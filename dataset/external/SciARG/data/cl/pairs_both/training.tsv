domain	doc_id	annotator	adu1_pos	adu2_pos	adu1_aty	adu2_aty	adu1_afu	adu2_afu	adu1_main	adu2_main	rel_direction	relation	adu1_text	adu2_text	adu1_id	adu2_id	rel_id
CL	D14-1002	pa	1	2	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	This paper presents a deep semantic similarity model ( DSSM ) , a special type of deep neural networks designed for text analysis , for recommending target documents to be of interest to a user based on a source document that she is reading .	We observe , identify , and detect naturally occurring signals of interestingness in click transitions on the Web between source and target documents , which we collect from commercial Web browser logs .	CL_D14-1002_pa_1	CL_D14-1002_pa_2	CL_D14-1002_pa_1_2
CL	D14-1002	pa	1	3	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	This paper presents a deep semantic similarity model ( DSSM ) , a special type of deep neural networks designed for text analysis , for recommending target documents to be of interest to a user based on a source document that she is reading .	The DSSM is trained on millions of Web transitions , and maps source-target document pairs to feature vectors in a latent space in such a way that the distance between source documents and their corresponding interesting targets in that space is minimized .	CL_D14-1002_pa_1	CL_D14-1002_pa_3	CL_D14-1002_pa_1_3
CL	D14-1002	pa	3	1	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	The DSSM is trained on millions of Web transitions , and maps source-target document pairs to feature vectors in a latent space in such a way that the distance between source documents and their corresponding interesting targets in that space is minimized .	This paper presents a deep semantic similarity model ( DSSM ) , a special type of deep neural networks designed for text analysis , for recommending target documents to be of interest to a user based on a source document that she is reading .	CL_D14-1002_pa_3	CL_D14-1002_pa_1	CL_D14-1002_pa_1_3
CL	D14-1002	pa	1	4	proposal	means	none	by-means	main	secondary	none	none	This paper presents a deep semantic similarity model ( DSSM ) , a special type of deep neural networks designed for text analysis , for recommending target documents to be of interest to a user based on a source document that she is reading .	The effectiveness of the DSSM is demonstrated using two interestingness tasks : automatic highlighting and contextual entity search .	CL_D14-1002_pa_1	CL_D14-1002_pa_4	CL_D14-1002_pa_1_4
CL	D14-1002	pa	1	5	proposal	result_means	none	support	main	secondary	back	support	This paper presents a deep semantic similarity model ( DSSM ) , a special type of deep neural networks designed for text analysis , for recommending target documents to be of interest to a user based on a source document that she is reading .	The results on large-scale , real-world datasets show that the semantics of documents are important for modeling interestingness and that the DSSM leads to significant quality improvement on both tasks , outperforming not only the classic document models that do not use semantics but also state-of-the-art topic models .	CL_D14-1002_pa_1	CL_D14-1002_pa_5	CL_D14-1002_pa_1_5
CL	D14-1002	pa	5	1	result_means	proposal	support	none	secondary	main	forw	support	The results on large-scale , real-world datasets show that the semantics of documents are important for modeling interestingness and that the DSSM leads to significant quality improvement on both tasks , outperforming not only the classic document models that do not use semantics but also state-of-the-art topic models .	This paper presents a deep semantic similarity model ( DSSM ) , a special type of deep neural networks designed for text analysis , for recommending target documents to be of interest to a user based on a source document that she is reading .	CL_D14-1002_pa_5	CL_D14-1002_pa_1	CL_D14-1002_pa_1_5
CL	D14-1002	pa	2	3	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	forw	elaboration	We observe , identify , and detect naturally occurring signals of interestingness in click transitions on the Web between source and target documents , which we collect from commercial Web browser logs .	The DSSM is trained on millions of Web transitions , and maps source-target document pairs to feature vectors in a latent space in such a way that the distance between source documents and their corresponding interesting targets in that space is minimized .	CL_D14-1002_pa_2	CL_D14-1002_pa_3	CL_D14-1002_pa_2_3
CL	D14-1002	pa	3	2	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	The DSSM is trained on millions of Web transitions , and maps source-target document pairs to feature vectors in a latent space in such a way that the distance between source documents and their corresponding interesting targets in that space is minimized .	We observe , identify , and detect naturally occurring signals of interestingness in click transitions on the Web between source and target documents , which we collect from commercial Web browser logs .	CL_D14-1002_pa_3	CL_D14-1002_pa_2	CL_D14-1002_pa_2_3
CL	D14-1002	pa	2	4	proposal_implementation	means	elaboration	by-means	secondary	secondary	none	none	We observe , identify , and detect naturally occurring signals of interestingness in click transitions on the Web between source and target documents , which we collect from commercial Web browser logs .	The effectiveness of the DSSM is demonstrated using two interestingness tasks : automatic highlighting and contextual entity search .	CL_D14-1002_pa_2	CL_D14-1002_pa_4	CL_D14-1002_pa_2_4
CL	D14-1002	pa	2	5	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	We observe , identify , and detect naturally occurring signals of interestingness in click transitions on the Web between source and target documents , which we collect from commercial Web browser logs .	The results on large-scale , real-world datasets show that the semantics of documents are important for modeling interestingness and that the DSSM leads to significant quality improvement on both tasks , outperforming not only the classic document models that do not use semantics but also state-of-the-art topic models .	CL_D14-1002_pa_2	CL_D14-1002_pa_5	CL_D14-1002_pa_2_5
CL	D14-1002	pa	3	4	proposal_implementation	means	elaboration	by-means	secondary	secondary	none	none	The DSSM is trained on millions of Web transitions , and maps source-target document pairs to feature vectors in a latent space in such a way that the distance between source documents and their corresponding interesting targets in that space is minimized .	The effectiveness of the DSSM is demonstrated using two interestingness tasks : automatic highlighting and contextual entity search .	CL_D14-1002_pa_3	CL_D14-1002_pa_4	CL_D14-1002_pa_3_4
CL	D14-1002	pa	3	5	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	The DSSM is trained on millions of Web transitions , and maps source-target document pairs to feature vectors in a latent space in such a way that the distance between source documents and their corresponding interesting targets in that space is minimized .	The results on large-scale , real-world datasets show that the semantics of documents are important for modeling interestingness and that the DSSM leads to significant quality improvement on both tasks , outperforming not only the classic document models that do not use semantics but also state-of-the-art topic models .	CL_D14-1002_pa_3	CL_D14-1002_pa_5	CL_D14-1002_pa_3_5
CL	D14-1002	pa	4	5	means	result_means	by-means	support	secondary	secondary	forw	by-means	The effectiveness of the DSSM is demonstrated using two interestingness tasks : automatic highlighting and contextual entity search .	The results on large-scale , real-world datasets show that the semantics of documents are important for modeling interestingness and that the DSSM leads to significant quality improvement on both tasks , outperforming not only the classic document models that do not use semantics but also state-of-the-art topic models .	CL_D14-1002_pa_4	CL_D14-1002_pa_5	CL_D14-1002_pa_4_5
CL	D14-1002	pa	5	4	result_means	means	support	by-means	secondary	secondary	back	by-means	The results on large-scale , real-world datasets show that the semantics of documents are important for modeling interestingness and that the DSSM leads to significant quality improvement on both tasks , outperforming not only the classic document models that do not use semantics but also state-of-the-art topic models .	The effectiveness of the DSSM is demonstrated using two interestingness tasks : automatic highlighting and contextual entity search .	CL_D14-1002_pa_5	CL_D14-1002_pa_4	CL_D14-1002_pa_4_5
CL	D14-1003	pa	1	2	proposal	proposal	none	elaboration	main	secondary	back	elaboration	This work presents two different translation models using recurrent neural networks .	The first one is a word-based approach using word alignments .	CL_D14-1003_pa_1	CL_D14-1003_pa_2	CL_D14-1003_pa_1_2
CL	D14-1003	pa	2	1	proposal	proposal	elaboration	none	secondary	main	forw	elaboration	The first one is a word-based approach using word alignments .	This work presents two different translation models using recurrent neural networks .	CL_D14-1003_pa_2	CL_D14-1003_pa_1	CL_D14-1003_pa_1_2
CL	D14-1003	pa	1	3	proposal	proposal	none	elaboration	main	secondary	back	elaboration	This work presents two different translation models using recurrent neural networks .	Second , we present phrase-based translation models that are more consistent with phrase-based decoding .	CL_D14-1003_pa_1	CL_D14-1003_pa_3	CL_D14-1003_pa_1_3
CL	D14-1003	pa	3	1	proposal	proposal	elaboration	none	secondary	main	forw	elaboration	Second , we present phrase-based translation models that are more consistent with phrase-based decoding .	This work presents two different translation models using recurrent neural networks .	CL_D14-1003_pa_3	CL_D14-1003_pa_1	CL_D14-1003_pa_1_3
CL	D14-1003	pa	1	4	proposal	proposal	none	elaboration	main	secondary	back	elaboration	This work presents two different translation models using recurrent neural networks .	Moreover , we introduce bidirectional recurrent neural models to the problem of machine translation , allowing us to use the full source sentence in our models , which is also of theoretical interest .	CL_D14-1003_pa_1	CL_D14-1003_pa_4	CL_D14-1003_pa_1_4
CL	D14-1003	pa	4	1	proposal	proposal	elaboration	none	secondary	main	forw	elaboration	Moreover , we introduce bidirectional recurrent neural models to the problem of machine translation , allowing us to use the full source sentence in our models , which is also of theoretical interest .	This work presents two different translation models using recurrent neural networks .	CL_D14-1003_pa_4	CL_D14-1003_pa_1	CL_D14-1003_pa_1_4
CL	D14-1003	pa	1	5	proposal	result_means	none	support	main	secondary	back	support	This work presents two different translation models using recurrent neural networks .	We demonstrate that our translation models are capable of improving strong baselines already including recurrent neural language models on three tasks : IWSLT 2013 German to English , BOLT Arabic to English and Chinese to English .	CL_D14-1003_pa_1	CL_D14-1003_pa_5	CL_D14-1003_pa_1_5
CL	D14-1003	pa	5	1	result_means	proposal	support	none	secondary	main	forw	support	We demonstrate that our translation models are capable of improving strong baselines already including recurrent neural language models on three tasks : IWSLT 2013 German to English , BOLT Arabic to English and Chinese to English .	This work presents two different translation models using recurrent neural networks .	CL_D14-1003_pa_5	CL_D14-1003_pa_1	CL_D14-1003_pa_1_5
CL	D14-1003	pa	1	6	proposal	observation	none	support	main	secondary	none	none	This work presents two different translation models using recurrent neural networks .	We obtain gains up to 1.6 % BLEU and 1.7 % TER by rescoring 1000-best lists .	CL_D14-1003_pa_1	CL_D14-1003_pa_6	CL_D14-1003_pa_1_6
CL	D14-1003	pa	2	3	proposal	proposal	elaboration	elaboration	secondary	secondary	none	none	The first one is a word-based approach using word alignments .	Second , we present phrase-based translation models that are more consistent with phrase-based decoding .	CL_D14-1003_pa_2	CL_D14-1003_pa_3	CL_D14-1003_pa_2_3
CL	D14-1003	pa	2	4	proposal	proposal	elaboration	elaboration	secondary	secondary	none	none	The first one is a word-based approach using word alignments .	Moreover , we introduce bidirectional recurrent neural models to the problem of machine translation , allowing us to use the full source sentence in our models , which is also of theoretical interest .	CL_D14-1003_pa_2	CL_D14-1003_pa_4	CL_D14-1003_pa_2_4
CL	D14-1003	pa	2	5	proposal	result_means	elaboration	support	secondary	secondary	none	none	The first one is a word-based approach using word alignments .	We demonstrate that our translation models are capable of improving strong baselines already including recurrent neural language models on three tasks : IWSLT 2013 German to English , BOLT Arabic to English and Chinese to English .	CL_D14-1003_pa_2	CL_D14-1003_pa_5	CL_D14-1003_pa_2_5
CL	D14-1003	pa	2	6	proposal	observation	elaboration	support	secondary	secondary	none	none	The first one is a word-based approach using word alignments .	We obtain gains up to 1.6 % BLEU and 1.7 % TER by rescoring 1000-best lists .	CL_D14-1003_pa_2	CL_D14-1003_pa_6	CL_D14-1003_pa_2_6
CL	D14-1003	pa	3	4	proposal	proposal	elaboration	elaboration	secondary	secondary	none	none	Second , we present phrase-based translation models that are more consistent with phrase-based decoding .	Moreover , we introduce bidirectional recurrent neural models to the problem of machine translation , allowing us to use the full source sentence in our models , which is also of theoretical interest .	CL_D14-1003_pa_3	CL_D14-1003_pa_4	CL_D14-1003_pa_3_4
CL	D14-1003	pa	3	5	proposal	result_means	elaboration	support	secondary	secondary	none	none	Second , we present phrase-based translation models that are more consistent with phrase-based decoding .	We demonstrate that our translation models are capable of improving strong baselines already including recurrent neural language models on three tasks : IWSLT 2013 German to English , BOLT Arabic to English and Chinese to English .	CL_D14-1003_pa_3	CL_D14-1003_pa_5	CL_D14-1003_pa_3_5
CL	D14-1003	pa	3	6	proposal	observation	elaboration	support	secondary	secondary	none	none	Second , we present phrase-based translation models that are more consistent with phrase-based decoding .	We obtain gains up to 1.6 % BLEU and 1.7 % TER by rescoring 1000-best lists .	CL_D14-1003_pa_3	CL_D14-1003_pa_6	CL_D14-1003_pa_3_6
CL	D14-1003	pa	4	5	proposal	result_means	elaboration	support	secondary	secondary	none	none	Moreover , we introduce bidirectional recurrent neural models to the problem of machine translation , allowing us to use the full source sentence in our models , which is also of theoretical interest .	We demonstrate that our translation models are capable of improving strong baselines already including recurrent neural language models on three tasks : IWSLT 2013 German to English , BOLT Arabic to English and Chinese to English .	CL_D14-1003_pa_4	CL_D14-1003_pa_5	CL_D14-1003_pa_4_5
CL	D14-1003	pa	4	6	proposal	observation	elaboration	support	secondary	secondary	none	none	Moreover , we introduce bidirectional recurrent neural models to the problem of machine translation , allowing us to use the full source sentence in our models , which is also of theoretical interest .	We obtain gains up to 1.6 % BLEU and 1.7 % TER by rescoring 1000-best lists .	CL_D14-1003_pa_4	CL_D14-1003_pa_6	CL_D14-1003_pa_4_6
CL	D14-1003	pa	5	6	result_means	observation	support	support	secondary	secondary	back	support	We demonstrate that our translation models are capable of improving strong baselines already including recurrent neural language models on three tasks : IWSLT 2013 German to English , BOLT Arabic to English and Chinese to English .	We obtain gains up to 1.6 % BLEU and 1.7 % TER by rescoring 1000-best lists .	CL_D14-1003_pa_5	CL_D14-1003_pa_6	CL_D14-1003_pa_5_6
CL	D14-1003	pa	6	5	observation	result_means	support	support	secondary	secondary	forw	support	We obtain gains up to 1.6 % BLEU and 1.7 % TER by rescoring 1000-best lists .	We demonstrate that our translation models are capable of improving strong baselines already including recurrent neural language models on three tasks : IWSLT 2013 German to English , BOLT Arabic to English and Chinese to English .	CL_D14-1003_pa_6	CL_D14-1003_pa_5	CL_D14-1003_pa_5_6
CL	D14-1004	pa	1	2	proposal	proposal	none	elaboration	secondary	main	back	elaboration	This paper investigates the use of neural networks for the acquisition of selectional preferences .	Inspired by recent advances of neural network models for NLP applications , we propose a neural network model that learns to discriminate between felicitous and infelicitous arguments for a particular predicate .	CL_D14-1004_pa_1	CL_D14-1004_pa_2	CL_D14-1004_pa_1_2
CL	D14-1004	pa	2	1	proposal	proposal	elaboration	none	main	secondary	forw	elaboration	Inspired by recent advances of neural network models for NLP applications , we propose a neural network model that learns to discriminate between felicitous and infelicitous arguments for a particular predicate .	This paper investigates the use of neural networks for the acquisition of selectional preferences .	CL_D14-1004_pa_2	CL_D14-1004_pa_1	CL_D14-1004_pa_1_2
CL	D14-1004	pa	1	3	proposal	proposal_implementation	none	elaboration	secondary	secondary	none	none	This paper investigates the use of neural networks for the acquisition of selectional preferences .	The model is entirely unsupervised - preferences are learned from unannotated corpus data .	CL_D14-1004_pa_1	CL_D14-1004_pa_3	CL_D14-1004_pa_1_3
CL	D14-1004	pa	1	4	proposal	proposal	none	elaboration	secondary	secondary	none	none	This paper investigates the use of neural networks for the acquisition of selectional preferences .	We propose two neural network architectures : one that handles standard two-way selectional preferences and one that is able to deal with multi-way selectional preferences .	CL_D14-1004_pa_1	CL_D14-1004_pa_4	CL_D14-1004_pa_1_4
CL	D14-1004	pa	1	5	proposal	result_means	none	support	secondary	secondary	none	none	This paper investigates the use of neural networks for the acquisition of selectional preferences .	The model's performance is evaluated on a pseudo-disambiguation task , on which it is shown to achieve state of the art performance .	CL_D14-1004_pa_1	CL_D14-1004_pa_5	CL_D14-1004_pa_1_5
CL	D14-1004	pa	2	3	proposal	proposal_implementation	elaboration	elaboration	main	secondary	back	elaboration	Inspired by recent advances of neural network models for NLP applications , we propose a neural network model that learns to discriminate between felicitous and infelicitous arguments for a particular predicate .	The model is entirely unsupervised - preferences are learned from unannotated corpus data .	CL_D14-1004_pa_2	CL_D14-1004_pa_3	CL_D14-1004_pa_2_3
CL	D14-1004	pa	3	2	proposal_implementation	proposal	elaboration	elaboration	secondary	main	forw	elaboration	The model is entirely unsupervised - preferences are learned from unannotated corpus data .	Inspired by recent advances of neural network models for NLP applications , we propose a neural network model that learns to discriminate between felicitous and infelicitous arguments for a particular predicate .	CL_D14-1004_pa_3	CL_D14-1004_pa_2	CL_D14-1004_pa_2_3
CL	D14-1004	pa	2	4	proposal	proposal	elaboration	elaboration	main	secondary	back	elaboration	Inspired by recent advances of neural network models for NLP applications , we propose a neural network model that learns to discriminate between felicitous and infelicitous arguments for a particular predicate .	We propose two neural network architectures : one that handles standard two-way selectional preferences and one that is able to deal with multi-way selectional preferences .	CL_D14-1004_pa_2	CL_D14-1004_pa_4	CL_D14-1004_pa_2_4
CL	D14-1004	pa	4	2	proposal	proposal	elaboration	elaboration	secondary	main	forw	elaboration	We propose two neural network architectures : one that handles standard two-way selectional preferences and one that is able to deal with multi-way selectional preferences .	Inspired by recent advances of neural network models for NLP applications , we propose a neural network model that learns to discriminate between felicitous and infelicitous arguments for a particular predicate .	CL_D14-1004_pa_4	CL_D14-1004_pa_2	CL_D14-1004_pa_2_4
CL	D14-1004	pa	2	5	proposal	result_means	elaboration	support	main	secondary	back	support	Inspired by recent advances of neural network models for NLP applications , we propose a neural network model that learns to discriminate between felicitous and infelicitous arguments for a particular predicate .	The model's performance is evaluated on a pseudo-disambiguation task , on which it is shown to achieve state of the art performance .	CL_D14-1004_pa_2	CL_D14-1004_pa_5	CL_D14-1004_pa_2_5
CL	D14-1004	pa	5	2	result_means	proposal	support	elaboration	secondary	main	forw	support	The model's performance is evaluated on a pseudo-disambiguation task , on which it is shown to achieve state of the art performance .	Inspired by recent advances of neural network models for NLP applications , we propose a neural network model that learns to discriminate between felicitous and infelicitous arguments for a particular predicate .	CL_D14-1004_pa_5	CL_D14-1004_pa_2	CL_D14-1004_pa_2_5
CL	D14-1004	pa	3	4	proposal_implementation	proposal	elaboration	elaboration	secondary	secondary	none	none	The model is entirely unsupervised - preferences are learned from unannotated corpus data .	We propose two neural network architectures : one that handles standard two-way selectional preferences and one that is able to deal with multi-way selectional preferences .	CL_D14-1004_pa_3	CL_D14-1004_pa_4	CL_D14-1004_pa_3_4
CL	D14-1004	pa	3	5	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	The model is entirely unsupervised - preferences are learned from unannotated corpus data .	The model's performance is evaluated on a pseudo-disambiguation task , on which it is shown to achieve state of the art performance .	CL_D14-1004_pa_3	CL_D14-1004_pa_5	CL_D14-1004_pa_3_5
CL	D14-1004	pa	4	5	proposal	result_means	elaboration	support	secondary	secondary	none	none	We propose two neural network architectures : one that handles standard two-way selectional preferences and one that is able to deal with multi-way selectional preferences .	The model's performance is evaluated on a pseudo-disambiguation task , on which it is shown to achieve state of the art performance .	CL_D14-1004_pa_4	CL_D14-1004_pa_5	CL_D14-1004_pa_4_5
CL	D14-1005	pa	1	2	proposal	conclusion	none	support	main	secondary	back	support	We construct multi-modal concept representations by concatenating a skip-gram linguistic representation vector with a visual concept representation vector computed using the feature extraction layers of a deep convolutional neural network ( CNN ) trained on a large labeled object recognition dataset .	This transfer learning approach brings a clear performance gain over features based on the traditional bag-of-visual-word approach .	CL_D14-1005_pa_1	CL_D14-1005_pa_2	CL_D14-1005_pa_1_2
CL	D14-1005	pa	2	1	conclusion	proposal	support	none	secondary	main	forw	support	This transfer learning approach brings a clear performance gain over features based on the traditional bag-of-visual-word approach .	We construct multi-modal concept representations by concatenating a skip-gram linguistic representation vector with a visual concept representation vector computed using the feature extraction layers of a deep convolutional neural network ( CNN ) trained on a large labeled object recognition dataset .	CL_D14-1005_pa_2	CL_D14-1005_pa_1	CL_D14-1005_pa_1_2
CL	D14-1005	pa	1	3	proposal	means	none	by-means	main	secondary	none	none	We construct multi-modal concept representations by concatenating a skip-gram linguistic representation vector with a visual concept representation vector computed using the feature extraction layers of a deep convolutional neural network ( CNN ) trained on a large labeled object recognition dataset .	Experimental results are reported on the WordSim353 and MEN semantic relatedness evaluation tasks .	CL_D14-1005_pa_1	CL_D14-1005_pa_3	CL_D14-1005_pa_1_3
CL	D14-1005	pa	1	4	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	We construct multi-modal concept representations by concatenating a skip-gram linguistic representation vector with a visual concept representation vector computed using the feature extraction layers of a deep convolutional neural network ( CNN ) trained on a large labeled object recognition dataset .	We use visual features computed using either ImageNet or ESP Game images .	CL_D14-1005_pa_1	CL_D14-1005_pa_4	CL_D14-1005_pa_1_4
CL	D14-1005	pa	4	1	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	We use visual features computed using either ImageNet or ESP Game images .	We construct multi-modal concept representations by concatenating a skip-gram linguistic representation vector with a visual concept representation vector computed using the feature extraction layers of a deep convolutional neural network ( CNN ) trained on a large labeled object recognition dataset .	CL_D14-1005_pa_4	CL_D14-1005_pa_1	CL_D14-1005_pa_1_4
CL	D14-1005	pa	2	3	conclusion	means	support	by-means	secondary	secondary	back	by-means	This transfer learning approach brings a clear performance gain over features based on the traditional bag-of-visual-word approach .	Experimental results are reported on the WordSim353 and MEN semantic relatedness evaluation tasks .	CL_D14-1005_pa_2	CL_D14-1005_pa_3	CL_D14-1005_pa_2_3
CL	D14-1005	pa	3	2	means	conclusion	by-means	support	secondary	secondary	forw	by-means	Experimental results are reported on the WordSim353 and MEN semantic relatedness evaluation tasks .	This transfer learning approach brings a clear performance gain over features based on the traditional bag-of-visual-word approach .	CL_D14-1005_pa_3	CL_D14-1005_pa_2	CL_D14-1005_pa_2_3
CL	D14-1005	pa	2	4	conclusion	proposal_implementation	support	elaboration	secondary	secondary	none	none	This transfer learning approach brings a clear performance gain over features based on the traditional bag-of-visual-word approach .	We use visual features computed using either ImageNet or ESP Game images .	CL_D14-1005_pa_2	CL_D14-1005_pa_4	CL_D14-1005_pa_2_4
CL	D14-1005	pa	3	4	means	proposal_implementation	by-means	elaboration	secondary	secondary	none	none	Experimental results are reported on the WordSim353 and MEN semantic relatedness evaluation tasks .	We use visual features computed using either ImageNet or ESP Game images .	CL_D14-1005_pa_3	CL_D14-1005_pa_4	CL_D14-1005_pa_3_4
CL	D14-1006	pa	1	2	proposal	information_additional	none	info-optional	main	secondary	back	info-optional	In this paper , we present a novel approach for identifying argumentative discourse structures in persuasive essays .	The structure of argumentation consists of several components ( i.e. claims and premises ) that are connected with argumentative relations .	CL_D14-1006_pa_1	CL_D14-1006_pa_2	CL_D14-1006_pa_1_2
CL	D14-1006	pa	2	1	information_additional	proposal	info-optional	none	secondary	main	forw	info-optional	The structure of argumentation consists of several components ( i.e. claims and premises ) that are connected with argumentative relations .	In this paper , we present a novel approach for identifying argumentative discourse structures in persuasive essays .	CL_D14-1006_pa_2	CL_D14-1006_pa_1	CL_D14-1006_pa_1_2
CL	D14-1006	pa	1	3	proposal	proposal	none	elaboration	main	secondary	back	elaboration	In this paper , we present a novel approach for identifying argumentative discourse structures in persuasive essays .	We consider this task in two consecutive steps .	CL_D14-1006_pa_1	CL_D14-1006_pa_3	CL_D14-1006_pa_1_3
CL	D14-1006	pa	3	1	proposal	proposal	elaboration	none	secondary	main	forw	elaboration	We consider this task in two consecutive steps .	In this paper , we present a novel approach for identifying argumentative discourse structures in persuasive essays .	CL_D14-1006_pa_3	CL_D14-1006_pa_1	CL_D14-1006_pa_1_3
CL	D14-1006	pa	1	4	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	In this paper , we present a novel approach for identifying argumentative discourse structures in persuasive essays .	First , we identify the components of arguments using multiclass classification .	CL_D14-1006_pa_1	CL_D14-1006_pa_4	CL_D14-1006_pa_1_4
CL	D14-1006	pa	1	5	proposal	proposal_implementation	none	sequence	main	secondary	none	none	In this paper , we present a novel approach for identifying argumentative discourse structures in persuasive essays .	Second , we classify a pair of argument components as either support or non-support for identifying the structure of argumentative discourse .	CL_D14-1006_pa_1	CL_D14-1006_pa_5	CL_D14-1006_pa_1_5
CL	D14-1006	pa	1	6	proposal	proposal	none	elaboration	main	secondary	none	none	In this paper , we present a novel approach for identifying argumentative discourse structures in persuasive essays .	For both tasks , we evaluate several classifiers and propose novel feature sets including structural , lexical , syntactic and contextual features .	CL_D14-1006_pa_1	CL_D14-1006_pa_6	CL_D14-1006_pa_1_6
CL	D14-1006	pa	1	7	proposal	observation	none	support	main	secondary	back	support	In this paper , we present a novel approach for identifying argumentative discourse structures in persuasive essays .	In our experiments , we obtain a macro F1-score of 0.726 for identifying argument components and 0.722 for argumentative relations .	CL_D14-1006_pa_1	CL_D14-1006_pa_7	CL_D14-1006_pa_1_7
CL	D14-1006	pa	7	1	observation	proposal	support	none	secondary	main	forw	support	In our experiments , we obtain a macro F1-score of 0.726 for identifying argument components and 0.722 for argumentative relations .	In this paper , we present a novel approach for identifying argumentative discourse structures in persuasive essays .	CL_D14-1006_pa_7	CL_D14-1006_pa_1	CL_D14-1006_pa_1_7
CL	D14-1006	pa	2	3	information_additional	proposal	info-optional	elaboration	secondary	secondary	none	none	The structure of argumentation consists of several components ( i.e. claims and premises ) that are connected with argumentative relations .	We consider this task in two consecutive steps .	CL_D14-1006_pa_2	CL_D14-1006_pa_3	CL_D14-1006_pa_2_3
CL	D14-1006	pa	2	4	information_additional	proposal_implementation	info-optional	elaboration	secondary	secondary	none	none	The structure of argumentation consists of several components ( i.e. claims and premises ) that are connected with argumentative relations .	First , we identify the components of arguments using multiclass classification .	CL_D14-1006_pa_2	CL_D14-1006_pa_4	CL_D14-1006_pa_2_4
CL	D14-1006	pa	2	5	information_additional	proposal_implementation	info-optional	sequence	secondary	secondary	none	none	The structure of argumentation consists of several components ( i.e. claims and premises ) that are connected with argumentative relations .	Second , we classify a pair of argument components as either support or non-support for identifying the structure of argumentative discourse .	CL_D14-1006_pa_2	CL_D14-1006_pa_5	CL_D14-1006_pa_2_5
CL	D14-1006	pa	2	6	information_additional	proposal	info-optional	elaboration	secondary	secondary	none	none	The structure of argumentation consists of several components ( i.e. claims and premises ) that are connected with argumentative relations .	For both tasks , we evaluate several classifiers and propose novel feature sets including structural , lexical , syntactic and contextual features .	CL_D14-1006_pa_2	CL_D14-1006_pa_6	CL_D14-1006_pa_2_6
CL	D14-1006	pa	2	7	information_additional	observation	info-optional	support	secondary	secondary	none	none	The structure of argumentation consists of several components ( i.e. claims and premises ) that are connected with argumentative relations .	In our experiments , we obtain a macro F1-score of 0.726 for identifying argument components and 0.722 for argumentative relations .	CL_D14-1006_pa_2	CL_D14-1006_pa_7	CL_D14-1006_pa_2_7
CL	D14-1006	pa	3	4	proposal	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	We consider this task in two consecutive steps .	First , we identify the components of arguments using multiclass classification .	CL_D14-1006_pa_3	CL_D14-1006_pa_4	CL_D14-1006_pa_3_4
CL	D14-1006	pa	4	3	proposal_implementation	proposal	elaboration	elaboration	secondary	secondary	forw	elaboration	First , we identify the components of arguments using multiclass classification .	We consider this task in two consecutive steps .	CL_D14-1006_pa_4	CL_D14-1006_pa_3	CL_D14-1006_pa_3_4
CL	D14-1006	pa	3	5	proposal	proposal_implementation	elaboration	sequence	secondary	secondary	none	none	We consider this task in two consecutive steps .	Second , we classify a pair of argument components as either support or non-support for identifying the structure of argumentative discourse .	CL_D14-1006_pa_3	CL_D14-1006_pa_5	CL_D14-1006_pa_3_5
CL	D14-1006	pa	3	6	proposal	proposal	elaboration	elaboration	secondary	secondary	back	elaboration	We consider this task in two consecutive steps .	For both tasks , we evaluate several classifiers and propose novel feature sets including structural , lexical , syntactic and contextual features .	CL_D14-1006_pa_3	CL_D14-1006_pa_6	CL_D14-1006_pa_3_6
CL	D14-1006	pa	6	3	proposal	proposal	elaboration	elaboration	secondary	secondary	forw	elaboration	For both tasks , we evaluate several classifiers and propose novel feature sets including structural , lexical , syntactic and contextual features .	We consider this task in two consecutive steps .	CL_D14-1006_pa_6	CL_D14-1006_pa_3	CL_D14-1006_pa_3_6
CL	D14-1006	pa	3	7	proposal	observation	elaboration	support	secondary	secondary	none	none	We consider this task in two consecutive steps .	In our experiments , we obtain a macro F1-score of 0.726 for identifying argument components and 0.722 for argumentative relations .	CL_D14-1006_pa_3	CL_D14-1006_pa_7	CL_D14-1006_pa_3_7
CL	D14-1006	pa	4	5	proposal_implementation	proposal_implementation	elaboration	sequence	secondary	secondary	back	sequence	First , we identify the components of arguments using multiclass classification .	Second , we classify a pair of argument components as either support or non-support for identifying the structure of argumentative discourse .	CL_D14-1006_pa_4	CL_D14-1006_pa_5	CL_D14-1006_pa_4_5
CL	D14-1006	pa	5	4	proposal_implementation	proposal_implementation	sequence	elaboration	secondary	secondary	forw	sequence	Second , we classify a pair of argument components as either support or non-support for identifying the structure of argumentative discourse .	First , we identify the components of arguments using multiclass classification .	CL_D14-1006_pa_5	CL_D14-1006_pa_4	CL_D14-1006_pa_4_5
CL	D14-1006	pa	4	6	proposal_implementation	proposal	elaboration	elaboration	secondary	secondary	none	none	First , we identify the components of arguments using multiclass classification .	For both tasks , we evaluate several classifiers and propose novel feature sets including structural , lexical , syntactic and contextual features .	CL_D14-1006_pa_4	CL_D14-1006_pa_6	CL_D14-1006_pa_4_6
CL	D14-1006	pa	4	7	proposal_implementation	observation	elaboration	support	secondary	secondary	none	none	First , we identify the components of arguments using multiclass classification .	In our experiments , we obtain a macro F1-score of 0.726 for identifying argument components and 0.722 for argumentative relations .	CL_D14-1006_pa_4	CL_D14-1006_pa_7	CL_D14-1006_pa_4_7
CL	D14-1006	pa	5	6	proposal_implementation	proposal	sequence	elaboration	secondary	secondary	none	none	Second , we classify a pair of argument components as either support or non-support for identifying the structure of argumentative discourse .	For both tasks , we evaluate several classifiers and propose novel feature sets including structural , lexical , syntactic and contextual features .	CL_D14-1006_pa_5	CL_D14-1006_pa_6	CL_D14-1006_pa_5_6
CL	D14-1006	pa	5	7	proposal_implementation	observation	sequence	support	secondary	secondary	none	none	Second , we classify a pair of argument components as either support or non-support for identifying the structure of argumentative discourse .	In our experiments , we obtain a macro F1-score of 0.726 for identifying argument components and 0.722 for argumentative relations .	CL_D14-1006_pa_5	CL_D14-1006_pa_7	CL_D14-1006_pa_5_7
CL	D14-1006	pa	6	7	proposal	observation	elaboration	support	secondary	secondary	none	none	For both tasks , we evaluate several classifiers and propose novel feature sets including structural , lexical , syntactic and contextual features .	In our experiments , we obtain a macro F1-score of 0.726 for identifying argument components and 0.722 for argumentative relations .	CL_D14-1006_pa_6	CL_D14-1006_pa_7	CL_D14-1006_pa_6_7
CL	D14-1007	pa	1	2	proposal	proposal	none	elaboration	main	secondary	back	elaboration	This paper proposes a Markov Decision Process and reinforcement learning based approach for domain selection in a multi-domain Spoken Dialogue System built on a distributed architecture .	In the proposed framework , the domain selection problem is treated as sequential planning instead of classification , such that confirmation and clarification interaction mechanisms are supported .	CL_D14-1007_pa_1	CL_D14-1007_pa_2	CL_D14-1007_pa_1_2
CL	D14-1007	pa	2	1	proposal	proposal	elaboration	none	secondary	main	forw	elaboration	In the proposed framework , the domain selection problem is treated as sequential planning instead of classification , such that confirmation and clarification interaction mechanisms are supported .	This paper proposes a Markov Decision Process and reinforcement learning based approach for domain selection in a multi-domain Spoken Dialogue System built on a distributed architecture .	CL_D14-1007_pa_2	CL_D14-1007_pa_1	CL_D14-1007_pa_1_2
CL	D14-1007	pa	1	3	proposal	result_means	none	support	main	secondary	back	support	This paper proposes a Markov Decision Process and reinforcement learning based approach for domain selection in a multi-domain Spoken Dialogue System built on a distributed architecture .	In addition , it is shown that by using a model parameter tying trick , the extensibility of the system can be preserved , where dialogue components in new domains can be easily plugged in , without re-training the domain selection policy .	CL_D14-1007_pa_1	CL_D14-1007_pa_3	CL_D14-1007_pa_1_3
CL	D14-1007	pa	3	1	result_means	proposal	support	none	secondary	main	forw	support	In addition , it is shown that by using a model parameter tying trick , the extensibility of the system can be preserved , where dialogue components in new domains can be easily plugged in , without re-training the domain selection policy .	This paper proposes a Markov Decision Process and reinforcement learning based approach for domain selection in a multi-domain Spoken Dialogue System built on a distributed architecture .	CL_D14-1007_pa_3	CL_D14-1007_pa_1	CL_D14-1007_pa_1_3
CL	D14-1007	pa	1	4	proposal	result	none	support	main	secondary	back	support	This paper proposes a Markov Decision Process and reinforcement learning based approach for domain selection in a multi-domain Spoken Dialogue System built on a distributed architecture .	The experimental results based on human subjects suggest that the proposed model marginally outperforms a non-trivial baseline .	CL_D14-1007_pa_1	CL_D14-1007_pa_4	CL_D14-1007_pa_1_4
CL	D14-1007	pa	4	1	result	proposal	support	none	secondary	main	forw	support	The experimental results based on human subjects suggest that the proposed model marginally outperforms a non-trivial baseline .	This paper proposes a Markov Decision Process and reinforcement learning based approach for domain selection in a multi-domain Spoken Dialogue System built on a distributed architecture .	CL_D14-1007_pa_4	CL_D14-1007_pa_1	CL_D14-1007_pa_1_4
CL	D14-1007	pa	2	3	proposal	result_means	elaboration	support	secondary	secondary	none	none	In the proposed framework , the domain selection problem is treated as sequential planning instead of classification , such that confirmation and clarification interaction mechanisms are supported .	In addition , it is shown that by using a model parameter tying trick , the extensibility of the system can be preserved , where dialogue components in new domains can be easily plugged in , without re-training the domain selection policy .	CL_D14-1007_pa_2	CL_D14-1007_pa_3	CL_D14-1007_pa_2_3
CL	D14-1007	pa	2	4	proposal	result	elaboration	support	secondary	secondary	none	none	In the proposed framework , the domain selection problem is treated as sequential planning instead of classification , such that confirmation and clarification interaction mechanisms are supported .	The experimental results based on human subjects suggest that the proposed model marginally outperforms a non-trivial baseline .	CL_D14-1007_pa_2	CL_D14-1007_pa_4	CL_D14-1007_pa_2_4
CL	D14-1007	pa	3	4	result_means	result	support	support	secondary	secondary	none	none	In addition , it is shown that by using a model parameter tying trick , the extensibility of the system can be preserved , where dialogue components in new domains can be easily plugged in , without re-training the domain selection policy .	The experimental results based on human subjects suggest that the proposed model marginally outperforms a non-trivial baseline .	CL_D14-1007_pa_3	CL_D14-1007_pa_4	CL_D14-1007_pa_3_4
CL	D14-1008	pa	1	2	motivation_background	proposal	support	none	secondary	secondary	forw	support	Discourse parsing is a challenging task and plays a critical role in discourse analysis .	In this paper , we focus on labeling full argument spans of discourse connectives in the Penn Discourse Treebank ( PDTB ) .	CL_D14-1008_pa_1	CL_D14-1008_pa_2	CL_D14-1008_pa_1_2
CL	D14-1008	pa	2	1	proposal	motivation_background	none	support	secondary	secondary	back	support	In this paper , we focus on labeling full argument spans of discourse connectives in the Penn Discourse Treebank ( PDTB ) .	Discourse parsing is a challenging task and plays a critical role in discourse analysis .	CL_D14-1008_pa_2	CL_D14-1008_pa_1	CL_D14-1008_pa_1_2
CL	D14-1008	pa	1	3	motivation_background	motivation_background	support	support	secondary	secondary	none	none	Discourse parsing is a challenging task and plays a critical role in discourse analysis .	Previous studies cast this task as a linear tagging or subtree extraction problem .	CL_D14-1008_pa_1	CL_D14-1008_pa_3	CL_D14-1008_pa_1_3
CL	D14-1008	pa	1	4	motivation_background	proposal	support	elaboration	secondary	main	none	none	Discourse parsing is a challenging task and plays a critical role in discourse analysis .	In this paper , we propose a novel constituent-based approach to argument labeling , which integrates the advantages of both linear tagging and subtree extraction .	CL_D14-1008_pa_1	CL_D14-1008_pa_4	CL_D14-1008_pa_1_4
CL	D14-1008	pa	1	5	motivation_background	proposal	support	elaboration	secondary	secondary	none	none	Discourse parsing is a challenging task and plays a critical role in discourse analysis .	In particular , the proposed approach unifies intra- and inter-sentence cases by treating the immediately preceding sentence as a special constituent .	CL_D14-1008_pa_1	CL_D14-1008_pa_5	CL_D14-1008_pa_1_5
CL	D14-1008	pa	1	6	motivation_background	proposal_implementation	support	elaboration	secondary	secondary	none	none	Discourse parsing is a challenging task and plays a critical role in discourse analysis .	Besides , a joint inference mechanism is introduced to incorporate global information across arguments into our constituent-based approach via integer linear programming .	CL_D14-1008_pa_1	CL_D14-1008_pa_6	CL_D14-1008_pa_1_6
CL	D14-1008	pa	1	7	motivation_background	result_means	support	support	secondary	secondary	none	none	Discourse parsing is a challenging task and plays a critical role in discourse analysis .	Evaluation on PDTB shows significant performance improvements of our constituent-based approach over the best state-of-the-art system .	CL_D14-1008_pa_1	CL_D14-1008_pa_7	CL_D14-1008_pa_1_7
CL	D14-1008	pa	1	8	motivation_background	result	support	elaboration	secondary	secondary	none	none	Discourse parsing is a challenging task and plays a critical role in discourse analysis .	It also shows the effectiveness of our joint inference mechanism in modeling global information across arguments .	CL_D14-1008_pa_1	CL_D14-1008_pa_8	CL_D14-1008_pa_1_8
CL	D14-1008	pa	2	3	proposal	motivation_background	none	support	secondary	secondary	none	none	In this paper , we focus on labeling full argument spans of discourse connectives in the Penn Discourse Treebank ( PDTB ) .	Previous studies cast this task as a linear tagging or subtree extraction problem .	CL_D14-1008_pa_2	CL_D14-1008_pa_3	CL_D14-1008_pa_2_3
CL	D14-1008	pa	2	4	proposal	proposal	none	elaboration	secondary	main	back	elaboration	In this paper , we focus on labeling full argument spans of discourse connectives in the Penn Discourse Treebank ( PDTB ) .	In this paper , we propose a novel constituent-based approach to argument labeling , which integrates the advantages of both linear tagging and subtree extraction .	CL_D14-1008_pa_2	CL_D14-1008_pa_4	CL_D14-1008_pa_2_4
CL	D14-1008	pa	4	2	proposal	proposal	elaboration	none	main	secondary	forw	elaboration	In this paper , we propose a novel constituent-based approach to argument labeling , which integrates the advantages of both linear tagging and subtree extraction .	In this paper , we focus on labeling full argument spans of discourse connectives in the Penn Discourse Treebank ( PDTB ) .	CL_D14-1008_pa_4	CL_D14-1008_pa_2	CL_D14-1008_pa_2_4
CL	D14-1008	pa	2	5	proposal	proposal	none	elaboration	secondary	secondary	none	none	In this paper , we focus on labeling full argument spans of discourse connectives in the Penn Discourse Treebank ( PDTB ) .	In particular , the proposed approach unifies intra- and inter-sentence cases by treating the immediately preceding sentence as a special constituent .	CL_D14-1008_pa_2	CL_D14-1008_pa_5	CL_D14-1008_pa_2_5
CL	D14-1008	pa	2	6	proposal	proposal_implementation	none	elaboration	secondary	secondary	none	none	In this paper , we focus on labeling full argument spans of discourse connectives in the Penn Discourse Treebank ( PDTB ) .	Besides , a joint inference mechanism is introduced to incorporate global information across arguments into our constituent-based approach via integer linear programming .	CL_D14-1008_pa_2	CL_D14-1008_pa_6	CL_D14-1008_pa_2_6
CL	D14-1008	pa	2	7	proposal	result_means	none	support	secondary	secondary	none	none	In this paper , we focus on labeling full argument spans of discourse connectives in the Penn Discourse Treebank ( PDTB ) .	Evaluation on PDTB shows significant performance improvements of our constituent-based approach over the best state-of-the-art system .	CL_D14-1008_pa_2	CL_D14-1008_pa_7	CL_D14-1008_pa_2_7
CL	D14-1008	pa	2	8	proposal	result	none	elaboration	secondary	secondary	none	none	In this paper , we focus on labeling full argument spans of discourse connectives in the Penn Discourse Treebank ( PDTB ) .	It also shows the effectiveness of our joint inference mechanism in modeling global information across arguments .	CL_D14-1008_pa_2	CL_D14-1008_pa_8	CL_D14-1008_pa_2_8
CL	D14-1008	pa	3	4	motivation_background	proposal	support	elaboration	secondary	main	forw	support	Previous studies cast this task as a linear tagging or subtree extraction problem .	In this paper , we propose a novel constituent-based approach to argument labeling , which integrates the advantages of both linear tagging and subtree extraction .	CL_D14-1008_pa_3	CL_D14-1008_pa_4	CL_D14-1008_pa_3_4
CL	D14-1008	pa	4	3	proposal	motivation_background	elaboration	support	main	secondary	back	support	In this paper , we propose a novel constituent-based approach to argument labeling , which integrates the advantages of both linear tagging and subtree extraction .	Previous studies cast this task as a linear tagging or subtree extraction problem .	CL_D14-1008_pa_4	CL_D14-1008_pa_3	CL_D14-1008_pa_3_4
CL	D14-1008	pa	3	5	motivation_background	proposal	support	elaboration	secondary	secondary	none	none	Previous studies cast this task as a linear tagging or subtree extraction problem .	In particular , the proposed approach unifies intra- and inter-sentence cases by treating the immediately preceding sentence as a special constituent .	CL_D14-1008_pa_3	CL_D14-1008_pa_5	CL_D14-1008_pa_3_5
CL	D14-1008	pa	3	6	motivation_background	proposal_implementation	support	elaboration	secondary	secondary	none	none	Previous studies cast this task as a linear tagging or subtree extraction problem .	Besides , a joint inference mechanism is introduced to incorporate global information across arguments into our constituent-based approach via integer linear programming .	CL_D14-1008_pa_3	CL_D14-1008_pa_6	CL_D14-1008_pa_3_6
CL	D14-1008	pa	3	7	motivation_background	result_means	support	support	secondary	secondary	none	none	Previous studies cast this task as a linear tagging or subtree extraction problem .	Evaluation on PDTB shows significant performance improvements of our constituent-based approach over the best state-of-the-art system .	CL_D14-1008_pa_3	CL_D14-1008_pa_7	CL_D14-1008_pa_3_7
CL	D14-1008	pa	3	8	motivation_background	result	support	elaboration	secondary	secondary	none	none	Previous studies cast this task as a linear tagging or subtree extraction problem .	It also shows the effectiveness of our joint inference mechanism in modeling global information across arguments .	CL_D14-1008_pa_3	CL_D14-1008_pa_8	CL_D14-1008_pa_3_8
CL	D14-1008	pa	4	5	proposal	proposal	elaboration	elaboration	main	secondary	back	elaboration	In this paper , we propose a novel constituent-based approach to argument labeling , which integrates the advantages of both linear tagging and subtree extraction .	In particular , the proposed approach unifies intra- and inter-sentence cases by treating the immediately preceding sentence as a special constituent .	CL_D14-1008_pa_4	CL_D14-1008_pa_5	CL_D14-1008_pa_4_5
CL	D14-1008	pa	5	4	proposal	proposal	elaboration	elaboration	secondary	main	forw	elaboration	In particular , the proposed approach unifies intra- and inter-sentence cases by treating the immediately preceding sentence as a special constituent .	In this paper , we propose a novel constituent-based approach to argument labeling , which integrates the advantages of both linear tagging and subtree extraction .	CL_D14-1008_pa_5	CL_D14-1008_pa_4	CL_D14-1008_pa_4_5
CL	D14-1008	pa	4	6	proposal	proposal_implementation	elaboration	elaboration	main	secondary	none	none	In this paper , we propose a novel constituent-based approach to argument labeling , which integrates the advantages of both linear tagging and subtree extraction .	Besides , a joint inference mechanism is introduced to incorporate global information across arguments into our constituent-based approach via integer linear programming .	CL_D14-1008_pa_4	CL_D14-1008_pa_6	CL_D14-1008_pa_4_6
CL	D14-1008	pa	4	7	proposal	result_means	elaboration	support	main	secondary	back	support	In this paper , we propose a novel constituent-based approach to argument labeling , which integrates the advantages of both linear tagging and subtree extraction .	Evaluation on PDTB shows significant performance improvements of our constituent-based approach over the best state-of-the-art system .	CL_D14-1008_pa_4	CL_D14-1008_pa_7	CL_D14-1008_pa_4_7
CL	D14-1008	pa	7	4	result_means	proposal	support	elaboration	secondary	main	forw	support	Evaluation on PDTB shows significant performance improvements of our constituent-based approach over the best state-of-the-art system .	In this paper , we propose a novel constituent-based approach to argument labeling , which integrates the advantages of both linear tagging and subtree extraction .	CL_D14-1008_pa_7	CL_D14-1008_pa_4	CL_D14-1008_pa_4_7
CL	D14-1008	pa	4	8	proposal	result	elaboration	elaboration	main	secondary	none	none	In this paper , we propose a novel constituent-based approach to argument labeling , which integrates the advantages of both linear tagging and subtree extraction .	It also shows the effectiveness of our joint inference mechanism in modeling global information across arguments .	CL_D14-1008_pa_4	CL_D14-1008_pa_8	CL_D14-1008_pa_4_8
CL	D14-1008	pa	5	6	proposal	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	In particular , the proposed approach unifies intra- and inter-sentence cases by treating the immediately preceding sentence as a special constituent .	Besides , a joint inference mechanism is introduced to incorporate global information across arguments into our constituent-based approach via integer linear programming .	CL_D14-1008_pa_5	CL_D14-1008_pa_6	CL_D14-1008_pa_5_6
CL	D14-1008	pa	6	5	proposal_implementation	proposal	elaboration	elaboration	secondary	secondary	forw	elaboration	Besides , a joint inference mechanism is introduced to incorporate global information across arguments into our constituent-based approach via integer linear programming .	In particular , the proposed approach unifies intra- and inter-sentence cases by treating the immediately preceding sentence as a special constituent .	CL_D14-1008_pa_6	CL_D14-1008_pa_5	CL_D14-1008_pa_5_6
CL	D14-1008	pa	5	7	proposal	result_means	elaboration	support	secondary	secondary	none	none	In particular , the proposed approach unifies intra- and inter-sentence cases by treating the immediately preceding sentence as a special constituent .	Evaluation on PDTB shows significant performance improvements of our constituent-based approach over the best state-of-the-art system .	CL_D14-1008_pa_5	CL_D14-1008_pa_7	CL_D14-1008_pa_5_7
CL	D14-1008	pa	5	8	proposal	result	elaboration	elaboration	secondary	secondary	none	none	In particular , the proposed approach unifies intra- and inter-sentence cases by treating the immediately preceding sentence as a special constituent .	It also shows the effectiveness of our joint inference mechanism in modeling global information across arguments .	CL_D14-1008_pa_5	CL_D14-1008_pa_8	CL_D14-1008_pa_5_8
CL	D14-1008	pa	6	7	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	Besides , a joint inference mechanism is introduced to incorporate global information across arguments into our constituent-based approach via integer linear programming .	Evaluation on PDTB shows significant performance improvements of our constituent-based approach over the best state-of-the-art system .	CL_D14-1008_pa_6	CL_D14-1008_pa_7	CL_D14-1008_pa_6_7
CL	D14-1008	pa	6	8	proposal_implementation	result	elaboration	elaboration	secondary	secondary	none	none	Besides , a joint inference mechanism is introduced to incorporate global information across arguments into our constituent-based approach via integer linear programming .	It also shows the effectiveness of our joint inference mechanism in modeling global information across arguments .	CL_D14-1008_pa_6	CL_D14-1008_pa_8	CL_D14-1008_pa_6_8
CL	D14-1008	pa	7	8	result_means	result	support	elaboration	secondary	secondary	back	elaboration	Evaluation on PDTB shows significant performance improvements of our constituent-based approach over the best state-of-the-art system .	It also shows the effectiveness of our joint inference mechanism in modeling global information across arguments .	CL_D14-1008_pa_7	CL_D14-1008_pa_8	CL_D14-1008_pa_7_8
CL	D14-1008	pa	8	7	result	result_means	elaboration	support	secondary	secondary	forw	elaboration	It also shows the effectiveness of our joint inference mechanism in modeling global information across arguments .	Evaluation on PDTB shows significant performance improvements of our constituent-based approach over the best state-of-the-art system .	CL_D14-1008_pa_8	CL_D14-1008_pa_7	CL_D14-1008_pa_7_8
CL	D14-1009	pa	1	2	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	We present STIR ( STrongly Incremental Repair detection ) , a system that detects speech repairs and edit terms on transcripts incrementally with minimal latency .	STIR uses information-theoretic measures from n-gram models as its principal decision features in a pipeline of classifiers detecting the different stages of repairs .	CL_D14-1009_pa_1	CL_D14-1009_pa_2	CL_D14-1009_pa_1_2
CL	D14-1009	pa	2	1	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	STIR uses information-theoretic measures from n-gram models as its principal decision features in a pipeline of classifiers detecting the different stages of repairs .	We present STIR ( STrongly Incremental Repair detection ) , a system that detects speech repairs and edit terms on transcripts incrementally with minimal latency .	CL_D14-1009_pa_2	CL_D14-1009_pa_1	CL_D14-1009_pa_1_2
CL	D14-1009	pa	1	3	proposal	result_means	none	support	main	secondary	back	support	We present STIR ( STrongly Incremental Repair detection ) , a system that detects speech repairs and edit terms on transcripts incrementally with minimal latency .	Results on the Switchboard disfluency tagged corpus show utterance-final accuracy on a par with state-of-the-art incremental repair detection methods , but with better incremental accuracy , faster time-to-detection and less computational overhead .	CL_D14-1009_pa_1	CL_D14-1009_pa_3	CL_D14-1009_pa_1_3
CL	D14-1009	pa	3	1	result_means	proposal	support	none	secondary	main	forw	support	Results on the Switchboard disfluency tagged corpus show utterance-final accuracy on a par with state-of-the-art incremental repair detection methods , but with better incremental accuracy , faster time-to-detection and less computational overhead .	We present STIR ( STrongly Incremental Repair detection ) , a system that detects speech repairs and edit terms on transcripts incrementally with minimal latency .	CL_D14-1009_pa_3	CL_D14-1009_pa_1	CL_D14-1009_pa_1_3
CL	D14-1009	pa	1	4	proposal	means	none	by-means	main	secondary	none	none	We present STIR ( STrongly Incremental Repair detection ) , a system that detects speech repairs and edit terms on transcripts incrementally with minimal latency .	We evaluate its performance using incremental metrics and propose new repair processing evaluation standards .	CL_D14-1009_pa_1	CL_D14-1009_pa_4	CL_D14-1009_pa_1_4
CL	D14-1009	pa	2	3	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	STIR uses information-theoretic measures from n-gram models as its principal decision features in a pipeline of classifiers detecting the different stages of repairs .	Results on the Switchboard disfluency tagged corpus show utterance-final accuracy on a par with state-of-the-art incremental repair detection methods , but with better incremental accuracy , faster time-to-detection and less computational overhead .	CL_D14-1009_pa_2	CL_D14-1009_pa_3	CL_D14-1009_pa_2_3
CL	D14-1009	pa	2	4	proposal_implementation	means	elaboration	by-means	secondary	secondary	none	none	STIR uses information-theoretic measures from n-gram models as its principal decision features in a pipeline of classifiers detecting the different stages of repairs .	We evaluate its performance using incremental metrics and propose new repair processing evaluation standards .	CL_D14-1009_pa_2	CL_D14-1009_pa_4	CL_D14-1009_pa_2_4
CL	D14-1009	pa	3	4	result_means	means	support	by-means	secondary	secondary	back	by-means	Results on the Switchboard disfluency tagged corpus show utterance-final accuracy on a par with state-of-the-art incremental repair detection methods , but with better incremental accuracy , faster time-to-detection and less computational overhead .	We evaluate its performance using incremental metrics and propose new repair processing evaluation standards .	CL_D14-1009_pa_3	CL_D14-1009_pa_4	CL_D14-1009_pa_3_4
CL	D14-1009	pa	4	3	means	result_means	by-means	support	secondary	secondary	forw	by-means	We evaluate its performance using incremental metrics and propose new repair processing evaluation standards .	Results on the Switchboard disfluency tagged corpus show utterance-final accuracy on a par with state-of-the-art incremental repair detection methods , but with better incremental accuracy , faster time-to-detection and less computational overhead .	CL_D14-1009_pa_4	CL_D14-1009_pa_3	CL_D14-1009_pa_3_4
CL	D14-1010	pa	1	2	motivation_background	motivation_background	support	elaboration	secondary	secondary	back	elaboration	There is rich knowledge encoded in online web data .	For example , punctuation and entity tags in Wikipedia data define some word boundaries in a sentence .	CL_D14-1010_pa_1	CL_D14-1010_pa_2	CL_D14-1010_pa_1_2
CL	D14-1010	pa	2	1	motivation_background	motivation_background	elaboration	support	secondary	secondary	forw	elaboration	For example , punctuation and entity tags in Wikipedia data define some word boundaries in a sentence .	There is rich knowledge encoded in online web data .	CL_D14-1010_pa_2	CL_D14-1010_pa_1	CL_D14-1010_pa_1_2
CL	D14-1010	pa	1	3	motivation_background	proposal	support	none	secondary	main	forw	support	There is rich knowledge encoded in online web data .	In this paper we adopt partial-label learning with conditional random fields to make use of this valuable knowledge for semi-supervised Chinese word segmentation .	CL_D14-1010_pa_1	CL_D14-1010_pa_3	CL_D14-1010_pa_1_3
CL	D14-1010	pa	3	1	proposal	motivation_background	none	support	main	secondary	back	support	In this paper we adopt partial-label learning with conditional random fields to make use of this valuable knowledge for semi-supervised Chinese word segmentation .	There is rich knowledge encoded in online web data .	CL_D14-1010_pa_3	CL_D14-1010_pa_1	CL_D14-1010_pa_1_3
CL	D14-1010	pa	1	4	motivation_background	information_additional	support	info-optional	secondary	secondary	none	none	There is rich knowledge encoded in online web data .	The basic idea of partial-label learning is to optimize a cost function that marginalizes the probability mass in the constrained space that encodes this knowledge .	CL_D14-1010_pa_1	CL_D14-1010_pa_4	CL_D14-1010_pa_1_4
CL	D14-1010	pa	1	5	motivation_background	result	support	support	secondary	secondary	none	none	There is rich knowledge encoded in online web data .	By integrating some domain adaptation techniques , such as EasyAdapt , our result reaches an F-measure of 95.98 % on the CTB-6 corpus , a significant improvement from both the supervised baseline and a previous proposed approach , namely constrained decode .	CL_D14-1010_pa_1	CL_D14-1010_pa_5	CL_D14-1010_pa_1_5
CL	D14-1010	pa	2	3	motivation_background	proposal	elaboration	none	secondary	main	none	none	For example , punctuation and entity tags in Wikipedia data define some word boundaries in a sentence .	In this paper we adopt partial-label learning with conditional random fields to make use of this valuable knowledge for semi-supervised Chinese word segmentation .	CL_D14-1010_pa_2	CL_D14-1010_pa_3	CL_D14-1010_pa_2_3
CL	D14-1010	pa	2	4	motivation_background	information_additional	elaboration	info-optional	secondary	secondary	none	none	For example , punctuation and entity tags in Wikipedia data define some word boundaries in a sentence .	The basic idea of partial-label learning is to optimize a cost function that marginalizes the probability mass in the constrained space that encodes this knowledge .	CL_D14-1010_pa_2	CL_D14-1010_pa_4	CL_D14-1010_pa_2_4
CL	D14-1010	pa	2	5	motivation_background	result	elaboration	support	secondary	secondary	none	none	For example , punctuation and entity tags in Wikipedia data define some word boundaries in a sentence .	By integrating some domain adaptation techniques , such as EasyAdapt , our result reaches an F-measure of 95.98 % on the CTB-6 corpus , a significant improvement from both the supervised baseline and a previous proposed approach , namely constrained decode .	CL_D14-1010_pa_2	CL_D14-1010_pa_5	CL_D14-1010_pa_2_5
CL	D14-1010	pa	3	4	proposal	information_additional	none	info-optional	main	secondary	back	info-optional	In this paper we adopt partial-label learning with conditional random fields to make use of this valuable knowledge for semi-supervised Chinese word segmentation .	The basic idea of partial-label learning is to optimize a cost function that marginalizes the probability mass in the constrained space that encodes this knowledge .	CL_D14-1010_pa_3	CL_D14-1010_pa_4	CL_D14-1010_pa_3_4
CL	D14-1010	pa	4	3	information_additional	proposal	info-optional	none	secondary	main	forw	info-optional	The basic idea of partial-label learning is to optimize a cost function that marginalizes the probability mass in the constrained space that encodes this knowledge .	In this paper we adopt partial-label learning with conditional random fields to make use of this valuable knowledge for semi-supervised Chinese word segmentation .	CL_D14-1010_pa_4	CL_D14-1010_pa_3	CL_D14-1010_pa_3_4
CL	D14-1010	pa	3	5	proposal	result	none	support	main	secondary	back	support	In this paper we adopt partial-label learning with conditional random fields to make use of this valuable knowledge for semi-supervised Chinese word segmentation .	By integrating some domain adaptation techniques , such as EasyAdapt , our result reaches an F-measure of 95.98 % on the CTB-6 corpus , a significant improvement from both the supervised baseline and a previous proposed approach , namely constrained decode .	CL_D14-1010_pa_3	CL_D14-1010_pa_5	CL_D14-1010_pa_3_5
CL	D14-1010	pa	5	3	result	proposal	support	none	secondary	main	forw	support	By integrating some domain adaptation techniques , such as EasyAdapt , our result reaches an F-measure of 95.98 % on the CTB-6 corpus , a significant improvement from both the supervised baseline and a previous proposed approach , namely constrained decode .	In this paper we adopt partial-label learning with conditional random fields to make use of this valuable knowledge for semi-supervised Chinese word segmentation .	CL_D14-1010_pa_5	CL_D14-1010_pa_3	CL_D14-1010_pa_3_5
CL	D14-1010	pa	4	5	information_additional	result	info-optional	support	secondary	secondary	none	none	The basic idea of partial-label learning is to optimize a cost function that marginalizes the probability mass in the constrained space that encodes this knowledge .	By integrating some domain adaptation techniques , such as EasyAdapt , our result reaches an F-measure of 95.98 % on the CTB-6 corpus , a significant improvement from both the supervised baseline and a previous proposed approach , namely constrained decode .	CL_D14-1010_pa_4	CL_D14-1010_pa_5	CL_D14-1010_pa_4_5
CL	D14-1011	pa	1	2	motivation_background	motivation_problem	info-required	support	secondary	secondary	forw	info-required	Microblogs have recently received widespread interest from NLP researchers .	However , current tools for Japanese word segmentation and POS tagging still perform poorly on microblog texts .	CL_D14-1011_pa_1	CL_D14-1011_pa_2	CL_D14-1011_pa_1_2
CL	D14-1011	pa	2	1	motivation_problem	motivation_background	support	info-required	secondary	secondary	back	info-required	However , current tools for Japanese word segmentation and POS tagging still perform poorly on microblog texts .	Microblogs have recently received widespread interest from NLP researchers .	CL_D14-1011_pa_2	CL_D14-1011_pa_1	CL_D14-1011_pa_1_2
CL	D14-1011	pa	1	3	motivation_background	proposal	info-required	none	secondary	main	none	none	Microblogs have recently received widespread interest from NLP researchers .	We developed an annotated corpus and proposed a joint model for over-coming this situation .	CL_D14-1011_pa_1	CL_D14-1011_pa_3	CL_D14-1011_pa_1_3
CL	D14-1011	pa	1	4	motivation_background	conclusion	info-required	support	secondary	secondary	none	none	Microblogs have recently received widespread interest from NLP researchers .	Our annotated corpus of microblog texts enables not only training of accurate statistical models but also quantitative evaluation of their performance .	CL_D14-1011_pa_1	CL_D14-1011_pa_4	CL_D14-1011_pa_1_4
CL	D14-1011	pa	1	5	motivation_background	conclusion	info-required	support	secondary	secondary	none	none	Microblogs have recently received widespread interest from NLP researchers .	Our joint model with lexical normalization handles the orthographic diversity of microblog texts .	CL_D14-1011_pa_1	CL_D14-1011_pa_5	CL_D14-1011_pa_1_5
CL	D14-1011	pa	1	6	motivation_background	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	Microblogs have recently received widespread interest from NLP researchers .	We conducted an experiment to demonstrate that the corpus and model substantially contribute to boosting accuracy .	CL_D14-1011_pa_1	CL_D14-1011_pa_6	CL_D14-1011_pa_1_6
CL	D14-1011	pa	2	3	motivation_problem	proposal	support	none	secondary	main	forw	support	However , current tools for Japanese word segmentation and POS tagging still perform poorly on microblog texts .	We developed an annotated corpus and proposed a joint model for over-coming this situation .	CL_D14-1011_pa_2	CL_D14-1011_pa_3	CL_D14-1011_pa_2_3
CL	D14-1011	pa	3	2	proposal	motivation_problem	none	support	main	secondary	back	support	We developed an annotated corpus and proposed a joint model for over-coming this situation .	However , current tools for Japanese word segmentation and POS tagging still perform poorly on microblog texts .	CL_D14-1011_pa_3	CL_D14-1011_pa_2	CL_D14-1011_pa_2_3
CL	D14-1011	pa	2	4	motivation_problem	conclusion	support	support	secondary	secondary	none	none	However , current tools for Japanese word segmentation and POS tagging still perform poorly on microblog texts .	Our annotated corpus of microblog texts enables not only training of accurate statistical models but also quantitative evaluation of their performance .	CL_D14-1011_pa_2	CL_D14-1011_pa_4	CL_D14-1011_pa_2_4
CL	D14-1011	pa	2	5	motivation_problem	conclusion	support	support	secondary	secondary	none	none	However , current tools for Japanese word segmentation and POS tagging still perform poorly on microblog texts .	Our joint model with lexical normalization handles the orthographic diversity of microblog texts .	CL_D14-1011_pa_2	CL_D14-1011_pa_5	CL_D14-1011_pa_2_5
CL	D14-1011	pa	2	6	motivation_problem	proposal_implementation	support	elaboration	secondary	secondary	none	none	However , current tools for Japanese word segmentation and POS tagging still perform poorly on microblog texts .	We conducted an experiment to demonstrate that the corpus and model substantially contribute to boosting accuracy .	CL_D14-1011_pa_2	CL_D14-1011_pa_6	CL_D14-1011_pa_2_6
CL	D14-1011	pa	3	4	proposal	conclusion	none	support	main	secondary	back	support	We developed an annotated corpus and proposed a joint model for over-coming this situation .	Our annotated corpus of microblog texts enables not only training of accurate statistical models but also quantitative evaluation of their performance .	CL_D14-1011_pa_3	CL_D14-1011_pa_4	CL_D14-1011_pa_3_4
CL	D14-1011	pa	4	3	conclusion	proposal	support	none	secondary	main	forw	support	Our annotated corpus of microblog texts enables not only training of accurate statistical models but also quantitative evaluation of their performance .	We developed an annotated corpus and proposed a joint model for over-coming this situation .	CL_D14-1011_pa_4	CL_D14-1011_pa_3	CL_D14-1011_pa_3_4
CL	D14-1011	pa	3	5	proposal	conclusion	none	support	main	secondary	back	support	We developed an annotated corpus and proposed a joint model for over-coming this situation .	Our joint model with lexical normalization handles the orthographic diversity of microblog texts .	CL_D14-1011_pa_3	CL_D14-1011_pa_5	CL_D14-1011_pa_3_5
CL	D14-1011	pa	5	3	conclusion	proposal	support	none	secondary	main	forw	support	Our joint model with lexical normalization handles the orthographic diversity of microblog texts .	We developed an annotated corpus and proposed a joint model for over-coming this situation .	CL_D14-1011_pa_5	CL_D14-1011_pa_3	CL_D14-1011_pa_3_5
CL	D14-1011	pa	3	6	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	We developed an annotated corpus and proposed a joint model for over-coming this situation .	We conducted an experiment to demonstrate that the corpus and model substantially contribute to boosting accuracy .	CL_D14-1011_pa_3	CL_D14-1011_pa_6	CL_D14-1011_pa_3_6
CL	D14-1011	pa	6	3	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	We conducted an experiment to demonstrate that the corpus and model substantially contribute to boosting accuracy .	We developed an annotated corpus and proposed a joint model for over-coming this situation .	CL_D14-1011_pa_6	CL_D14-1011_pa_3	CL_D14-1011_pa_3_6
CL	D14-1011	pa	4	5	conclusion	conclusion	support	support	secondary	secondary	none	none	Our annotated corpus of microblog texts enables not only training of accurate statistical models but also quantitative evaluation of their performance .	Our joint model with lexical normalization handles the orthographic diversity of microblog texts .	CL_D14-1011_pa_4	CL_D14-1011_pa_5	CL_D14-1011_pa_4_5
CL	D14-1011	pa	4	6	conclusion	proposal_implementation	support	elaboration	secondary	secondary	none	none	Our annotated corpus of microblog texts enables not only training of accurate statistical models but also quantitative evaluation of their performance .	We conducted an experiment to demonstrate that the corpus and model substantially contribute to boosting accuracy .	CL_D14-1011_pa_4	CL_D14-1011_pa_6	CL_D14-1011_pa_4_6
CL	D14-1011	pa	5	6	conclusion	proposal_implementation	support	elaboration	secondary	secondary	none	none	Our joint model with lexical normalization handles the orthographic diversity of microblog texts .	We conducted an experiment to demonstrate that the corpus and model substantially contribute to boosting accuracy .	CL_D14-1011_pa_5	CL_D14-1011_pa_6	CL_D14-1011_pa_5_6
CL	D14-1012	pa	1	2	motivation_background	motivation_problem	info-required	support	secondary	secondary	forw	info-required	Recent work has shown success in using continuous word embeddings learned from unlabeled data as features to improve supervised NLP systems , which is regarded as a simple semi-supervised learning mechanism .	However , fundamental problems on effectively incorporating the word embedding features within the framework of linear models remain .	CL_D14-1012_pa_1	CL_D14-1012_pa_2	CL_D14-1012_pa_1_2
CL	D14-1012	pa	2	1	motivation_problem	motivation_background	support	info-required	secondary	secondary	back	info-required	However , fundamental problems on effectively incorporating the word embedding features within the framework of linear models remain .	Recent work has shown success in using continuous word embeddings learned from unlabeled data as features to improve supervised NLP systems , which is regarded as a simple semi-supervised learning mechanism .	CL_D14-1012_pa_2	CL_D14-1012_pa_1	CL_D14-1012_pa_1_2
CL	D14-1012	pa	1	3	motivation_background	proposal	info-required	none	secondary	main	none	none	Recent work has shown success in using continuous word embeddings learned from unlabeled data as features to improve supervised NLP systems , which is regarded as a simple semi-supervised learning mechanism .	In this study , we investigate and analyze three different approaches , including a new proposed distributional prototype approach , for utilizing the embedding features .	CL_D14-1012_pa_1	CL_D14-1012_pa_3	CL_D14-1012_pa_1_3
CL	D14-1012	pa	1	4	motivation_background	conclusion	info-required	support	secondary	secondary	none	none	Recent work has shown success in using continuous word embeddings learned from unlabeled data as features to improve supervised NLP systems , which is regarded as a simple semi-supervised learning mechanism .	The presented approaches can be integrated into most of the classical linear models in NLP .	CL_D14-1012_pa_1	CL_D14-1012_pa_4	CL_D14-1012_pa_1_4
CL	D14-1012	pa	1	5	motivation_background	result_means	info-required	support	secondary	secondary	none	none	Recent work has shown success in using continuous word embeddings learned from unlabeled data as features to improve supervised NLP systems , which is regarded as a simple semi-supervised learning mechanism .	Experiments on the task of named entity recognition show that each of the proposed approaches can better utilize the word embedding features , among which the distributional prototype approach performs the best .	CL_D14-1012_pa_1	CL_D14-1012_pa_5	CL_D14-1012_pa_1_5
CL	D14-1012	pa	1	6	motivation_background	result_means	info-required	elaboration	secondary	secondary	none	none	Recent work has shown success in using continuous word embeddings learned from unlabeled data as features to improve supervised NLP systems , which is regarded as a simple semi-supervised learning mechanism .	Moreover , the combination of the approaches provides additive improvements , outperforming the dense and continuous embedding features by nearly 2 points of F1 score .	CL_D14-1012_pa_1	CL_D14-1012_pa_6	CL_D14-1012_pa_1_6
CL	D14-1012	pa	2	3	motivation_problem	proposal	support	none	secondary	main	forw	support	However , fundamental problems on effectively incorporating the word embedding features within the framework of linear models remain .	In this study , we investigate and analyze three different approaches , including a new proposed distributional prototype approach , for utilizing the embedding features .	CL_D14-1012_pa_2	CL_D14-1012_pa_3	CL_D14-1012_pa_2_3
CL	D14-1012	pa	3	2	proposal	motivation_problem	none	support	main	secondary	back	support	In this study , we investigate and analyze three different approaches , including a new proposed distributional prototype approach , for utilizing the embedding features .	However , fundamental problems on effectively incorporating the word embedding features within the framework of linear models remain .	CL_D14-1012_pa_3	CL_D14-1012_pa_2	CL_D14-1012_pa_2_3
CL	D14-1012	pa	2	4	motivation_problem	conclusion	support	support	secondary	secondary	none	none	However , fundamental problems on effectively incorporating the word embedding features within the framework of linear models remain .	The presented approaches can be integrated into most of the classical linear models in NLP .	CL_D14-1012_pa_2	CL_D14-1012_pa_4	CL_D14-1012_pa_2_4
CL	D14-1012	pa	2	5	motivation_problem	result_means	support	support	secondary	secondary	none	none	However , fundamental problems on effectively incorporating the word embedding features within the framework of linear models remain .	Experiments on the task of named entity recognition show that each of the proposed approaches can better utilize the word embedding features , among which the distributional prototype approach performs the best .	CL_D14-1012_pa_2	CL_D14-1012_pa_5	CL_D14-1012_pa_2_5
CL	D14-1012	pa	2	6	motivation_problem	result_means	support	elaboration	secondary	secondary	none	none	However , fundamental problems on effectively incorporating the word embedding features within the framework of linear models remain .	Moreover , the combination of the approaches provides additive improvements , outperforming the dense and continuous embedding features by nearly 2 points of F1 score .	CL_D14-1012_pa_2	CL_D14-1012_pa_6	CL_D14-1012_pa_2_6
CL	D14-1012	pa	3	4	proposal	conclusion	none	support	main	secondary	back	support	In this study , we investigate and analyze three different approaches , including a new proposed distributional prototype approach , for utilizing the embedding features .	The presented approaches can be integrated into most of the classical linear models in NLP .	CL_D14-1012_pa_3	CL_D14-1012_pa_4	CL_D14-1012_pa_3_4
CL	D14-1012	pa	4	3	conclusion	proposal	support	none	secondary	main	forw	support	The presented approaches can be integrated into most of the classical linear models in NLP .	In this study , we investigate and analyze three different approaches , including a new proposed distributional prototype approach , for utilizing the embedding features .	CL_D14-1012_pa_4	CL_D14-1012_pa_3	CL_D14-1012_pa_3_4
CL	D14-1012	pa	3	5	proposal	result_means	none	support	main	secondary	back	support	In this study , we investigate and analyze three different approaches , including a new proposed distributional prototype approach , for utilizing the embedding features .	Experiments on the task of named entity recognition show that each of the proposed approaches can better utilize the word embedding features , among which the distributional prototype approach performs the best .	CL_D14-1012_pa_3	CL_D14-1012_pa_5	CL_D14-1012_pa_3_5
CL	D14-1012	pa	5	3	result_means	proposal	support	none	secondary	main	forw	support	Experiments on the task of named entity recognition show that each of the proposed approaches can better utilize the word embedding features , among which the distributional prototype approach performs the best .	In this study , we investigate and analyze three different approaches , including a new proposed distributional prototype approach , for utilizing the embedding features .	CL_D14-1012_pa_5	CL_D14-1012_pa_3	CL_D14-1012_pa_3_5
CL	D14-1012	pa	3	6	proposal	result_means	none	elaboration	main	secondary	none	none	In this study , we investigate and analyze three different approaches , including a new proposed distributional prototype approach , for utilizing the embedding features .	Moreover , the combination of the approaches provides additive improvements , outperforming the dense and continuous embedding features by nearly 2 points of F1 score .	CL_D14-1012_pa_3	CL_D14-1012_pa_6	CL_D14-1012_pa_3_6
CL	D14-1012	pa	4	5	conclusion	result_means	support	support	secondary	secondary	none	none	The presented approaches can be integrated into most of the classical linear models in NLP .	Experiments on the task of named entity recognition show that each of the proposed approaches can better utilize the word embedding features , among which the distributional prototype approach performs the best .	CL_D14-1012_pa_4	CL_D14-1012_pa_5	CL_D14-1012_pa_4_5
CL	D14-1012	pa	4	6	conclusion	result_means	support	elaboration	secondary	secondary	none	none	The presented approaches can be integrated into most of the classical linear models in NLP .	Moreover , the combination of the approaches provides additive improvements , outperforming the dense and continuous embedding features by nearly 2 points of F1 score .	CL_D14-1012_pa_4	CL_D14-1012_pa_6	CL_D14-1012_pa_4_6
CL	D14-1012	pa	5	6	result_means	result_means	support	elaboration	secondary	secondary	back	elaboration	Experiments on the task of named entity recognition show that each of the proposed approaches can better utilize the word embedding features , among which the distributional prototype approach performs the best .	Moreover , the combination of the approaches provides additive improvements , outperforming the dense and continuous embedding features by nearly 2 points of F1 score .	CL_D14-1012_pa_5	CL_D14-1012_pa_6	CL_D14-1012_pa_5_6
CL	D14-1012	pa	6	5	result_means	result_means	elaboration	support	secondary	secondary	forw	elaboration	Moreover , the combination of the approaches provides additive improvements , outperforming the dense and continuous embedding features by nearly 2 points of F1 score .	Experiments on the task of named entity recognition show that each of the proposed approaches can better utilize the word embedding features , among which the distributional prototype approach performs the best .	CL_D14-1012_pa_6	CL_D14-1012_pa_5	CL_D14-1012_pa_5_6
CL	D14-1013	pa	1	2	motivation_background	motivation_hypothesis	info-required	support	secondary	secondary	forw	info-required	Punctuation prediction and disfluency prediction can improve downstream natural language processing tasks such as machine translation and information extraction .	Combining the two tasks can potentially improve the efficiency of the overall pipeline system and reduce error propagation .	CL_D14-1013_pa_1	CL_D14-1013_pa_2	CL_D14-1013_pa_1_2
CL	D14-1013	pa	2	1	motivation_hypothesis	motivation_background	support	info-required	secondary	secondary	back	info-required	Combining the two tasks can potentially improve the efficiency of the overall pipeline system and reduce error propagation .	Punctuation prediction and disfluency prediction can improve downstream natural language processing tasks such as machine translation and information extraction .	CL_D14-1013_pa_2	CL_D14-1013_pa_1	CL_D14-1013_pa_1_2
CL	D14-1013	pa	1	3	motivation_background	proposal	info-required	none	secondary	main	none	none	Punctuation prediction and disfluency prediction can improve downstream natural language processing tasks such as machine translation and information extraction .	In this work , we compare various methods for combining punctuation prediction ( PU ) and disfluency prediction ( DF ) on the Switchboard corpus .	CL_D14-1013_pa_1	CL_D14-1013_pa_3	CL_D14-1013_pa_1_3
CL	D14-1013	pa	1	4	motivation_background	proposal	info-required	elaboration	secondary	secondary	none	none	Punctuation prediction and disfluency prediction can improve downstream natural language processing tasks such as machine translation and information extraction .	We compare an isolated prediction approach with a cascade approach , a rescoring approach , and three joint model approaches .	CL_D14-1013_pa_1	CL_D14-1013_pa_4	CL_D14-1013_pa_1_4
CL	D14-1013	pa	1	5	motivation_background	result	info-required	support	secondary	secondary	none	none	Punctuation prediction and disfluency prediction can improve downstream natural language processing tasks such as machine translation and information extraction .	For the cascade approach , we show that the soft cascade method is better than the hard cascade method .	CL_D14-1013_pa_1	CL_D14-1013_pa_5	CL_D14-1013_pa_1_5
CL	D14-1013	pa	1	6	motivation_background	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	Punctuation prediction and disfluency prediction can improve downstream natural language processing tasks such as machine translation and information extraction .	We also use the cascade models to generate an n-best list , use the bi-directional cascade models to perform rescoring , and compare that with the results of the cascade models .	CL_D14-1013_pa_1	CL_D14-1013_pa_6	CL_D14-1013_pa_1_6
CL	D14-1013	pa	1	7	motivation_background	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	Punctuation prediction and disfluency prediction can improve downstream natural language processing tasks such as machine translation and information extraction .	For the joint model approach , we compare mixed-label Linear-chain Conditional Random Field ( LCRF ) , cross-product LCRF and 2-layer Factorial Conditional Random Field ( FCRF ) with soft-cascade LCRF .	CL_D14-1013_pa_1	CL_D14-1013_pa_7	CL_D14-1013_pa_1_7
CL	D14-1013	pa	1	8	motivation_background	result	info-required	support	secondary	secondary	none	none	Punctuation prediction and disfluency prediction can improve downstream natural language processing tasks such as machine translation and information extraction .	Our results show that the various methods linking the two tasks are not significantly different from one another , although they perform better than the isolated prediction method by 0.5-1.5 % in the F1 score .	CL_D14-1013_pa_1	CL_D14-1013_pa_8	CL_D14-1013_pa_1_8
CL	D14-1013	pa	1	9	motivation_background	result	info-required	elaboration	secondary	secondary	none	none	Punctuation prediction and disfluency prediction can improve downstream natural language processing tasks such as machine translation and information extraction .	Moreover , the clique order of features also shows a marked difference .	CL_D14-1013_pa_1	CL_D14-1013_pa_9	CL_D14-1013_pa_1_9
CL	D14-1013	pa	2	3	motivation_hypothesis	proposal	support	none	secondary	main	forw	support	Combining the two tasks can potentially improve the efficiency of the overall pipeline system and reduce error propagation .	In this work , we compare various methods for combining punctuation prediction ( PU ) and disfluency prediction ( DF ) on the Switchboard corpus .	CL_D14-1013_pa_2	CL_D14-1013_pa_3	CL_D14-1013_pa_2_3
CL	D14-1013	pa	3	2	proposal	motivation_hypothesis	none	support	main	secondary	back	support	In this work , we compare various methods for combining punctuation prediction ( PU ) and disfluency prediction ( DF ) on the Switchboard corpus .	Combining the two tasks can potentially improve the efficiency of the overall pipeline system and reduce error propagation .	CL_D14-1013_pa_3	CL_D14-1013_pa_2	CL_D14-1013_pa_2_3
CL	D14-1013	pa	2	4	motivation_hypothesis	proposal	support	elaboration	secondary	secondary	none	none	Combining the two tasks can potentially improve the efficiency of the overall pipeline system and reduce error propagation .	We compare an isolated prediction approach with a cascade approach , a rescoring approach , and three joint model approaches .	CL_D14-1013_pa_2	CL_D14-1013_pa_4	CL_D14-1013_pa_2_4
CL	D14-1013	pa	2	5	motivation_hypothesis	result	support	support	secondary	secondary	none	none	Combining the two tasks can potentially improve the efficiency of the overall pipeline system and reduce error propagation .	For the cascade approach , we show that the soft cascade method is better than the hard cascade method .	CL_D14-1013_pa_2	CL_D14-1013_pa_5	CL_D14-1013_pa_2_5
CL	D14-1013	pa	2	6	motivation_hypothesis	proposal_implementation	support	elaboration	secondary	secondary	none	none	Combining the two tasks can potentially improve the efficiency of the overall pipeline system and reduce error propagation .	We also use the cascade models to generate an n-best list , use the bi-directional cascade models to perform rescoring , and compare that with the results of the cascade models .	CL_D14-1013_pa_2	CL_D14-1013_pa_6	CL_D14-1013_pa_2_6
CL	D14-1013	pa	2	7	motivation_hypothesis	proposal_implementation	support	elaboration	secondary	secondary	none	none	Combining the two tasks can potentially improve the efficiency of the overall pipeline system and reduce error propagation .	For the joint model approach , we compare mixed-label Linear-chain Conditional Random Field ( LCRF ) , cross-product LCRF and 2-layer Factorial Conditional Random Field ( FCRF ) with soft-cascade LCRF .	CL_D14-1013_pa_2	CL_D14-1013_pa_7	CL_D14-1013_pa_2_7
CL	D14-1013	pa	2	8	motivation_hypothesis	result	support	support	secondary	secondary	none	none	Combining the two tasks can potentially improve the efficiency of the overall pipeline system and reduce error propagation .	Our results show that the various methods linking the two tasks are not significantly different from one another , although they perform better than the isolated prediction method by 0.5-1.5 % in the F1 score .	CL_D14-1013_pa_2	CL_D14-1013_pa_8	CL_D14-1013_pa_2_8
CL	D14-1013	pa	2	9	motivation_hypothesis	result	support	elaboration	secondary	secondary	none	none	Combining the two tasks can potentially improve the efficiency of the overall pipeline system and reduce error propagation .	Moreover , the clique order of features also shows a marked difference .	CL_D14-1013_pa_2	CL_D14-1013_pa_9	CL_D14-1013_pa_2_9
CL	D14-1013	pa	3	4	proposal	proposal	none	elaboration	main	secondary	back	elaboration	In this work , we compare various methods for combining punctuation prediction ( PU ) and disfluency prediction ( DF ) on the Switchboard corpus .	We compare an isolated prediction approach with a cascade approach , a rescoring approach , and three joint model approaches .	CL_D14-1013_pa_3	CL_D14-1013_pa_4	CL_D14-1013_pa_3_4
CL	D14-1013	pa	4	3	proposal	proposal	elaboration	none	secondary	main	forw	elaboration	We compare an isolated prediction approach with a cascade approach , a rescoring approach , and three joint model approaches .	In this work , we compare various methods for combining punctuation prediction ( PU ) and disfluency prediction ( DF ) on the Switchboard corpus .	CL_D14-1013_pa_4	CL_D14-1013_pa_3	CL_D14-1013_pa_3_4
CL	D14-1013	pa	3	5	proposal	result	none	support	main	secondary	none	none	In this work , we compare various methods for combining punctuation prediction ( PU ) and disfluency prediction ( DF ) on the Switchboard corpus .	For the cascade approach , we show that the soft cascade method is better than the hard cascade method .	CL_D14-1013_pa_3	CL_D14-1013_pa_5	CL_D14-1013_pa_3_5
CL	D14-1013	pa	3	6	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	In this work , we compare various methods for combining punctuation prediction ( PU ) and disfluency prediction ( DF ) on the Switchboard corpus .	We also use the cascade models to generate an n-best list , use the bi-directional cascade models to perform rescoring , and compare that with the results of the cascade models .	CL_D14-1013_pa_3	CL_D14-1013_pa_6	CL_D14-1013_pa_3_6
CL	D14-1013	pa	3	7	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	In this work , we compare various methods for combining punctuation prediction ( PU ) and disfluency prediction ( DF ) on the Switchboard corpus .	For the joint model approach , we compare mixed-label Linear-chain Conditional Random Field ( LCRF ) , cross-product LCRF and 2-layer Factorial Conditional Random Field ( FCRF ) with soft-cascade LCRF .	CL_D14-1013_pa_3	CL_D14-1013_pa_7	CL_D14-1013_pa_3_7
CL	D14-1013	pa	3	8	proposal	result	none	support	main	secondary	back	support	In this work , we compare various methods for combining punctuation prediction ( PU ) and disfluency prediction ( DF ) on the Switchboard corpus .	Our results show that the various methods linking the two tasks are not significantly different from one another , although they perform better than the isolated prediction method by 0.5-1.5 % in the F1 score .	CL_D14-1013_pa_3	CL_D14-1013_pa_8	CL_D14-1013_pa_3_8
CL	D14-1013	pa	8	3	result	proposal	support	none	secondary	main	forw	support	Our results show that the various methods linking the two tasks are not significantly different from one another , although they perform better than the isolated prediction method by 0.5-1.5 % in the F1 score .	In this work , we compare various methods for combining punctuation prediction ( PU ) and disfluency prediction ( DF ) on the Switchboard corpus .	CL_D14-1013_pa_8	CL_D14-1013_pa_3	CL_D14-1013_pa_3_8
CL	D14-1013	pa	3	9	proposal	result	none	elaboration	main	secondary	none	none	In this work , we compare various methods for combining punctuation prediction ( PU ) and disfluency prediction ( DF ) on the Switchboard corpus .	Moreover , the clique order of features also shows a marked difference .	CL_D14-1013_pa_3	CL_D14-1013_pa_9	CL_D14-1013_pa_3_9
CL	D14-1013	pa	4	5	proposal	result	elaboration	support	secondary	secondary	back	support	We compare an isolated prediction approach with a cascade approach , a rescoring approach , and three joint model approaches .	For the cascade approach , we show that the soft cascade method is better than the hard cascade method .	CL_D14-1013_pa_4	CL_D14-1013_pa_5	CL_D14-1013_pa_4_5
CL	D14-1013	pa	5	4	result	proposal	support	elaboration	secondary	secondary	forw	support	For the cascade approach , we show that the soft cascade method is better than the hard cascade method .	We compare an isolated prediction approach with a cascade approach , a rescoring approach , and three joint model approaches .	CL_D14-1013_pa_5	CL_D14-1013_pa_4	CL_D14-1013_pa_4_5
CL	D14-1013	pa	4	6	proposal	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	We compare an isolated prediction approach with a cascade approach , a rescoring approach , and three joint model approaches .	We also use the cascade models to generate an n-best list , use the bi-directional cascade models to perform rescoring , and compare that with the results of the cascade models .	CL_D14-1013_pa_4	CL_D14-1013_pa_6	CL_D14-1013_pa_4_6
CL	D14-1013	pa	6	4	proposal_implementation	proposal	elaboration	elaboration	secondary	secondary	forw	elaboration	We also use the cascade models to generate an n-best list , use the bi-directional cascade models to perform rescoring , and compare that with the results of the cascade models .	We compare an isolated prediction approach with a cascade approach , a rescoring approach , and three joint model approaches .	CL_D14-1013_pa_6	CL_D14-1013_pa_4	CL_D14-1013_pa_4_6
CL	D14-1013	pa	4	7	proposal	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	We compare an isolated prediction approach with a cascade approach , a rescoring approach , and three joint model approaches .	For the joint model approach , we compare mixed-label Linear-chain Conditional Random Field ( LCRF ) , cross-product LCRF and 2-layer Factorial Conditional Random Field ( FCRF ) with soft-cascade LCRF .	CL_D14-1013_pa_4	CL_D14-1013_pa_7	CL_D14-1013_pa_4_7
CL	D14-1013	pa	7	4	proposal_implementation	proposal	elaboration	elaboration	secondary	secondary	forw	elaboration	For the joint model approach , we compare mixed-label Linear-chain Conditional Random Field ( LCRF ) , cross-product LCRF and 2-layer Factorial Conditional Random Field ( FCRF ) with soft-cascade LCRF .	We compare an isolated prediction approach with a cascade approach , a rescoring approach , and three joint model approaches .	CL_D14-1013_pa_7	CL_D14-1013_pa_4	CL_D14-1013_pa_4_7
CL	D14-1013	pa	4	8	proposal	result	elaboration	support	secondary	secondary	none	none	We compare an isolated prediction approach with a cascade approach , a rescoring approach , and three joint model approaches .	Our results show that the various methods linking the two tasks are not significantly different from one another , although they perform better than the isolated prediction method by 0.5-1.5 % in the F1 score .	CL_D14-1013_pa_4	CL_D14-1013_pa_8	CL_D14-1013_pa_4_8
CL	D14-1013	pa	4	9	proposal	result	elaboration	elaboration	secondary	secondary	none	none	We compare an isolated prediction approach with a cascade approach , a rescoring approach , and three joint model approaches .	Moreover , the clique order of features also shows a marked difference .	CL_D14-1013_pa_4	CL_D14-1013_pa_9	CL_D14-1013_pa_4_9
CL	D14-1013	pa	5	6	result	proposal_implementation	support	elaboration	secondary	secondary	none	none	For the cascade approach , we show that the soft cascade method is better than the hard cascade method .	We also use the cascade models to generate an n-best list , use the bi-directional cascade models to perform rescoring , and compare that with the results of the cascade models .	CL_D14-1013_pa_5	CL_D14-1013_pa_6	CL_D14-1013_pa_5_6
CL	D14-1013	pa	5	7	result	proposal_implementation	support	elaboration	secondary	secondary	none	none	For the cascade approach , we show that the soft cascade method is better than the hard cascade method .	For the joint model approach , we compare mixed-label Linear-chain Conditional Random Field ( LCRF ) , cross-product LCRF and 2-layer Factorial Conditional Random Field ( FCRF ) with soft-cascade LCRF .	CL_D14-1013_pa_5	CL_D14-1013_pa_7	CL_D14-1013_pa_5_7
CL	D14-1013	pa	5	8	result	result	support	support	secondary	secondary	none	none	For the cascade approach , we show that the soft cascade method is better than the hard cascade method .	Our results show that the various methods linking the two tasks are not significantly different from one another , although they perform better than the isolated prediction method by 0.5-1.5 % in the F1 score .	CL_D14-1013_pa_5	CL_D14-1013_pa_8	CL_D14-1013_pa_5_8
CL	D14-1013	pa	5	9	result	result	support	elaboration	secondary	secondary	none	none	For the cascade approach , we show that the soft cascade method is better than the hard cascade method .	Moreover , the clique order of features also shows a marked difference .	CL_D14-1013_pa_5	CL_D14-1013_pa_9	CL_D14-1013_pa_5_9
CL	D14-1013	pa	6	7	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	none	none	We also use the cascade models to generate an n-best list , use the bi-directional cascade models to perform rescoring , and compare that with the results of the cascade models .	For the joint model approach , we compare mixed-label Linear-chain Conditional Random Field ( LCRF ) , cross-product LCRF and 2-layer Factorial Conditional Random Field ( FCRF ) with soft-cascade LCRF .	CL_D14-1013_pa_6	CL_D14-1013_pa_7	CL_D14-1013_pa_6_7
CL	D14-1013	pa	6	8	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	We also use the cascade models to generate an n-best list , use the bi-directional cascade models to perform rescoring , and compare that with the results of the cascade models .	Our results show that the various methods linking the two tasks are not significantly different from one another , although they perform better than the isolated prediction method by 0.5-1.5 % in the F1 score .	CL_D14-1013_pa_6	CL_D14-1013_pa_8	CL_D14-1013_pa_6_8
CL	D14-1013	pa	6	9	proposal_implementation	result	elaboration	elaboration	secondary	secondary	none	none	We also use the cascade models to generate an n-best list , use the bi-directional cascade models to perform rescoring , and compare that with the results of the cascade models .	Moreover , the clique order of features also shows a marked difference .	CL_D14-1013_pa_6	CL_D14-1013_pa_9	CL_D14-1013_pa_6_9
CL	D14-1013	pa	7	8	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	For the joint model approach , we compare mixed-label Linear-chain Conditional Random Field ( LCRF ) , cross-product LCRF and 2-layer Factorial Conditional Random Field ( FCRF ) with soft-cascade LCRF .	Our results show that the various methods linking the two tasks are not significantly different from one another , although they perform better than the isolated prediction method by 0.5-1.5 % in the F1 score .	CL_D14-1013_pa_7	CL_D14-1013_pa_8	CL_D14-1013_pa_7_8
CL	D14-1013	pa	7	9	proposal_implementation	result	elaboration	elaboration	secondary	secondary	none	none	For the joint model approach , we compare mixed-label Linear-chain Conditional Random Field ( LCRF ) , cross-product LCRF and 2-layer Factorial Conditional Random Field ( FCRF ) with soft-cascade LCRF .	Moreover , the clique order of features also shows a marked difference .	CL_D14-1013_pa_7	CL_D14-1013_pa_9	CL_D14-1013_pa_7_9
CL	D14-1013	pa	8	9	result	result	support	elaboration	secondary	secondary	back	elaboration	Our results show that the various methods linking the two tasks are not significantly different from one another , although they perform better than the isolated prediction method by 0.5-1.5 % in the F1 score .	Moreover , the clique order of features also shows a marked difference .	CL_D14-1013_pa_8	CL_D14-1013_pa_9	CL_D14-1013_pa_8_9
CL	D14-1013	pa	9	8	result	result	elaboration	support	secondary	secondary	forw	elaboration	Moreover , the clique order of features also shows a marked difference .	Our results show that the various methods linking the two tasks are not significantly different from one another , although they perform better than the isolated prediction method by 0.5-1.5 % in the F1 score .	CL_D14-1013_pa_9	CL_D14-1013_pa_8	CL_D14-1013_pa_8_9
CL	D14-1014	pa	1	2	proposal	conclusion	none	support	main	secondary	back	support	We introduce submodular optimization to the problem of training data subset selection for statistical machine translation ( SMT ) .	By explicitly formulating data selection as a submodular program , we obtain fast scalable selection algorithms with mathematical performance guarantees , resulting in a unified framework that clarifies existing approaches and also makes both new and many previous approaches easily accessible .	CL_D14-1014_pa_1	CL_D14-1014_pa_2	CL_D14-1014_pa_1_2
CL	D14-1014	pa	2	1	conclusion	proposal	support	none	secondary	main	forw	support	By explicitly formulating data selection as a submodular program , we obtain fast scalable selection algorithms with mathematical performance guarantees , resulting in a unified framework that clarifies existing approaches and also makes both new and many previous approaches easily accessible .	We introduce submodular optimization to the problem of training data subset selection for statistical machine translation ( SMT ) .	CL_D14-1014_pa_2	CL_D14-1014_pa_1	CL_D14-1014_pa_1_2
CL	D14-1014	pa	1	3	proposal	proposal	none	elaboration	main	secondary	back	elaboration	We introduce submodular optimization to the problem of training data subset selection for statistical machine translation ( SMT ) .	We present a new class of submodular functions designed specifically for SMT and evaluate them on two different translation tasks .	CL_D14-1014_pa_1	CL_D14-1014_pa_3	CL_D14-1014_pa_1_3
CL	D14-1014	pa	3	1	proposal	proposal	elaboration	none	secondary	main	forw	elaboration	We present a new class of submodular functions designed specifically for SMT and evaluate them on two different translation tasks .	We introduce submodular optimization to the problem of training data subset selection for statistical machine translation ( SMT ) .	CL_D14-1014_pa_3	CL_D14-1014_pa_1	CL_D14-1014_pa_1_3
CL	D14-1014	pa	1	4	proposal	result	none	support	main	secondary	none	none	We introduce submodular optimization to the problem of training data subset selection for statistical machine translation ( SMT ) .	Our results show that our best submodular method significantly outperforms several baseline methods , including the widely-used cross-entropy based data selection method .	CL_D14-1014_pa_1	CL_D14-1014_pa_4	CL_D14-1014_pa_1_4
CL	D14-1014	pa	1	5	proposal	conclusion	none	elaboration	main	secondary	none	none	We introduce submodular optimization to the problem of training data subset selection for statistical machine translation ( SMT ) .	In addition , our approach easily scales to large data sets and is applicable to other data selection problems in natural language processing .	CL_D14-1014_pa_1	CL_D14-1014_pa_5	CL_D14-1014_pa_1_5
CL	D14-1014	pa	2	3	conclusion	proposal	support	elaboration	secondary	secondary	none	none	By explicitly formulating data selection as a submodular program , we obtain fast scalable selection algorithms with mathematical performance guarantees , resulting in a unified framework that clarifies existing approaches and also makes both new and many previous approaches easily accessible .	We present a new class of submodular functions designed specifically for SMT and evaluate them on two different translation tasks .	CL_D14-1014_pa_2	CL_D14-1014_pa_3	CL_D14-1014_pa_2_3
CL	D14-1014	pa	2	4	conclusion	result	support	support	secondary	secondary	back	support	By explicitly formulating data selection as a submodular program , we obtain fast scalable selection algorithms with mathematical performance guarantees , resulting in a unified framework that clarifies existing approaches and also makes both new and many previous approaches easily accessible .	Our results show that our best submodular method significantly outperforms several baseline methods , including the widely-used cross-entropy based data selection method .	CL_D14-1014_pa_2	CL_D14-1014_pa_4	CL_D14-1014_pa_2_4
CL	D14-1014	pa	4	2	result	conclusion	support	support	secondary	secondary	forw	support	Our results show that our best submodular method significantly outperforms several baseline methods , including the widely-used cross-entropy based data selection method .	By explicitly formulating data selection as a submodular program , we obtain fast scalable selection algorithms with mathematical performance guarantees , resulting in a unified framework that clarifies existing approaches and also makes both new and many previous approaches easily accessible .	CL_D14-1014_pa_4	CL_D14-1014_pa_2	CL_D14-1014_pa_2_4
CL	D14-1014	pa	2	5	conclusion	conclusion	support	elaboration	secondary	secondary	back	elaboration	By explicitly formulating data selection as a submodular program , we obtain fast scalable selection algorithms with mathematical performance guarantees , resulting in a unified framework that clarifies existing approaches and also makes both new and many previous approaches easily accessible .	In addition , our approach easily scales to large data sets and is applicable to other data selection problems in natural language processing .	CL_D14-1014_pa_2	CL_D14-1014_pa_5	CL_D14-1014_pa_2_5
CL	D14-1014	pa	5	2	conclusion	conclusion	elaboration	support	secondary	secondary	forw	elaboration	In addition , our approach easily scales to large data sets and is applicable to other data selection problems in natural language processing .	By explicitly formulating data selection as a submodular program , we obtain fast scalable selection algorithms with mathematical performance guarantees , resulting in a unified framework that clarifies existing approaches and also makes both new and many previous approaches easily accessible .	CL_D14-1014_pa_5	CL_D14-1014_pa_2	CL_D14-1014_pa_2_5
CL	D14-1014	pa	3	4	proposal	result	elaboration	support	secondary	secondary	none	none	We present a new class of submodular functions designed specifically for SMT and evaluate them on two different translation tasks .	Our results show that our best submodular method significantly outperforms several baseline methods , including the widely-used cross-entropy based data selection method .	CL_D14-1014_pa_3	CL_D14-1014_pa_4	CL_D14-1014_pa_3_4
CL	D14-1014	pa	3	5	proposal	conclusion	elaboration	elaboration	secondary	secondary	none	none	We present a new class of submodular functions designed specifically for SMT and evaluate them on two different translation tasks .	In addition , our approach easily scales to large data sets and is applicable to other data selection problems in natural language processing .	CL_D14-1014_pa_3	CL_D14-1014_pa_5	CL_D14-1014_pa_3_5
CL	D14-1014	pa	4	5	result	conclusion	support	elaboration	secondary	secondary	none	none	Our results show that our best submodular method significantly outperforms several baseline methods , including the widely-used cross-entropy based data selection method .	In addition , our approach easily scales to large data sets and is applicable to other data selection problems in natural language processing .	CL_D14-1014_pa_4	CL_D14-1014_pa_5	CL_D14-1014_pa_4_5
CL	D14-1015	pa	1	2	proposal	motivation_problem	none	support	main	secondary	back	support	We investigate how to improve bilingual embedding which has been successfully used as a feature in phrase-based statistical machine translation ( SMT ) .	Despite bilingual embedding's success , the contextual information , which is of critical importance to translation quality , was ignored in previous work .	CL_D14-1015_pa_1	CL_D14-1015_pa_2	CL_D14-1015_pa_1_2
CL	D14-1015	pa	2	1	motivation_problem	proposal	support	none	secondary	main	forw	support	Despite bilingual embedding's success , the contextual information , which is of critical importance to translation quality , was ignored in previous work .	We investigate how to improve bilingual embedding which has been successfully used as a feature in phrase-based statistical machine translation ( SMT ) .	CL_D14-1015_pa_2	CL_D14-1015_pa_1	CL_D14-1015_pa_1_2
CL	D14-1015	pa	1	3	proposal	proposal	none	elaboration	main	secondary	back	elaboration	We investigate how to improve bilingual embedding which has been successfully used as a feature in phrase-based statistical machine translation ( SMT ) .	To employ the contextual information , we propose a simple and memory-efficient model for learning bilingual embedding , taking both the source phrase and context around the phrase into account .	CL_D14-1015_pa_1	CL_D14-1015_pa_3	CL_D14-1015_pa_1_3
CL	D14-1015	pa	3	1	proposal	proposal	elaboration	none	secondary	main	forw	elaboration	To employ the contextual information , we propose a simple and memory-efficient model for learning bilingual embedding , taking both the source phrase and context around the phrase into account .	We investigate how to improve bilingual embedding which has been successfully used as a feature in phrase-based statistical machine translation ( SMT ) .	CL_D14-1015_pa_3	CL_D14-1015_pa_1	CL_D14-1015_pa_1_3
CL	D14-1015	pa	1	4	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	We investigate how to improve bilingual embedding which has been successfully used as a feature in phrase-based statistical machine translation ( SMT ) .	Bilingual translation scores generated from our proposed bilingual embedding model are used as features in our SMT system .	CL_D14-1015_pa_1	CL_D14-1015_pa_4	CL_D14-1015_pa_1_4
CL	D14-1015	pa	1	5	proposal	result	none	support	main	secondary	none	none	We investigate how to improve bilingual embedding which has been successfully used as a feature in phrase-based statistical machine translation ( SMT ) .	Experimental results show that the proposed method achieves significant improvements on large-scale Chinese-English translation task .	CL_D14-1015_pa_1	CL_D14-1015_pa_5	CL_D14-1015_pa_1_5
CL	D14-1015	pa	2	3	motivation_problem	proposal	support	elaboration	secondary	secondary	none	none	Despite bilingual embedding's success , the contextual information , which is of critical importance to translation quality , was ignored in previous work .	To employ the contextual information , we propose a simple and memory-efficient model for learning bilingual embedding , taking both the source phrase and context around the phrase into account .	CL_D14-1015_pa_2	CL_D14-1015_pa_3	CL_D14-1015_pa_2_3
CL	D14-1015	pa	2	4	motivation_problem	proposal_implementation	support	elaboration	secondary	secondary	none	none	Despite bilingual embedding's success , the contextual information , which is of critical importance to translation quality , was ignored in previous work .	Bilingual translation scores generated from our proposed bilingual embedding model are used as features in our SMT system .	CL_D14-1015_pa_2	CL_D14-1015_pa_4	CL_D14-1015_pa_2_4
CL	D14-1015	pa	2	5	motivation_problem	result	support	support	secondary	secondary	none	none	Despite bilingual embedding's success , the contextual information , which is of critical importance to translation quality , was ignored in previous work .	Experimental results show that the proposed method achieves significant improvements on large-scale Chinese-English translation task .	CL_D14-1015_pa_2	CL_D14-1015_pa_5	CL_D14-1015_pa_2_5
CL	D14-1015	pa	3	4	proposal	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	To employ the contextual information , we propose a simple and memory-efficient model for learning bilingual embedding , taking both the source phrase and context around the phrase into account .	Bilingual translation scores generated from our proposed bilingual embedding model are used as features in our SMT system .	CL_D14-1015_pa_3	CL_D14-1015_pa_4	CL_D14-1015_pa_3_4
CL	D14-1015	pa	4	3	proposal_implementation	proposal	elaboration	elaboration	secondary	secondary	forw	elaboration	Bilingual translation scores generated from our proposed bilingual embedding model are used as features in our SMT system .	To employ the contextual information , we propose a simple and memory-efficient model for learning bilingual embedding , taking both the source phrase and context around the phrase into account .	CL_D14-1015_pa_4	CL_D14-1015_pa_3	CL_D14-1015_pa_3_4
CL	D14-1015	pa	3	5	proposal	result	elaboration	support	secondary	secondary	back	support	To employ the contextual information , we propose a simple and memory-efficient model for learning bilingual embedding , taking both the source phrase and context around the phrase into account .	Experimental results show that the proposed method achieves significant improvements on large-scale Chinese-English translation task .	CL_D14-1015_pa_3	CL_D14-1015_pa_5	CL_D14-1015_pa_3_5
CL	D14-1015	pa	5	3	result	proposal	support	elaboration	secondary	secondary	forw	support	Experimental results show that the proposed method achieves significant improvements on large-scale Chinese-English translation task .	To employ the contextual information , we propose a simple and memory-efficient model for learning bilingual embedding , taking both the source phrase and context around the phrase into account .	CL_D14-1015_pa_5	CL_D14-1015_pa_3	CL_D14-1015_pa_3_5
CL	D14-1015	pa	4	5	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	Bilingual translation scores generated from our proposed bilingual embedding model are used as features in our SMT system .	Experimental results show that the proposed method achieves significant improvements on large-scale Chinese-English translation task .	CL_D14-1015_pa_4	CL_D14-1015_pa_5	CL_D14-1015_pa_4_5
CL	D14-1016	pa	1	2	proposal	motivation_background	none	info-required	secondary	secondary	none	none	We present a novel approach to improve word alignment for statistical machine translation ( SMT ) .	Conventional word alignment methods allow discontinuous alignment , meaning that a source ( or target ) word links to several target ( or source ) words whose positions are discontinuous .	CL_D14-1016_pa_1	CL_D14-1016_pa_2	CL_D14-1016_pa_1_2
CL	D14-1016	pa	1	3	proposal	motivation_problem	none	support	secondary	secondary	none	none	We present a novel approach to improve word alignment for statistical machine translation ( SMT ) .	However , we cannot extract phrase pairs from this kind of alignments as they break the alignment consistency constraint .	CL_D14-1016_pa_1	CL_D14-1016_pa_3	CL_D14-1016_pa_1_3
CL	D14-1016	pa	1	4	proposal	proposal	none	elaboration	secondary	main	back	elaboration	We present a novel approach to improve word alignment for statistical machine translation ( SMT ) .	In this paper , we use a weighted vote method to transform discontinuous word alignment to continuous alignment , which enables SMT systems extract more phrase pairs .	CL_D14-1016_pa_1	CL_D14-1016_pa_4	CL_D14-1016_pa_1_4
CL	D14-1016	pa	4	1	proposal	proposal	elaboration	none	main	secondary	forw	elaboration	In this paper , we use a weighted vote method to transform discontinuous word alignment to continuous alignment , which enables SMT systems extract more phrase pairs .	We present a novel approach to improve word alignment for statistical machine translation ( SMT ) .	CL_D14-1016_pa_4	CL_D14-1016_pa_1	CL_D14-1016_pa_1_4
CL	D14-1016	pa	1	5	proposal	means	none	by-means	secondary	secondary	none	none	We present a novel approach to improve word alignment for statistical machine translation ( SMT ) .	We carry out experiments on large scale Chinese-to-English and German-to-English translation tasks .	CL_D14-1016_pa_1	CL_D14-1016_pa_5	CL_D14-1016_pa_1_5
CL	D14-1016	pa	1	6	proposal	result	none	support	secondary	secondary	none	none	We present a novel approach to improve word alignment for statistical machine translation ( SMT ) .	Experimental results show statistically significant improvements of BLEU score in both cases over the baseline systems .	CL_D14-1016_pa_1	CL_D14-1016_pa_6	CL_D14-1016_pa_1_6
CL	D14-1016	pa	1	7	proposal	observation	none	support	secondary	secondary	none	none	We present a novel approach to improve word alignment for statistical machine translation ( SMT ) .	Our method produces a gain of +1.68 BLEU on NIST OpenMT04 for the phrase-based system , and a gain of +1.28 BLEU on NIST OpenMT06 for the hierarchical phrase-based system .	CL_D14-1016_pa_1	CL_D14-1016_pa_7	CL_D14-1016_pa_1_7
CL	D14-1016	pa	2	3	motivation_background	motivation_problem	info-required	support	secondary	secondary	forw	info-required	Conventional word alignment methods allow discontinuous alignment , meaning that a source ( or target ) word links to several target ( or source ) words whose positions are discontinuous .	However , we cannot extract phrase pairs from this kind of alignments as they break the alignment consistency constraint .	CL_D14-1016_pa_2	CL_D14-1016_pa_3	CL_D14-1016_pa_2_3
CL	D14-1016	pa	3	2	motivation_problem	motivation_background	support	info-required	secondary	secondary	back	info-required	However , we cannot extract phrase pairs from this kind of alignments as they break the alignment consistency constraint .	Conventional word alignment methods allow discontinuous alignment , meaning that a source ( or target ) word links to several target ( or source ) words whose positions are discontinuous .	CL_D14-1016_pa_3	CL_D14-1016_pa_2	CL_D14-1016_pa_2_3
CL	D14-1016	pa	2	4	motivation_background	proposal	info-required	elaboration	secondary	main	none	none	Conventional word alignment methods allow discontinuous alignment , meaning that a source ( or target ) word links to several target ( or source ) words whose positions are discontinuous .	In this paper , we use a weighted vote method to transform discontinuous word alignment to continuous alignment , which enables SMT systems extract more phrase pairs .	CL_D14-1016_pa_2	CL_D14-1016_pa_4	CL_D14-1016_pa_2_4
CL	D14-1016	pa	2	5	motivation_background	means	info-required	by-means	secondary	secondary	none	none	Conventional word alignment methods allow discontinuous alignment , meaning that a source ( or target ) word links to several target ( or source ) words whose positions are discontinuous .	We carry out experiments on large scale Chinese-to-English and German-to-English translation tasks .	CL_D14-1016_pa_2	CL_D14-1016_pa_5	CL_D14-1016_pa_2_5
CL	D14-1016	pa	2	6	motivation_background	result	info-required	support	secondary	secondary	none	none	Conventional word alignment methods allow discontinuous alignment , meaning that a source ( or target ) word links to several target ( or source ) words whose positions are discontinuous .	Experimental results show statistically significant improvements of BLEU score in both cases over the baseline systems .	CL_D14-1016_pa_2	CL_D14-1016_pa_6	CL_D14-1016_pa_2_6
CL	D14-1016	pa	2	7	motivation_background	observation	info-required	support	secondary	secondary	none	none	Conventional word alignment methods allow discontinuous alignment , meaning that a source ( or target ) word links to several target ( or source ) words whose positions are discontinuous .	Our method produces a gain of +1.68 BLEU on NIST OpenMT04 for the phrase-based system , and a gain of +1.28 BLEU on NIST OpenMT06 for the hierarchical phrase-based system .	CL_D14-1016_pa_2	CL_D14-1016_pa_7	CL_D14-1016_pa_2_7
CL	D14-1016	pa	3	4	motivation_problem	proposal	support	elaboration	secondary	main	forw	support	However , we cannot extract phrase pairs from this kind of alignments as they break the alignment consistency constraint .	In this paper , we use a weighted vote method to transform discontinuous word alignment to continuous alignment , which enables SMT systems extract more phrase pairs .	CL_D14-1016_pa_3	CL_D14-1016_pa_4	CL_D14-1016_pa_3_4
CL	D14-1016	pa	4	3	proposal	motivation_problem	elaboration	support	main	secondary	back	support	In this paper , we use a weighted vote method to transform discontinuous word alignment to continuous alignment , which enables SMT systems extract more phrase pairs .	However , we cannot extract phrase pairs from this kind of alignments as they break the alignment consistency constraint .	CL_D14-1016_pa_4	CL_D14-1016_pa_3	CL_D14-1016_pa_3_4
CL	D14-1016	pa	3	5	motivation_problem	means	support	by-means	secondary	secondary	none	none	However , we cannot extract phrase pairs from this kind of alignments as they break the alignment consistency constraint .	We carry out experiments on large scale Chinese-to-English and German-to-English translation tasks .	CL_D14-1016_pa_3	CL_D14-1016_pa_5	CL_D14-1016_pa_3_5
CL	D14-1016	pa	3	6	motivation_problem	result	support	support	secondary	secondary	none	none	However , we cannot extract phrase pairs from this kind of alignments as they break the alignment consistency constraint .	Experimental results show statistically significant improvements of BLEU score in both cases over the baseline systems .	CL_D14-1016_pa_3	CL_D14-1016_pa_6	CL_D14-1016_pa_3_6
CL	D14-1016	pa	3	7	motivation_problem	observation	support	support	secondary	secondary	none	none	However , we cannot extract phrase pairs from this kind of alignments as they break the alignment consistency constraint .	Our method produces a gain of +1.68 BLEU on NIST OpenMT04 for the phrase-based system , and a gain of +1.28 BLEU on NIST OpenMT06 for the hierarchical phrase-based system .	CL_D14-1016_pa_3	CL_D14-1016_pa_7	CL_D14-1016_pa_3_7
CL	D14-1016	pa	4	5	proposal	means	elaboration	by-means	main	secondary	none	none	In this paper , we use a weighted vote method to transform discontinuous word alignment to continuous alignment , which enables SMT systems extract more phrase pairs .	We carry out experiments on large scale Chinese-to-English and German-to-English translation tasks .	CL_D14-1016_pa_4	CL_D14-1016_pa_5	CL_D14-1016_pa_4_5
CL	D14-1016	pa	4	6	proposal	result	elaboration	support	main	secondary	back	support	In this paper , we use a weighted vote method to transform discontinuous word alignment to continuous alignment , which enables SMT systems extract more phrase pairs .	Experimental results show statistically significant improvements of BLEU score in both cases over the baseline systems .	CL_D14-1016_pa_4	CL_D14-1016_pa_6	CL_D14-1016_pa_4_6
CL	D14-1016	pa	6	4	result	proposal	support	elaboration	secondary	main	forw	support	Experimental results show statistically significant improvements of BLEU score in both cases over the baseline systems .	In this paper , we use a weighted vote method to transform discontinuous word alignment to continuous alignment , which enables SMT systems extract more phrase pairs .	CL_D14-1016_pa_6	CL_D14-1016_pa_4	CL_D14-1016_pa_4_6
CL	D14-1016	pa	4	7	proposal	observation	elaboration	support	main	secondary	none	none	In this paper , we use a weighted vote method to transform discontinuous word alignment to continuous alignment , which enables SMT systems extract more phrase pairs .	Our method produces a gain of +1.68 BLEU on NIST OpenMT04 for the phrase-based system , and a gain of +1.28 BLEU on NIST OpenMT06 for the hierarchical phrase-based system .	CL_D14-1016_pa_4	CL_D14-1016_pa_7	CL_D14-1016_pa_4_7
CL	D14-1016	pa	5	6	means	result	by-means	support	secondary	secondary	forw	by-means	We carry out experiments on large scale Chinese-to-English and German-to-English translation tasks .	Experimental results show statistically significant improvements of BLEU score in both cases over the baseline systems .	CL_D14-1016_pa_5	CL_D14-1016_pa_6	CL_D14-1016_pa_5_6
CL	D14-1016	pa	6	5	result	means	support	by-means	secondary	secondary	back	by-means	Experimental results show statistically significant improvements of BLEU score in both cases over the baseline systems .	We carry out experiments on large scale Chinese-to-English and German-to-English translation tasks .	CL_D14-1016_pa_6	CL_D14-1016_pa_5	CL_D14-1016_pa_5_6
CL	D14-1016	pa	5	7	means	observation	by-means	support	secondary	secondary	none	none	We carry out experiments on large scale Chinese-to-English and German-to-English translation tasks .	Our method produces a gain of +1.68 BLEU on NIST OpenMT04 for the phrase-based system , and a gain of +1.28 BLEU on NIST OpenMT06 for the hierarchical phrase-based system .	CL_D14-1016_pa_5	CL_D14-1016_pa_7	CL_D14-1016_pa_5_7
CL	D14-1016	pa	6	7	result	observation	support	support	secondary	secondary	back	support	Experimental results show statistically significant improvements of BLEU score in both cases over the baseline systems .	Our method produces a gain of +1.68 BLEU on NIST OpenMT04 for the phrase-based system , and a gain of +1.28 BLEU on NIST OpenMT06 for the hierarchical phrase-based system .	CL_D14-1016_pa_6	CL_D14-1016_pa_7	CL_D14-1016_pa_6_7
CL	D14-1016	pa	7	6	observation	result	support	support	secondary	secondary	forw	support	Our method produces a gain of +1.68 BLEU on NIST OpenMT04 for the phrase-based system , and a gain of +1.28 BLEU on NIST OpenMT06 for the hierarchical phrase-based system .	Experimental results show statistically significant improvements of BLEU score in both cases over the baseline systems .	CL_D14-1016_pa_7	CL_D14-1016_pa_6	CL_D14-1016_pa_6_7
CL	D14-1017	pa	1	2	motivation_problem	motivation_problem	info-required	support	secondary	secondary	forw	info-required	Generative word alignment models , such as IBM Models , are restricted to one-to-many alignment , and cannot explicitly represent many-to-many relationships in a bilingual text .	The problem is partially solved either by introducing heuristics or by agreement constraints such that two directional word alignments agree with each other .	CL_D14-1017_pa_1	CL_D14-1017_pa_2	CL_D14-1017_pa_1_2
CL	D14-1017	pa	2	1	motivation_problem	motivation_problem	support	info-required	secondary	secondary	back	info-required	The problem is partially solved either by introducing heuristics or by agreement constraints such that two directional word alignments agree with each other .	Generative word alignment models , such as IBM Models , are restricted to one-to-many alignment , and cannot explicitly represent many-to-many relationships in a bilingual text .	CL_D14-1017_pa_2	CL_D14-1017_pa_1	CL_D14-1017_pa_1_2
CL	D14-1017	pa	1	3	motivation_problem	proposal	info-required	none	secondary	main	none	none	Generative word alignment models , such as IBM Models , are restricted to one-to-many alignment , and cannot explicitly represent many-to-many relationships in a bilingual text .	In this paper , we focus on the posterior regularization frame-work ( Ganchev et al. , 2010 ) that can force two directional word alignment models to agree with each other during training , and propose new constraints that can take into account the difference between function words and content words .	CL_D14-1017_pa_1	CL_D14-1017_pa_3	CL_D14-1017_pa_1_3
CL	D14-1017	pa	1	4	motivation_problem	result_means	info-required	support	secondary	secondary	none	none	Generative word alignment models , such as IBM Models , are restricted to one-to-many alignment , and cannot explicitly represent many-to-many relationships in a bilingual text .	Experimental results on French-to-English and Japanese-to-English alignment tasks show statistically significant gains over the previous posterior regularization baseline .	CL_D14-1017_pa_1	CL_D14-1017_pa_4	CL_D14-1017_pa_1_4
CL	D14-1017	pa	1	5	motivation_problem	result_means	info-required	elaboration	secondary	secondary	none	none	Generative word alignment models , such as IBM Models , are restricted to one-to-many alignment , and cannot explicitly represent many-to-many relationships in a bilingual text .	We also observed gains in Japanese-to-English translation tasks , which prove the effectiveness of our methods under grammatically different language pairs .	CL_D14-1017_pa_1	CL_D14-1017_pa_5	CL_D14-1017_pa_1_5
CL	D14-1017	pa	2	3	motivation_problem	proposal	support	none	secondary	main	forw	support	The problem is partially solved either by introducing heuristics or by agreement constraints such that two directional word alignments agree with each other .	In this paper , we focus on the posterior regularization frame-work ( Ganchev et al. , 2010 ) that can force two directional word alignment models to agree with each other during training , and propose new constraints that can take into account the difference between function words and content words .	CL_D14-1017_pa_2	CL_D14-1017_pa_3	CL_D14-1017_pa_2_3
CL	D14-1017	pa	3	2	proposal	motivation_problem	none	support	main	secondary	back	support	In this paper , we focus on the posterior regularization frame-work ( Ganchev et al. , 2010 ) that can force two directional word alignment models to agree with each other during training , and propose new constraints that can take into account the difference between function words and content words .	The problem is partially solved either by introducing heuristics or by agreement constraints such that two directional word alignments agree with each other .	CL_D14-1017_pa_3	CL_D14-1017_pa_2	CL_D14-1017_pa_2_3
CL	D14-1017	pa	2	4	motivation_problem	result_means	support	support	secondary	secondary	none	none	The problem is partially solved either by introducing heuristics or by agreement constraints such that two directional word alignments agree with each other .	Experimental results on French-to-English and Japanese-to-English alignment tasks show statistically significant gains over the previous posterior regularization baseline .	CL_D14-1017_pa_2	CL_D14-1017_pa_4	CL_D14-1017_pa_2_4
CL	D14-1017	pa	2	5	motivation_problem	result_means	support	elaboration	secondary	secondary	none	none	The problem is partially solved either by introducing heuristics or by agreement constraints such that two directional word alignments agree with each other .	We also observed gains in Japanese-to-English translation tasks , which prove the effectiveness of our methods under grammatically different language pairs .	CL_D14-1017_pa_2	CL_D14-1017_pa_5	CL_D14-1017_pa_2_5
CL	D14-1017	pa	3	4	proposal	result_means	none	support	main	secondary	back	support	In this paper , we focus on the posterior regularization frame-work ( Ganchev et al. , 2010 ) that can force two directional word alignment models to agree with each other during training , and propose new constraints that can take into account the difference between function words and content words .	Experimental results on French-to-English and Japanese-to-English alignment tasks show statistically significant gains over the previous posterior regularization baseline .	CL_D14-1017_pa_3	CL_D14-1017_pa_4	CL_D14-1017_pa_3_4
CL	D14-1017	pa	4	3	result_means	proposal	support	none	secondary	main	forw	support	Experimental results on French-to-English and Japanese-to-English alignment tasks show statistically significant gains over the previous posterior regularization baseline .	In this paper , we focus on the posterior regularization frame-work ( Ganchev et al. , 2010 ) that can force two directional word alignment models to agree with each other during training , and propose new constraints that can take into account the difference between function words and content words .	CL_D14-1017_pa_4	CL_D14-1017_pa_3	CL_D14-1017_pa_3_4
CL	D14-1017	pa	3	5	proposal	result_means	none	elaboration	main	secondary	none	none	In this paper , we focus on the posterior regularization frame-work ( Ganchev et al. , 2010 ) that can force two directional word alignment models to agree with each other during training , and propose new constraints that can take into account the difference between function words and content words .	We also observed gains in Japanese-to-English translation tasks , which prove the effectiveness of our methods under grammatically different language pairs .	CL_D14-1017_pa_3	CL_D14-1017_pa_5	CL_D14-1017_pa_3_5
CL	D14-1017	pa	4	5	result_means	result_means	support	elaboration	secondary	secondary	back	elaboration	Experimental results on French-to-English and Japanese-to-English alignment tasks show statistically significant gains over the previous posterior regularization baseline .	We also observed gains in Japanese-to-English translation tasks , which prove the effectiveness of our methods under grammatically different language pairs .	CL_D14-1017_pa_4	CL_D14-1017_pa_5	CL_D14-1017_pa_4_5
CL	D14-1017	pa	5	4	result_means	result_means	elaboration	support	secondary	secondary	forw	elaboration	We also observed gains in Japanese-to-English translation tasks , which prove the effectiveness of our methods under grammatically different language pairs .	Experimental results on French-to-English and Japanese-to-English alignment tasks show statistically significant gains over the previous posterior regularization baseline .	CL_D14-1017_pa_5	CL_D14-1017_pa_4	CL_D14-1017_pa_4_5
CL	D14-1018	pa	1	2	motivation_background	motivation_background	info-required	info-required	secondary	secondary	forw	info-required	Distinct properties of translated text have been the subject of research in linguistics for many year ( Baker , 1993 ) .	In recent years computational methods have been developed to empirically verify the linguistic theories about translated text ( Baroni and Bernardini , 2006 ) .	CL_D14-1018_pa_1	CL_D14-1018_pa_2	CL_D14-1018_pa_1_2
CL	D14-1018	pa	2	1	motivation_background	motivation_background	info-required	info-required	secondary	secondary	back	info-required	In recent years computational methods have been developed to empirically verify the linguistic theories about translated text ( Baroni and Bernardini , 2006 ) .	Distinct properties of translated text have been the subject of research in linguistics for many year ( Baker , 1993 ) .	CL_D14-1018_pa_2	CL_D14-1018_pa_1	CL_D14-1018_pa_1_2
CL	D14-1018	pa	1	3	motivation_background	motivation_problem	info-required	support	secondary	secondary	none	none	Distinct properties of translated text have been the subject of research in linguistics for many year ( Baker , 1993 ) .	While many characteristics of translated text are more apparent in comparison to the original text , most of the prior research has focused on monolingual features of translated and original text .	CL_D14-1018_pa_1	CL_D14-1018_pa_3	CL_D14-1018_pa_1_3
CL	D14-1018	pa	1	4	motivation_background	proposal	info-required	none	secondary	main	none	none	Distinct properties of translated text have been the subject of research in linguistics for many year ( Baker , 1993 ) .	The contribution of this work is introducing bilingual features that are capable of explaining differences in translation direction using localized linguistic phenomena at the phrase or sentence level , rather than using monolingual statistics at the document level .	CL_D14-1018_pa_1	CL_D14-1018_pa_4	CL_D14-1018_pa_1_4
CL	D14-1018	pa	1	5	motivation_background	conclusion	info-required	support	secondary	secondary	none	none	Distinct properties of translated text have been the subject of research in linguistics for many year ( Baker , 1993 ) .	We show that these bilingual features out-perform the monolingual features used in prior work ( Kurokawa et al. , 2009 ) for the task of classifying translation direction .	CL_D14-1018_pa_1	CL_D14-1018_pa_5	CL_D14-1018_pa_1_5
CL	D14-1018	pa	2	3	motivation_background	motivation_problem	info-required	support	secondary	secondary	forw	info-required	In recent years computational methods have been developed to empirically verify the linguistic theories about translated text ( Baroni and Bernardini , 2006 ) .	While many characteristics of translated text are more apparent in comparison to the original text , most of the prior research has focused on monolingual features of translated and original text .	CL_D14-1018_pa_2	CL_D14-1018_pa_3	CL_D14-1018_pa_2_3
CL	D14-1018	pa	3	2	motivation_problem	motivation_background	support	info-required	secondary	secondary	back	info-required	While many characteristics of translated text are more apparent in comparison to the original text , most of the prior research has focused on monolingual features of translated and original text .	In recent years computational methods have been developed to empirically verify the linguistic theories about translated text ( Baroni and Bernardini , 2006 ) .	CL_D14-1018_pa_3	CL_D14-1018_pa_2	CL_D14-1018_pa_2_3
CL	D14-1018	pa	2	4	motivation_background	proposal	info-required	none	secondary	main	none	none	In recent years computational methods have been developed to empirically verify the linguistic theories about translated text ( Baroni and Bernardini , 2006 ) .	The contribution of this work is introducing bilingual features that are capable of explaining differences in translation direction using localized linguistic phenomena at the phrase or sentence level , rather than using monolingual statistics at the document level .	CL_D14-1018_pa_2	CL_D14-1018_pa_4	CL_D14-1018_pa_2_4
CL	D14-1018	pa	2	5	motivation_background	conclusion	info-required	support	secondary	secondary	none	none	In recent years computational methods have been developed to empirically verify the linguistic theories about translated text ( Baroni and Bernardini , 2006 ) .	We show that these bilingual features out-perform the monolingual features used in prior work ( Kurokawa et al. , 2009 ) for the task of classifying translation direction .	CL_D14-1018_pa_2	CL_D14-1018_pa_5	CL_D14-1018_pa_2_5
CL	D14-1018	pa	3	4	motivation_problem	proposal	support	none	secondary	main	forw	support	While many characteristics of translated text are more apparent in comparison to the original text , most of the prior research has focused on monolingual features of translated and original text .	The contribution of this work is introducing bilingual features that are capable of explaining differences in translation direction using localized linguistic phenomena at the phrase or sentence level , rather than using monolingual statistics at the document level .	CL_D14-1018_pa_3	CL_D14-1018_pa_4	CL_D14-1018_pa_3_4
CL	D14-1018	pa	4	3	proposal	motivation_problem	none	support	main	secondary	back	support	The contribution of this work is introducing bilingual features that are capable of explaining differences in translation direction using localized linguistic phenomena at the phrase or sentence level , rather than using monolingual statistics at the document level .	While many characteristics of translated text are more apparent in comparison to the original text , most of the prior research has focused on monolingual features of translated and original text .	CL_D14-1018_pa_4	CL_D14-1018_pa_3	CL_D14-1018_pa_3_4
CL	D14-1018	pa	3	5	motivation_problem	conclusion	support	support	secondary	secondary	none	none	While many characteristics of translated text are more apparent in comparison to the original text , most of the prior research has focused on monolingual features of translated and original text .	We show that these bilingual features out-perform the monolingual features used in prior work ( Kurokawa et al. , 2009 ) for the task of classifying translation direction .	CL_D14-1018_pa_3	CL_D14-1018_pa_5	CL_D14-1018_pa_3_5
CL	D14-1018	pa	4	5	proposal	conclusion	none	support	main	secondary	back	support	The contribution of this work is introducing bilingual features that are capable of explaining differences in translation direction using localized linguistic phenomena at the phrase or sentence level , rather than using monolingual statistics at the document level .	We show that these bilingual features out-perform the monolingual features used in prior work ( Kurokawa et al. , 2009 ) for the task of classifying translation direction .	CL_D14-1018_pa_4	CL_D14-1018_pa_5	CL_D14-1018_pa_4_5
CL	D14-1018	pa	5	4	conclusion	proposal	support	none	secondary	main	forw	support	We show that these bilingual features out-perform the monolingual features used in prior work ( Kurokawa et al. , 2009 ) for the task of classifying translation direction .	The contribution of this work is introducing bilingual features that are capable of explaining differences in translation direction using localized linguistic phenomena at the phrase or sentence level , rather than using monolingual statistics at the document level .	CL_D14-1018_pa_5	CL_D14-1018_pa_4	CL_D14-1018_pa_4_5
CL	D14-1019	pa	1	2	motivation_background	motivation_problem	info-required	support	secondary	secondary	forw	info-required	Recently , syntactic information has helped significantly to improve statistical machine translation .	However , the use of syntactic information may have a negative impact on the speed of translation because of the large number of rules , especially when syntax labels are projected from a parser in syntax-augmented machine translation .	CL_D14-1019_pa_1	CL_D14-1019_pa_2	CL_D14-1019_pa_1_2
CL	D14-1019	pa	2	1	motivation_problem	motivation_background	support	info-required	secondary	secondary	back	info-required	However , the use of syntactic information may have a negative impact on the speed of translation because of the large number of rules , especially when syntax labels are projected from a parser in syntax-augmented machine translation .	Recently , syntactic information has helped significantly to improve statistical machine translation .	CL_D14-1019_pa_2	CL_D14-1019_pa_1	CL_D14-1019_pa_1_2
CL	D14-1019	pa	1	3	motivation_background	proposal	info-required	none	secondary	main	none	none	Recently , syntactic information has helped significantly to improve statistical machine translation .	In this paper , we propose a syntax-label clustering method that uses an exchange algorithm in which syntax labels are clustered together to reduce the number of rules .	CL_D14-1019_pa_1	CL_D14-1019_pa_3	CL_D14-1019_pa_1_3
CL	D14-1019	pa	1	4	motivation_background	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	Recently , syntactic information has helped significantly to improve statistical machine translation .	The proposed method achieves clustering by directly maximizing the likelihood of synchronous rules , whereas previous work considered only the similarity of probabilistic distributions of labels .	CL_D14-1019_pa_1	CL_D14-1019_pa_4	CL_D14-1019_pa_1_4
CL	D14-1019	pa	1	5	motivation_background	result_means	info-required	support	secondary	secondary	none	none	Recently , syntactic information has helped significantly to improve statistical machine translation .	We tested the proposed method on Japanese-English and Chinese-English translation tasks and found order-of-magnitude higher clustering speeds for reducing labels and gains in translation quality compared with previous clustering method .	CL_D14-1019_pa_1	CL_D14-1019_pa_5	CL_D14-1019_pa_1_5
CL	D14-1019	pa	2	3	motivation_problem	proposal	support	none	secondary	main	forw	support	However , the use of syntactic information may have a negative impact on the speed of translation because of the large number of rules , especially when syntax labels are projected from a parser in syntax-augmented machine translation .	In this paper , we propose a syntax-label clustering method that uses an exchange algorithm in which syntax labels are clustered together to reduce the number of rules .	CL_D14-1019_pa_2	CL_D14-1019_pa_3	CL_D14-1019_pa_2_3
CL	D14-1019	pa	3	2	proposal	motivation_problem	none	support	main	secondary	back	support	In this paper , we propose a syntax-label clustering method that uses an exchange algorithm in which syntax labels are clustered together to reduce the number of rules .	However , the use of syntactic information may have a negative impact on the speed of translation because of the large number of rules , especially when syntax labels are projected from a parser in syntax-augmented machine translation .	CL_D14-1019_pa_3	CL_D14-1019_pa_2	CL_D14-1019_pa_2_3
CL	D14-1019	pa	2	4	motivation_problem	proposal_implementation	support	elaboration	secondary	secondary	none	none	However , the use of syntactic information may have a negative impact on the speed of translation because of the large number of rules , especially when syntax labels are projected from a parser in syntax-augmented machine translation .	The proposed method achieves clustering by directly maximizing the likelihood of synchronous rules , whereas previous work considered only the similarity of probabilistic distributions of labels .	CL_D14-1019_pa_2	CL_D14-1019_pa_4	CL_D14-1019_pa_2_4
CL	D14-1019	pa	2	5	motivation_problem	result_means	support	support	secondary	secondary	none	none	However , the use of syntactic information may have a negative impact on the speed of translation because of the large number of rules , especially when syntax labels are projected from a parser in syntax-augmented machine translation .	We tested the proposed method on Japanese-English and Chinese-English translation tasks and found order-of-magnitude higher clustering speeds for reducing labels and gains in translation quality compared with previous clustering method .	CL_D14-1019_pa_2	CL_D14-1019_pa_5	CL_D14-1019_pa_2_5
CL	D14-1019	pa	3	4	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	In this paper , we propose a syntax-label clustering method that uses an exchange algorithm in which syntax labels are clustered together to reduce the number of rules .	The proposed method achieves clustering by directly maximizing the likelihood of synchronous rules , whereas previous work considered only the similarity of probabilistic distributions of labels .	CL_D14-1019_pa_3	CL_D14-1019_pa_4	CL_D14-1019_pa_3_4
CL	D14-1019	pa	4	3	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	The proposed method achieves clustering by directly maximizing the likelihood of synchronous rules , whereas previous work considered only the similarity of probabilistic distributions of labels .	In this paper , we propose a syntax-label clustering method that uses an exchange algorithm in which syntax labels are clustered together to reduce the number of rules .	CL_D14-1019_pa_4	CL_D14-1019_pa_3	CL_D14-1019_pa_3_4
CL	D14-1019	pa	3	5	proposal	result_means	none	support	main	secondary	back	support	In this paper , we propose a syntax-label clustering method that uses an exchange algorithm in which syntax labels are clustered together to reduce the number of rules .	We tested the proposed method on Japanese-English and Chinese-English translation tasks and found order-of-magnitude higher clustering speeds for reducing labels and gains in translation quality compared with previous clustering method .	CL_D14-1019_pa_3	CL_D14-1019_pa_5	CL_D14-1019_pa_3_5
CL	D14-1019	pa	5	3	result_means	proposal	support	none	secondary	main	forw	support	We tested the proposed method on Japanese-English and Chinese-English translation tasks and found order-of-magnitude higher clustering speeds for reducing labels and gains in translation quality compared with previous clustering method .	In this paper , we propose a syntax-label clustering method that uses an exchange algorithm in which syntax labels are clustered together to reduce the number of rules .	CL_D14-1019_pa_5	CL_D14-1019_pa_3	CL_D14-1019_pa_3_5
CL	D14-1019	pa	4	5	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	The proposed method achieves clustering by directly maximizing the likelihood of synchronous rules , whereas previous work considered only the similarity of probabilistic distributions of labels .	We tested the proposed method on Japanese-English and Chinese-English translation tasks and found order-of-magnitude higher clustering speeds for reducing labels and gains in translation quality compared with previous clustering method .	CL_D14-1019_pa_4	CL_D14-1019_pa_5	CL_D14-1019_pa_4_5
CL	D14-1020	pa	1	2	motivation_background	motivation_background	info-required	info-required	secondary	secondary	forw	info-required	Automatic metrics are widely used in machine translation as a substitute for human assessment .	With the introduction of any new metric comes the question of just how well that metric mimics human assessment of translation quality .	CL_D14-1020_pa_1	CL_D14-1020_pa_2	CL_D14-1020_pa_1_2
CL	D14-1020	pa	2	1	motivation_background	motivation_background	info-required	info-required	secondary	secondary	back	info-required	With the introduction of any new metric comes the question of just how well that metric mimics human assessment of translation quality .	Automatic metrics are widely used in machine translation as a substitute for human assessment .	CL_D14-1020_pa_2	CL_D14-1020_pa_1	CL_D14-1020_pa_1_2
CL	D14-1020	pa	1	3	motivation_background	motivation_background	info-required	elaboration	secondary	secondary	none	none	Automatic metrics are widely used in machine translation as a substitute for human assessment .	This is often measured by correlation with human judgment .	CL_D14-1020_pa_1	CL_D14-1020_pa_3	CL_D14-1020_pa_1_3
CL	D14-1020	pa	1	4	motivation_background	motivation_problem	info-required	support	secondary	secondary	none	none	Automatic metrics are widely used in machine translation as a substitute for human assessment .	Significance tests are generally not used to establish whether improvements over existing methods such as BLEU are statistically significant or have occurred simply by chance , however .	CL_D14-1020_pa_1	CL_D14-1020_pa_4	CL_D14-1020_pa_1_4
CL	D14-1020	pa	1	5	motivation_background	proposal	info-required	none	secondary	main	none	none	Automatic metrics are widely used in machine translation as a substitute for human assessment .	In this paper , we introduce a significance test for comparing correlations of two metrics , along with an open-source implementation of the test .	CL_D14-1020_pa_1	CL_D14-1020_pa_5	CL_D14-1020_pa_1_5
CL	D14-1020	pa	1	6	motivation_background	result_means	info-required	support	secondary	secondary	none	none	Automatic metrics are widely used in machine translation as a substitute for human assessment .	When applied to a range of metrics across seven language pairs , tests show that for a high proportion of metrics , there is insufficient evidence to conclude significant improvement over BLEU .	CL_D14-1020_pa_1	CL_D14-1020_pa_6	CL_D14-1020_pa_1_6
CL	D14-1020	pa	2	3	motivation_background	motivation_background	info-required	elaboration	secondary	secondary	back	elaboration	With the introduction of any new metric comes the question of just how well that metric mimics human assessment of translation quality .	This is often measured by correlation with human judgment .	CL_D14-1020_pa_2	CL_D14-1020_pa_3	CL_D14-1020_pa_2_3
CL	D14-1020	pa	3	2	motivation_background	motivation_background	elaboration	info-required	secondary	secondary	forw	elaboration	This is often measured by correlation with human judgment .	With the introduction of any new metric comes the question of just how well that metric mimics human assessment of translation quality .	CL_D14-1020_pa_3	CL_D14-1020_pa_2	CL_D14-1020_pa_2_3
CL	D14-1020	pa	2	4	motivation_background	motivation_problem	info-required	support	secondary	secondary	forw	info-required	With the introduction of any new metric comes the question of just how well that metric mimics human assessment of translation quality .	Significance tests are generally not used to establish whether improvements over existing methods such as BLEU are statistically significant or have occurred simply by chance , however .	CL_D14-1020_pa_2	CL_D14-1020_pa_4	CL_D14-1020_pa_2_4
CL	D14-1020	pa	4	2	motivation_problem	motivation_background	support	info-required	secondary	secondary	back	info-required	Significance tests are generally not used to establish whether improvements over existing methods such as BLEU are statistically significant or have occurred simply by chance , however .	With the introduction of any new metric comes the question of just how well that metric mimics human assessment of translation quality .	CL_D14-1020_pa_4	CL_D14-1020_pa_2	CL_D14-1020_pa_2_4
CL	D14-1020	pa	2	5	motivation_background	proposal	info-required	none	secondary	main	none	none	With the introduction of any new metric comes the question of just how well that metric mimics human assessment of translation quality .	In this paper , we introduce a significance test for comparing correlations of two metrics , along with an open-source implementation of the test .	CL_D14-1020_pa_2	CL_D14-1020_pa_5	CL_D14-1020_pa_2_5
CL	D14-1020	pa	2	6	motivation_background	result_means	info-required	support	secondary	secondary	none	none	With the introduction of any new metric comes the question of just how well that metric mimics human assessment of translation quality .	When applied to a range of metrics across seven language pairs , tests show that for a high proportion of metrics , there is insufficient evidence to conclude significant improvement over BLEU .	CL_D14-1020_pa_2	CL_D14-1020_pa_6	CL_D14-1020_pa_2_6
CL	D14-1020	pa	3	4	motivation_background	motivation_problem	elaboration	support	secondary	secondary	none	none	This is often measured by correlation with human judgment .	Significance tests are generally not used to establish whether improvements over existing methods such as BLEU are statistically significant or have occurred simply by chance , however .	CL_D14-1020_pa_3	CL_D14-1020_pa_4	CL_D14-1020_pa_3_4
CL	D14-1020	pa	3	5	motivation_background	proposal	elaboration	none	secondary	main	none	none	This is often measured by correlation with human judgment .	In this paper , we introduce a significance test for comparing correlations of two metrics , along with an open-source implementation of the test .	CL_D14-1020_pa_3	CL_D14-1020_pa_5	CL_D14-1020_pa_3_5
CL	D14-1020	pa	3	6	motivation_background	result_means	elaboration	support	secondary	secondary	none	none	This is often measured by correlation with human judgment .	When applied to a range of metrics across seven language pairs , tests show that for a high proportion of metrics , there is insufficient evidence to conclude significant improvement over BLEU .	CL_D14-1020_pa_3	CL_D14-1020_pa_6	CL_D14-1020_pa_3_6
CL	D14-1020	pa	4	5	motivation_problem	proposal	support	none	secondary	main	forw	support	Significance tests are generally not used to establish whether improvements over existing methods such as BLEU are statistically significant or have occurred simply by chance , however .	In this paper , we introduce a significance test for comparing correlations of two metrics , along with an open-source implementation of the test .	CL_D14-1020_pa_4	CL_D14-1020_pa_5	CL_D14-1020_pa_4_5
CL	D14-1020	pa	5	4	proposal	motivation_problem	none	support	main	secondary	back	support	In this paper , we introduce a significance test for comparing correlations of two metrics , along with an open-source implementation of the test .	Significance tests are generally not used to establish whether improvements over existing methods such as BLEU are statistically significant or have occurred simply by chance , however .	CL_D14-1020_pa_5	CL_D14-1020_pa_4	CL_D14-1020_pa_4_5
CL	D14-1020	pa	4	6	motivation_problem	result_means	support	support	secondary	secondary	none	none	Significance tests are generally not used to establish whether improvements over existing methods such as BLEU are statistically significant or have occurred simply by chance , however .	When applied to a range of metrics across seven language pairs , tests show that for a high proportion of metrics , there is insufficient evidence to conclude significant improvement over BLEU .	CL_D14-1020_pa_4	CL_D14-1020_pa_6	CL_D14-1020_pa_4_6
CL	D14-1020	pa	5	6	proposal	result_means	none	support	main	secondary	back	support	In this paper , we introduce a significance test for comparing correlations of two metrics , along with an open-source implementation of the test .	When applied to a range of metrics across seven language pairs , tests show that for a high proportion of metrics , there is insufficient evidence to conclude significant improvement over BLEU .	CL_D14-1020_pa_5	CL_D14-1020_pa_6	CL_D14-1020_pa_5_6
CL	D14-1020	pa	6	5	result_means	proposal	support	none	secondary	main	forw	support	When applied to a range of metrics across seven language pairs , tests show that for a high proportion of metrics , there is insufficient evidence to conclude significant improvement over BLEU .	In this paper , we introduce a significance test for comparing correlations of two metrics , along with an open-source implementation of the test .	CL_D14-1020_pa_6	CL_D14-1020_pa_5	CL_D14-1020_pa_5_6
CL	D14-1021	pa	1	2	proposal	proposal	none	elaboration	main	secondary	back	elaboration	We study a novel architecture for syntactic SMT .	In contrast to the dominant approach in the literature , the system does not rely on translation rules , but treat translation as an unconstrained target sentence generation task , using soft features to capture lexical and syntactic correspondences between the source and target languages .	CL_D14-1021_pa_1	CL_D14-1021_pa_2	CL_D14-1021_pa_1_2
CL	D14-1021	pa	2	1	proposal	proposal	elaboration	none	secondary	main	forw	elaboration	In contrast to the dominant approach in the literature , the system does not rely on translation rules , but treat translation as an unconstrained target sentence generation task , using soft features to capture lexical and syntactic correspondences between the source and target languages .	We study a novel architecture for syntactic SMT .	CL_D14-1021_pa_2	CL_D14-1021_pa_1	CL_D14-1021_pa_1_2
CL	D14-1021	pa	1	3	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	We study a novel architecture for syntactic SMT .	Target syntax features and bilingual translation features are trained consistently in a discriminative model .	CL_D14-1021_pa_1	CL_D14-1021_pa_3	CL_D14-1021_pa_1_3
CL	D14-1021	pa	1	4	proposal	result_means	none	support	main	secondary	back	support	We study a novel architecture for syntactic SMT .	Experiments using the IWSLT 2010 dataset show that the system achieves BLEU comparable to the state-of-the-art syntactic SMT systems .	CL_D14-1021_pa_1	CL_D14-1021_pa_4	CL_D14-1021_pa_1_4
CL	D14-1021	pa	4	1	result_means	proposal	support	none	secondary	main	forw	support	Experiments using the IWSLT 2010 dataset show that the system achieves BLEU comparable to the state-of-the-art syntactic SMT systems .	We study a novel architecture for syntactic SMT .	CL_D14-1021_pa_4	CL_D14-1021_pa_1	CL_D14-1021_pa_1_4
CL	D14-1021	pa	2	3	proposal	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	In contrast to the dominant approach in the literature , the system does not rely on translation rules , but treat translation as an unconstrained target sentence generation task , using soft features to capture lexical and syntactic correspondences between the source and target languages .	Target syntax features and bilingual translation features are trained consistently in a discriminative model .	CL_D14-1021_pa_2	CL_D14-1021_pa_3	CL_D14-1021_pa_2_3
CL	D14-1021	pa	3	2	proposal_implementation	proposal	elaboration	elaboration	secondary	secondary	forw	elaboration	Target syntax features and bilingual translation features are trained consistently in a discriminative model .	In contrast to the dominant approach in the literature , the system does not rely on translation rules , but treat translation as an unconstrained target sentence generation task , using soft features to capture lexical and syntactic correspondences between the source and target languages .	CL_D14-1021_pa_3	CL_D14-1021_pa_2	CL_D14-1021_pa_2_3
CL	D14-1021	pa	2	4	proposal	result_means	elaboration	support	secondary	secondary	none	none	In contrast to the dominant approach in the literature , the system does not rely on translation rules , but treat translation as an unconstrained target sentence generation task , using soft features to capture lexical and syntactic correspondences between the source and target languages .	Experiments using the IWSLT 2010 dataset show that the system achieves BLEU comparable to the state-of-the-art syntactic SMT systems .	CL_D14-1021_pa_2	CL_D14-1021_pa_4	CL_D14-1021_pa_2_4
CL	D14-1021	pa	3	4	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	Target syntax features and bilingual translation features are trained consistently in a discriminative model .	Experiments using the IWSLT 2010 dataset show that the system achieves BLEU comparable to the state-of-the-art syntactic SMT systems .	CL_D14-1021_pa_3	CL_D14-1021_pa_4	CL_D14-1021_pa_3_4
CL	D14-1022	pa	1	2	proposal	proposal	none	elaboration	main	secondary	back	elaboration	We propose a simple and effective approach to learn translation spans for the hierarchical phrase-based translation model .	Our model evaluates if a source span should be covered by translation rules during decoding , which is integrated into the translation system as soft constraints .	CL_D14-1022_pa_1	CL_D14-1022_pa_2	CL_D14-1022_pa_1_2
CL	D14-1022	pa	2	1	proposal	proposal	elaboration	none	secondary	main	forw	elaboration	Our model evaluates if a source span should be covered by translation rules during decoding , which is integrated into the translation system as soft constraints .	We propose a simple and effective approach to learn translation spans for the hierarchical phrase-based translation model .	CL_D14-1022_pa_2	CL_D14-1022_pa_1	CL_D14-1022_pa_1_2
CL	D14-1022	pa	1	3	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	We propose a simple and effective approach to learn translation spans for the hierarchical phrase-based translation model .	Compared to syntactic constraints , our model is directly acquired from an aligned parallel corpus and does not require parsers .	CL_D14-1022_pa_1	CL_D14-1022_pa_3	CL_D14-1022_pa_1_3
CL	D14-1022	pa	1	4	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	We propose a simple and effective approach to learn translation spans for the hierarchical phrase-based translation model .	Rich source side contextual features and advanced machine learning methods were utilized for this learning task .	CL_D14-1022_pa_1	CL_D14-1022_pa_4	CL_D14-1022_pa_1_4
CL	D14-1022	pa	1	5	proposal	result_means	none	support	main	secondary	back	support	We propose a simple and effective approach to learn translation spans for the hierarchical phrase-based translation model .	The proposed approach was evaluated on NTCIR-9 Chinese-English and Japanese-English translation tasks and showed significant improvement over the baseline system .	CL_D14-1022_pa_1	CL_D14-1022_pa_5	CL_D14-1022_pa_1_5
CL	D14-1022	pa	5	1	result_means	proposal	support	none	secondary	main	forw	support	The proposed approach was evaluated on NTCIR-9 Chinese-English and Japanese-English translation tasks and showed significant improvement over the baseline system .	We propose a simple and effective approach to learn translation spans for the hierarchical phrase-based translation model .	CL_D14-1022_pa_5	CL_D14-1022_pa_1	CL_D14-1022_pa_1_5
CL	D14-1022	pa	2	3	proposal	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	Our model evaluates if a source span should be covered by translation rules during decoding , which is integrated into the translation system as soft constraints .	Compared to syntactic constraints , our model is directly acquired from an aligned parallel corpus and does not require parsers .	CL_D14-1022_pa_2	CL_D14-1022_pa_3	CL_D14-1022_pa_2_3
CL	D14-1022	pa	3	2	proposal_implementation	proposal	elaboration	elaboration	secondary	secondary	forw	elaboration	Compared to syntactic constraints , our model is directly acquired from an aligned parallel corpus and does not require parsers .	Our model evaluates if a source span should be covered by translation rules during decoding , which is integrated into the translation system as soft constraints .	CL_D14-1022_pa_3	CL_D14-1022_pa_2	CL_D14-1022_pa_2_3
CL	D14-1022	pa	2	4	proposal	proposal_implementation	elaboration	elaboration	secondary	secondary	none	none	Our model evaluates if a source span should be covered by translation rules during decoding , which is integrated into the translation system as soft constraints .	Rich source side contextual features and advanced machine learning methods were utilized for this learning task .	CL_D14-1022_pa_2	CL_D14-1022_pa_4	CL_D14-1022_pa_2_4
CL	D14-1022	pa	2	5	proposal	result_means	elaboration	support	secondary	secondary	none	none	Our model evaluates if a source span should be covered by translation rules during decoding , which is integrated into the translation system as soft constraints .	The proposed approach was evaluated on NTCIR-9 Chinese-English and Japanese-English translation tasks and showed significant improvement over the baseline system .	CL_D14-1022_pa_2	CL_D14-1022_pa_5	CL_D14-1022_pa_2_5
CL	D14-1022	pa	3	4	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	Compared to syntactic constraints , our model is directly acquired from an aligned parallel corpus and does not require parsers .	Rich source side contextual features and advanced machine learning methods were utilized for this learning task .	CL_D14-1022_pa_3	CL_D14-1022_pa_4	CL_D14-1022_pa_3_4
CL	D14-1022	pa	4	3	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	forw	elaboration	Rich source side contextual features and advanced machine learning methods were utilized for this learning task .	Compared to syntactic constraints , our model is directly acquired from an aligned parallel corpus and does not require parsers .	CL_D14-1022_pa_4	CL_D14-1022_pa_3	CL_D14-1022_pa_3_4
CL	D14-1022	pa	3	5	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	Compared to syntactic constraints , our model is directly acquired from an aligned parallel corpus and does not require parsers .	The proposed approach was evaluated on NTCIR-9 Chinese-English and Japanese-English translation tasks and showed significant improvement over the baseline system .	CL_D14-1022_pa_3	CL_D14-1022_pa_5	CL_D14-1022_pa_3_5
CL	D14-1022	pa	4	5	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	Rich source side contextual features and advanced machine learning methods were utilized for this learning task .	The proposed approach was evaluated on NTCIR-9 Chinese-English and Japanese-English translation tasks and showed significant improvement over the baseline system .	CL_D14-1022_pa_4	CL_D14-1022_pa_5	CL_D14-1022_pa_4_5
CL	D14-1023	pa	1	2	motivation_background	motivation_problem	info-required	support	secondary	secondary	forw	info-required	Since larger n-gram Language Model ( LM ) usually performs better in Statistical Machine Translation ( SMT ) , how to construct efficient large LM is an important topic in SMT .	However , most of the existing LM growing methods need an extra monolingual corpus , where additional LM adaption technology is necessary .	CL_D14-1023_pa_1	CL_D14-1023_pa_2	CL_D14-1023_pa_1_2
CL	D14-1023	pa	2	1	motivation_problem	motivation_background	support	info-required	secondary	secondary	back	info-required	However , most of the existing LM growing methods need an extra monolingual corpus , where additional LM adaption technology is necessary .	Since larger n-gram Language Model ( LM ) usually performs better in Statistical Machine Translation ( SMT ) , how to construct efficient large LM is an important topic in SMT .	CL_D14-1023_pa_2	CL_D14-1023_pa_1	CL_D14-1023_pa_1_2
CL	D14-1023	pa	1	3	motivation_background	proposal	info-required	none	secondary	main	none	none	Since larger n-gram Language Model ( LM ) usually performs better in Statistical Machine Translation ( SMT ) , how to construct efficient large LM is an important topic in SMT .	In this paper , we propose a novel neural network based bilingual LM growing method , only using the bilingual parallel corpus in SMT .	CL_D14-1023_pa_1	CL_D14-1023_pa_3	CL_D14-1023_pa_1_3
CL	D14-1023	pa	1	4	motivation_background	result	info-required	support	secondary	secondary	none	none	Since larger n-gram Language Model ( LM ) usually performs better in Statistical Machine Translation ( SMT ) , how to construct efficient large LM is an important topic in SMT .	The results show that our method can improve both the perplexity score for LM evaluation and BLEU score for SMT , and significantly outperforms the existing LM growing methods without extra corpus .	CL_D14-1023_pa_1	CL_D14-1023_pa_4	CL_D14-1023_pa_1_4
CL	D14-1023	pa	2	3	motivation_problem	proposal	support	none	secondary	main	forw	support	However , most of the existing LM growing methods need an extra monolingual corpus , where additional LM adaption technology is necessary .	In this paper , we propose a novel neural network based bilingual LM growing method , only using the bilingual parallel corpus in SMT .	CL_D14-1023_pa_2	CL_D14-1023_pa_3	CL_D14-1023_pa_2_3
CL	D14-1023	pa	3	2	proposal	motivation_problem	none	support	main	secondary	back	support	In this paper , we propose a novel neural network based bilingual LM growing method , only using the bilingual parallel corpus in SMT .	However , most of the existing LM growing methods need an extra monolingual corpus , where additional LM adaption technology is necessary .	CL_D14-1023_pa_3	CL_D14-1023_pa_2	CL_D14-1023_pa_2_3
CL	D14-1023	pa	2	4	motivation_problem	result	support	support	secondary	secondary	none	none	However , most of the existing LM growing methods need an extra monolingual corpus , where additional LM adaption technology is necessary .	The results show that our method can improve both the perplexity score for LM evaluation and BLEU score for SMT , and significantly outperforms the existing LM growing methods without extra corpus .	CL_D14-1023_pa_2	CL_D14-1023_pa_4	CL_D14-1023_pa_2_4
CL	D14-1023	pa	3	4	proposal	result	none	support	main	secondary	back	support	In this paper , we propose a novel neural network based bilingual LM growing method , only using the bilingual parallel corpus in SMT .	The results show that our method can improve both the perplexity score for LM evaluation and BLEU score for SMT , and significantly outperforms the existing LM growing methods without extra corpus .	CL_D14-1023_pa_3	CL_D14-1023_pa_4	CL_D14-1023_pa_3_4
CL	D14-1023	pa	4	3	result	proposal	support	none	secondary	main	forw	support	The results show that our method can improve both the perplexity score for LM evaluation and BLEU score for SMT , and significantly outperforms the existing LM growing methods without extra corpus .	In this paper , we propose a novel neural network based bilingual LM growing method , only using the bilingual parallel corpus in SMT .	CL_D14-1023_pa_4	CL_D14-1023_pa_3	CL_D14-1023_pa_3_4
CL	D14-1024	pa	1	2	proposal	result_means	none	support	main	secondary	none	none	This article describes a linguistically informed method for integrating phrasal verbs into statistical machine translation ( SMT ) systems .	In a case study involving English to Bulgarian SMT , we show that our method does not only improve translation quality but also outperforms similar methods previously applied to the same task .	CL_D14-1024_pa_1	CL_D14-1024_pa_2	CL_D14-1024_pa_1_2
CL	D14-1024	pa	1	3	proposal	conclusion	none	support	main	secondary	back	support	This article describes a linguistically informed method for integrating phrasal verbs into statistical machine translation ( SMT ) systems .	We attribute this to the fact that , in contrast to previous work on the subject , we employ detailed linguistic information .	CL_D14-1024_pa_1	CL_D14-1024_pa_3	CL_D14-1024_pa_1_3
CL	D14-1024	pa	3	1	conclusion	proposal	support	none	secondary	main	forw	support	We attribute this to the fact that , in contrast to previous work on the subject , we employ detailed linguistic information .	This article describes a linguistically informed method for integrating phrasal verbs into statistical machine translation ( SMT ) systems .	CL_D14-1024_pa_3	CL_D14-1024_pa_1	CL_D14-1024_pa_1_3
CL	D14-1024	pa	1	4	proposal	result	none	elaboration	main	secondary	none	none	This article describes a linguistically informed method for integrating phrasal verbs into statistical machine translation ( SMT ) systems .	We found out that features which describe phrasal verbs as idiomatic or compositional contribute most to the better translation quality achieved by our method .	CL_D14-1024_pa_1	CL_D14-1024_pa_4	CL_D14-1024_pa_1_4
CL	D14-1024	pa	2	3	result_means	conclusion	support	support	secondary	secondary	forw	support	In a case study involving English to Bulgarian SMT , we show that our method does not only improve translation quality but also outperforms similar methods previously applied to the same task .	We attribute this to the fact that , in contrast to previous work on the subject , we employ detailed linguistic information .	CL_D14-1024_pa_2	CL_D14-1024_pa_3	CL_D14-1024_pa_2_3
CL	D14-1024	pa	3	2	conclusion	result_means	support	support	secondary	secondary	back	support	We attribute this to the fact that , in contrast to previous work on the subject , we employ detailed linguistic information .	In a case study involving English to Bulgarian SMT , we show that our method does not only improve translation quality but also outperforms similar methods previously applied to the same task .	CL_D14-1024_pa_3	CL_D14-1024_pa_2	CL_D14-1024_pa_2_3
CL	D14-1024	pa	2	4	result_means	result	support	elaboration	secondary	secondary	back	elaboration	In a case study involving English to Bulgarian SMT , we show that our method does not only improve translation quality but also outperforms similar methods previously applied to the same task .	We found out that features which describe phrasal verbs as idiomatic or compositional contribute most to the better translation quality achieved by our method .	CL_D14-1024_pa_2	CL_D14-1024_pa_4	CL_D14-1024_pa_2_4
CL	D14-1024	pa	4	2	result	result_means	elaboration	support	secondary	secondary	forw	elaboration	We found out that features which describe phrasal verbs as idiomatic or compositional contribute most to the better translation quality achieved by our method .	In a case study involving English to Bulgarian SMT , we show that our method does not only improve translation quality but also outperforms similar methods previously applied to the same task .	CL_D14-1024_pa_4	CL_D14-1024_pa_2	CL_D14-1024_pa_2_4
CL	D14-1024	pa	3	4	conclusion	result	support	elaboration	secondary	secondary	none	none	We attribute this to the fact that , in contrast to previous work on the subject , we employ detailed linguistic information .	We found out that features which describe phrasal verbs as idiomatic or compositional contribute most to the better translation quality achieved by our method .	CL_D14-1024_pa_3	CL_D14-1024_pa_4	CL_D14-1024_pa_3_4
CL	D14-1025	pa	1	2	motivation_background	motivation_problem	info-required	support	secondary	secondary	forw	info-required	Sentence level evaluation in MT has turned out far more difficult than corpus level evaluation .	Existing sentence level metrics employ a limited set of features , most of which are rather sparse at the sentence level , and their intricate models are rarely trained for ranking .	CL_D14-1025_pa_1	CL_D14-1025_pa_2	CL_D14-1025_pa_1_2
CL	D14-1025	pa	2	1	motivation_problem	motivation_background	support	info-required	secondary	secondary	back	info-required	Existing sentence level metrics employ a limited set of features , most of which are rather sparse at the sentence level , and their intricate models are rarely trained for ranking .	Sentence level evaluation in MT has turned out far more difficult than corpus level evaluation .	CL_D14-1025_pa_2	CL_D14-1025_pa_1	CL_D14-1025_pa_1_2
CL	D14-1025	pa	1	3	motivation_background	proposal	info-required	none	secondary	main	none	none	Sentence level evaluation in MT has turned out far more difficult than corpus level evaluation .	This paper presents a simple linear model exploiting 33 relatively dense features , some of which are novel while others are known but seldom used , and train it under the learning-to-rank framework .	CL_D14-1025_pa_1	CL_D14-1025_pa_3	CL_D14-1025_pa_1_3
CL	D14-1025	pa	1	4	motivation_background	result_means	info-required	support	secondary	secondary	none	none	Sentence level evaluation in MT has turned out far more difficult than corpus level evaluation .	We evaluate our metric on the standard WMT12 data showing that it outperforms the strong baseline METEOR .	CL_D14-1025_pa_1	CL_D14-1025_pa_4	CL_D14-1025_pa_1_4
CL	D14-1025	pa	1	5	motivation_background	proposal	info-required	elaboration	secondary	secondary	none	none	Sentence level evaluation in MT has turned out far more difficult than corpus level evaluation .	We also analyze the contribution of individual features and the choice of training data , language-pair vs. target-language data , providing new insights into this task .	CL_D14-1025_pa_1	CL_D14-1025_pa_5	CL_D14-1025_pa_1_5
CL	D14-1025	pa	2	3	motivation_problem	proposal	support	none	secondary	main	forw	support	Existing sentence level metrics employ a limited set of features , most of which are rather sparse at the sentence level , and their intricate models are rarely trained for ranking .	This paper presents a simple linear model exploiting 33 relatively dense features , some of which are novel while others are known but seldom used , and train it under the learning-to-rank framework .	CL_D14-1025_pa_2	CL_D14-1025_pa_3	CL_D14-1025_pa_2_3
CL	D14-1025	pa	3	2	proposal	motivation_problem	none	support	main	secondary	back	support	This paper presents a simple linear model exploiting 33 relatively dense features , some of which are novel while others are known but seldom used , and train it under the learning-to-rank framework .	Existing sentence level metrics employ a limited set of features , most of which are rather sparse at the sentence level , and their intricate models are rarely trained for ranking .	CL_D14-1025_pa_3	CL_D14-1025_pa_2	CL_D14-1025_pa_2_3
CL	D14-1025	pa	2	4	motivation_problem	result_means	support	support	secondary	secondary	none	none	Existing sentence level metrics employ a limited set of features , most of which are rather sparse at the sentence level , and their intricate models are rarely trained for ranking .	We evaluate our metric on the standard WMT12 data showing that it outperforms the strong baseline METEOR .	CL_D14-1025_pa_2	CL_D14-1025_pa_4	CL_D14-1025_pa_2_4
CL	D14-1025	pa	2	5	motivation_problem	proposal	support	elaboration	secondary	secondary	none	none	Existing sentence level metrics employ a limited set of features , most of which are rather sparse at the sentence level , and their intricate models are rarely trained for ranking .	We also analyze the contribution of individual features and the choice of training data , language-pair vs. target-language data , providing new insights into this task .	CL_D14-1025_pa_2	CL_D14-1025_pa_5	CL_D14-1025_pa_2_5
CL	D14-1025	pa	3	4	proposal	result_means	none	support	main	secondary	back	support	This paper presents a simple linear model exploiting 33 relatively dense features , some of which are novel while others are known but seldom used , and train it under the learning-to-rank framework .	We evaluate our metric on the standard WMT12 data showing that it outperforms the strong baseline METEOR .	CL_D14-1025_pa_3	CL_D14-1025_pa_4	CL_D14-1025_pa_3_4
CL	D14-1025	pa	4	3	result_means	proposal	support	none	secondary	main	forw	support	We evaluate our metric on the standard WMT12 data showing that it outperforms the strong baseline METEOR .	This paper presents a simple linear model exploiting 33 relatively dense features , some of which are novel while others are known but seldom used , and train it under the learning-to-rank framework .	CL_D14-1025_pa_4	CL_D14-1025_pa_3	CL_D14-1025_pa_3_4
CL	D14-1025	pa	3	5	proposal	proposal	none	elaboration	main	secondary	back	elaboration	This paper presents a simple linear model exploiting 33 relatively dense features , some of which are novel while others are known but seldom used , and train it under the learning-to-rank framework .	We also analyze the contribution of individual features and the choice of training data , language-pair vs. target-language data , providing new insights into this task .	CL_D14-1025_pa_3	CL_D14-1025_pa_5	CL_D14-1025_pa_3_5
CL	D14-1025	pa	5	3	proposal	proposal	elaboration	none	secondary	main	forw	elaboration	We also analyze the contribution of individual features and the choice of training data , language-pair vs. target-language data , providing new insights into this task .	This paper presents a simple linear model exploiting 33 relatively dense features , some of which are novel while others are known but seldom used , and train it under the learning-to-rank framework .	CL_D14-1025_pa_5	CL_D14-1025_pa_3	CL_D14-1025_pa_3_5
CL	D14-1025	pa	4	5	result_means	proposal	support	elaboration	secondary	secondary	none	none	We evaluate our metric on the standard WMT12 data showing that it outperforms the strong baseline METEOR .	We also analyze the contribution of individual features and the choice of training data , language-pair vs. target-language data , providing new insights into this task .	CL_D14-1025_pa_4	CL_D14-1025_pa_5	CL_D14-1025_pa_4_5
CL	D14-1026	pa	1	2	proposal	conclusion	none	support	main	secondary	back	support	We present a human judgments dataset and an adapted metric for evaluation of Arabic machine translation .	Our mediumscale dataset is the first of its kind for Arabic with high annotation quality .	CL_D14-1026_pa_1	CL_D14-1026_pa_2	CL_D14-1026_pa_1_2
CL	D14-1026	pa	2	1	conclusion	proposal	support	none	secondary	main	forw	support	Our mediumscale dataset is the first of its kind for Arabic with high annotation quality .	We present a human judgments dataset and an adapted metric for evaluation of Arabic machine translation .	CL_D14-1026_pa_2	CL_D14-1026_pa_1	CL_D14-1026_pa_1_2
CL	D14-1026	pa	1	3	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	We present a human judgments dataset and an adapted metric for evaluation of Arabic machine translation .	We use the dataset to adapt the BLEU score for Arabic .	CL_D14-1026_pa_1	CL_D14-1026_pa_3	CL_D14-1026_pa_1_3
CL	D14-1026	pa	1	4	proposal	proposal	none	elaboration	main	secondary	back	elaboration	We present a human judgments dataset and an adapted metric for evaluation of Arabic machine translation .	Our score ( AL-BLEU ) provides partial credits for stem and morphological matchings of hypothesis and reference words .	CL_D14-1026_pa_1	CL_D14-1026_pa_4	CL_D14-1026_pa_1_4
CL	D14-1026	pa	4	1	proposal	proposal	elaboration	none	secondary	main	forw	elaboration	Our score ( AL-BLEU ) provides partial credits for stem and morphological matchings of hypothesis and reference words .	We present a human judgments dataset and an adapted metric for evaluation of Arabic machine translation .	CL_D14-1026_pa_4	CL_D14-1026_pa_1	CL_D14-1026_pa_1_4
CL	D14-1026	pa	1	5	proposal	result_means	none	support	main	secondary	back	support	We present a human judgments dataset and an adapted metric for evaluation of Arabic machine translation .	We evaluate BLEU , METEOR and AL-BLEU on our human judgments corpus and show that AL-BLEU has the highest correlation with human judgments .	CL_D14-1026_pa_1	CL_D14-1026_pa_5	CL_D14-1026_pa_1_5
CL	D14-1026	pa	5	1	result_means	proposal	support	none	secondary	main	forw	support	We evaluate BLEU , METEOR and AL-BLEU on our human judgments corpus and show that AL-BLEU has the highest correlation with human judgments .	We present a human judgments dataset and an adapted metric for evaluation of Arabic machine translation .	CL_D14-1026_pa_5	CL_D14-1026_pa_1	CL_D14-1026_pa_1_5
CL	D14-1026	pa	1	6	proposal	information_additional	none	info-optional	main	secondary	none	none	We present a human judgments dataset and an adapted metric for evaluation of Arabic machine translation .	We are releasing the dataset and software to the research community .	CL_D14-1026_pa_1	CL_D14-1026_pa_6	CL_D14-1026_pa_1_6
CL	D14-1026	pa	2	3	conclusion	proposal_implementation	support	elaboration	secondary	secondary	none	none	Our mediumscale dataset is the first of its kind for Arabic with high annotation quality .	We use the dataset to adapt the BLEU score for Arabic .	CL_D14-1026_pa_2	CL_D14-1026_pa_3	CL_D14-1026_pa_2_3
CL	D14-1026	pa	2	4	conclusion	proposal	support	elaboration	secondary	secondary	none	none	Our mediumscale dataset is the first of its kind for Arabic with high annotation quality .	Our score ( AL-BLEU ) provides partial credits for stem and morphological matchings of hypothesis and reference words .	CL_D14-1026_pa_2	CL_D14-1026_pa_4	CL_D14-1026_pa_2_4
CL	D14-1026	pa	2	5	conclusion	result_means	support	support	secondary	secondary	none	none	Our mediumscale dataset is the first of its kind for Arabic with high annotation quality .	We evaluate BLEU , METEOR and AL-BLEU on our human judgments corpus and show that AL-BLEU has the highest correlation with human judgments .	CL_D14-1026_pa_2	CL_D14-1026_pa_5	CL_D14-1026_pa_2_5
CL	D14-1026	pa	2	6	conclusion	information_additional	support	info-optional	secondary	secondary	back	info-optional	Our mediumscale dataset is the first of its kind for Arabic with high annotation quality .	We are releasing the dataset and software to the research community .	CL_D14-1026_pa_2	CL_D14-1026_pa_6	CL_D14-1026_pa_2_6
CL	D14-1026	pa	6	2	information_additional	conclusion	info-optional	support	secondary	secondary	forw	info-optional	We are releasing the dataset and software to the research community .	Our mediumscale dataset is the first of its kind for Arabic with high annotation quality .	CL_D14-1026_pa_6	CL_D14-1026_pa_2	CL_D14-1026_pa_2_6
CL	D14-1026	pa	3	4	proposal_implementation	proposal	elaboration	elaboration	secondary	secondary	forw	elaboration	We use the dataset to adapt the BLEU score for Arabic .	Our score ( AL-BLEU ) provides partial credits for stem and morphological matchings of hypothesis and reference words .	CL_D14-1026_pa_3	CL_D14-1026_pa_4	CL_D14-1026_pa_3_4
CL	D14-1026	pa	4	3	proposal	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	Our score ( AL-BLEU ) provides partial credits for stem and morphological matchings of hypothesis and reference words .	We use the dataset to adapt the BLEU score for Arabic .	CL_D14-1026_pa_4	CL_D14-1026_pa_3	CL_D14-1026_pa_3_4
CL	D14-1026	pa	3	5	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	We use the dataset to adapt the BLEU score for Arabic .	We evaluate BLEU , METEOR and AL-BLEU on our human judgments corpus and show that AL-BLEU has the highest correlation with human judgments .	CL_D14-1026_pa_3	CL_D14-1026_pa_5	CL_D14-1026_pa_3_5
CL	D14-1026	pa	3	6	proposal_implementation	information_additional	elaboration	info-optional	secondary	secondary	none	none	We use the dataset to adapt the BLEU score for Arabic .	We are releasing the dataset and software to the research community .	CL_D14-1026_pa_3	CL_D14-1026_pa_6	CL_D14-1026_pa_3_6
CL	D14-1026	pa	4	5	proposal	result_means	elaboration	support	secondary	secondary	none	none	Our score ( AL-BLEU ) provides partial credits for stem and morphological matchings of hypothesis and reference words .	We evaluate BLEU , METEOR and AL-BLEU on our human judgments corpus and show that AL-BLEU has the highest correlation with human judgments .	CL_D14-1026_pa_4	CL_D14-1026_pa_5	CL_D14-1026_pa_4_5
CL	D14-1026	pa	4	6	proposal	information_additional	elaboration	info-optional	secondary	secondary	none	none	Our score ( AL-BLEU ) provides partial credits for stem and morphological matchings of hypothesis and reference words .	We are releasing the dataset and software to the research community .	CL_D14-1026_pa_4	CL_D14-1026_pa_6	CL_D14-1026_pa_4_6
CL	D14-1026	pa	5	6	result_means	information_additional	support	info-optional	secondary	secondary	none	none	We evaluate BLEU , METEOR and AL-BLEU on our human judgments corpus and show that AL-BLEU has the highest correlation with human judgments .	We are releasing the dataset and software to the research community .	CL_D14-1026_pa_5	CL_D14-1026_pa_6	CL_D14-1026_pa_5_6
CL	D14-1027	pa	1	2	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	We present a pairwise learning-to-rank approach to machine translation evaluation that learns to differentiate better from worse translations in the context of a given reference .	We integrate several layers of linguistic information encapsulated in tree-based structures , making use of both the reference and the system output simultaneously , thus bringing our ranking closer to how humans evaluate translations .	CL_D14-1027_pa_1	CL_D14-1027_pa_2	CL_D14-1027_pa_1_2
CL	D14-1027	pa	2	1	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	We integrate several layers of linguistic information encapsulated in tree-based structures , making use of both the reference and the system output simultaneously , thus bringing our ranking closer to how humans evaluate translations .	We present a pairwise learning-to-rank approach to machine translation evaluation that learns to differentiate better from worse translations in the context of a given reference .	CL_D14-1027_pa_2	CL_D14-1027_pa_1	CL_D14-1027_pa_1_2
CL	D14-1027	pa	1	3	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	We present a pairwise learning-to-rank approach to machine translation evaluation that learns to differentiate better from worse translations in the context of a given reference .	Most importantly , instead of deciding upfront which types of features are important , we use the learning framework of preference re-ranking kernels to learn the features automatically .	CL_D14-1027_pa_1	CL_D14-1027_pa_3	CL_D14-1027_pa_1_3
CL	D14-1027	pa	1	4	proposal	result	none	support	main	secondary	back	support	We present a pairwise learning-to-rank approach to machine translation evaluation that learns to differentiate better from worse translations in the context of a given reference .	The evaluation results show that learning in the proposed framework yields better correlation with humans than computing the direct similarity over the same type of structures .	CL_D14-1027_pa_1	CL_D14-1027_pa_4	CL_D14-1027_pa_1_4
CL	D14-1027	pa	4	1	result	proposal	support	none	secondary	main	forw	support	The evaluation results show that learning in the proposed framework yields better correlation with humans than computing the direct similarity over the same type of structures .	We present a pairwise learning-to-rank approach to machine translation evaluation that learns to differentiate better from worse translations in the context of a given reference .	CL_D14-1027_pa_4	CL_D14-1027_pa_1	CL_D14-1027_pa_1_4
CL	D14-1027	pa	1	5	proposal	conclusion	none	support	main	secondary	back	support	We present a pairwise learning-to-rank approach to machine translation evaluation that learns to differentiate better from worse translations in the context of a given reference .	Also , we show our structural kernel learning ( SKL ) can be a general framework for MT evaluation , in which syntactic and semantic information can be naturally incorporated .	CL_D14-1027_pa_1	CL_D14-1027_pa_5	CL_D14-1027_pa_1_5
CL	D14-1027	pa	5	1	conclusion	proposal	support	none	secondary	main	forw	support	Also , we show our structural kernel learning ( SKL ) can be a general framework for MT evaluation , in which syntactic and semantic information can be naturally incorporated .	We present a pairwise learning-to-rank approach to machine translation evaluation that learns to differentiate better from worse translations in the context of a given reference .	CL_D14-1027_pa_5	CL_D14-1027_pa_1	CL_D14-1027_pa_1_5
CL	D14-1027	pa	2	3	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	We integrate several layers of linguistic information encapsulated in tree-based structures , making use of both the reference and the system output simultaneously , thus bringing our ranking closer to how humans evaluate translations .	Most importantly , instead of deciding upfront which types of features are important , we use the learning framework of preference re-ranking kernels to learn the features automatically .	CL_D14-1027_pa_2	CL_D14-1027_pa_3	CL_D14-1027_pa_2_3
CL	D14-1027	pa	3	2	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	forw	elaboration	Most importantly , instead of deciding upfront which types of features are important , we use the learning framework of preference re-ranking kernels to learn the features automatically .	We integrate several layers of linguistic information encapsulated in tree-based structures , making use of both the reference and the system output simultaneously , thus bringing our ranking closer to how humans evaluate translations .	CL_D14-1027_pa_3	CL_D14-1027_pa_2	CL_D14-1027_pa_2_3
CL	D14-1027	pa	2	4	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	We integrate several layers of linguistic information encapsulated in tree-based structures , making use of both the reference and the system output simultaneously , thus bringing our ranking closer to how humans evaluate translations .	The evaluation results show that learning in the proposed framework yields better correlation with humans than computing the direct similarity over the same type of structures .	CL_D14-1027_pa_2	CL_D14-1027_pa_4	CL_D14-1027_pa_2_4
CL	D14-1027	pa	2	5	proposal_implementation	conclusion	elaboration	support	secondary	secondary	none	none	We integrate several layers of linguistic information encapsulated in tree-based structures , making use of both the reference and the system output simultaneously , thus bringing our ranking closer to how humans evaluate translations .	Also , we show our structural kernel learning ( SKL ) can be a general framework for MT evaluation , in which syntactic and semantic information can be naturally incorporated .	CL_D14-1027_pa_2	CL_D14-1027_pa_5	CL_D14-1027_pa_2_5
CL	D14-1027	pa	3	4	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	Most importantly , instead of deciding upfront which types of features are important , we use the learning framework of preference re-ranking kernels to learn the features automatically .	The evaluation results show that learning in the proposed framework yields better correlation with humans than computing the direct similarity over the same type of structures .	CL_D14-1027_pa_3	CL_D14-1027_pa_4	CL_D14-1027_pa_3_4
CL	D14-1027	pa	3	5	proposal_implementation	conclusion	elaboration	support	secondary	secondary	none	none	Most importantly , instead of deciding upfront which types of features are important , we use the learning framework of preference re-ranking kernels to learn the features automatically .	Also , we show our structural kernel learning ( SKL ) can be a general framework for MT evaluation , in which syntactic and semantic information can be naturally incorporated .	CL_D14-1027_pa_3	CL_D14-1027_pa_5	CL_D14-1027_pa_3_5
CL	D14-1027	pa	4	5	result	conclusion	support	support	secondary	secondary	none	none	The evaluation results show that learning in the proposed framework yields better correlation with humans than computing the direct similarity over the same type of structures .	Also , we show our structural kernel learning ( SKL ) can be a general framework for MT evaluation , in which syntactic and semantic information can be naturally incorporated .	CL_D14-1027_pa_4	CL_D14-1027_pa_5	CL_D14-1027_pa_4_5
CL	D14-1028	pa	1	2	motivation_background	motivation_problem	info-required	support	secondary	secondary	forw	info-required	Left-to-right ( LR ) decoding ( Watanabe et al. , 2006 ) is promising decoding algorithm for hierarchical phrase-based translation ( Hiero ) that visits input spans in arbitrary order producing the output translation in left to right order .	This leads to far fewer language model calls , but while LR decoding is more efficient than CKY decoding , it is unable to capture some hierarchical phrase alignments reachable using CKY decoding and suffers from lower translation quality as a result .	CL_D14-1028_pa_1	CL_D14-1028_pa_2	CL_D14-1028_pa_1_2
CL	D14-1028	pa	2	1	motivation_problem	motivation_background	support	info-required	secondary	secondary	back	info-required	This leads to far fewer language model calls , but while LR decoding is more efficient than CKY decoding , it is unable to capture some hierarchical phrase alignments reachable using CKY decoding and suffers from lower translation quality as a result .	Left-to-right ( LR ) decoding ( Watanabe et al. , 2006 ) is promising decoding algorithm for hierarchical phrase-based translation ( Hiero ) that visits input spans in arbitrary order producing the output translation in left to right order .	CL_D14-1028_pa_2	CL_D14-1028_pa_1	CL_D14-1028_pa_1_2
CL	D14-1028	pa	1	3	motivation_background	proposal	info-required	none	secondary	main	none	none	Left-to-right ( LR ) decoding ( Watanabe et al. , 2006 ) is promising decoding algorithm for hierarchical phrase-based translation ( Hiero ) that visits input spans in arbitrary order producing the output translation in left to right order .	This paper introduces two improvements to LR decoding that make it comparable in translation quality to CKY-based Hiero .	CL_D14-1028_pa_1	CL_D14-1028_pa_3	CL_D14-1028_pa_1_3
CL	D14-1028	pa	2	3	motivation_problem	proposal	support	none	secondary	main	forw	support	This leads to far fewer language model calls , but while LR decoding is more efficient than CKY decoding , it is unable to capture some hierarchical phrase alignments reachable using CKY decoding and suffers from lower translation quality as a result .	This paper introduces two improvements to LR decoding that make it comparable in translation quality to CKY-based Hiero .	CL_D14-1028_pa_2	CL_D14-1028_pa_3	CL_D14-1028_pa_2_3
CL	D14-1028	pa	3	2	proposal	motivation_problem	none	support	main	secondary	back	support	This paper introduces two improvements to LR decoding that make it comparable in translation quality to CKY-based Hiero .	This leads to far fewer language model calls , but while LR decoding is more efficient than CKY decoding , it is unable to capture some hierarchical phrase alignments reachable using CKY decoding and suffers from lower translation quality as a result .	CL_D14-1028_pa_3	CL_D14-1028_pa_2	CL_D14-1028_pa_2_3
CL	D14-1029	pa	1	2	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	In this paper , we present a novel extension of a forest-to-string machine translation system with a reordering model .	We predict reordering probabilities for every pair of source words with a model using features observed from the input parse forest .	CL_D14-1029_pa_1	CL_D14-1029_pa_2	CL_D14-1029_pa_1_2
CL	D14-1029	pa	2	1	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	We predict reordering probabilities for every pair of source words with a model using features observed from the input parse forest .	In this paper , we present a novel extension of a forest-to-string machine translation system with a reordering model .	CL_D14-1029_pa_2	CL_D14-1029_pa_1	CL_D14-1029_pa_1_2
CL	D14-1029	pa	1	3	proposal	proposal	none	elaboration	main	secondary	back	elaboration	In this paper , we present a novel extension of a forest-to-string machine translation system with a reordering model .	Our approach naturally deals with the ambiguity present in the input parse forest , but , at the same time , takes into account only the parts of the input forest used by the current translation hypothesis .	CL_D14-1029_pa_1	CL_D14-1029_pa_3	CL_D14-1029_pa_1_3
CL	D14-1029	pa	3	1	proposal	proposal	elaboration	none	secondary	main	forw	elaboration	Our approach naturally deals with the ambiguity present in the input parse forest , but , at the same time , takes into account only the parts of the input forest used by the current translation hypothesis .	In this paper , we present a novel extension of a forest-to-string machine translation system with a reordering model .	CL_D14-1029_pa_3	CL_D14-1029_pa_1	CL_D14-1029_pa_1_3
CL	D14-1029	pa	1	4	proposal	observation	none	support	main	secondary	back	support	In this paper , we present a novel extension of a forest-to-string machine translation system with a reordering model .	The method provides improvement from 0.6 up to 1.0 point measured by ( Ter  Bleu ) /2 metric .	CL_D14-1029_pa_1	CL_D14-1029_pa_4	CL_D14-1029_pa_1_4
CL	D14-1029	pa	4	1	observation	proposal	support	none	secondary	main	forw	support	The method provides improvement from 0.6 up to 1.0 point measured by ( Ter  Bleu ) /2 metric .	In this paper , we present a novel extension of a forest-to-string machine translation system with a reordering model .	CL_D14-1029_pa_4	CL_D14-1029_pa_1	CL_D14-1029_pa_1_4
CL	D14-1029	pa	2	3	proposal_implementation	proposal	elaboration	elaboration	secondary	secondary	none	none	We predict reordering probabilities for every pair of source words with a model using features observed from the input parse forest .	Our approach naturally deals with the ambiguity present in the input parse forest , but , at the same time , takes into account only the parts of the input forest used by the current translation hypothesis .	CL_D14-1029_pa_2	CL_D14-1029_pa_3	CL_D14-1029_pa_2_3
CL	D14-1029	pa	2	4	proposal_implementation	observation	elaboration	support	secondary	secondary	none	none	We predict reordering probabilities for every pair of source words with a model using features observed from the input parse forest .	The method provides improvement from 0.6 up to 1.0 point measured by ( Ter  Bleu ) /2 metric .	CL_D14-1029_pa_2	CL_D14-1029_pa_4	CL_D14-1029_pa_2_4
CL	D14-1029	pa	3	4	proposal	observation	elaboration	support	secondary	secondary	none	none	Our approach naturally deals with the ambiguity present in the input parse forest , but , at the same time , takes into account only the parts of the input forest used by the current translation hypothesis .	The method provides improvement from 0.6 up to 1.0 point measured by ( Ter  Bleu ) /2 metric .	CL_D14-1029_pa_3	CL_D14-1029_pa_4	CL_D14-1029_pa_3_4
CL	D14-1030	pa	1	2	motivation_background	motivation_background	info-required	info-required	secondary	secondary	forw	info-required	Many statistical models for natural language processing exist , including context-based neural networks that ( 1 ) model the previously seen context as a latent feature vector , ( 2 ) integrate successive words into the context using some learned representation ( embedding ) , and ( 3 ) compute output probabilities for incoming words given the context .	On the other hand , brain imaging studies have suggested that during reading , the brain ( a ) continuously builds a context from the successive words and every time it encounters a word it ( b ) fetches its properties from memory and ( c ) integrates it with the previous context with a degree of effort that is inversely proportional to how probable the word is .	CL_D14-1030_pa_1	CL_D14-1030_pa_2	CL_D14-1030_pa_1_2
CL	D14-1030	pa	2	1	motivation_background	motivation_background	info-required	info-required	secondary	secondary	back	info-required	On the other hand , brain imaging studies have suggested that during reading , the brain ( a ) continuously builds a context from the successive words and every time it encounters a word it ( b ) fetches its properties from memory and ( c ) integrates it with the previous context with a degree of effort that is inversely proportional to how probable the word is .	Many statistical models for natural language processing exist , including context-based neural networks that ( 1 ) model the previously seen context as a latent feature vector , ( 2 ) integrate successive words into the context using some learned representation ( embedding ) , and ( 3 ) compute output probabilities for incoming words given the context .	CL_D14-1030_pa_2	CL_D14-1030_pa_1	CL_D14-1030_pa_1_2
CL	D14-1030	pa	1	3	motivation_background	motivation_hypothesis	info-required	support	secondary	secondary	none	none	Many statistical models for natural language processing exist , including context-based neural networks that ( 1 ) model the previously seen context as a latent feature vector , ( 2 ) integrate successive words into the context using some learned representation ( embedding ) , and ( 3 ) compute output probabilities for incoming words given the context .	This hints to a parallelism between the neural networks and the brain in modeling context ( 1 and a ) , representing the incoming words ( 2 and b ) and integrating it ( 3 and c ) .	CL_D14-1030_pa_1	CL_D14-1030_pa_3	CL_D14-1030_pa_1_3
CL	D14-1030	pa	1	4	motivation_background	proposal	info-required	none	secondary	main	none	none	Many statistical models for natural language processing exist , including context-based neural networks that ( 1 ) model the previously seen context as a latent feature vector , ( 2 ) integrate successive words into the context using some learned representation ( embedding ) , and ( 3 ) compute output probabilities for incoming words given the context .	We explore this parallelism to better understand the brain processes and the neural networks representations .	CL_D14-1030_pa_1	CL_D14-1030_pa_4	CL_D14-1030_pa_1_4
CL	D14-1030	pa	1	5	motivation_background	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	Many statistical models for natural language processing exist , including context-based neural networks that ( 1 ) model the previously seen context as a latent feature vector , ( 2 ) integrate successive words into the context using some learned representation ( embedding ) , and ( 3 ) compute output probabilities for incoming words given the context .	We study the alignment between the latent vectors used by neural networks and brain activity observed via Magnetoencephalography ( MEG ) when subjects read a story .	CL_D14-1030_pa_1	CL_D14-1030_pa_5	CL_D14-1030_pa_1_5
CL	D14-1030	pa	1	6	motivation_background	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	Many statistical models for natural language processing exist , including context-based neural networks that ( 1 ) model the previously seen context as a latent feature vector , ( 2 ) integrate successive words into the context using some learned representation ( embedding ) , and ( 3 ) compute output probabilities for incoming words given the context .	For that purpose we apply the neural network to the same text the subjects are reading , and explore the ability of these three vector representations to predict the observed word-by-word brain activity .	CL_D14-1030_pa_1	CL_D14-1030_pa_6	CL_D14-1030_pa_1_6
CL	D14-1030	pa	1	7	motivation_background	result	info-required	support	secondary	secondary	none	none	Many statistical models for natural language processing exist , including context-based neural networks that ( 1 ) model the previously seen context as a latent feature vector , ( 2 ) integrate successive words into the context using some learned representation ( embedding ) , and ( 3 ) compute output probabilities for incoming words given the context .	Our novel results show that : before a new word i is read , brain activity is well predicted by the neural network latent representation of context and the predictability decreases as the brain integrates the word and changes its own representation of context .	CL_D14-1030_pa_1	CL_D14-1030_pa_7	CL_D14-1030_pa_1_7
CL	D14-1030	pa	1	8	motivation_background	result	info-required	support	secondary	secondary	none	none	Many statistical models for natural language processing exist , including context-based neural networks that ( 1 ) model the previously seen context as a latent feature vector , ( 2 ) integrate successive words into the context using some learned representation ( embedding ) , and ( 3 ) compute output probabilities for incoming words given the context .	Secondly , the neural network embedding of word i can predict the MEG activity when word i is presented to the subject , revealing that it is correlated with the brain's own representation of word i .	CL_D14-1030_pa_1	CL_D14-1030_pa_8	CL_D14-1030_pa_1_8
CL	D14-1030	pa	1	9	motivation_background	result	info-required	elaboration	secondary	secondary	none	none	Many statistical models for natural language processing exist , including context-based neural networks that ( 1 ) model the previously seen context as a latent feature vector , ( 2 ) integrate successive words into the context using some learned representation ( embedding ) , and ( 3 ) compute output probabilities for incoming words given the context .	Moreover , we obtain that the activity is predicted in different regions of the brain with varying delay .	CL_D14-1030_pa_1	CL_D14-1030_pa_9	CL_D14-1030_pa_1_9
CL	D14-1030	pa	1	10	motivation_background	result	info-required	elaboration	secondary	secondary	none	none	Many statistical models for natural language processing exist , including context-based neural networks that ( 1 ) model the previously seen context as a latent feature vector , ( 2 ) integrate successive words into the context using some learned representation ( embedding ) , and ( 3 ) compute output probabilities for incoming words given the context .	The delay is consistent with the placement of each region on the processing pathway that starts in the visual cortex and moves to higher level regions .	CL_D14-1030_pa_1	CL_D14-1030_pa_10	CL_D14-1030_pa_1_10
CL	D14-1030	pa	1	11	motivation_background	result	info-required	support	secondary	secondary	none	none	Many statistical models for natural language processing exist , including context-based neural networks that ( 1 ) model the previously seen context as a latent feature vector , ( 2 ) integrate successive words into the context using some learned representation ( embedding ) , and ( 3 ) compute output probabilities for incoming words given the context .	Finally , we show that the output probability computed by the neural networks agrees with the brain's own assessment of the probability of word i , as it can be used to predict the brain activity after the word i's properties have been fetched from memory and the brain is in the process of integrating it into the context .	CL_D14-1030_pa_1	CL_D14-1030_pa_11	CL_D14-1030_pa_1_11
CL	D14-1030	pa	2	3	motivation_background	motivation_hypothesis	info-required	support	secondary	secondary	forw	info-required	On the other hand , brain imaging studies have suggested that during reading , the brain ( a ) continuously builds a context from the successive words and every time it encounters a word it ( b ) fetches its properties from memory and ( c ) integrates it with the previous context with a degree of effort that is inversely proportional to how probable the word is .	This hints to a parallelism between the neural networks and the brain in modeling context ( 1 and a ) , representing the incoming words ( 2 and b ) and integrating it ( 3 and c ) .	CL_D14-1030_pa_2	CL_D14-1030_pa_3	CL_D14-1030_pa_2_3
CL	D14-1030	pa	3	2	motivation_hypothesis	motivation_background	support	info-required	secondary	secondary	back	info-required	This hints to a parallelism between the neural networks and the brain in modeling context ( 1 and a ) , representing the incoming words ( 2 and b ) and integrating it ( 3 and c ) .	On the other hand , brain imaging studies have suggested that during reading , the brain ( a ) continuously builds a context from the successive words and every time it encounters a word it ( b ) fetches its properties from memory and ( c ) integrates it with the previous context with a degree of effort that is inversely proportional to how probable the word is .	CL_D14-1030_pa_3	CL_D14-1030_pa_2	CL_D14-1030_pa_2_3
CL	D14-1030	pa	2	4	motivation_background	proposal	info-required	none	secondary	main	none	none	On the other hand , brain imaging studies have suggested that during reading , the brain ( a ) continuously builds a context from the successive words and every time it encounters a word it ( b ) fetches its properties from memory and ( c ) integrates it with the previous context with a degree of effort that is inversely proportional to how probable the word is .	We explore this parallelism to better understand the brain processes and the neural networks representations .	CL_D14-1030_pa_2	CL_D14-1030_pa_4	CL_D14-1030_pa_2_4
CL	D14-1030	pa	2	5	motivation_background	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	On the other hand , brain imaging studies have suggested that during reading , the brain ( a ) continuously builds a context from the successive words and every time it encounters a word it ( b ) fetches its properties from memory and ( c ) integrates it with the previous context with a degree of effort that is inversely proportional to how probable the word is .	We study the alignment between the latent vectors used by neural networks and brain activity observed via Magnetoencephalography ( MEG ) when subjects read a story .	CL_D14-1030_pa_2	CL_D14-1030_pa_5	CL_D14-1030_pa_2_5
CL	D14-1030	pa	2	6	motivation_background	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	On the other hand , brain imaging studies have suggested that during reading , the brain ( a ) continuously builds a context from the successive words and every time it encounters a word it ( b ) fetches its properties from memory and ( c ) integrates it with the previous context with a degree of effort that is inversely proportional to how probable the word is .	For that purpose we apply the neural network to the same text the subjects are reading , and explore the ability of these three vector representations to predict the observed word-by-word brain activity .	CL_D14-1030_pa_2	CL_D14-1030_pa_6	CL_D14-1030_pa_2_6
CL	D14-1030	pa	2	7	motivation_background	result	info-required	support	secondary	secondary	none	none	On the other hand , brain imaging studies have suggested that during reading , the brain ( a ) continuously builds a context from the successive words and every time it encounters a word it ( b ) fetches its properties from memory and ( c ) integrates it with the previous context with a degree of effort that is inversely proportional to how probable the word is .	Our novel results show that : before a new word i is read , brain activity is well predicted by the neural network latent representation of context and the predictability decreases as the brain integrates the word and changes its own representation of context .	CL_D14-1030_pa_2	CL_D14-1030_pa_7	CL_D14-1030_pa_2_7
CL	D14-1030	pa	2	8	motivation_background	result	info-required	support	secondary	secondary	none	none	On the other hand , brain imaging studies have suggested that during reading , the brain ( a ) continuously builds a context from the successive words and every time it encounters a word it ( b ) fetches its properties from memory and ( c ) integrates it with the previous context with a degree of effort that is inversely proportional to how probable the word is .	Secondly , the neural network embedding of word i can predict the MEG activity when word i is presented to the subject , revealing that it is correlated with the brain's own representation of word i .	CL_D14-1030_pa_2	CL_D14-1030_pa_8	CL_D14-1030_pa_2_8
CL	D14-1030	pa	2	9	motivation_background	result	info-required	elaboration	secondary	secondary	none	none	On the other hand , brain imaging studies have suggested that during reading , the brain ( a ) continuously builds a context from the successive words and every time it encounters a word it ( b ) fetches its properties from memory and ( c ) integrates it with the previous context with a degree of effort that is inversely proportional to how probable the word is .	Moreover , we obtain that the activity is predicted in different regions of the brain with varying delay .	CL_D14-1030_pa_2	CL_D14-1030_pa_9	CL_D14-1030_pa_2_9
CL	D14-1030	pa	2	10	motivation_background	result	info-required	elaboration	secondary	secondary	none	none	On the other hand , brain imaging studies have suggested that during reading , the brain ( a ) continuously builds a context from the successive words and every time it encounters a word it ( b ) fetches its properties from memory and ( c ) integrates it with the previous context with a degree of effort that is inversely proportional to how probable the word is .	The delay is consistent with the placement of each region on the processing pathway that starts in the visual cortex and moves to higher level regions .	CL_D14-1030_pa_2	CL_D14-1030_pa_10	CL_D14-1030_pa_2_10
CL	D14-1030	pa	2	11	motivation_background	result	info-required	support	secondary	secondary	none	none	On the other hand , brain imaging studies have suggested that during reading , the brain ( a ) continuously builds a context from the successive words and every time it encounters a word it ( b ) fetches its properties from memory and ( c ) integrates it with the previous context with a degree of effort that is inversely proportional to how probable the word is .	Finally , we show that the output probability computed by the neural networks agrees with the brain's own assessment of the probability of word i , as it can be used to predict the brain activity after the word i's properties have been fetched from memory and the brain is in the process of integrating it into the context .	CL_D14-1030_pa_2	CL_D14-1030_pa_11	CL_D14-1030_pa_2_11
CL	D14-1030	pa	3	4	motivation_hypothesis	proposal	support	none	secondary	main	forw	support	This hints to a parallelism between the neural networks and the brain in modeling context ( 1 and a ) , representing the incoming words ( 2 and b ) and integrating it ( 3 and c ) .	We explore this parallelism to better understand the brain processes and the neural networks representations .	CL_D14-1030_pa_3	CL_D14-1030_pa_4	CL_D14-1030_pa_3_4
CL	D14-1030	pa	4	3	proposal	motivation_hypothesis	none	support	main	secondary	back	support	We explore this parallelism to better understand the brain processes and the neural networks representations .	This hints to a parallelism between the neural networks and the brain in modeling context ( 1 and a ) , representing the incoming words ( 2 and b ) and integrating it ( 3 and c ) .	CL_D14-1030_pa_4	CL_D14-1030_pa_3	CL_D14-1030_pa_3_4
CL	D14-1030	pa	3	5	motivation_hypothesis	proposal_implementation	support	elaboration	secondary	secondary	none	none	This hints to a parallelism between the neural networks and the brain in modeling context ( 1 and a ) , representing the incoming words ( 2 and b ) and integrating it ( 3 and c ) .	We study the alignment between the latent vectors used by neural networks and brain activity observed via Magnetoencephalography ( MEG ) when subjects read a story .	CL_D14-1030_pa_3	CL_D14-1030_pa_5	CL_D14-1030_pa_3_5
CL	D14-1030	pa	3	6	motivation_hypothesis	proposal_implementation	support	elaboration	secondary	secondary	none	none	This hints to a parallelism between the neural networks and the brain in modeling context ( 1 and a ) , representing the incoming words ( 2 and b ) and integrating it ( 3 and c ) .	For that purpose we apply the neural network to the same text the subjects are reading , and explore the ability of these three vector representations to predict the observed word-by-word brain activity .	CL_D14-1030_pa_3	CL_D14-1030_pa_6	CL_D14-1030_pa_3_6
CL	D14-1030	pa	3	7	motivation_hypothesis	result	support	support	secondary	secondary	none	none	This hints to a parallelism between the neural networks and the brain in modeling context ( 1 and a ) , representing the incoming words ( 2 and b ) and integrating it ( 3 and c ) .	Our novel results show that : before a new word i is read , brain activity is well predicted by the neural network latent representation of context and the predictability decreases as the brain integrates the word and changes its own representation of context .	CL_D14-1030_pa_3	CL_D14-1030_pa_7	CL_D14-1030_pa_3_7
CL	D14-1030	pa	3	8	motivation_hypothesis	result	support	support	secondary	secondary	none	none	This hints to a parallelism between the neural networks and the brain in modeling context ( 1 and a ) , representing the incoming words ( 2 and b ) and integrating it ( 3 and c ) .	Secondly , the neural network embedding of word i can predict the MEG activity when word i is presented to the subject , revealing that it is correlated with the brain's own representation of word i .	CL_D14-1030_pa_3	CL_D14-1030_pa_8	CL_D14-1030_pa_3_8
CL	D14-1030	pa	3	9	motivation_hypothesis	result	support	elaboration	secondary	secondary	none	none	This hints to a parallelism between the neural networks and the brain in modeling context ( 1 and a ) , representing the incoming words ( 2 and b ) and integrating it ( 3 and c ) .	Moreover , we obtain that the activity is predicted in different regions of the brain with varying delay .	CL_D14-1030_pa_3	CL_D14-1030_pa_9	CL_D14-1030_pa_3_9
CL	D14-1030	pa	3	10	motivation_hypothesis	result	support	elaboration	secondary	secondary	none	none	This hints to a parallelism between the neural networks and the brain in modeling context ( 1 and a ) , representing the incoming words ( 2 and b ) and integrating it ( 3 and c ) .	The delay is consistent with the placement of each region on the processing pathway that starts in the visual cortex and moves to higher level regions .	CL_D14-1030_pa_3	CL_D14-1030_pa_10	CL_D14-1030_pa_3_10
CL	D14-1030	pa	3	11	motivation_hypothesis	result	support	support	secondary	secondary	none	none	This hints to a parallelism between the neural networks and the brain in modeling context ( 1 and a ) , representing the incoming words ( 2 and b ) and integrating it ( 3 and c ) .	Finally , we show that the output probability computed by the neural networks agrees with the brain's own assessment of the probability of word i , as it can be used to predict the brain activity after the word i's properties have been fetched from memory and the brain is in the process of integrating it into the context .	CL_D14-1030_pa_3	CL_D14-1030_pa_11	CL_D14-1030_pa_3_11
CL	D14-1030	pa	4	5	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	We explore this parallelism to better understand the brain processes and the neural networks representations .	We study the alignment between the latent vectors used by neural networks and brain activity observed via Magnetoencephalography ( MEG ) when subjects read a story .	CL_D14-1030_pa_4	CL_D14-1030_pa_5	CL_D14-1030_pa_4_5
CL	D14-1030	pa	5	4	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	We study the alignment between the latent vectors used by neural networks and brain activity observed via Magnetoencephalography ( MEG ) when subjects read a story .	We explore this parallelism to better understand the brain processes and the neural networks representations .	CL_D14-1030_pa_5	CL_D14-1030_pa_4	CL_D14-1030_pa_4_5
CL	D14-1030	pa	4	6	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	We explore this parallelism to better understand the brain processes and the neural networks representations .	For that purpose we apply the neural network to the same text the subjects are reading , and explore the ability of these three vector representations to predict the observed word-by-word brain activity .	CL_D14-1030_pa_4	CL_D14-1030_pa_6	CL_D14-1030_pa_4_6
CL	D14-1030	pa	4	7	proposal	result	none	support	main	secondary	back	support	We explore this parallelism to better understand the brain processes and the neural networks representations .	Our novel results show that : before a new word i is read , brain activity is well predicted by the neural network latent representation of context and the predictability decreases as the brain integrates the word and changes its own representation of context .	CL_D14-1030_pa_4	CL_D14-1030_pa_7	CL_D14-1030_pa_4_7
CL	D14-1030	pa	7	4	result	proposal	support	none	secondary	main	forw	support	Our novel results show that : before a new word i is read , brain activity is well predicted by the neural network latent representation of context and the predictability decreases as the brain integrates the word and changes its own representation of context .	We explore this parallelism to better understand the brain processes and the neural networks representations .	CL_D14-1030_pa_7	CL_D14-1030_pa_4	CL_D14-1030_pa_4_7
CL	D14-1030	pa	4	8	proposal	result	none	support	main	secondary	back	support	We explore this parallelism to better understand the brain processes and the neural networks representations .	Secondly , the neural network embedding of word i can predict the MEG activity when word i is presented to the subject , revealing that it is correlated with the brain's own representation of word i .	CL_D14-1030_pa_4	CL_D14-1030_pa_8	CL_D14-1030_pa_4_8
CL	D14-1030	pa	8	4	result	proposal	support	none	secondary	main	forw	support	Secondly , the neural network embedding of word i can predict the MEG activity when word i is presented to the subject , revealing that it is correlated with the brain's own representation of word i .	We explore this parallelism to better understand the brain processes and the neural networks representations .	CL_D14-1030_pa_8	CL_D14-1030_pa_4	CL_D14-1030_pa_4_8
CL	D14-1030	pa	4	9	proposal	result	none	elaboration	main	secondary	none	none	We explore this parallelism to better understand the brain processes and the neural networks representations .	Moreover , we obtain that the activity is predicted in different regions of the brain with varying delay .	CL_D14-1030_pa_4	CL_D14-1030_pa_9	CL_D14-1030_pa_4_9
CL	D14-1030	pa	4	10	proposal	result	none	elaboration	main	secondary	none	none	We explore this parallelism to better understand the brain processes and the neural networks representations .	The delay is consistent with the placement of each region on the processing pathway that starts in the visual cortex and moves to higher level regions .	CL_D14-1030_pa_4	CL_D14-1030_pa_10	CL_D14-1030_pa_4_10
CL	D14-1030	pa	4	11	proposal	result	none	support	main	secondary	back	support	We explore this parallelism to better understand the brain processes and the neural networks representations .	Finally , we show that the output probability computed by the neural networks agrees with the brain's own assessment of the probability of word i , as it can be used to predict the brain activity after the word i's properties have been fetched from memory and the brain is in the process of integrating it into the context .	CL_D14-1030_pa_4	CL_D14-1030_pa_11	CL_D14-1030_pa_4_11
CL	D14-1030	pa	11	4	result	proposal	support	none	secondary	main	forw	support	Finally , we show that the output probability computed by the neural networks agrees with the brain's own assessment of the probability of word i , as it can be used to predict the brain activity after the word i's properties have been fetched from memory and the brain is in the process of integrating it into the context .	We explore this parallelism to better understand the brain processes and the neural networks representations .	CL_D14-1030_pa_11	CL_D14-1030_pa_4	CL_D14-1030_pa_4_11
CL	D14-1030	pa	5	6	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	We study the alignment between the latent vectors used by neural networks and brain activity observed via Magnetoencephalography ( MEG ) when subjects read a story .	For that purpose we apply the neural network to the same text the subjects are reading , and explore the ability of these three vector representations to predict the observed word-by-word brain activity .	CL_D14-1030_pa_5	CL_D14-1030_pa_6	CL_D14-1030_pa_5_6
CL	D14-1030	pa	6	5	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	forw	elaboration	For that purpose we apply the neural network to the same text the subjects are reading , and explore the ability of these three vector representations to predict the observed word-by-word brain activity .	We study the alignment between the latent vectors used by neural networks and brain activity observed via Magnetoencephalography ( MEG ) when subjects read a story .	CL_D14-1030_pa_6	CL_D14-1030_pa_5	CL_D14-1030_pa_5_6
CL	D14-1030	pa	5	7	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	We study the alignment between the latent vectors used by neural networks and brain activity observed via Magnetoencephalography ( MEG ) when subjects read a story .	Our novel results show that : before a new word i is read , brain activity is well predicted by the neural network latent representation of context and the predictability decreases as the brain integrates the word and changes its own representation of context .	CL_D14-1030_pa_5	CL_D14-1030_pa_7	CL_D14-1030_pa_5_7
CL	D14-1030	pa	5	8	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	We study the alignment between the latent vectors used by neural networks and brain activity observed via Magnetoencephalography ( MEG ) when subjects read a story .	Secondly , the neural network embedding of word i can predict the MEG activity when word i is presented to the subject , revealing that it is correlated with the brain's own representation of word i .	CL_D14-1030_pa_5	CL_D14-1030_pa_8	CL_D14-1030_pa_5_8
CL	D14-1030	pa	5	9	proposal_implementation	result	elaboration	elaboration	secondary	secondary	none	none	We study the alignment between the latent vectors used by neural networks and brain activity observed via Magnetoencephalography ( MEG ) when subjects read a story .	Moreover , we obtain that the activity is predicted in different regions of the brain with varying delay .	CL_D14-1030_pa_5	CL_D14-1030_pa_9	CL_D14-1030_pa_5_9
CL	D14-1030	pa	5	10	proposal_implementation	result	elaboration	elaboration	secondary	secondary	none	none	We study the alignment between the latent vectors used by neural networks and brain activity observed via Magnetoencephalography ( MEG ) when subjects read a story .	The delay is consistent with the placement of each region on the processing pathway that starts in the visual cortex and moves to higher level regions .	CL_D14-1030_pa_5	CL_D14-1030_pa_10	CL_D14-1030_pa_5_10
CL	D14-1030	pa	5	11	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	We study the alignment between the latent vectors used by neural networks and brain activity observed via Magnetoencephalography ( MEG ) when subjects read a story .	Finally , we show that the output probability computed by the neural networks agrees with the brain's own assessment of the probability of word i , as it can be used to predict the brain activity after the word i's properties have been fetched from memory and the brain is in the process of integrating it into the context .	CL_D14-1030_pa_5	CL_D14-1030_pa_11	CL_D14-1030_pa_5_11
CL	D14-1030	pa	6	7	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	For that purpose we apply the neural network to the same text the subjects are reading , and explore the ability of these three vector representations to predict the observed word-by-word brain activity .	Our novel results show that : before a new word i is read , brain activity is well predicted by the neural network latent representation of context and the predictability decreases as the brain integrates the word and changes its own representation of context .	CL_D14-1030_pa_6	CL_D14-1030_pa_7	CL_D14-1030_pa_6_7
CL	D14-1030	pa	6	8	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	For that purpose we apply the neural network to the same text the subjects are reading , and explore the ability of these three vector representations to predict the observed word-by-word brain activity .	Secondly , the neural network embedding of word i can predict the MEG activity when word i is presented to the subject , revealing that it is correlated with the brain's own representation of word i .	CL_D14-1030_pa_6	CL_D14-1030_pa_8	CL_D14-1030_pa_6_8
CL	D14-1030	pa	6	9	proposal_implementation	result	elaboration	elaboration	secondary	secondary	none	none	For that purpose we apply the neural network to the same text the subjects are reading , and explore the ability of these three vector representations to predict the observed word-by-word brain activity .	Moreover , we obtain that the activity is predicted in different regions of the brain with varying delay .	CL_D14-1030_pa_6	CL_D14-1030_pa_9	CL_D14-1030_pa_6_9
CL	D14-1030	pa	6	10	proposal_implementation	result	elaboration	elaboration	secondary	secondary	none	none	For that purpose we apply the neural network to the same text the subjects are reading , and explore the ability of these three vector representations to predict the observed word-by-word brain activity .	The delay is consistent with the placement of each region on the processing pathway that starts in the visual cortex and moves to higher level regions .	CL_D14-1030_pa_6	CL_D14-1030_pa_10	CL_D14-1030_pa_6_10
CL	D14-1030	pa	6	11	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	For that purpose we apply the neural network to the same text the subjects are reading , and explore the ability of these three vector representations to predict the observed word-by-word brain activity .	Finally , we show that the output probability computed by the neural networks agrees with the brain's own assessment of the probability of word i , as it can be used to predict the brain activity after the word i's properties have been fetched from memory and the brain is in the process of integrating it into the context .	CL_D14-1030_pa_6	CL_D14-1030_pa_11	CL_D14-1030_pa_6_11
CL	D14-1030	pa	7	8	result	result	support	support	secondary	secondary	none	none	Our novel results show that : before a new word i is read , brain activity is well predicted by the neural network latent representation of context and the predictability decreases as the brain integrates the word and changes its own representation of context .	Secondly , the neural network embedding of word i can predict the MEG activity when word i is presented to the subject , revealing that it is correlated with the brain's own representation of word i .	CL_D14-1030_pa_7	CL_D14-1030_pa_8	CL_D14-1030_pa_7_8
CL	D14-1030	pa	7	9	result	result	support	elaboration	secondary	secondary	none	none	Our novel results show that : before a new word i is read , brain activity is well predicted by the neural network latent representation of context and the predictability decreases as the brain integrates the word and changes its own representation of context .	Moreover , we obtain that the activity is predicted in different regions of the brain with varying delay .	CL_D14-1030_pa_7	CL_D14-1030_pa_9	CL_D14-1030_pa_7_9
CL	D14-1030	pa	7	10	result	result	support	elaboration	secondary	secondary	none	none	Our novel results show that : before a new word i is read , brain activity is well predicted by the neural network latent representation of context and the predictability decreases as the brain integrates the word and changes its own representation of context .	The delay is consistent with the placement of each region on the processing pathway that starts in the visual cortex and moves to higher level regions .	CL_D14-1030_pa_7	CL_D14-1030_pa_10	CL_D14-1030_pa_7_10
CL	D14-1030	pa	7	11	result	result	support	support	secondary	secondary	none	none	Our novel results show that : before a new word i is read , brain activity is well predicted by the neural network latent representation of context and the predictability decreases as the brain integrates the word and changes its own representation of context .	Finally , we show that the output probability computed by the neural networks agrees with the brain's own assessment of the probability of word i , as it can be used to predict the brain activity after the word i's properties have been fetched from memory and the brain is in the process of integrating it into the context .	CL_D14-1030_pa_7	CL_D14-1030_pa_11	CL_D14-1030_pa_7_11
CL	D14-1030	pa	8	9	result	result	support	elaboration	secondary	secondary	back	elaboration	Secondly , the neural network embedding of word i can predict the MEG activity when word i is presented to the subject , revealing that it is correlated with the brain's own representation of word i .	Moreover , we obtain that the activity is predicted in different regions of the brain with varying delay .	CL_D14-1030_pa_8	CL_D14-1030_pa_9	CL_D14-1030_pa_8_9
CL	D14-1030	pa	9	8	result	result	elaboration	support	secondary	secondary	forw	elaboration	Moreover , we obtain that the activity is predicted in different regions of the brain with varying delay .	Secondly , the neural network embedding of word i can predict the MEG activity when word i is presented to the subject , revealing that it is correlated with the brain's own representation of word i .	CL_D14-1030_pa_9	CL_D14-1030_pa_8	CL_D14-1030_pa_8_9
CL	D14-1030	pa	8	10	result	result	support	elaboration	secondary	secondary	none	none	Secondly , the neural network embedding of word i can predict the MEG activity when word i is presented to the subject , revealing that it is correlated with the brain's own representation of word i .	The delay is consistent with the placement of each region on the processing pathway that starts in the visual cortex and moves to higher level regions .	CL_D14-1030_pa_8	CL_D14-1030_pa_10	CL_D14-1030_pa_8_10
CL	D14-1030	pa	8	11	result	result	support	support	secondary	secondary	none	none	Secondly , the neural network embedding of word i can predict the MEG activity when word i is presented to the subject , revealing that it is correlated with the brain's own representation of word i .	Finally , we show that the output probability computed by the neural networks agrees with the brain's own assessment of the probability of word i , as it can be used to predict the brain activity after the word i's properties have been fetched from memory and the brain is in the process of integrating it into the context .	CL_D14-1030_pa_8	CL_D14-1030_pa_11	CL_D14-1030_pa_8_11
CL	D14-1030	pa	9	10	result	result	elaboration	elaboration	secondary	secondary	back	elaboration	Moreover , we obtain that the activity is predicted in different regions of the brain with varying delay .	The delay is consistent with the placement of each region on the processing pathway that starts in the visual cortex and moves to higher level regions .	CL_D14-1030_pa_9	CL_D14-1030_pa_10	CL_D14-1030_pa_9_10
CL	D14-1030	pa	10	9	result	result	elaboration	elaboration	secondary	secondary	forw	elaboration	The delay is consistent with the placement of each region on the processing pathway that starts in the visual cortex and moves to higher level regions .	Moreover , we obtain that the activity is predicted in different regions of the brain with varying delay .	CL_D14-1030_pa_10	CL_D14-1030_pa_9	CL_D14-1030_pa_9_10
CL	D14-1030	pa	9	11	result	result	elaboration	support	secondary	secondary	none	none	Moreover , we obtain that the activity is predicted in different regions of the brain with varying delay .	Finally , we show that the output probability computed by the neural networks agrees with the brain's own assessment of the probability of word i , as it can be used to predict the brain activity after the word i's properties have been fetched from memory and the brain is in the process of integrating it into the context .	CL_D14-1030_pa_9	CL_D14-1030_pa_11	CL_D14-1030_pa_9_11
CL	D14-1030	pa	10	11	result	result	elaboration	support	secondary	secondary	none	none	The delay is consistent with the placement of each region on the processing pathway that starts in the visual cortex and moves to higher level regions .	Finally , we show that the output probability computed by the neural networks agrees with the brain's own assessment of the probability of word i , as it can be used to predict the brain activity after the word i's properties have been fetched from memory and the brain is in the process of integrating it into the context .	CL_D14-1030_pa_10	CL_D14-1030_pa_11	CL_D14-1030_pa_10_11
CL	D14-1031	pa	1	2	motivation_background	motivation_background	info-required	support	secondary	secondary	forw	info-required	Child semantic development includes learning the meaning of words as well as the semantic relations among words .	A presumed outcome of semantic development is the formation of a semantic network that reflects this knowledge .	CL_D14-1031_pa_1	CL_D14-1031_pa_2	CL_D14-1031_pa_1_2
CL	D14-1031	pa	2	1	motivation_background	motivation_background	support	info-required	secondary	secondary	back	info-required	A presumed outcome of semantic development is the formation of a semantic network that reflects this knowledge .	Child semantic development includes learning the meaning of words as well as the semantic relations among words .	CL_D14-1031_pa_2	CL_D14-1031_pa_1	CL_D14-1031_pa_1_2
CL	D14-1031	pa	1	3	motivation_background	proposal	info-required	none	secondary	main	none	none	Child semantic development includes learning the meaning of words as well as the semantic relations among words .	We present an algorithm for simultaneously learning word meanings and gradually growing a semantic network , which adheres to the cognitive plausibility requirements of incrementality and limited computations .	CL_D14-1031_pa_1	CL_D14-1031_pa_3	CL_D14-1031_pa_1_3
CL	D14-1031	pa	1	4	motivation_background	conclusion	info-required	support	secondary	secondary	none	none	Child semantic development includes learning the meaning of words as well as the semantic relations among words .	We demonstrate that the semantic connections among words in addition to their context is necessary in forming a semantic network that resembles an adult's semantic knowledge .	CL_D14-1031_pa_1	CL_D14-1031_pa_4	CL_D14-1031_pa_1_4
CL	D14-1031	pa	2	3	motivation_background	proposal	support	none	secondary	main	forw	support	A presumed outcome of semantic development is the formation of a semantic network that reflects this knowledge .	We present an algorithm for simultaneously learning word meanings and gradually growing a semantic network , which adheres to the cognitive plausibility requirements of incrementality and limited computations .	CL_D14-1031_pa_2	CL_D14-1031_pa_3	CL_D14-1031_pa_2_3
CL	D14-1031	pa	3	2	proposal	motivation_background	none	support	main	secondary	back	support	We present an algorithm for simultaneously learning word meanings and gradually growing a semantic network , which adheres to the cognitive plausibility requirements of incrementality and limited computations .	A presumed outcome of semantic development is the formation of a semantic network that reflects this knowledge .	CL_D14-1031_pa_3	CL_D14-1031_pa_2	CL_D14-1031_pa_2_3
CL	D14-1031	pa	2	4	motivation_background	conclusion	support	support	secondary	secondary	none	none	A presumed outcome of semantic development is the formation of a semantic network that reflects this knowledge .	We demonstrate that the semantic connections among words in addition to their context is necessary in forming a semantic network that resembles an adult's semantic knowledge .	CL_D14-1031_pa_2	CL_D14-1031_pa_4	CL_D14-1031_pa_2_4
CL	D14-1031	pa	3	4	proposal	conclusion	none	support	main	secondary	back	support	We present an algorithm for simultaneously learning word meanings and gradually growing a semantic network , which adheres to the cognitive plausibility requirements of incrementality and limited computations .	We demonstrate that the semantic connections among words in addition to their context is necessary in forming a semantic network that resembles an adult's semantic knowledge .	CL_D14-1031_pa_3	CL_D14-1031_pa_4	CL_D14-1031_pa_3_4
CL	D14-1031	pa	4	3	conclusion	proposal	support	none	secondary	main	forw	support	We demonstrate that the semantic connections among words in addition to their context is necessary in forming a semantic network that resembles an adult's semantic knowledge .	We present an algorithm for simultaneously learning word meanings and gradually growing a semantic network , which adheres to the cognitive plausibility requirements of incrementality and limited computations .	CL_D14-1031_pa_4	CL_D14-1031_pa_3	CL_D14-1031_pa_3_4
CL	D14-1032	pa	1	2	motivation_background	motivation_background	info-required	info-required	secondary	secondary	forw	info-required	Models that acquire semantic representations from both linguistic and perceptual input are of interest to researchers in NLP because of the obvious parallels with human language learning .	Performance advantages of the multi-modal approach over language-only models have been clearly established when models are required to learn concrete noun concepts .	CL_D14-1032_pa_1	CL_D14-1032_pa_2	CL_D14-1032_pa_1_2
CL	D14-1032	pa	2	1	motivation_background	motivation_background	info-required	info-required	secondary	secondary	back	info-required	Performance advantages of the multi-modal approach over language-only models have been clearly established when models are required to learn concrete noun concepts .	Models that acquire semantic representations from both linguistic and perceptual input are of interest to researchers in NLP because of the obvious parallels with human language learning .	CL_D14-1032_pa_2	CL_D14-1032_pa_1	CL_D14-1032_pa_1_2
CL	D14-1032	pa	1	3	motivation_background	motivation_problem	info-required	support	secondary	secondary	none	none	Models that acquire semantic representations from both linguistic and perceptual input are of interest to researchers in NLP because of the obvious parallels with human language learning .	However , such concepts are comparatively rare in everyday language .	CL_D14-1032_pa_1	CL_D14-1032_pa_3	CL_D14-1032_pa_1_3
CL	D14-1032	pa	1	4	motivation_background	proposal	info-required	none	secondary	main	none	none	Models that acquire semantic representations from both linguistic and perceptual input are of interest to researchers in NLP because of the obvious parallels with human language learning .	In this work , we present a new means of extending the scope of multi-modal models to more commonly-occurring abstract lexical concepts via an approach that learns multi-modal embeddings .	CL_D14-1032_pa_1	CL_D14-1032_pa_4	CL_D14-1032_pa_1_4
CL	D14-1032	pa	1	5	motivation_background	result	info-required	support	secondary	secondary	none	none	Models that acquire semantic representations from both linguistic and perceptual input are of interest to researchers in NLP because of the obvious parallels with human language learning .	Our architecture outperforms previous approaches in combining input from distinct modalities , and propagates perceptual information on concrete concepts to abstract concepts more effectively than alternatives .	CL_D14-1032_pa_1	CL_D14-1032_pa_5	CL_D14-1032_pa_1_5
CL	D14-1032	pa	1	6	motivation_background	proposal	info-required	elaboration	secondary	secondary	none	none	Models that acquire semantic representations from both linguistic and perceptual input are of interest to researchers in NLP because of the obvious parallels with human language learning .	We discuss the implications of our results both for optimizing the performance of multi-modal models and for theories of abstract conceptual representation .	CL_D14-1032_pa_1	CL_D14-1032_pa_6	CL_D14-1032_pa_1_6
CL	D14-1032	pa	2	3	motivation_background	motivation_problem	info-required	support	secondary	secondary	forw	info-required	Performance advantages of the multi-modal approach over language-only models have been clearly established when models are required to learn concrete noun concepts .	However , such concepts are comparatively rare in everyday language .	CL_D14-1032_pa_2	CL_D14-1032_pa_3	CL_D14-1032_pa_2_3
CL	D14-1032	pa	3	2	motivation_problem	motivation_background	support	info-required	secondary	secondary	back	info-required	However , such concepts are comparatively rare in everyday language .	Performance advantages of the multi-modal approach over language-only models have been clearly established when models are required to learn concrete noun concepts .	CL_D14-1032_pa_3	CL_D14-1032_pa_2	CL_D14-1032_pa_2_3
CL	D14-1032	pa	2	4	motivation_background	proposal	info-required	none	secondary	main	none	none	Performance advantages of the multi-modal approach over language-only models have been clearly established when models are required to learn concrete noun concepts .	In this work , we present a new means of extending the scope of multi-modal models to more commonly-occurring abstract lexical concepts via an approach that learns multi-modal embeddings .	CL_D14-1032_pa_2	CL_D14-1032_pa_4	CL_D14-1032_pa_2_4
CL	D14-1032	pa	2	5	motivation_background	result	info-required	support	secondary	secondary	none	none	Performance advantages of the multi-modal approach over language-only models have been clearly established when models are required to learn concrete noun concepts .	Our architecture outperforms previous approaches in combining input from distinct modalities , and propagates perceptual information on concrete concepts to abstract concepts more effectively than alternatives .	CL_D14-1032_pa_2	CL_D14-1032_pa_5	CL_D14-1032_pa_2_5
CL	D14-1032	pa	2	6	motivation_background	proposal	info-required	elaboration	secondary	secondary	none	none	Performance advantages of the multi-modal approach over language-only models have been clearly established when models are required to learn concrete noun concepts .	We discuss the implications of our results both for optimizing the performance of multi-modal models and for theories of abstract conceptual representation .	CL_D14-1032_pa_2	CL_D14-1032_pa_6	CL_D14-1032_pa_2_6
CL	D14-1032	pa	3	4	motivation_problem	proposal	support	none	secondary	main	forw	support	However , such concepts are comparatively rare in everyday language .	In this work , we present a new means of extending the scope of multi-modal models to more commonly-occurring abstract lexical concepts via an approach that learns multi-modal embeddings .	CL_D14-1032_pa_3	CL_D14-1032_pa_4	CL_D14-1032_pa_3_4
CL	D14-1032	pa	4	3	proposal	motivation_problem	none	support	main	secondary	back	support	In this work , we present a new means of extending the scope of multi-modal models to more commonly-occurring abstract lexical concepts via an approach that learns multi-modal embeddings .	However , such concepts are comparatively rare in everyday language .	CL_D14-1032_pa_4	CL_D14-1032_pa_3	CL_D14-1032_pa_3_4
CL	D14-1032	pa	3	5	motivation_problem	result	support	support	secondary	secondary	none	none	However , such concepts are comparatively rare in everyday language .	Our architecture outperforms previous approaches in combining input from distinct modalities , and propagates perceptual information on concrete concepts to abstract concepts more effectively than alternatives .	CL_D14-1032_pa_3	CL_D14-1032_pa_5	CL_D14-1032_pa_3_5
CL	D14-1032	pa	3	6	motivation_problem	proposal	support	elaboration	secondary	secondary	none	none	However , such concepts are comparatively rare in everyday language .	We discuss the implications of our results both for optimizing the performance of multi-modal models and for theories of abstract conceptual representation .	CL_D14-1032_pa_3	CL_D14-1032_pa_6	CL_D14-1032_pa_3_6
CL	D14-1032	pa	4	5	proposal	result	none	support	main	secondary	back	support	In this work , we present a new means of extending the scope of multi-modal models to more commonly-occurring abstract lexical concepts via an approach that learns multi-modal embeddings .	Our architecture outperforms previous approaches in combining input from distinct modalities , and propagates perceptual information on concrete concepts to abstract concepts more effectively than alternatives .	CL_D14-1032_pa_4	CL_D14-1032_pa_5	CL_D14-1032_pa_4_5
CL	D14-1032	pa	5	4	result	proposal	support	none	secondary	main	forw	support	Our architecture outperforms previous approaches in combining input from distinct modalities , and propagates perceptual information on concrete concepts to abstract concepts more effectively than alternatives .	In this work , we present a new means of extending the scope of multi-modal models to more commonly-occurring abstract lexical concepts via an approach that learns multi-modal embeddings .	CL_D14-1032_pa_5	CL_D14-1032_pa_4	CL_D14-1032_pa_4_5
CL	D14-1032	pa	4	6	proposal	proposal	none	elaboration	main	secondary	back	elaboration	In this work , we present a new means of extending the scope of multi-modal models to more commonly-occurring abstract lexical concepts via an approach that learns multi-modal embeddings .	We discuss the implications of our results both for optimizing the performance of multi-modal models and for theories of abstract conceptual representation .	CL_D14-1032_pa_4	CL_D14-1032_pa_6	CL_D14-1032_pa_4_6
CL	D14-1032	pa	6	4	proposal	proposal	elaboration	none	secondary	main	forw	elaboration	We discuss the implications of our results both for optimizing the performance of multi-modal models and for theories of abstract conceptual representation .	In this work , we present a new means of extending the scope of multi-modal models to more commonly-occurring abstract lexical concepts via an approach that learns multi-modal embeddings .	CL_D14-1032_pa_6	CL_D14-1032_pa_4	CL_D14-1032_pa_4_6
CL	D14-1032	pa	5	6	result	proposal	support	elaboration	secondary	secondary	none	none	Our architecture outperforms previous approaches in combining input from distinct modalities , and propagates perceptual information on concrete concepts to abstract concepts more effectively than alternatives .	We discuss the implications of our results both for optimizing the performance of multi-modal models and for theories of abstract conceptual representation .	CL_D14-1032_pa_5	CL_D14-1032_pa_6	CL_D14-1032_pa_5_6
CL	D14-1033	pa	1	2	motivation_background	proposal	support	none	secondary	main	forw	support	State-of-art systems for grammar error correction often correct errors based on word sequences or phrases .	In this paper , we describe a grammar error correction system which corrects grammatical errors at tree level directly .	CL_D14-1033_pa_1	CL_D14-1033_pa_2	CL_D14-1033_pa_1_2
CL	D14-1033	pa	2	1	proposal	motivation_background	none	support	main	secondary	back	support	In this paper , we describe a grammar error correction system which corrects grammatical errors at tree level directly .	State-of-art systems for grammar error correction often correct errors based on word sequences or phrases .	CL_D14-1033_pa_2	CL_D14-1033_pa_1	CL_D14-1033_pa_1_2
CL	D14-1033	pa	1	3	motivation_background	proposal_implementation	support	elaboration	secondary	secondary	none	none	State-of-art systems for grammar error correction often correct errors based on word sequences or phrases .	We cluster all error into two groups and divide our system into two modules correspondingly : the general module and the special module .	CL_D14-1033_pa_1	CL_D14-1033_pa_3	CL_D14-1033_pa_1_3
CL	D14-1033	pa	1	4	motivation_background	proposal_implementation	support	elaboration	secondary	secondary	none	none	State-of-art systems for grammar error correction often correct errors based on word sequences or phrases .	In the general module , we propose a TreeNode Language Model to correct errors related to verbs and nouns .	CL_D14-1033_pa_1	CL_D14-1033_pa_4	CL_D14-1033_pa_1_4
CL	D14-1033	pa	1	5	motivation_background	conclusion	support	support	secondary	secondary	none	none	State-of-art systems for grammar error correction often correct errors based on word sequences or phrases .	The TreeNode Language Model is easy to train and the decoding is efficient .	CL_D14-1033_pa_1	CL_D14-1033_pa_5	CL_D14-1033_pa_1_5
CL	D14-1033	pa	1	6	motivation_background	proposal_implementation	support	elaboration	secondary	secondary	none	none	State-of-art systems for grammar error correction often correct errors based on word sequences or phrases .	In the special module , two extra classification models are trained to correct errors related to determiners and prepositions .	CL_D14-1033_pa_1	CL_D14-1033_pa_6	CL_D14-1033_pa_1_6
CL	D14-1033	pa	1	7	motivation_background	result	support	support	secondary	secondary	none	none	State-of-art systems for grammar error correction often correct errors based on word sequences or phrases .	Experiments show that our system outperforms the state-of-art systems and improves the F1 score .	CL_D14-1033_pa_1	CL_D14-1033_pa_7	CL_D14-1033_pa_1_7
CL	D14-1033	pa	2	3	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	In this paper , we describe a grammar error correction system which corrects grammatical errors at tree level directly .	We cluster all error into two groups and divide our system into two modules correspondingly : the general module and the special module .	CL_D14-1033_pa_2	CL_D14-1033_pa_3	CL_D14-1033_pa_2_3
CL	D14-1033	pa	3	2	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	We cluster all error into two groups and divide our system into two modules correspondingly : the general module and the special module .	In this paper , we describe a grammar error correction system which corrects grammatical errors at tree level directly .	CL_D14-1033_pa_3	CL_D14-1033_pa_2	CL_D14-1033_pa_2_3
CL	D14-1033	pa	2	4	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	In this paper , we describe a grammar error correction system which corrects grammatical errors at tree level directly .	In the general module , we propose a TreeNode Language Model to correct errors related to verbs and nouns .	CL_D14-1033_pa_2	CL_D14-1033_pa_4	CL_D14-1033_pa_2_4
CL	D14-1033	pa	2	5	proposal	conclusion	none	support	main	secondary	none	none	In this paper , we describe a grammar error correction system which corrects grammatical errors at tree level directly .	The TreeNode Language Model is easy to train and the decoding is efficient .	CL_D14-1033_pa_2	CL_D14-1033_pa_5	CL_D14-1033_pa_2_5
CL	D14-1033	pa	2	6	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	In this paper , we describe a grammar error correction system which corrects grammatical errors at tree level directly .	In the special module , two extra classification models are trained to correct errors related to determiners and prepositions .	CL_D14-1033_pa_2	CL_D14-1033_pa_6	CL_D14-1033_pa_2_6
CL	D14-1033	pa	2	7	proposal	result	none	support	main	secondary	back	support	In this paper , we describe a grammar error correction system which corrects grammatical errors at tree level directly .	Experiments show that our system outperforms the state-of-art systems and improves the F1 score .	CL_D14-1033_pa_2	CL_D14-1033_pa_7	CL_D14-1033_pa_2_7
CL	D14-1033	pa	7	2	result	proposal	support	none	secondary	main	forw	support	Experiments show that our system outperforms the state-of-art systems and improves the F1 score .	In this paper , we describe a grammar error correction system which corrects grammatical errors at tree level directly .	CL_D14-1033_pa_7	CL_D14-1033_pa_2	CL_D14-1033_pa_2_7
CL	D14-1033	pa	3	4	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	We cluster all error into two groups and divide our system into two modules correspondingly : the general module and the special module .	In the general module , we propose a TreeNode Language Model to correct errors related to verbs and nouns .	CL_D14-1033_pa_3	CL_D14-1033_pa_4	CL_D14-1033_pa_3_4
CL	D14-1033	pa	4	3	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	forw	elaboration	In the general module , we propose a TreeNode Language Model to correct errors related to verbs and nouns .	We cluster all error into two groups and divide our system into two modules correspondingly : the general module and the special module .	CL_D14-1033_pa_4	CL_D14-1033_pa_3	CL_D14-1033_pa_3_4
CL	D14-1033	pa	3	5	proposal_implementation	conclusion	elaboration	support	secondary	secondary	none	none	We cluster all error into two groups and divide our system into two modules correspondingly : the general module and the special module .	The TreeNode Language Model is easy to train and the decoding is efficient .	CL_D14-1033_pa_3	CL_D14-1033_pa_5	CL_D14-1033_pa_3_5
CL	D14-1033	pa	3	6	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	We cluster all error into two groups and divide our system into two modules correspondingly : the general module and the special module .	In the special module , two extra classification models are trained to correct errors related to determiners and prepositions .	CL_D14-1033_pa_3	CL_D14-1033_pa_6	CL_D14-1033_pa_3_6
CL	D14-1033	pa	6	3	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	forw	elaboration	In the special module , two extra classification models are trained to correct errors related to determiners and prepositions .	We cluster all error into two groups and divide our system into two modules correspondingly : the general module and the special module .	CL_D14-1033_pa_6	CL_D14-1033_pa_3	CL_D14-1033_pa_3_6
CL	D14-1033	pa	3	7	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	We cluster all error into two groups and divide our system into two modules correspondingly : the general module and the special module .	Experiments show that our system outperforms the state-of-art systems and improves the F1 score .	CL_D14-1033_pa_3	CL_D14-1033_pa_7	CL_D14-1033_pa_3_7
CL	D14-1033	pa	4	5	proposal_implementation	conclusion	elaboration	support	secondary	secondary	back	support	In the general module , we propose a TreeNode Language Model to correct errors related to verbs and nouns .	The TreeNode Language Model is easy to train and the decoding is efficient .	CL_D14-1033_pa_4	CL_D14-1033_pa_5	CL_D14-1033_pa_4_5
CL	D14-1033	pa	5	4	conclusion	proposal_implementation	support	elaboration	secondary	secondary	forw	support	The TreeNode Language Model is easy to train and the decoding is efficient .	In the general module , we propose a TreeNode Language Model to correct errors related to verbs and nouns .	CL_D14-1033_pa_5	CL_D14-1033_pa_4	CL_D14-1033_pa_4_5
CL	D14-1033	pa	4	6	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	none	none	In the general module , we propose a TreeNode Language Model to correct errors related to verbs and nouns .	In the special module , two extra classification models are trained to correct errors related to determiners and prepositions .	CL_D14-1033_pa_4	CL_D14-1033_pa_6	CL_D14-1033_pa_4_6
CL	D14-1033	pa	4	7	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	In the general module , we propose a TreeNode Language Model to correct errors related to verbs and nouns .	Experiments show that our system outperforms the state-of-art systems and improves the F1 score .	CL_D14-1033_pa_4	CL_D14-1033_pa_7	CL_D14-1033_pa_4_7
CL	D14-1033	pa	5	6	conclusion	proposal_implementation	support	elaboration	secondary	secondary	none	none	The TreeNode Language Model is easy to train and the decoding is efficient .	In the special module , two extra classification models are trained to correct errors related to determiners and prepositions .	CL_D14-1033_pa_5	CL_D14-1033_pa_6	CL_D14-1033_pa_5_6
CL	D14-1033	pa	5	7	conclusion	result	support	support	secondary	secondary	none	none	The TreeNode Language Model is easy to train and the decoding is efficient .	Experiments show that our system outperforms the state-of-art systems and improves the F1 score .	CL_D14-1033_pa_5	CL_D14-1033_pa_7	CL_D14-1033_pa_5_7
CL	D14-1033	pa	6	7	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	In the special module , two extra classification models are trained to correct errors related to determiners and prepositions .	Experiments show that our system outperforms the state-of-art systems and improves the F1 score .	CL_D14-1033_pa_6	CL_D14-1033_pa_7	CL_D14-1033_pa_6_7
CL	D14-1034	pa	1	2	motivation_background	motivation_problem	info-required	support	secondary	secondary	forw	info-required	Most existing systems for subcategorization frame ( SCF ) acquisition rely on supervised parsing and infer SCF distributions at type , rather than instance level .	These systems suffer from poor portability across domains and their benefit for NLP tasks that involve sentence-level processing is limited .	CL_D14-1034_pa_1	CL_D14-1034_pa_2	CL_D14-1034_pa_1_2
CL	D14-1034	pa	2	1	motivation_problem	motivation_background	support	info-required	secondary	secondary	back	info-required	These systems suffer from poor portability across domains and their benefit for NLP tasks that involve sentence-level processing is limited .	Most existing systems for subcategorization frame ( SCF ) acquisition rely on supervised parsing and infer SCF distributions at type , rather than instance level .	CL_D14-1034_pa_2	CL_D14-1034_pa_1	CL_D14-1034_pa_1_2
CL	D14-1034	pa	1	3	motivation_background	proposal	info-required	none	secondary	main	none	none	Most existing systems for subcategorization frame ( SCF ) acquisition rely on supervised parsing and infer SCF distributions at type , rather than instance level .	We propose a new unsupervised , Markov Random Field-based model for SCF acquisition which is designed to address these problems .	CL_D14-1034_pa_1	CL_D14-1034_pa_3	CL_D14-1034_pa_1_3
CL	D14-1034	pa	1	4	motivation_background	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	Most existing systems for subcategorization frame ( SCF ) acquisition rely on supervised parsing and infer SCF distributions at type , rather than instance level .	The system relies on supervised POS tagging rather than parsing , and is capable of learning SCFs at instance level .	CL_D14-1034_pa_1	CL_D14-1034_pa_4	CL_D14-1034_pa_1_4
CL	D14-1034	pa	1	5	motivation_background	result_means	info-required	support	secondary	secondary	none	none	Most existing systems for subcategorization frame ( SCF ) acquisition rely on supervised parsing and infer SCF distributions at type , rather than instance level .	We perform evaluation against gold standard data which shows that our system outperforms several supervised and type-level SCF baselines .	CL_D14-1034_pa_1	CL_D14-1034_pa_5	CL_D14-1034_pa_1_5
CL	D14-1034	pa	1	6	motivation_background	result_means	info-required	support	secondary	secondary	none	none	Most existing systems for subcategorization frame ( SCF ) acquisition rely on supervised parsing and infer SCF distributions at type , rather than instance level .	We also conduct task-based evaluation in the context of verb similarity prediction , demonstrating that a vector space model based on our SCFs substantially outperforms a lexical model and a model based on a supervised parser .	CL_D14-1034_pa_1	CL_D14-1034_pa_6	CL_D14-1034_pa_1_6
CL	D14-1034	pa	2	3	motivation_problem	proposal	support	none	secondary	main	forw	support	These systems suffer from poor portability across domains and their benefit for NLP tasks that involve sentence-level processing is limited .	We propose a new unsupervised , Markov Random Field-based model for SCF acquisition which is designed to address these problems .	CL_D14-1034_pa_2	CL_D14-1034_pa_3	CL_D14-1034_pa_2_3
CL	D14-1034	pa	3	2	proposal	motivation_problem	none	support	main	secondary	back	support	We propose a new unsupervised , Markov Random Field-based model for SCF acquisition which is designed to address these problems .	These systems suffer from poor portability across domains and their benefit for NLP tasks that involve sentence-level processing is limited .	CL_D14-1034_pa_3	CL_D14-1034_pa_2	CL_D14-1034_pa_2_3
CL	D14-1034	pa	2	4	motivation_problem	proposal_implementation	support	elaboration	secondary	secondary	none	none	These systems suffer from poor portability across domains and their benefit for NLP tasks that involve sentence-level processing is limited .	The system relies on supervised POS tagging rather than parsing , and is capable of learning SCFs at instance level .	CL_D14-1034_pa_2	CL_D14-1034_pa_4	CL_D14-1034_pa_2_4
CL	D14-1034	pa	2	5	motivation_problem	result_means	support	support	secondary	secondary	none	none	These systems suffer from poor portability across domains and their benefit for NLP tasks that involve sentence-level processing is limited .	We perform evaluation against gold standard data which shows that our system outperforms several supervised and type-level SCF baselines .	CL_D14-1034_pa_2	CL_D14-1034_pa_5	CL_D14-1034_pa_2_5
CL	D14-1034	pa	2	6	motivation_problem	result_means	support	support	secondary	secondary	none	none	These systems suffer from poor portability across domains and their benefit for NLP tasks that involve sentence-level processing is limited .	We also conduct task-based evaluation in the context of verb similarity prediction , demonstrating that a vector space model based on our SCFs substantially outperforms a lexical model and a model based on a supervised parser .	CL_D14-1034_pa_2	CL_D14-1034_pa_6	CL_D14-1034_pa_2_6
CL	D14-1034	pa	3	4	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	We propose a new unsupervised , Markov Random Field-based model for SCF acquisition which is designed to address these problems .	The system relies on supervised POS tagging rather than parsing , and is capable of learning SCFs at instance level .	CL_D14-1034_pa_3	CL_D14-1034_pa_4	CL_D14-1034_pa_3_4
CL	D14-1034	pa	4	3	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	The system relies on supervised POS tagging rather than parsing , and is capable of learning SCFs at instance level .	We propose a new unsupervised , Markov Random Field-based model for SCF acquisition which is designed to address these problems .	CL_D14-1034_pa_4	CL_D14-1034_pa_3	CL_D14-1034_pa_3_4
CL	D14-1034	pa	3	5	proposal	result_means	none	support	main	secondary	back	support	We propose a new unsupervised , Markov Random Field-based model for SCF acquisition which is designed to address these problems .	We perform evaluation against gold standard data which shows that our system outperforms several supervised and type-level SCF baselines .	CL_D14-1034_pa_3	CL_D14-1034_pa_5	CL_D14-1034_pa_3_5
CL	D14-1034	pa	5	3	result_means	proposal	support	none	secondary	main	forw	support	We perform evaluation against gold standard data which shows that our system outperforms several supervised and type-level SCF baselines .	We propose a new unsupervised , Markov Random Field-based model for SCF acquisition which is designed to address these problems .	CL_D14-1034_pa_5	CL_D14-1034_pa_3	CL_D14-1034_pa_3_5
CL	D14-1034	pa	3	6	proposal	result_means	none	support	main	secondary	back	support	We propose a new unsupervised , Markov Random Field-based model for SCF acquisition which is designed to address these problems .	We also conduct task-based evaluation in the context of verb similarity prediction , demonstrating that a vector space model based on our SCFs substantially outperforms a lexical model and a model based on a supervised parser .	CL_D14-1034_pa_3	CL_D14-1034_pa_6	CL_D14-1034_pa_3_6
CL	D14-1034	pa	6	3	result_means	proposal	support	none	secondary	main	forw	support	We also conduct task-based evaluation in the context of verb similarity prediction , demonstrating that a vector space model based on our SCFs substantially outperforms a lexical model and a model based on a supervised parser .	We propose a new unsupervised , Markov Random Field-based model for SCF acquisition which is designed to address these problems .	CL_D14-1034_pa_6	CL_D14-1034_pa_3	CL_D14-1034_pa_3_6
CL	D14-1034	pa	4	5	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	The system relies on supervised POS tagging rather than parsing , and is capable of learning SCFs at instance level .	We perform evaluation against gold standard data which shows that our system outperforms several supervised and type-level SCF baselines .	CL_D14-1034_pa_4	CL_D14-1034_pa_5	CL_D14-1034_pa_4_5
CL	D14-1034	pa	4	6	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	The system relies on supervised POS tagging rather than parsing , and is capable of learning SCFs at instance level .	We also conduct task-based evaluation in the context of verb similarity prediction , demonstrating that a vector space model based on our SCFs substantially outperforms a lexical model and a model based on a supervised parser .	CL_D14-1034_pa_4	CL_D14-1034_pa_6	CL_D14-1034_pa_4_6
CL	D14-1034	pa	5	6	result_means	result_means	support	support	secondary	secondary	none	none	We perform evaluation against gold standard data which shows that our system outperforms several supervised and type-level SCF baselines .	We also conduct task-based evaluation in the context of verb similarity prediction , demonstrating that a vector space model based on our SCFs substantially outperforms a lexical model and a model based on a supervised parser .	CL_D14-1034_pa_5	CL_D14-1034_pa_6	CL_D14-1034_pa_5_6
CL	D14-1035	pa	1	2	motivation_background	proposal	support	none	secondary	main	forw	support	PCFGs with latent annotations have been shown to be a very effective model for phrase structure parsing .	We present a Bayesian model and algorithms based on a Gibbs sampler for parsing with a grammar with latent annotations .	CL_D14-1035_pa_1	CL_D14-1035_pa_2	CL_D14-1035_pa_1_2
CL	D14-1035	pa	2	1	proposal	motivation_background	none	support	main	secondary	back	support	We present a Bayesian model and algorithms based on a Gibbs sampler for parsing with a grammar with latent annotations .	PCFGs with latent annotations have been shown to be a very effective model for phrase structure parsing .	CL_D14-1035_pa_2	CL_D14-1035_pa_1	CL_D14-1035_pa_1_2
CL	D14-1035	pa	1	3	motivation_background	proposal_implementation	support	elaboration	secondary	secondary	none	none	PCFGs with latent annotations have been shown to be a very effective model for phrase structure parsing .	For PCFG-LA , we present an additional Gibbs sampler algorithm to learn annotations from training data , which are parse trees with coarse ( unannotated ) symbols .	CL_D14-1035_pa_1	CL_D14-1035_pa_3	CL_D14-1035_pa_1_3
CL	D14-1035	pa	1	4	motivation_background	conclusion	support	support	secondary	secondary	none	none	PCFGs with latent annotations have been shown to be a very effective model for phrase structure parsing .	We show that a Gibbs sampling technique is capable of parsing sentences in a wide variety of languages and producing results that are on-par with or surpass previous approaches .	CL_D14-1035_pa_1	CL_D14-1035_pa_4	CL_D14-1035_pa_1_4
CL	D14-1035	pa	1	5	motivation_background	result_means	support	support	secondary	secondary	none	none	PCFGs with latent annotations have been shown to be a very effective model for phrase structure parsing .	Our results for Kinyarwanda and Malagasy in particular demonstrate that low-resource language parsing can benefit substantially from a Bayesian approach .	CL_D14-1035_pa_1	CL_D14-1035_pa_5	CL_D14-1035_pa_1_5
CL	D14-1035	pa	2	3	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	We present a Bayesian model and algorithms based on a Gibbs sampler for parsing with a grammar with latent annotations .	For PCFG-LA , we present an additional Gibbs sampler algorithm to learn annotations from training data , which are parse trees with coarse ( unannotated ) symbols .	CL_D14-1035_pa_2	CL_D14-1035_pa_3	CL_D14-1035_pa_2_3
CL	D14-1035	pa	3	2	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	For PCFG-LA , we present an additional Gibbs sampler algorithm to learn annotations from training data , which are parse trees with coarse ( unannotated ) symbols .	We present a Bayesian model and algorithms based on a Gibbs sampler for parsing with a grammar with latent annotations .	CL_D14-1035_pa_3	CL_D14-1035_pa_2	CL_D14-1035_pa_2_3
CL	D14-1035	pa	2	4	proposal	conclusion	none	support	main	secondary	back	support	We present a Bayesian model and algorithms based on a Gibbs sampler for parsing with a grammar with latent annotations .	We show that a Gibbs sampling technique is capable of parsing sentences in a wide variety of languages and producing results that are on-par with or surpass previous approaches .	CL_D14-1035_pa_2	CL_D14-1035_pa_4	CL_D14-1035_pa_2_4
CL	D14-1035	pa	4	2	conclusion	proposal	support	none	secondary	main	forw	support	We show that a Gibbs sampling technique is capable of parsing sentences in a wide variety of languages and producing results that are on-par with or surpass previous approaches .	We present a Bayesian model and algorithms based on a Gibbs sampler for parsing with a grammar with latent annotations .	CL_D14-1035_pa_4	CL_D14-1035_pa_2	CL_D14-1035_pa_2_4
CL	D14-1035	pa	2	5	proposal	result_means	none	support	main	secondary	none	none	We present a Bayesian model and algorithms based on a Gibbs sampler for parsing with a grammar with latent annotations .	Our results for Kinyarwanda and Malagasy in particular demonstrate that low-resource language parsing can benefit substantially from a Bayesian approach .	CL_D14-1035_pa_2	CL_D14-1035_pa_5	CL_D14-1035_pa_2_5
CL	D14-1035	pa	3	4	proposal_implementation	conclusion	elaboration	support	secondary	secondary	none	none	For PCFG-LA , we present an additional Gibbs sampler algorithm to learn annotations from training data , which are parse trees with coarse ( unannotated ) symbols .	We show that a Gibbs sampling technique is capable of parsing sentences in a wide variety of languages and producing results that are on-par with or surpass previous approaches .	CL_D14-1035_pa_3	CL_D14-1035_pa_4	CL_D14-1035_pa_3_4
CL	D14-1035	pa	3	5	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	For PCFG-LA , we present an additional Gibbs sampler algorithm to learn annotations from training data , which are parse trees with coarse ( unannotated ) symbols .	Our results for Kinyarwanda and Malagasy in particular demonstrate that low-resource language parsing can benefit substantially from a Bayesian approach .	CL_D14-1035_pa_3	CL_D14-1035_pa_5	CL_D14-1035_pa_3_5
CL	D14-1035	pa	4	5	conclusion	result_means	support	support	secondary	secondary	back	support	We show that a Gibbs sampling technique is capable of parsing sentences in a wide variety of languages and producing results that are on-par with or surpass previous approaches .	Our results for Kinyarwanda and Malagasy in particular demonstrate that low-resource language parsing can benefit substantially from a Bayesian approach .	CL_D14-1035_pa_4	CL_D14-1035_pa_5	CL_D14-1035_pa_4_5
CL	D14-1035	pa	5	4	result_means	conclusion	support	support	secondary	secondary	forw	support	Our results for Kinyarwanda and Malagasy in particular demonstrate that low-resource language parsing can benefit substantially from a Bayesian approach .	We show that a Gibbs sampling technique is capable of parsing sentences in a wide variety of languages and producing results that are on-par with or surpass previous approaches .	CL_D14-1035_pa_5	CL_D14-1035_pa_4	CL_D14-1035_pa_4_5
CL	D14-1036	pa	1	2	proposal	information_additional	none	info-optional	main	secondary	back	info-optional	We introduce the task of incremental semantic role labeling ( iSRL ) , in which semantic roles are assigned to incomplete input ( sentence prefixes ) .	iSRL is the semantic equivalent of incremental parsing , and is useful for language modeling , sentence completion , machine translation , and psycholinguistic modeling .	CL_D14-1036_pa_1	CL_D14-1036_pa_2	CL_D14-1036_pa_1_2
CL	D14-1036	pa	2	1	information_additional	proposal	info-optional	none	secondary	main	forw	info-optional	iSRL is the semantic equivalent of incremental parsing , and is useful for language modeling , sentence completion , machine translation , and psycholinguistic modeling .	We introduce the task of incremental semantic role labeling ( iSRL ) , in which semantic roles are assigned to incomplete input ( sentence prefixes ) .	CL_D14-1036_pa_2	CL_D14-1036_pa_1	CL_D14-1036_pa_1_2
CL	D14-1036	pa	1	3	proposal	proposal	none	elaboration	main	secondary	back	elaboration	We introduce the task of incremental semantic role labeling ( iSRL ) , in which semantic roles are assigned to incomplete input ( sentence prefixes ) .	We propose an iSRL system that combines an incremental TAG parser with a semantically enriched lexicon , a role propagation algorithm , and a cascade of classifiers .	CL_D14-1036_pa_1	CL_D14-1036_pa_3	CL_D14-1036_pa_1_3
CL	D14-1036	pa	3	1	proposal	proposal	elaboration	none	secondary	main	forw	elaboration	We propose an iSRL system that combines an incremental TAG parser with a semantically enriched lexicon , a role propagation algorithm , and a cascade of classifiers .	We introduce the task of incremental semantic role labeling ( iSRL ) , in which semantic roles are assigned to incomplete input ( sentence prefixes ) .	CL_D14-1036_pa_3	CL_D14-1036_pa_1	CL_D14-1036_pa_1_3
CL	D14-1036	pa	1	4	proposal	observation	none	support	main	secondary	none	none	We introduce the task of incremental semantic role labeling ( iSRL ) , in which semantic roles are assigned to incomplete input ( sentence prefixes ) .	Our approach achieves an SRL F-score of 78.38 % on the standard CoNLL 2009 dataset .	CL_D14-1036_pa_1	CL_D14-1036_pa_4	CL_D14-1036_pa_1_4
CL	D14-1036	pa	1	5	proposal	result_means	none	support	main	secondary	none	none	We introduce the task of incremental semantic role labeling ( iSRL ) , in which semantic roles are assigned to incomplete input ( sentence prefixes ) .	It substantially outperforms a strong baseline that combines gold-standard syntactic dependencies with heuristic role assignment , as well as a baseline based on Nivre's incremental dependency parser .	CL_D14-1036_pa_1	CL_D14-1036_pa_5	CL_D14-1036_pa_1_5
CL	D14-1036	pa	2	3	information_additional	proposal	info-optional	elaboration	secondary	secondary	none	none	iSRL is the semantic equivalent of incremental parsing , and is useful for language modeling , sentence completion , machine translation , and psycholinguistic modeling .	We propose an iSRL system that combines an incremental TAG parser with a semantically enriched lexicon , a role propagation algorithm , and a cascade of classifiers .	CL_D14-1036_pa_2	CL_D14-1036_pa_3	CL_D14-1036_pa_2_3
CL	D14-1036	pa	2	4	information_additional	observation	info-optional	support	secondary	secondary	none	none	iSRL is the semantic equivalent of incremental parsing , and is useful for language modeling , sentence completion , machine translation , and psycholinguistic modeling .	Our approach achieves an SRL F-score of 78.38 % on the standard CoNLL 2009 dataset .	CL_D14-1036_pa_2	CL_D14-1036_pa_4	CL_D14-1036_pa_2_4
CL	D14-1036	pa	2	5	information_additional	result_means	info-optional	support	secondary	secondary	none	none	iSRL is the semantic equivalent of incremental parsing , and is useful for language modeling , sentence completion , machine translation , and psycholinguistic modeling .	It substantially outperforms a strong baseline that combines gold-standard syntactic dependencies with heuristic role assignment , as well as a baseline based on Nivre's incremental dependency parser .	CL_D14-1036_pa_2	CL_D14-1036_pa_5	CL_D14-1036_pa_2_5
CL	D14-1036	pa	3	4	proposal	observation	elaboration	support	secondary	secondary	none	none	We propose an iSRL system that combines an incremental TAG parser with a semantically enriched lexicon , a role propagation algorithm , and a cascade of classifiers .	Our approach achieves an SRL F-score of 78.38 % on the standard CoNLL 2009 dataset .	CL_D14-1036_pa_3	CL_D14-1036_pa_4	CL_D14-1036_pa_3_4
CL	D14-1036	pa	3	5	proposal	result_means	elaboration	support	secondary	secondary	back	support	We propose an iSRL system that combines an incremental TAG parser with a semantically enriched lexicon , a role propagation algorithm , and a cascade of classifiers .	It substantially outperforms a strong baseline that combines gold-standard syntactic dependencies with heuristic role assignment , as well as a baseline based on Nivre's incremental dependency parser .	CL_D14-1036_pa_3	CL_D14-1036_pa_5	CL_D14-1036_pa_3_5
CL	D14-1036	pa	5	3	result_means	proposal	support	elaboration	secondary	secondary	forw	support	It substantially outperforms a strong baseline that combines gold-standard syntactic dependencies with heuristic role assignment , as well as a baseline based on Nivre's incremental dependency parser .	We propose an iSRL system that combines an incremental TAG parser with a semantically enriched lexicon , a role propagation algorithm , and a cascade of classifiers .	CL_D14-1036_pa_5	CL_D14-1036_pa_3	CL_D14-1036_pa_3_5
CL	D14-1036	pa	4	5	observation	result_means	support	support	secondary	secondary	forw	support	Our approach achieves an SRL F-score of 78.38 % on the standard CoNLL 2009 dataset .	It substantially outperforms a strong baseline that combines gold-standard syntactic dependencies with heuristic role assignment , as well as a baseline based on Nivre's incremental dependency parser .	CL_D14-1036_pa_4	CL_D14-1036_pa_5	CL_D14-1036_pa_4_5
CL	D14-1036	pa	5	4	result_means	observation	support	support	secondary	secondary	back	support	It substantially outperforms a strong baseline that combines gold-standard syntactic dependencies with heuristic role assignment , as well as a baseline based on Nivre's incremental dependency parser .	Our approach achieves an SRL F-score of 78.38 % on the standard CoNLL 2009 dataset .	CL_D14-1036_pa_5	CL_D14-1036_pa_4	CL_D14-1036_pa_4_5
CL	D14-1037	pa	1	2	motivation_problem	motivation_background	info-required	support	secondary	secondary	forw	info-required	The informal nature of social media text renders it very difficult to be automatically processed by natural language processing tools .	Text normalization , which corresponds to restoring the non-standard words to their canonical forms , provides a solution to this challenge .	CL_D14-1037_pa_1	CL_D14-1037_pa_2	CL_D14-1037_pa_1_2
CL	D14-1037	pa	2	1	motivation_background	motivation_problem	support	info-required	secondary	secondary	back	info-required	Text normalization , which corresponds to restoring the non-standard words to their canonical forms , provides a solution to this challenge .	The informal nature of social media text renders it very difficult to be automatically processed by natural language processing tools .	CL_D14-1037_pa_2	CL_D14-1037_pa_1	CL_D14-1037_pa_1_2
CL	D14-1037	pa	1	3	motivation_problem	proposal	info-required	none	secondary	main	none	none	The informal nature of social media text renders it very difficult to be automatically processed by natural language processing tools .	We introduce an unsupervised text normalization approach that utilizes not only lexical , but also contextual and grammatical features of social text .	CL_D14-1037_pa_1	CL_D14-1037_pa_3	CL_D14-1037_pa_1_3
CL	D14-1037	pa	1	4	motivation_problem	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	The informal nature of social media text renders it very difficult to be automatically processed by natural language processing tools .	The contextual and grammatical features are extracted from a word association graph built by using a large unlabeled social media text corpus .	CL_D14-1037_pa_1	CL_D14-1037_pa_4	CL_D14-1037_pa_1_4
CL	D14-1037	pa	1	5	motivation_problem	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	The informal nature of social media text renders it very difficult to be automatically processed by natural language processing tools .	The graph encodes the relative positions of the words with respect to each other , as well as their part-of-speech tags .	CL_D14-1037_pa_1	CL_D14-1037_pa_5	CL_D14-1037_pa_1_5
CL	D14-1037	pa	1	6	motivation_problem	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	The informal nature of social media text renders it very difficult to be automatically processed by natural language processing tools .	The lexical features are obtained by using the longest common subsequence ratio and edit distance measures to encode the surface similarity among words , and the double metaphone algorithm to represent the phonetic similarity .	CL_D14-1037_pa_1	CL_D14-1037_pa_6	CL_D14-1037_pa_1_6
CL	D14-1037	pa	1	7	motivation_problem	proposal	info-required	elaboration	secondary	secondary	none	none	The informal nature of social media text renders it very difficult to be automatically processed by natural language processing tools .	Unlike most of the recent approaches that are based on generating normalization dictionaries , the proposed approach performs normalization by considering the context of the non-standard words in the input text .	CL_D14-1037_pa_1	CL_D14-1037_pa_7	CL_D14-1037_pa_1_7
CL	D14-1037	pa	1	8	motivation_problem	result	info-required	support	secondary	secondary	none	none	The informal nature of social media text renders it very difficult to be automatically processed by natural language processing tools .	Our results show that it achieves state-of-the-art F-score performance on standard datasets .	CL_D14-1037_pa_1	CL_D14-1037_pa_8	CL_D14-1037_pa_1_8
CL	D14-1037	pa	1	9	motivation_problem	conclusion	info-required	support	secondary	secondary	none	none	The informal nature of social media text renders it very difficult to be automatically processed by natural language processing tools .	In addition , the system can be tuned to achieve very high precision without sacrificing much from recall .	CL_D14-1037_pa_1	CL_D14-1037_pa_9	CL_D14-1037_pa_1_9
CL	D14-1037	pa	2	3	motivation_background	proposal	support	none	secondary	main	forw	support	Text normalization , which corresponds to restoring the non-standard words to their canonical forms , provides a solution to this challenge .	We introduce an unsupervised text normalization approach that utilizes not only lexical , but also contextual and grammatical features of social text .	CL_D14-1037_pa_2	CL_D14-1037_pa_3	CL_D14-1037_pa_2_3
CL	D14-1037	pa	3	2	proposal	motivation_background	none	support	main	secondary	back	support	We introduce an unsupervised text normalization approach that utilizes not only lexical , but also contextual and grammatical features of social text .	Text normalization , which corresponds to restoring the non-standard words to their canonical forms , provides a solution to this challenge .	CL_D14-1037_pa_3	CL_D14-1037_pa_2	CL_D14-1037_pa_2_3
CL	D14-1037	pa	2	4	motivation_background	proposal_implementation	support	elaboration	secondary	secondary	none	none	Text normalization , which corresponds to restoring the non-standard words to their canonical forms , provides a solution to this challenge .	The contextual and grammatical features are extracted from a word association graph built by using a large unlabeled social media text corpus .	CL_D14-1037_pa_2	CL_D14-1037_pa_4	CL_D14-1037_pa_2_4
CL	D14-1037	pa	2	5	motivation_background	proposal_implementation	support	elaboration	secondary	secondary	none	none	Text normalization , which corresponds to restoring the non-standard words to their canonical forms , provides a solution to this challenge .	The graph encodes the relative positions of the words with respect to each other , as well as their part-of-speech tags .	CL_D14-1037_pa_2	CL_D14-1037_pa_5	CL_D14-1037_pa_2_5
CL	D14-1037	pa	2	6	motivation_background	proposal_implementation	support	elaboration	secondary	secondary	none	none	Text normalization , which corresponds to restoring the non-standard words to their canonical forms , provides a solution to this challenge .	The lexical features are obtained by using the longest common subsequence ratio and edit distance measures to encode the surface similarity among words , and the double metaphone algorithm to represent the phonetic similarity .	CL_D14-1037_pa_2	CL_D14-1037_pa_6	CL_D14-1037_pa_2_6
CL	D14-1037	pa	2	7	motivation_background	proposal	support	elaboration	secondary	secondary	none	none	Text normalization , which corresponds to restoring the non-standard words to their canonical forms , provides a solution to this challenge .	Unlike most of the recent approaches that are based on generating normalization dictionaries , the proposed approach performs normalization by considering the context of the non-standard words in the input text .	CL_D14-1037_pa_2	CL_D14-1037_pa_7	CL_D14-1037_pa_2_7
CL	D14-1037	pa	2	8	motivation_background	result	support	support	secondary	secondary	none	none	Text normalization , which corresponds to restoring the non-standard words to their canonical forms , provides a solution to this challenge .	Our results show that it achieves state-of-the-art F-score performance on standard datasets .	CL_D14-1037_pa_2	CL_D14-1037_pa_8	CL_D14-1037_pa_2_8
CL	D14-1037	pa	2	9	motivation_background	conclusion	support	support	secondary	secondary	none	none	Text normalization , which corresponds to restoring the non-standard words to their canonical forms , provides a solution to this challenge .	In addition , the system can be tuned to achieve very high precision without sacrificing much from recall .	CL_D14-1037_pa_2	CL_D14-1037_pa_9	CL_D14-1037_pa_2_9
CL	D14-1037	pa	3	4	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	We introduce an unsupervised text normalization approach that utilizes not only lexical , but also contextual and grammatical features of social text .	The contextual and grammatical features are extracted from a word association graph built by using a large unlabeled social media text corpus .	CL_D14-1037_pa_3	CL_D14-1037_pa_4	CL_D14-1037_pa_3_4
CL	D14-1037	pa	4	3	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	The contextual and grammatical features are extracted from a word association graph built by using a large unlabeled social media text corpus .	We introduce an unsupervised text normalization approach that utilizes not only lexical , but also contextual and grammatical features of social text .	CL_D14-1037_pa_4	CL_D14-1037_pa_3	CL_D14-1037_pa_3_4
CL	D14-1037	pa	3	5	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	We introduce an unsupervised text normalization approach that utilizes not only lexical , but also contextual and grammatical features of social text .	The graph encodes the relative positions of the words with respect to each other , as well as their part-of-speech tags .	CL_D14-1037_pa_3	CL_D14-1037_pa_5	CL_D14-1037_pa_3_5
CL	D14-1037	pa	3	6	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	We introduce an unsupervised text normalization approach that utilizes not only lexical , but also contextual and grammatical features of social text .	The lexical features are obtained by using the longest common subsequence ratio and edit distance measures to encode the surface similarity among words , and the double metaphone algorithm to represent the phonetic similarity .	CL_D14-1037_pa_3	CL_D14-1037_pa_6	CL_D14-1037_pa_3_6
CL	D14-1037	pa	3	7	proposal	proposal	none	elaboration	main	secondary	back	elaboration	We introduce an unsupervised text normalization approach that utilizes not only lexical , but also contextual and grammatical features of social text .	Unlike most of the recent approaches that are based on generating normalization dictionaries , the proposed approach performs normalization by considering the context of the non-standard words in the input text .	CL_D14-1037_pa_3	CL_D14-1037_pa_7	CL_D14-1037_pa_3_7
CL	D14-1037	pa	7	3	proposal	proposal	elaboration	none	secondary	main	forw	elaboration	Unlike most of the recent approaches that are based on generating normalization dictionaries , the proposed approach performs normalization by considering the context of the non-standard words in the input text .	We introduce an unsupervised text normalization approach that utilizes not only lexical , but also contextual and grammatical features of social text .	CL_D14-1037_pa_7	CL_D14-1037_pa_3	CL_D14-1037_pa_3_7
CL	D14-1037	pa	3	8	proposal	result	none	support	main	secondary	back	support	We introduce an unsupervised text normalization approach that utilizes not only lexical , but also contextual and grammatical features of social text .	Our results show that it achieves state-of-the-art F-score performance on standard datasets .	CL_D14-1037_pa_3	CL_D14-1037_pa_8	CL_D14-1037_pa_3_8
CL	D14-1037	pa	8	3	result	proposal	support	none	secondary	main	forw	support	Our results show that it achieves state-of-the-art F-score performance on standard datasets .	We introduce an unsupervised text normalization approach that utilizes not only lexical , but also contextual and grammatical features of social text .	CL_D14-1037_pa_8	CL_D14-1037_pa_3	CL_D14-1037_pa_3_8
CL	D14-1037	pa	3	9	proposal	conclusion	none	support	main	secondary	back	support	We introduce an unsupervised text normalization approach that utilizes not only lexical , but also contextual and grammatical features of social text .	In addition , the system can be tuned to achieve very high precision without sacrificing much from recall .	CL_D14-1037_pa_3	CL_D14-1037_pa_9	CL_D14-1037_pa_3_9
CL	D14-1037	pa	9	3	conclusion	proposal	support	none	secondary	main	forw	support	In addition , the system can be tuned to achieve very high precision without sacrificing much from recall .	We introduce an unsupervised text normalization approach that utilizes not only lexical , but also contextual and grammatical features of social text .	CL_D14-1037_pa_9	CL_D14-1037_pa_3	CL_D14-1037_pa_3_9
CL	D14-1037	pa	4	5	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	The contextual and grammatical features are extracted from a word association graph built by using a large unlabeled social media text corpus .	The graph encodes the relative positions of the words with respect to each other , as well as their part-of-speech tags .	CL_D14-1037_pa_4	CL_D14-1037_pa_5	CL_D14-1037_pa_4_5
CL	D14-1037	pa	5	4	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	forw	elaboration	The graph encodes the relative positions of the words with respect to each other , as well as their part-of-speech tags .	The contextual and grammatical features are extracted from a word association graph built by using a large unlabeled social media text corpus .	CL_D14-1037_pa_5	CL_D14-1037_pa_4	CL_D14-1037_pa_4_5
CL	D14-1037	pa	4	6	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	The contextual and grammatical features are extracted from a word association graph built by using a large unlabeled social media text corpus .	The lexical features are obtained by using the longest common subsequence ratio and edit distance measures to encode the surface similarity among words , and the double metaphone algorithm to represent the phonetic similarity .	CL_D14-1037_pa_4	CL_D14-1037_pa_6	CL_D14-1037_pa_4_6
CL	D14-1037	pa	6	4	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	forw	elaboration	The lexical features are obtained by using the longest common subsequence ratio and edit distance measures to encode the surface similarity among words , and the double metaphone algorithm to represent the phonetic similarity .	The contextual and grammatical features are extracted from a word association graph built by using a large unlabeled social media text corpus .	CL_D14-1037_pa_6	CL_D14-1037_pa_4	CL_D14-1037_pa_4_6
CL	D14-1037	pa	4	7	proposal_implementation	proposal	elaboration	elaboration	secondary	secondary	none	none	The contextual and grammatical features are extracted from a word association graph built by using a large unlabeled social media text corpus .	Unlike most of the recent approaches that are based on generating normalization dictionaries , the proposed approach performs normalization by considering the context of the non-standard words in the input text .	CL_D14-1037_pa_4	CL_D14-1037_pa_7	CL_D14-1037_pa_4_7
CL	D14-1037	pa	4	8	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	The contextual and grammatical features are extracted from a word association graph built by using a large unlabeled social media text corpus .	Our results show that it achieves state-of-the-art F-score performance on standard datasets .	CL_D14-1037_pa_4	CL_D14-1037_pa_8	CL_D14-1037_pa_4_8
CL	D14-1037	pa	4	9	proposal_implementation	conclusion	elaboration	support	secondary	secondary	none	none	The contextual and grammatical features are extracted from a word association graph built by using a large unlabeled social media text corpus .	In addition , the system can be tuned to achieve very high precision without sacrificing much from recall .	CL_D14-1037_pa_4	CL_D14-1037_pa_9	CL_D14-1037_pa_4_9
CL	D14-1037	pa	5	6	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	none	none	The graph encodes the relative positions of the words with respect to each other , as well as their part-of-speech tags .	The lexical features are obtained by using the longest common subsequence ratio and edit distance measures to encode the surface similarity among words , and the double metaphone algorithm to represent the phonetic similarity .	CL_D14-1037_pa_5	CL_D14-1037_pa_6	CL_D14-1037_pa_5_6
CL	D14-1037	pa	5	7	proposal_implementation	proposal	elaboration	elaboration	secondary	secondary	none	none	The graph encodes the relative positions of the words with respect to each other , as well as their part-of-speech tags .	Unlike most of the recent approaches that are based on generating normalization dictionaries , the proposed approach performs normalization by considering the context of the non-standard words in the input text .	CL_D14-1037_pa_5	CL_D14-1037_pa_7	CL_D14-1037_pa_5_7
CL	D14-1037	pa	5	8	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	The graph encodes the relative positions of the words with respect to each other , as well as their part-of-speech tags .	Our results show that it achieves state-of-the-art F-score performance on standard datasets .	CL_D14-1037_pa_5	CL_D14-1037_pa_8	CL_D14-1037_pa_5_8
CL	D14-1037	pa	5	9	proposal_implementation	conclusion	elaboration	support	secondary	secondary	none	none	The graph encodes the relative positions of the words with respect to each other , as well as their part-of-speech tags .	In addition , the system can be tuned to achieve very high precision without sacrificing much from recall .	CL_D14-1037_pa_5	CL_D14-1037_pa_9	CL_D14-1037_pa_5_9
CL	D14-1037	pa	6	7	proposal_implementation	proposal	elaboration	elaboration	secondary	secondary	none	none	The lexical features are obtained by using the longest common subsequence ratio and edit distance measures to encode the surface similarity among words , and the double metaphone algorithm to represent the phonetic similarity .	Unlike most of the recent approaches that are based on generating normalization dictionaries , the proposed approach performs normalization by considering the context of the non-standard words in the input text .	CL_D14-1037_pa_6	CL_D14-1037_pa_7	CL_D14-1037_pa_6_7
CL	D14-1037	pa	6	8	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	The lexical features are obtained by using the longest common subsequence ratio and edit distance measures to encode the surface similarity among words , and the double metaphone algorithm to represent the phonetic similarity .	Our results show that it achieves state-of-the-art F-score performance on standard datasets .	CL_D14-1037_pa_6	CL_D14-1037_pa_8	CL_D14-1037_pa_6_8
CL	D14-1037	pa	6	9	proposal_implementation	conclusion	elaboration	support	secondary	secondary	none	none	The lexical features are obtained by using the longest common subsequence ratio and edit distance measures to encode the surface similarity among words , and the double metaphone algorithm to represent the phonetic similarity .	In addition , the system can be tuned to achieve very high precision without sacrificing much from recall .	CL_D14-1037_pa_6	CL_D14-1037_pa_9	CL_D14-1037_pa_6_9
CL	D14-1037	pa	7	8	proposal	result	elaboration	support	secondary	secondary	none	none	Unlike most of the recent approaches that are based on generating normalization dictionaries , the proposed approach performs normalization by considering the context of the non-standard words in the input text .	Our results show that it achieves state-of-the-art F-score performance on standard datasets .	CL_D14-1037_pa_7	CL_D14-1037_pa_8	CL_D14-1037_pa_7_8
CL	D14-1037	pa	7	9	proposal	conclusion	elaboration	support	secondary	secondary	none	none	Unlike most of the recent approaches that are based on generating normalization dictionaries , the proposed approach performs normalization by considering the context of the non-standard words in the input text .	In addition , the system can be tuned to achieve very high precision without sacrificing much from recall .	CL_D14-1037_pa_7	CL_D14-1037_pa_9	CL_D14-1037_pa_7_9
CL	D14-1037	pa	8	9	result	conclusion	support	support	secondary	secondary	none	none	Our results show that it achieves state-of-the-art F-score performance on standard datasets .	In addition , the system can be tuned to achieve very high precision without sacrificing much from recall .	CL_D14-1037_pa_8	CL_D14-1037_pa_9	CL_D14-1037_pa_8_9
CL	D14-1038	pa	1	2	motivation_background	motivation_problem	info-required	info-required	secondary	secondary	forw	info-required	Search engines are increasingly relying on large knowledge bases of facts to provide direct answers to users' queries .	However , the construction of these knowledge bases is largely manual and does not scale to the long and heavy tail of facts .	CL_D14-1038_pa_1	CL_D14-1038_pa_2	CL_D14-1038_pa_1_2
CL	D14-1038	pa	2	1	motivation_problem	motivation_background	info-required	info-required	secondary	secondary	back	info-required	However , the construction of these knowledge bases is largely manual and does not scale to the long and heavy tail of facts .	Search engines are increasingly relying on large knowledge bases of facts to provide direct answers to users' queries .	CL_D14-1038_pa_2	CL_D14-1038_pa_1	CL_D14-1038_pa_1_2
CL	D14-1038	pa	1	3	motivation_background	motivation_problem	info-required	support	secondary	secondary	none	none	Search engines are increasingly relying on large knowledge bases of facts to provide direct answers to users' queries .	Open information extraction tries to address this challenge , but typically assumes that facts are expressed with verb phrases , and therefore has had difficulty extracting facts for noun-based relations .	CL_D14-1038_pa_1	CL_D14-1038_pa_3	CL_D14-1038_pa_1_3
CL	D14-1038	pa	1	4	motivation_background	proposal	info-required	none	secondary	main	none	none	Search engines are increasingly relying on large knowledge bases of facts to provide direct answers to users' queries .	We describe ReNoun , an open information extraction system that complements previous efforts by focusing on nominal attributes and on the long tail .	CL_D14-1038_pa_1	CL_D14-1038_pa_4	CL_D14-1038_pa_1_4
CL	D14-1038	pa	1	5	motivation_background	proposal	info-required	elaboration	secondary	secondary	none	none	Search engines are increasingly relying on large knowledge bases of facts to provide direct answers to users' queries .	ReNoun's approach is based on leveraging a large ontology of noun attributes mined from a text corpus and from user queries .	CL_D14-1038_pa_1	CL_D14-1038_pa_5	CL_D14-1038_pa_1_5
CL	D14-1038	pa	1	6	motivation_background	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	Search engines are increasingly relying on large knowledge bases of facts to provide direct answers to users' queries .	ReNoun creates a seed set of training data by using specialized patterns and requiring that the facts mention an attribute in the ontology .	CL_D14-1038_pa_1	CL_D14-1038_pa_6	CL_D14-1038_pa_1_6
CL	D14-1038	pa	1	7	motivation_background	proposal_implementation	info-required	sequence	secondary	secondary	none	none	Search engines are increasingly relying on large knowledge bases of facts to provide direct answers to users' queries .	ReNoun then generalizes from this seed set to produce a much larger set of extractions that are then scored .	CL_D14-1038_pa_1	CL_D14-1038_pa_7	CL_D14-1038_pa_1_7
CL	D14-1038	pa	1	8	motivation_background	result	info-required	support	secondary	secondary	none	none	Search engines are increasingly relying on large knowledge bases of facts to provide direct answers to users' queries .	We describe experiments that show that we extract facts with high precision and for attributes that cannot be extracted with verb-based techniques .	CL_D14-1038_pa_1	CL_D14-1038_pa_8	CL_D14-1038_pa_1_8
CL	D14-1038	pa	2	3	motivation_problem	motivation_problem	info-required	support	secondary	secondary	forw	info-required	However , the construction of these knowledge bases is largely manual and does not scale to the long and heavy tail of facts .	Open information extraction tries to address this challenge , but typically assumes that facts are expressed with verb phrases , and therefore has had difficulty extracting facts for noun-based relations .	CL_D14-1038_pa_2	CL_D14-1038_pa_3	CL_D14-1038_pa_2_3
CL	D14-1038	pa	3	2	motivation_problem	motivation_problem	support	info-required	secondary	secondary	back	info-required	Open information extraction tries to address this challenge , but typically assumes that facts are expressed with verb phrases , and therefore has had difficulty extracting facts for noun-based relations .	However , the construction of these knowledge bases is largely manual and does not scale to the long and heavy tail of facts .	CL_D14-1038_pa_3	CL_D14-1038_pa_2	CL_D14-1038_pa_2_3
CL	D14-1038	pa	2	4	motivation_problem	proposal	info-required	none	secondary	main	none	none	However , the construction of these knowledge bases is largely manual and does not scale to the long and heavy tail of facts .	We describe ReNoun , an open information extraction system that complements previous efforts by focusing on nominal attributes and on the long tail .	CL_D14-1038_pa_2	CL_D14-1038_pa_4	CL_D14-1038_pa_2_4
CL	D14-1038	pa	2	5	motivation_problem	proposal	info-required	elaboration	secondary	secondary	none	none	However , the construction of these knowledge bases is largely manual and does not scale to the long and heavy tail of facts .	ReNoun's approach is based on leveraging a large ontology of noun attributes mined from a text corpus and from user queries .	CL_D14-1038_pa_2	CL_D14-1038_pa_5	CL_D14-1038_pa_2_5
CL	D14-1038	pa	2	6	motivation_problem	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	However , the construction of these knowledge bases is largely manual and does not scale to the long and heavy tail of facts .	ReNoun creates a seed set of training data by using specialized patterns and requiring that the facts mention an attribute in the ontology .	CL_D14-1038_pa_2	CL_D14-1038_pa_6	CL_D14-1038_pa_2_6
CL	D14-1038	pa	2	7	motivation_problem	proposal_implementation	info-required	sequence	secondary	secondary	none	none	However , the construction of these knowledge bases is largely manual and does not scale to the long and heavy tail of facts .	ReNoun then generalizes from this seed set to produce a much larger set of extractions that are then scored .	CL_D14-1038_pa_2	CL_D14-1038_pa_7	CL_D14-1038_pa_2_7
CL	D14-1038	pa	2	8	motivation_problem	result	info-required	support	secondary	secondary	none	none	However , the construction of these knowledge bases is largely manual and does not scale to the long and heavy tail of facts .	We describe experiments that show that we extract facts with high precision and for attributes that cannot be extracted with verb-based techniques .	CL_D14-1038_pa_2	CL_D14-1038_pa_8	CL_D14-1038_pa_2_8
CL	D14-1038	pa	3	4	motivation_problem	proposal	support	none	secondary	main	forw	support	Open information extraction tries to address this challenge , but typically assumes that facts are expressed with verb phrases , and therefore has had difficulty extracting facts for noun-based relations .	We describe ReNoun , an open information extraction system that complements previous efforts by focusing on nominal attributes and on the long tail .	CL_D14-1038_pa_3	CL_D14-1038_pa_4	CL_D14-1038_pa_3_4
CL	D14-1038	pa	4	3	proposal	motivation_problem	none	support	main	secondary	back	support	We describe ReNoun , an open information extraction system that complements previous efforts by focusing on nominal attributes and on the long tail .	Open information extraction tries to address this challenge , but typically assumes that facts are expressed with verb phrases , and therefore has had difficulty extracting facts for noun-based relations .	CL_D14-1038_pa_4	CL_D14-1038_pa_3	CL_D14-1038_pa_3_4
CL	D14-1038	pa	3	5	motivation_problem	proposal	support	elaboration	secondary	secondary	none	none	Open information extraction tries to address this challenge , but typically assumes that facts are expressed with verb phrases , and therefore has had difficulty extracting facts for noun-based relations .	ReNoun's approach is based on leveraging a large ontology of noun attributes mined from a text corpus and from user queries .	CL_D14-1038_pa_3	CL_D14-1038_pa_5	CL_D14-1038_pa_3_5
CL	D14-1038	pa	3	6	motivation_problem	proposal_implementation	support	elaboration	secondary	secondary	none	none	Open information extraction tries to address this challenge , but typically assumes that facts are expressed with verb phrases , and therefore has had difficulty extracting facts for noun-based relations .	ReNoun creates a seed set of training data by using specialized patterns and requiring that the facts mention an attribute in the ontology .	CL_D14-1038_pa_3	CL_D14-1038_pa_6	CL_D14-1038_pa_3_6
CL	D14-1038	pa	3	7	motivation_problem	proposal_implementation	support	sequence	secondary	secondary	none	none	Open information extraction tries to address this challenge , but typically assumes that facts are expressed with verb phrases , and therefore has had difficulty extracting facts for noun-based relations .	ReNoun then generalizes from this seed set to produce a much larger set of extractions that are then scored .	CL_D14-1038_pa_3	CL_D14-1038_pa_7	CL_D14-1038_pa_3_7
CL	D14-1038	pa	3	8	motivation_problem	result	support	support	secondary	secondary	none	none	Open information extraction tries to address this challenge , but typically assumes that facts are expressed with verb phrases , and therefore has had difficulty extracting facts for noun-based relations .	We describe experiments that show that we extract facts with high precision and for attributes that cannot be extracted with verb-based techniques .	CL_D14-1038_pa_3	CL_D14-1038_pa_8	CL_D14-1038_pa_3_8
CL	D14-1038	pa	4	5	proposal	proposal	none	elaboration	main	secondary	back	elaboration	We describe ReNoun , an open information extraction system that complements previous efforts by focusing on nominal attributes and on the long tail .	ReNoun's approach is based on leveraging a large ontology of noun attributes mined from a text corpus and from user queries .	CL_D14-1038_pa_4	CL_D14-1038_pa_5	CL_D14-1038_pa_4_5
CL	D14-1038	pa	5	4	proposal	proposal	elaboration	none	secondary	main	forw	elaboration	ReNoun's approach is based on leveraging a large ontology of noun attributes mined from a text corpus and from user queries .	We describe ReNoun , an open information extraction system that complements previous efforts by focusing on nominal attributes and on the long tail .	CL_D14-1038_pa_5	CL_D14-1038_pa_4	CL_D14-1038_pa_4_5
CL	D14-1038	pa	4	6	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	We describe ReNoun , an open information extraction system that complements previous efforts by focusing on nominal attributes and on the long tail .	ReNoun creates a seed set of training data by using specialized patterns and requiring that the facts mention an attribute in the ontology .	CL_D14-1038_pa_4	CL_D14-1038_pa_6	CL_D14-1038_pa_4_6
CL	D14-1038	pa	4	7	proposal	proposal_implementation	none	sequence	main	secondary	none	none	We describe ReNoun , an open information extraction system that complements previous efforts by focusing on nominal attributes and on the long tail .	ReNoun then generalizes from this seed set to produce a much larger set of extractions that are then scored .	CL_D14-1038_pa_4	CL_D14-1038_pa_7	CL_D14-1038_pa_4_7
CL	D14-1038	pa	4	8	proposal	result	none	support	main	secondary	back	support	We describe ReNoun , an open information extraction system that complements previous efforts by focusing on nominal attributes and on the long tail .	We describe experiments that show that we extract facts with high precision and for attributes that cannot be extracted with verb-based techniques .	CL_D14-1038_pa_4	CL_D14-1038_pa_8	CL_D14-1038_pa_4_8
CL	D14-1038	pa	8	4	result	proposal	support	none	secondary	main	forw	support	We describe experiments that show that we extract facts with high precision and for attributes that cannot be extracted with verb-based techniques .	We describe ReNoun , an open information extraction system that complements previous efforts by focusing on nominal attributes and on the long tail .	CL_D14-1038_pa_8	CL_D14-1038_pa_4	CL_D14-1038_pa_4_8
CL	D14-1038	pa	5	6	proposal	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	ReNoun's approach is based on leveraging a large ontology of noun attributes mined from a text corpus and from user queries .	ReNoun creates a seed set of training data by using specialized patterns and requiring that the facts mention an attribute in the ontology .	CL_D14-1038_pa_5	CL_D14-1038_pa_6	CL_D14-1038_pa_5_6
CL	D14-1038	pa	6	5	proposal_implementation	proposal	elaboration	elaboration	secondary	secondary	forw	elaboration	ReNoun creates a seed set of training data by using specialized patterns and requiring that the facts mention an attribute in the ontology .	ReNoun's approach is based on leveraging a large ontology of noun attributes mined from a text corpus and from user queries .	CL_D14-1038_pa_6	CL_D14-1038_pa_5	CL_D14-1038_pa_5_6
CL	D14-1038	pa	5	7	proposal	proposal_implementation	elaboration	sequence	secondary	secondary	none	none	ReNoun's approach is based on leveraging a large ontology of noun attributes mined from a text corpus and from user queries .	ReNoun then generalizes from this seed set to produce a much larger set of extractions that are then scored .	CL_D14-1038_pa_5	CL_D14-1038_pa_7	CL_D14-1038_pa_5_7
CL	D14-1038	pa	5	8	proposal	result	elaboration	support	secondary	secondary	none	none	ReNoun's approach is based on leveraging a large ontology of noun attributes mined from a text corpus and from user queries .	We describe experiments that show that we extract facts with high precision and for attributes that cannot be extracted with verb-based techniques .	CL_D14-1038_pa_5	CL_D14-1038_pa_8	CL_D14-1038_pa_5_8
CL	D14-1038	pa	6	7	proposal_implementation	proposal_implementation	elaboration	sequence	secondary	secondary	back	sequence	ReNoun creates a seed set of training data by using specialized patterns and requiring that the facts mention an attribute in the ontology .	ReNoun then generalizes from this seed set to produce a much larger set of extractions that are then scored .	CL_D14-1038_pa_6	CL_D14-1038_pa_7	CL_D14-1038_pa_6_7
CL	D14-1038	pa	7	6	proposal_implementation	proposal_implementation	sequence	elaboration	secondary	secondary	forw	sequence	ReNoun then generalizes from this seed set to produce a much larger set of extractions that are then scored .	ReNoun creates a seed set of training data by using specialized patterns and requiring that the facts mention an attribute in the ontology .	CL_D14-1038_pa_7	CL_D14-1038_pa_6	CL_D14-1038_pa_6_7
CL	D14-1038	pa	6	8	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	ReNoun creates a seed set of training data by using specialized patterns and requiring that the facts mention an attribute in the ontology .	We describe experiments that show that we extract facts with high precision and for attributes that cannot be extracted with verb-based techniques .	CL_D14-1038_pa_6	CL_D14-1038_pa_8	CL_D14-1038_pa_6_8
CL	D14-1038	pa	7	8	proposal_implementation	result	sequence	support	secondary	secondary	none	none	ReNoun then generalizes from this seed set to produce a much larger set of extractions that are then scored .	We describe experiments that show that we extract facts with high precision and for attributes that cannot be extracted with verb-based techniques .	CL_D14-1038_pa_7	CL_D14-1038_pa_8	CL_D14-1038_pa_7_8
CL	D14-1039	pa	1	2	motivation_background	motivation_problem	info-required	support	secondary	secondary	forw	info-required	Text-based document geolocation is commonly rooted in language-based information retrieval techniques over geodesic grids .	These methods ignore the natural hierarchy of cells in such grids and fall afoul of independence assumptions .	CL_D14-1039_pa_1	CL_D14-1039_pa_2	CL_D14-1039_pa_1_2
CL	D14-1039	pa	2	1	motivation_problem	motivation_background	support	info-required	secondary	secondary	back	info-required	These methods ignore the natural hierarchy of cells in such grids and fall afoul of independence assumptions .	Text-based document geolocation is commonly rooted in language-based information retrieval techniques over geodesic grids .	CL_D14-1039_pa_2	CL_D14-1039_pa_1	CL_D14-1039_pa_1_2
CL	D14-1039	pa	1	3	motivation_background	conclusion	info-required	none	secondary	main	none	none	Text-based document geolocation is commonly rooted in language-based information retrieval techniques over geodesic grids .	We demonstrate the effectiveness of using logistic regression models on a hierarchy of nodes in the grid , which improves upon the state of the art accuracy by several percent and reduces mean error distances by hundreds of kilometers on data from Twitter , Wikipedia , and Flickr .	CL_D14-1039_pa_1	CL_D14-1039_pa_3	CL_D14-1039_pa_1_3
CL	D14-1039	pa	1	4	motivation_background	conclusion	info-required	support	secondary	secondary	none	none	Text-based document geolocation is commonly rooted in language-based information retrieval techniques over geodesic grids .	We also show that logistic regression performs feature selection effectively , assigning high weights to geocentric terms .	CL_D14-1039_pa_1	CL_D14-1039_pa_4	CL_D14-1039_pa_1_4
CL	D14-1039	pa	2	3	motivation_problem	conclusion	support	none	secondary	main	forw	support	These methods ignore the natural hierarchy of cells in such grids and fall afoul of independence assumptions .	We demonstrate the effectiveness of using logistic regression models on a hierarchy of nodes in the grid , which improves upon the state of the art accuracy by several percent and reduces mean error distances by hundreds of kilometers on data from Twitter , Wikipedia , and Flickr .	CL_D14-1039_pa_2	CL_D14-1039_pa_3	CL_D14-1039_pa_2_3
CL	D14-1039	pa	3	2	conclusion	motivation_problem	none	support	main	secondary	back	support	We demonstrate the effectiveness of using logistic regression models on a hierarchy of nodes in the grid , which improves upon the state of the art accuracy by several percent and reduces mean error distances by hundreds of kilometers on data from Twitter , Wikipedia , and Flickr .	These methods ignore the natural hierarchy of cells in such grids and fall afoul of independence assumptions .	CL_D14-1039_pa_3	CL_D14-1039_pa_2	CL_D14-1039_pa_2_3
CL	D14-1039	pa	2	4	motivation_problem	conclusion	support	support	secondary	secondary	none	none	These methods ignore the natural hierarchy of cells in such grids and fall afoul of independence assumptions .	We also show that logistic regression performs feature selection effectively , assigning high weights to geocentric terms .	CL_D14-1039_pa_2	CL_D14-1039_pa_4	CL_D14-1039_pa_2_4
CL	D14-1039	pa	3	4	conclusion	conclusion	none	support	main	secondary	back	support	We demonstrate the effectiveness of using logistic regression models on a hierarchy of nodes in the grid , which improves upon the state of the art accuracy by several percent and reduces mean error distances by hundreds of kilometers on data from Twitter , Wikipedia , and Flickr .	We also show that logistic regression performs feature selection effectively , assigning high weights to geocentric terms .	CL_D14-1039_pa_3	CL_D14-1039_pa_4	CL_D14-1039_pa_3_4
CL	D14-1039	pa	4	3	conclusion	conclusion	support	none	secondary	main	forw	support	We also show that logistic regression performs feature selection effectively , assigning high weights to geocentric terms .	We demonstrate the effectiveness of using logistic regression models on a hierarchy of nodes in the grid , which improves upon the state of the art accuracy by several percent and reduces mean error distances by hundreds of kilometers on data from Twitter , Wikipedia , and Flickr .	CL_D14-1039_pa_4	CL_D14-1039_pa_3	CL_D14-1039_pa_3_4
CL	D14-1070	mn	1	2	motivation_background	motivation_problem	info-required	support	secondary	secondary	forw	info-required	Text classification methods for tasks like factoid question answering typically use manually defined string matching rules or bag of words representations .	These methods are ineffective when question text contains very few individual words ( e.g. , named entities ) that are indicative of the answer .	CL_D14-1070_mn_1	CL_D14-1070_mn_2	CL_D14-1070_mn_1_2
CL	D14-1070	mn	2	1	motivation_problem	motivation_background	support	info-required	secondary	secondary	back	info-required	These methods are ineffective when question text contains very few individual words ( e.g. , named entities ) that are indicative of the answer .	Text classification methods for tasks like factoid question answering typically use manually defined string matching rules or bag of words representations .	CL_D14-1070_mn_2	CL_D14-1070_mn_1	CL_D14-1070_mn_1_2
CL	D14-1070	mn	1	3	motivation_background	proposal	info-required	none	secondary	main	none	none	Text classification methods for tasks like factoid question answering typically use manually defined string matching rules or bag of words representations .	We introduce a recursive neural network ( rnn ) model that can reason over such input by modeling textual compositionality .	CL_D14-1070_mn_1	CL_D14-1070_mn_3	CL_D14-1070_mn_1_3
CL	D14-1070	mn	1	4	motivation_background	means	info-required	by-means	secondary	secondary	none	none	Text classification methods for tasks like factoid question answering typically use manually defined string matching rules or bag of words representations .	We apply our model , qanta , to a dataset of questions from a trivia competition called quiz bowl .	CL_D14-1070_mn_1	CL_D14-1070_mn_4	CL_D14-1070_mn_1_4
CL	D14-1070	mn	1	5	motivation_background	conclusion	info-required	support	secondary	secondary	none	none	Text classification methods for tasks like factoid question answering typically use manually defined string matching rules or bag of words representations .	Unlike previous rnn models , qanta learns word and phrase-level representations that combine across sentences to reason about entities .	CL_D14-1070_mn_1	CL_D14-1070_mn_5	CL_D14-1070_mn_1_5
CL	D14-1070	mn	1	6	motivation_background	result	info-required	support	secondary	secondary	none	none	Text classification methods for tasks like factoid question answering typically use manually defined string matching rules or bag of words representations .	The model outperforms multiple baselines and , when combined with information retrieval methods , ri-vals the best human players .	CL_D14-1070_mn_1	CL_D14-1070_mn_6	CL_D14-1070_mn_1_6
CL	D14-1070	mn	2	3	motivation_problem	proposal	support	none	secondary	main	forw	support	These methods are ineffective when question text contains very few individual words ( e.g. , named entities ) that are indicative of the answer .	We introduce a recursive neural network ( rnn ) model that can reason over such input by modeling textual compositionality .	CL_D14-1070_mn_2	CL_D14-1070_mn_3	CL_D14-1070_mn_2_3
CL	D14-1070	mn	3	2	proposal	motivation_problem	none	support	main	secondary	back	support	We introduce a recursive neural network ( rnn ) model that can reason over such input by modeling textual compositionality .	These methods are ineffective when question text contains very few individual words ( e.g. , named entities ) that are indicative of the answer .	CL_D14-1070_mn_3	CL_D14-1070_mn_2	CL_D14-1070_mn_2_3
CL	D14-1070	mn	2	4	motivation_problem	means	support	by-means	secondary	secondary	none	none	These methods are ineffective when question text contains very few individual words ( e.g. , named entities ) that are indicative of the answer .	We apply our model , qanta , to a dataset of questions from a trivia competition called quiz bowl .	CL_D14-1070_mn_2	CL_D14-1070_mn_4	CL_D14-1070_mn_2_4
CL	D14-1070	mn	2	5	motivation_problem	conclusion	support	support	secondary	secondary	none	none	These methods are ineffective when question text contains very few individual words ( e.g. , named entities ) that are indicative of the answer .	Unlike previous rnn models , qanta learns word and phrase-level representations that combine across sentences to reason about entities .	CL_D14-1070_mn_2	CL_D14-1070_mn_5	CL_D14-1070_mn_2_5
CL	D14-1070	mn	2	6	motivation_problem	result	support	support	secondary	secondary	none	none	These methods are ineffective when question text contains very few individual words ( e.g. , named entities ) that are indicative of the answer .	The model outperforms multiple baselines and , when combined with information retrieval methods , ri-vals the best human players .	CL_D14-1070_mn_2	CL_D14-1070_mn_6	CL_D14-1070_mn_2_6
CL	D14-1070	mn	3	4	proposal	means	none	by-means	main	secondary	none	none	We introduce a recursive neural network ( rnn ) model that can reason over such input by modeling textual compositionality .	We apply our model , qanta , to a dataset of questions from a trivia competition called quiz bowl .	CL_D14-1070_mn_3	CL_D14-1070_mn_4	CL_D14-1070_mn_3_4
CL	D14-1070	mn	3	5	proposal	conclusion	none	support	main	secondary	back	support	We introduce a recursive neural network ( rnn ) model that can reason over such input by modeling textual compositionality .	Unlike previous rnn models , qanta learns word and phrase-level representations that combine across sentences to reason about entities .	CL_D14-1070_mn_3	CL_D14-1070_mn_5	CL_D14-1070_mn_3_5
CL	D14-1070	mn	5	3	conclusion	proposal	support	none	secondary	main	forw	support	Unlike previous rnn models , qanta learns word and phrase-level representations that combine across sentences to reason about entities .	We introduce a recursive neural network ( rnn ) model that can reason over such input by modeling textual compositionality .	CL_D14-1070_mn_5	CL_D14-1070_mn_3	CL_D14-1070_mn_3_5
CL	D14-1070	mn	3	6	proposal	result	none	support	main	secondary	none	none	We introduce a recursive neural network ( rnn ) model that can reason over such input by modeling textual compositionality .	The model outperforms multiple baselines and , when combined with information retrieval methods , ri-vals the best human players .	CL_D14-1070_mn_3	CL_D14-1070_mn_6	CL_D14-1070_mn_3_6
CL	D14-1070	mn	4	5	means	conclusion	by-means	support	secondary	secondary	none	none	We apply our model , qanta , to a dataset of questions from a trivia competition called quiz bowl .	Unlike previous rnn models , qanta learns word and phrase-level representations that combine across sentences to reason about entities .	CL_D14-1070_mn_4	CL_D14-1070_mn_5	CL_D14-1070_mn_4_5
CL	D14-1070	mn	4	6	means	result	by-means	support	secondary	secondary	forw	by-means	We apply our model , qanta , to a dataset of questions from a trivia competition called quiz bowl .	The model outperforms multiple baselines and , when combined with information retrieval methods , ri-vals the best human players .	CL_D14-1070_mn_4	CL_D14-1070_mn_6	CL_D14-1070_mn_4_6
CL	D14-1070	mn	6	4	result	means	support	by-means	secondary	secondary	back	by-means	The model outperforms multiple baselines and , when combined with information retrieval methods , ri-vals the best human players .	We apply our model , qanta , to a dataset of questions from a trivia competition called quiz bowl .	CL_D14-1070_mn_6	CL_D14-1070_mn_4	CL_D14-1070_mn_4_6
CL	D14-1070	mn	5	6	conclusion	result	support	support	secondary	secondary	back	support	Unlike previous rnn models , qanta learns word and phrase-level representations that combine across sentences to reason about entities .	The model outperforms multiple baselines and , when combined with information retrieval methods , ri-vals the best human players .	CL_D14-1070_mn_5	CL_D14-1070_mn_6	CL_D14-1070_mn_5_6
CL	D14-1070	mn	6	5	result	conclusion	support	support	secondary	secondary	forw	support	The model outperforms multiple baselines and , when combined with information retrieval methods , ri-vals the best human players .	Unlike previous rnn models , qanta learns word and phrase-level representations that combine across sentences to reason about entities .	CL_D14-1070_mn_6	CL_D14-1070_mn_5	CL_D14-1070_mn_5_6
CL	D14-1071	mn	1	2	motivation_background	proposal	support	none	secondary	main	forw	support	Transforming a natural language ( NL ) question into a corresponding logical form ( LF ) is central to the knowledge-based question answering ( KB-QA ) task .	Unlike most previous methods that achieve this goal based on mappings between lexicalized phrases and logical predicates , this paper goes one step further and proposes a novel embedding-based approach that maps NL-questions into LFs for KB-QA by leveraging semantic associations between lexical representations and KB-properties in the latent space .	CL_D14-1071_mn_1	CL_D14-1071_mn_2	CL_D14-1071_mn_1_2
CL	D14-1071	mn	2	1	proposal	motivation_background	none	support	main	secondary	back	support	Unlike most previous methods that achieve this goal based on mappings between lexicalized phrases and logical predicates , this paper goes one step further and proposes a novel embedding-based approach that maps NL-questions into LFs for KB-QA by leveraging semantic associations between lexical representations and KB-properties in the latent space .	Transforming a natural language ( NL ) question into a corresponding logical form ( LF ) is central to the knowledge-based question answering ( KB-QA ) task .	CL_D14-1071_mn_2	CL_D14-1071_mn_1	CL_D14-1071_mn_1_2
CL	D14-1071	mn	1	3	motivation_background	result_means	support	support	secondary	secondary	none	none	Transforming a natural language ( NL ) question into a corresponding logical form ( LF ) is central to the knowledge-based question answering ( KB-QA ) task .	Experimental results demonstrate that our proposed method outperforms three KB-QA baseline methods on two publicly released QA data sets .	CL_D14-1071_mn_1	CL_D14-1071_mn_3	CL_D14-1071_mn_1_3
CL	D14-1071	mn	2	3	proposal	result_means	none	support	main	secondary	back	support	Unlike most previous methods that achieve this goal based on mappings between lexicalized phrases and logical predicates , this paper goes one step further and proposes a novel embedding-based approach that maps NL-questions into LFs for KB-QA by leveraging semantic associations between lexical representations and KB-properties in the latent space .	Experimental results demonstrate that our proposed method outperforms three KB-QA baseline methods on two publicly released QA data sets .	CL_D14-1071_mn_2	CL_D14-1071_mn_3	CL_D14-1071_mn_2_3
CL	D14-1071	mn	3	2	result_means	proposal	support	none	secondary	main	forw	support	Experimental results demonstrate that our proposed method outperforms three KB-QA baseline methods on two publicly released QA data sets .	Unlike most previous methods that achieve this goal based on mappings between lexicalized phrases and logical predicates , this paper goes one step further and proposes a novel embedding-based approach that maps NL-questions into LFs for KB-QA by leveraging semantic associations between lexical representations and KB-properties in the latent space .	CL_D14-1071_mn_3	CL_D14-1071_mn_2	CL_D14-1071_mn_2_3
CL	D14-1072	mn	1	2	motivation_problem	proposal	support	none	secondary	main	forw	support	Wikipedia's link structure is a valuable resource for natural language processing tasks , but only a fraction of the concepts mentioned in each article are annotated with hyperlinks .	In this paper , we study how to augment Wikipedia with additional high-precision links .	CL_D14-1072_mn_1	CL_D14-1072_mn_2	CL_D14-1072_mn_1_2
CL	D14-1072	mn	2	1	proposal	motivation_problem	none	support	main	secondary	back	support	In this paper , we study how to augment Wikipedia with additional high-precision links .	Wikipedia's link structure is a valuable resource for natural language processing tasks , but only a fraction of the concepts mentioned in each article are annotated with hyperlinks .	CL_D14-1072_mn_2	CL_D14-1072_mn_1	CL_D14-1072_mn_1_2
CL	D14-1072	mn	1	3	motivation_problem	proposal_implementation	support	elaboration	secondary	secondary	none	none	Wikipedia's link structure is a valuable resource for natural language processing tasks , but only a fraction of the concepts mentioned in each article are annotated with hyperlinks .	We present 3W , a system that identifies concept mentions in Wikipedia text , and links each mention to its referent page .	CL_D14-1072_mn_1	CL_D14-1072_mn_3	CL_D14-1072_mn_1_3
CL	D14-1072	mn	1	4	motivation_problem	proposal_implementation	support	elaboration	secondary	secondary	none	none	Wikipedia's link structure is a valuable resource for natural language processing tasks , but only a fraction of the concepts mentioned in each article are annotated with hyperlinks .	3W leverages rich semantic information present in Wikipedia to achieve high precision .	CL_D14-1072_mn_1	CL_D14-1072_mn_4	CL_D14-1072_mn_1_4
CL	D14-1072	mn	1	5	motivation_problem	observation	support	support	secondary	secondary	none	none	Wikipedia's link structure is a valuable resource for natural language processing tasks , but only a fraction of the concepts mentioned in each article are annotated with hyperlinks .	Our experiments demonstrate that 3W can add an average of seven new links to each Wikipedia article , at a precision of 0.98 .	CL_D14-1072_mn_1	CL_D14-1072_mn_5	CL_D14-1072_mn_1_5
CL	D14-1072	mn	2	3	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	In this paper , we study how to augment Wikipedia with additional high-precision links .	We present 3W , a system that identifies concept mentions in Wikipedia text , and links each mention to its referent page .	CL_D14-1072_mn_2	CL_D14-1072_mn_3	CL_D14-1072_mn_2_3
CL	D14-1072	mn	3	2	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	We present 3W , a system that identifies concept mentions in Wikipedia text , and links each mention to its referent page .	In this paper , we study how to augment Wikipedia with additional high-precision links .	CL_D14-1072_mn_3	CL_D14-1072_mn_2	CL_D14-1072_mn_2_3
CL	D14-1072	mn	2	4	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	In this paper , we study how to augment Wikipedia with additional high-precision links .	3W leverages rich semantic information present in Wikipedia to achieve high precision .	CL_D14-1072_mn_2	CL_D14-1072_mn_4	CL_D14-1072_mn_2_4
CL	D14-1072	mn	2	5	proposal	observation	none	support	main	secondary	back	support	In this paper , we study how to augment Wikipedia with additional high-precision links .	Our experiments demonstrate that 3W can add an average of seven new links to each Wikipedia article , at a precision of 0.98 .	CL_D14-1072_mn_2	CL_D14-1072_mn_5	CL_D14-1072_mn_2_5
CL	D14-1072	mn	5	2	observation	proposal	support	none	secondary	main	forw	support	Our experiments demonstrate that 3W can add an average of seven new links to each Wikipedia article , at a precision of 0.98 .	In this paper , we study how to augment Wikipedia with additional high-precision links .	CL_D14-1072_mn_5	CL_D14-1072_mn_2	CL_D14-1072_mn_2_5
CL	D14-1072	mn	3	4	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	We present 3W , a system that identifies concept mentions in Wikipedia text , and links each mention to its referent page .	3W leverages rich semantic information present in Wikipedia to achieve high precision .	CL_D14-1072_mn_3	CL_D14-1072_mn_4	CL_D14-1072_mn_3_4
CL	D14-1072	mn	4	3	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	forw	elaboration	3W leverages rich semantic information present in Wikipedia to achieve high precision .	We present 3W , a system that identifies concept mentions in Wikipedia text , and links each mention to its referent page .	CL_D14-1072_mn_4	CL_D14-1072_mn_3	CL_D14-1072_mn_3_4
CL	D14-1072	mn	3	5	proposal_implementation	observation	elaboration	support	secondary	secondary	none	none	We present 3W , a system that identifies concept mentions in Wikipedia text , and links each mention to its referent page .	Our experiments demonstrate that 3W can add an average of seven new links to each Wikipedia article , at a precision of 0.98 .	CL_D14-1072_mn_3	CL_D14-1072_mn_5	CL_D14-1072_mn_3_5
CL	D14-1072	mn	4	5	proposal_implementation	observation	elaboration	support	secondary	secondary	none	none	3W leverages rich semantic information present in Wikipedia to achieve high precision .	Our experiments demonstrate that 3W can add an average of seven new links to each Wikipedia article , at a precision of 0.98 .	CL_D14-1072_mn_4	CL_D14-1072_mn_5	CL_D14-1072_mn_4_5
CL	D14-1073	mn	1	2	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	In this paper we present our task-based evaluation of query biased summarization for cross-language information retrieval ( CLIR ) using relevance prediction .	We describe our 13 summarization methods each from one of four summarization strategies .	CL_D14-1073_mn_1	CL_D14-1073_mn_2	CL_D14-1073_mn_1_2
CL	D14-1073	mn	2	1	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	We describe our 13 summarization methods each from one of four summarization strategies .	In this paper we present our task-based evaluation of query biased summarization for cross-language information retrieval ( CLIR ) using relevance prediction .	CL_D14-1073_mn_2	CL_D14-1073_mn_1	CL_D14-1073_mn_1_2
CL	D14-1073	mn	1	3	proposal	means	none	by-means	main	secondary	none	none	In this paper we present our task-based evaluation of query biased summarization for cross-language information retrieval ( CLIR ) using relevance prediction .	We show how well our methods perform using Farsi text from the CLEF 2008 shared-task , which we translated to English automtatically .	CL_D14-1073_mn_1	CL_D14-1073_mn_3	CL_D14-1073_mn_1_3
CL	D14-1073	mn	1	4	proposal	means	none	by-means	main	secondary	none	none	In this paper we present our task-based evaluation of query biased summarization for cross-language information retrieval ( CLIR ) using relevance prediction .	We report precision/recall/F1 , accuracy and time-on-task .	CL_D14-1073_mn_1	CL_D14-1073_mn_4	CL_D14-1073_mn_1_4
CL	D14-1073	mn	1	5	proposal	result	none	elaboration	main	secondary	none	none	In this paper we present our task-based evaluation of query biased summarization for cross-language information retrieval ( CLIR ) using relevance prediction .	We found that different summarization methods perform optimally for different evaluation metrics , but overall query biased word clouds are the best summarization strategy .	CL_D14-1073_mn_1	CL_D14-1073_mn_5	CL_D14-1073_mn_1_5
CL	D14-1073	mn	1	6	proposal	result	none	support	main	secondary	none	none	In this paper we present our task-based evaluation of query biased summarization for cross-language information retrieval ( CLIR ) using relevance prediction .	In our analysis , we demonstrate that using the ROUGE metric on our sentence-based summaries cannot make the same kinds of distinctions as our evaluation framework does .	CL_D14-1073_mn_1	CL_D14-1073_mn_6	CL_D14-1073_mn_1_6
CL	D14-1073	mn	1	7	proposal	conclusion	none	support	main	secondary	back	support	In this paper we present our task-based evaluation of query biased summarization for cross-language information retrieval ( CLIR ) using relevance prediction .	Finally , we present our recommendations for creating muchneeded evaluation standards and datasets .	CL_D14-1073_mn_1	CL_D14-1073_mn_7	CL_D14-1073_mn_1_7
CL	D14-1073	mn	7	1	conclusion	proposal	support	none	secondary	main	forw	support	Finally , we present our recommendations for creating muchneeded evaluation standards and datasets .	In this paper we present our task-based evaluation of query biased summarization for cross-language information retrieval ( CLIR ) using relevance prediction .	CL_D14-1073_mn_7	CL_D14-1073_mn_1	CL_D14-1073_mn_1_7
CL	D14-1073	mn	2	3	proposal_implementation	means	elaboration	by-means	secondary	secondary	none	none	We describe our 13 summarization methods each from one of four summarization strategies .	We show how well our methods perform using Farsi text from the CLEF 2008 shared-task , which we translated to English automtatically .	CL_D14-1073_mn_2	CL_D14-1073_mn_3	CL_D14-1073_mn_2_3
CL	D14-1073	mn	2	4	proposal_implementation	means	elaboration	by-means	secondary	secondary	none	none	We describe our 13 summarization methods each from one of four summarization strategies .	We report precision/recall/F1 , accuracy and time-on-task .	CL_D14-1073_mn_2	CL_D14-1073_mn_4	CL_D14-1073_mn_2_4
CL	D14-1073	mn	2	5	proposal_implementation	result	elaboration	elaboration	secondary	secondary	none	none	We describe our 13 summarization methods each from one of four summarization strategies .	We found that different summarization methods perform optimally for different evaluation metrics , but overall query biased word clouds are the best summarization strategy .	CL_D14-1073_mn_2	CL_D14-1073_mn_5	CL_D14-1073_mn_2_5
CL	D14-1073	mn	2	6	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	We describe our 13 summarization methods each from one of four summarization strategies .	In our analysis , we demonstrate that using the ROUGE metric on our sentence-based summaries cannot make the same kinds of distinctions as our evaluation framework does .	CL_D14-1073_mn_2	CL_D14-1073_mn_6	CL_D14-1073_mn_2_6
CL	D14-1073	mn	2	7	proposal_implementation	conclusion	elaboration	support	secondary	secondary	none	none	We describe our 13 summarization methods each from one of four summarization strategies .	Finally , we present our recommendations for creating muchneeded evaluation standards and datasets .	CL_D14-1073_mn_2	CL_D14-1073_mn_7	CL_D14-1073_mn_2_7
CL	D14-1073	mn	3	4	means	means	by-means	by-means	secondary	secondary	none	none	We show how well our methods perform using Farsi text from the CLEF 2008 shared-task , which we translated to English automtatically .	We report precision/recall/F1 , accuracy and time-on-task .	CL_D14-1073_mn_3	CL_D14-1073_mn_4	CL_D14-1073_mn_3_4
CL	D14-1073	mn	3	5	means	result	by-means	elaboration	secondary	secondary	forw	by-means	We show how well our methods perform using Farsi text from the CLEF 2008 shared-task , which we translated to English automtatically .	We found that different summarization methods perform optimally for different evaluation metrics , but overall query biased word clouds are the best summarization strategy .	CL_D14-1073_mn_3	CL_D14-1073_mn_5	CL_D14-1073_mn_3_5
CL	D14-1073	mn	5	3	result	means	elaboration	by-means	secondary	secondary	back	by-means	We found that different summarization methods perform optimally for different evaluation metrics , but overall query biased word clouds are the best summarization strategy .	We show how well our methods perform using Farsi text from the CLEF 2008 shared-task , which we translated to English automtatically .	CL_D14-1073_mn_5	CL_D14-1073_mn_3	CL_D14-1073_mn_3_5
CL	D14-1073	mn	3	6	means	result	by-means	support	secondary	secondary	none	none	We show how well our methods perform using Farsi text from the CLEF 2008 shared-task , which we translated to English automtatically .	In our analysis , we demonstrate that using the ROUGE metric on our sentence-based summaries cannot make the same kinds of distinctions as our evaluation framework does .	CL_D14-1073_mn_3	CL_D14-1073_mn_6	CL_D14-1073_mn_3_6
CL	D14-1073	mn	3	7	means	conclusion	by-means	support	secondary	secondary	none	none	We show how well our methods perform using Farsi text from the CLEF 2008 shared-task , which we translated to English automtatically .	Finally , we present our recommendations for creating muchneeded evaluation standards and datasets .	CL_D14-1073_mn_3	CL_D14-1073_mn_7	CL_D14-1073_mn_3_7
CL	D14-1073	mn	4	5	means	result	by-means	elaboration	secondary	secondary	forw	by-means	We report precision/recall/F1 , accuracy and time-on-task .	We found that different summarization methods perform optimally for different evaluation metrics , but overall query biased word clouds are the best summarization strategy .	CL_D14-1073_mn_4	CL_D14-1073_mn_5	CL_D14-1073_mn_4_5
CL	D14-1073	mn	5	4	result	means	elaboration	by-means	secondary	secondary	back	by-means	We found that different summarization methods perform optimally for different evaluation metrics , but overall query biased word clouds are the best summarization strategy .	We report precision/recall/F1 , accuracy and time-on-task .	CL_D14-1073_mn_5	CL_D14-1073_mn_4	CL_D14-1073_mn_4_5
CL	D14-1073	mn	4	6	means	result	by-means	support	secondary	secondary	none	none	We report precision/recall/F1 , accuracy and time-on-task .	In our analysis , we demonstrate that using the ROUGE metric on our sentence-based summaries cannot make the same kinds of distinctions as our evaluation framework does .	CL_D14-1073_mn_4	CL_D14-1073_mn_6	CL_D14-1073_mn_4_6
CL	D14-1073	mn	4	7	means	conclusion	by-means	support	secondary	secondary	none	none	We report precision/recall/F1 , accuracy and time-on-task .	Finally , we present our recommendations for creating muchneeded evaluation standards and datasets .	CL_D14-1073_mn_4	CL_D14-1073_mn_7	CL_D14-1073_mn_4_7
CL	D14-1073	mn	5	6	result	result	elaboration	support	secondary	secondary	forw	elaboration	We found that different summarization methods perform optimally for different evaluation metrics , but overall query biased word clouds are the best summarization strategy .	In our analysis , we demonstrate that using the ROUGE metric on our sentence-based summaries cannot make the same kinds of distinctions as our evaluation framework does .	CL_D14-1073_mn_5	CL_D14-1073_mn_6	CL_D14-1073_mn_5_6
CL	D14-1073	mn	6	5	result	result	support	elaboration	secondary	secondary	back	elaboration	In our analysis , we demonstrate that using the ROUGE metric on our sentence-based summaries cannot make the same kinds of distinctions as our evaluation framework does .	We found that different summarization methods perform optimally for different evaluation metrics , but overall query biased word clouds are the best summarization strategy .	CL_D14-1073_mn_6	CL_D14-1073_mn_5	CL_D14-1073_mn_5_6
CL	D14-1073	mn	5	7	result	conclusion	elaboration	support	secondary	secondary	none	none	We found that different summarization methods perform optimally for different evaluation metrics , but overall query biased word clouds are the best summarization strategy .	Finally , we present our recommendations for creating muchneeded evaluation standards and datasets .	CL_D14-1073_mn_5	CL_D14-1073_mn_7	CL_D14-1073_mn_5_7
CL	D14-1073	mn	6	7	result	conclusion	support	support	secondary	secondary	forw	support	In our analysis , we demonstrate that using the ROUGE metric on our sentence-based summaries cannot make the same kinds of distinctions as our evaluation framework does .	Finally , we present our recommendations for creating muchneeded evaluation standards and datasets .	CL_D14-1073_mn_6	CL_D14-1073_mn_7	CL_D14-1073_mn_6_7
CL	D14-1073	mn	7	6	conclusion	result	support	support	secondary	secondary	back	support	Finally , we present our recommendations for creating muchneeded evaluation standards and datasets .	In our analysis , we demonstrate that using the ROUGE metric on our sentence-based summaries cannot make the same kinds of distinctions as our evaluation framework does .	CL_D14-1073_mn_7	CL_D14-1073_mn_6	CL_D14-1073_mn_6_7
CL	D14-1074	mn	1	2	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	We propose a model for Chinese poem generation based on recurrent neural networks which we argue is ideally suited to capturing poetic content and form .	Our generator jointly performs content selection ( "what to say" ) and surface realization ( "how to say" ) by learning representations of individual characters , and their combinations into one or more lines as well as how these mutually reinforce and constrain each other .	CL_D14-1074_mn_1	CL_D14-1074_mn_2	CL_D14-1074_mn_1_2
CL	D14-1074	mn	2	1	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	Our generator jointly performs content selection ( "what to say" ) and surface realization ( "how to say" ) by learning representations of individual characters , and their combinations into one or more lines as well as how these mutually reinforce and constrain each other .	We propose a model for Chinese poem generation based on recurrent neural networks which we argue is ideally suited to capturing poetic content and form .	CL_D14-1074_mn_2	CL_D14-1074_mn_1	CL_D14-1074_mn_1_2
CL	D14-1074	mn	1	3	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	We propose a model for Chinese poem generation based on recurrent neural networks which we argue is ideally suited to capturing poetic content and form .	Poem lines are generated incrementally by taking into account the entire history of what has been generated so far rather than the limited horizon imposed by the previous line or lexical n-grams .	CL_D14-1074_mn_1	CL_D14-1074_mn_3	CL_D14-1074_mn_1_3
CL	D14-1074	mn	1	4	proposal	result	none	support	main	secondary	back	support	We propose a model for Chinese poem generation based on recurrent neural networks which we argue is ideally suited to capturing poetic content and form .	Experimental results show that our model outperforms competitive Chinese poetry generation systems using both automatic and manual evaluation methods .	CL_D14-1074_mn_1	CL_D14-1074_mn_4	CL_D14-1074_mn_1_4
CL	D14-1074	mn	4	1	result	proposal	support	none	secondary	main	forw	support	Experimental results show that our model outperforms competitive Chinese poetry generation systems using both automatic and manual evaluation methods .	We propose a model for Chinese poem generation based on recurrent neural networks which we argue is ideally suited to capturing poetic content and form .	CL_D14-1074_mn_4	CL_D14-1074_mn_1	CL_D14-1074_mn_1_4
CL	D14-1074	mn	2	3	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	Our generator jointly performs content selection ( "what to say" ) and surface realization ( "how to say" ) by learning representations of individual characters , and their combinations into one or more lines as well as how these mutually reinforce and constrain each other .	Poem lines are generated incrementally by taking into account the entire history of what has been generated so far rather than the limited horizon imposed by the previous line or lexical n-grams .	CL_D14-1074_mn_2	CL_D14-1074_mn_3	CL_D14-1074_mn_2_3
CL	D14-1074	mn	3	2	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	forw	elaboration	Poem lines are generated incrementally by taking into account the entire history of what has been generated so far rather than the limited horizon imposed by the previous line or lexical n-grams .	Our generator jointly performs content selection ( "what to say" ) and surface realization ( "how to say" ) by learning representations of individual characters , and their combinations into one or more lines as well as how these mutually reinforce and constrain each other .	CL_D14-1074_mn_3	CL_D14-1074_mn_2	CL_D14-1074_mn_2_3
CL	D14-1074	mn	2	4	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	Our generator jointly performs content selection ( "what to say" ) and surface realization ( "how to say" ) by learning representations of individual characters , and their combinations into one or more lines as well as how these mutually reinforce and constrain each other .	Experimental results show that our model outperforms competitive Chinese poetry generation systems using both automatic and manual evaluation methods .	CL_D14-1074_mn_2	CL_D14-1074_mn_4	CL_D14-1074_mn_2_4
CL	D14-1074	mn	3	4	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	Poem lines are generated incrementally by taking into account the entire history of what has been generated so far rather than the limited horizon imposed by the previous line or lexical n-grams .	Experimental results show that our model outperforms competitive Chinese poetry generation systems using both automatic and manual evaluation methods .	CL_D14-1074_mn_3	CL_D14-1074_mn_4	CL_D14-1074_mn_3_4
CL	D14-1075	mn	1	2	proposal	result	none	support	main	secondary	back	support	This paper explores alternate algorithms , reward functions and feature sets for performing multi-document summarization using reinforcement learning with a high focus on reproducibility .	We show that ROUGE results can be improved using a unigram and bigram similarity metric when training a learner to select sentences for summarization .	CL_D14-1075_mn_1	CL_D14-1075_mn_2	CL_D14-1075_mn_1_2
CL	D14-1075	mn	2	1	result	proposal	support	none	secondary	main	forw	support	We show that ROUGE results can be improved using a unigram and bigram similarity metric when training a learner to select sentences for summarization .	This paper explores alternate algorithms , reward functions and feature sets for performing multi-document summarization using reinforcement learning with a high focus on reproducibility .	CL_D14-1075_mn_2	CL_D14-1075_mn_1	CL_D14-1075_mn_1_2
CL	D14-1075	mn	1	3	proposal	means	none	by-means	main	secondary	none	none	This paper explores alternate algorithms , reward functions and feature sets for performing multi-document summarization using reinforcement learning with a high focus on reproducibility .	Learners are trained to summarize document clusters based on various algorithms and reward functions and then evaluated using ROUGE .	CL_D14-1075_mn_1	CL_D14-1075_mn_3	CL_D14-1075_mn_1_3
CL	D14-1075	mn	1	4	proposal	observation	none	support	main	secondary	none	none	This paper explores alternate algorithms , reward functions and feature sets for performing multi-document summarization using reinforcement learning with a high focus on reproducibility .	Our experiments show a statistically significant improvement of 1.33 % , 1.58 % , and 2.25 % for ROUGE-1 , ROUGE-2 and ROUGE-L scores , respectively , when compared with the performance of the state of the art in automatic summarization with reinforcement learning on the DUC2004 dataset .	CL_D14-1075_mn_1	CL_D14-1075_mn_4	CL_D14-1075_mn_1_4
CL	D14-1075	mn	1	5	proposal	observation	none	support	main	secondary	none	none	This paper explores alternate algorithms , reward functions and feature sets for performing multi-document summarization using reinforcement learning with a high focus on reproducibility .	Furthermore query focused extensions of our approach show an improvement of 1.37 % and 2.31 % for ROUGE-2 and ROUGE-SU4 respectively over query focused extensions of the state of the art with reinforcement learning on the DUC2006 dataset .	CL_D14-1075_mn_1	CL_D14-1075_mn_5	CL_D14-1075_mn_1_5
CL	D14-1075	mn	2	3	result	means	support	by-means	secondary	secondary	back	by-means	We show that ROUGE results can be improved using a unigram and bigram similarity metric when training a learner to select sentences for summarization .	Learners are trained to summarize document clusters based on various algorithms and reward functions and then evaluated using ROUGE .	CL_D14-1075_mn_2	CL_D14-1075_mn_3	CL_D14-1075_mn_2_3
CL	D14-1075	mn	3	2	means	result	by-means	support	secondary	secondary	forw	by-means	Learners are trained to summarize document clusters based on various algorithms and reward functions and then evaluated using ROUGE .	We show that ROUGE results can be improved using a unigram and bigram similarity metric when training a learner to select sentences for summarization .	CL_D14-1075_mn_3	CL_D14-1075_mn_2	CL_D14-1075_mn_2_3
CL	D14-1075	mn	2	4	result	observation	support	support	secondary	secondary	back	support	We show that ROUGE results can be improved using a unigram and bigram similarity metric when training a learner to select sentences for summarization .	Our experiments show a statistically significant improvement of 1.33 % , 1.58 % , and 2.25 % for ROUGE-1 , ROUGE-2 and ROUGE-L scores , respectively , when compared with the performance of the state of the art in automatic summarization with reinforcement learning on the DUC2004 dataset .	CL_D14-1075_mn_2	CL_D14-1075_mn_4	CL_D14-1075_mn_2_4
CL	D14-1075	mn	4	2	observation	result	support	support	secondary	secondary	forw	support	Our experiments show a statistically significant improvement of 1.33 % , 1.58 % , and 2.25 % for ROUGE-1 , ROUGE-2 and ROUGE-L scores , respectively , when compared with the performance of the state of the art in automatic summarization with reinforcement learning on the DUC2004 dataset .	We show that ROUGE results can be improved using a unigram and bigram similarity metric when training a learner to select sentences for summarization .	CL_D14-1075_mn_4	CL_D14-1075_mn_2	CL_D14-1075_mn_2_4
CL	D14-1075	mn	2	5	result	observation	support	support	secondary	secondary	back	support	We show that ROUGE results can be improved using a unigram and bigram similarity metric when training a learner to select sentences for summarization .	Furthermore query focused extensions of our approach show an improvement of 1.37 % and 2.31 % for ROUGE-2 and ROUGE-SU4 respectively over query focused extensions of the state of the art with reinforcement learning on the DUC2006 dataset .	CL_D14-1075_mn_2	CL_D14-1075_mn_5	CL_D14-1075_mn_2_5
CL	D14-1075	mn	5	2	observation	result	support	support	secondary	secondary	forw	support	Furthermore query focused extensions of our approach show an improvement of 1.37 % and 2.31 % for ROUGE-2 and ROUGE-SU4 respectively over query focused extensions of the state of the art with reinforcement learning on the DUC2006 dataset .	We show that ROUGE results can be improved using a unigram and bigram similarity metric when training a learner to select sentences for summarization .	CL_D14-1075_mn_5	CL_D14-1075_mn_2	CL_D14-1075_mn_2_5
CL	D14-1075	mn	3	4	means	observation	by-means	support	secondary	secondary	none	none	Learners are trained to summarize document clusters based on various algorithms and reward functions and then evaluated using ROUGE .	Our experiments show a statistically significant improvement of 1.33 % , 1.58 % , and 2.25 % for ROUGE-1 , ROUGE-2 and ROUGE-L scores , respectively , when compared with the performance of the state of the art in automatic summarization with reinforcement learning on the DUC2004 dataset .	CL_D14-1075_mn_3	CL_D14-1075_mn_4	CL_D14-1075_mn_3_4
CL	D14-1075	mn	3	5	means	observation	by-means	support	secondary	secondary	none	none	Learners are trained to summarize document clusters based on various algorithms and reward functions and then evaluated using ROUGE .	Furthermore query focused extensions of our approach show an improvement of 1.37 % and 2.31 % for ROUGE-2 and ROUGE-SU4 respectively over query focused extensions of the state of the art with reinforcement learning on the DUC2006 dataset .	CL_D14-1075_mn_3	CL_D14-1075_mn_5	CL_D14-1075_mn_3_5
CL	D14-1075	mn	4	5	observation	observation	support	support	secondary	secondary	none	none	Our experiments show a statistically significant improvement of 1.33 % , 1.58 % , and 2.25 % for ROUGE-1 , ROUGE-2 and ROUGE-L scores , respectively , when compared with the performance of the state of the art in automatic summarization with reinforcement learning on the DUC2004 dataset .	Furthermore query focused extensions of our approach show an improvement of 1.37 % and 2.31 % for ROUGE-2 and ROUGE-SU4 respectively over query focused extensions of the state of the art with reinforcement learning on the DUC2006 dataset .	CL_D14-1075_mn_4	CL_D14-1075_mn_5	CL_D14-1075_mn_4_5
CL	D14-1076	mn	1	2	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	In this paper , we focus on the problem of using sentence compression techniques to improve multi-document summarization .	We propose an innovative sentence compression method by considering every node in the constituent parse tree and deciding its status - remove or retain .	CL_D14-1076_mn_1	CL_D14-1076_mn_2	CL_D14-1076_mn_1_2
CL	D14-1076	mn	2	1	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	We propose an innovative sentence compression method by considering every node in the constituent parse tree and deciding its status - remove or retain .	In this paper , we focus on the problem of using sentence compression techniques to improve multi-document summarization .	CL_D14-1076_mn_2	CL_D14-1076_mn_1	CL_D14-1076_mn_1_2
CL	D14-1076	mn	1	3	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	In this paper , we focus on the problem of using sentence compression techniques to improve multi-document summarization .	Integer liner programming with discriminative training is used to solve the problem .	CL_D14-1076_mn_1	CL_D14-1076_mn_3	CL_D14-1076_mn_1_3
CL	D14-1076	mn	1	4	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	In this paper , we focus on the problem of using sentence compression techniques to improve multi-document summarization .	Under this model , we incorporate various constraints to improve the linguistic quality of the compressed sentences .	CL_D14-1076_mn_1	CL_D14-1076_mn_4	CL_D14-1076_mn_1_4
CL	D14-1076	mn	1	5	proposal	proposal_implementation	none	sequence	main	secondary	none	none	In this paper , we focus on the problem of using sentence compression techniques to improve multi-document summarization .	Then we utilize a pipeline summarization framework where sentences are first compressed by our proposed compression model to obtain top-n candidates and then a sentence selection module is used to generate the final summary .	CL_D14-1076_mn_1	CL_D14-1076_mn_5	CL_D14-1076_mn_1_5
CL	D14-1076	mn	1	6	proposal	result_means	none	support	main	secondary	back	support	In this paper , we focus on the problem of using sentence compression techniques to improve multi-document summarization .	Compared with state-of-the-art algorithms , our model has similar ROUGE-2 scores but better linguistic quality on TAC data .	CL_D14-1076_mn_1	CL_D14-1076_mn_6	CL_D14-1076_mn_1_6
CL	D14-1076	mn	6	1	result_means	proposal	support	none	secondary	main	forw	support	Compared with state-of-the-art algorithms , our model has similar ROUGE-2 scores but better linguistic quality on TAC data .	In this paper , we focus on the problem of using sentence compression techniques to improve multi-document summarization .	CL_D14-1076_mn_6	CL_D14-1076_mn_1	CL_D14-1076_mn_1_6
CL	D14-1076	mn	2	3	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	We propose an innovative sentence compression method by considering every node in the constituent parse tree and deciding its status - remove or retain .	Integer liner programming with discriminative training is used to solve the problem .	CL_D14-1076_mn_2	CL_D14-1076_mn_3	CL_D14-1076_mn_2_3
CL	D14-1076	mn	3	2	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	forw	elaboration	Integer liner programming with discriminative training is used to solve the problem .	We propose an innovative sentence compression method by considering every node in the constituent parse tree and deciding its status - remove or retain .	CL_D14-1076_mn_3	CL_D14-1076_mn_2	CL_D14-1076_mn_2_3
CL	D14-1076	mn	2	4	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	none	none	We propose an innovative sentence compression method by considering every node in the constituent parse tree and deciding its status - remove or retain .	Under this model , we incorporate various constraints to improve the linguistic quality of the compressed sentences .	CL_D14-1076_mn_2	CL_D14-1076_mn_4	CL_D14-1076_mn_2_4
CL	D14-1076	mn	2	5	proposal_implementation	proposal_implementation	elaboration	sequence	secondary	secondary	none	none	We propose an innovative sentence compression method by considering every node in the constituent parse tree and deciding its status - remove or retain .	Then we utilize a pipeline summarization framework where sentences are first compressed by our proposed compression model to obtain top-n candidates and then a sentence selection module is used to generate the final summary .	CL_D14-1076_mn_2	CL_D14-1076_mn_5	CL_D14-1076_mn_2_5
CL	D14-1076	mn	2	6	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	We propose an innovative sentence compression method by considering every node in the constituent parse tree and deciding its status - remove or retain .	Compared with state-of-the-art algorithms , our model has similar ROUGE-2 scores but better linguistic quality on TAC data .	CL_D14-1076_mn_2	CL_D14-1076_mn_6	CL_D14-1076_mn_2_6
CL	D14-1076	mn	3	4	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	Integer liner programming with discriminative training is used to solve the problem .	Under this model , we incorporate various constraints to improve the linguistic quality of the compressed sentences .	CL_D14-1076_mn_3	CL_D14-1076_mn_4	CL_D14-1076_mn_3_4
CL	D14-1076	mn	4	3	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	forw	elaboration	Under this model , we incorporate various constraints to improve the linguistic quality of the compressed sentences .	Integer liner programming with discriminative training is used to solve the problem .	CL_D14-1076_mn_4	CL_D14-1076_mn_3	CL_D14-1076_mn_3_4
CL	D14-1076	mn	3	5	proposal_implementation	proposal_implementation	elaboration	sequence	secondary	secondary	none	none	Integer liner programming with discriminative training is used to solve the problem .	Then we utilize a pipeline summarization framework where sentences are first compressed by our proposed compression model to obtain top-n candidates and then a sentence selection module is used to generate the final summary .	CL_D14-1076_mn_3	CL_D14-1076_mn_5	CL_D14-1076_mn_3_5
CL	D14-1076	mn	3	6	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	Integer liner programming with discriminative training is used to solve the problem .	Compared with state-of-the-art algorithms , our model has similar ROUGE-2 scores but better linguistic quality on TAC data .	CL_D14-1076_mn_3	CL_D14-1076_mn_6	CL_D14-1076_mn_3_6
CL	D14-1076	mn	4	5	proposal_implementation	proposal_implementation	elaboration	sequence	secondary	secondary	back	sequence	Under this model , we incorporate various constraints to improve the linguistic quality of the compressed sentences .	Then we utilize a pipeline summarization framework where sentences are first compressed by our proposed compression model to obtain top-n candidates and then a sentence selection module is used to generate the final summary .	CL_D14-1076_mn_4	CL_D14-1076_mn_5	CL_D14-1076_mn_4_5
CL	D14-1076	mn	5	4	proposal_implementation	proposal_implementation	sequence	elaboration	secondary	secondary	forw	sequence	Then we utilize a pipeline summarization framework where sentences are first compressed by our proposed compression model to obtain top-n candidates and then a sentence selection module is used to generate the final summary .	Under this model , we incorporate various constraints to improve the linguistic quality of the compressed sentences .	CL_D14-1076_mn_5	CL_D14-1076_mn_4	CL_D14-1076_mn_4_5
CL	D14-1076	mn	4	6	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	Under this model , we incorporate various constraints to improve the linguistic quality of the compressed sentences .	Compared with state-of-the-art algorithms , our model has similar ROUGE-2 scores but better linguistic quality on TAC data .	CL_D14-1076_mn_4	CL_D14-1076_mn_6	CL_D14-1076_mn_4_6
CL	D14-1076	mn	5	6	proposal_implementation	result_means	sequence	support	secondary	secondary	none	none	Then we utilize a pipeline summarization framework where sentences are first compressed by our proposed compression model to obtain top-n candidates and then a sentence selection module is used to generate the final summary .	Compared with state-of-the-art algorithms , our model has similar ROUGE-2 scores but better linguistic quality on TAC data .	CL_D14-1076_mn_5	CL_D14-1076_mn_6	CL_D14-1076_mn_5_6
CL	D14-1077	mn	1	2	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	In this study , we analyzed the effects of applying different levels of stemming approaches such as fixed-length word truncation and morphological analysis for multi-document summarization ( MDS ) on Turkish , which is an agglutinative and morphologically rich language .	We constructed a manually annotated MDS data set , and to our best knowledge , reported the first results on Turkish MDS .	CL_D14-1077_mn_1	CL_D14-1077_mn_2	CL_D14-1077_mn_1_2
CL	D14-1077	mn	2	1	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	We constructed a manually annotated MDS data set , and to our best knowledge , reported the first results on Turkish MDS .	In this study , we analyzed the effects of applying different levels of stemming approaches such as fixed-length word truncation and morphological analysis for multi-document summarization ( MDS ) on Turkish , which is an agglutinative and morphologically rich language .	CL_D14-1077_mn_2	CL_D14-1077_mn_1	CL_D14-1077_mn_1_2
CL	D14-1077	mn	1	3	proposal	result	none	support	main	secondary	back	support	In this study , we analyzed the effects of applying different levels of stemming approaches such as fixed-length word truncation and morphological analysis for multi-document summarization ( MDS ) on Turkish , which is an agglutinative and morphologically rich language .	Our results show that a simple fixed-length word truncation approach performs slightly better than no stemming , whereas applying complex morphological analysis does not improve Turkish MDS .	CL_D14-1077_mn_1	CL_D14-1077_mn_3	CL_D14-1077_mn_1_3
CL	D14-1077	mn	3	1	result	proposal	support	none	secondary	main	forw	support	Our results show that a simple fixed-length word truncation approach performs slightly better than no stemming , whereas applying complex morphological analysis does not improve Turkish MDS .	In this study , we analyzed the effects of applying different levels of stemming approaches such as fixed-length word truncation and morphological analysis for multi-document summarization ( MDS ) on Turkish , which is an agglutinative and morphologically rich language .	CL_D14-1077_mn_3	CL_D14-1077_mn_1	CL_D14-1077_mn_1_3
CL	D14-1077	mn	2	3	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	We constructed a manually annotated MDS data set , and to our best knowledge , reported the first results on Turkish MDS .	Our results show that a simple fixed-length word truncation approach performs slightly better than no stemming , whereas applying complex morphological analysis does not improve Turkish MDS .	CL_D14-1077_mn_2	CL_D14-1077_mn_3	CL_D14-1077_mn_2_3
CL	D14-1078	mn	1	2	motivation_background	motivation_background	info-required	info-required	secondary	secondary	forw	info-required	The ability to learn from user interactions can give systems access to unprecedented amounts of knowledge .	This is evident in search engines , recommender systems , and electronic commerce , and it can be the key to solving other knowledge in-tensive tasks .	CL_D14-1078_mn_1	CL_D14-1078_mn_2	CL_D14-1078_mn_1_2
CL	D14-1078	mn	2	1	motivation_background	motivation_background	info-required	info-required	secondary	secondary	back	info-required	This is evident in search engines , recommender systems , and electronic commerce , and it can be the key to solving other knowledge in-tensive tasks .	The ability to learn from user interactions can give systems access to unprecedented amounts of knowledge .	CL_D14-1078_mn_2	CL_D14-1078_mn_1	CL_D14-1078_mn_1_2
CL	D14-1078	mn	1	3	motivation_background	motivation_problem	info-required	support	secondary	secondary	none	none	The ability to learn from user interactions can give systems access to unprecedented amounts of knowledge .	However , extracting the knowledge conveyed by user interactions is less straightforward than standard machine learning , since it requires learning systems that explicitly account for human decision making , human motivation , and human abilities .	CL_D14-1078_mn_1	CL_D14-1078_mn_3	CL_D14-1078_mn_1_3
CL	D14-1078	mn	1	4	motivation_background	motivation_hypothesis	info-required	support	secondary	secondary	none	none	The ability to learn from user interactions can give systems access to unprecedented amounts of knowledge .	In this talk , I argue that the design space of such interactive learning systems encompasses not only the machine learning algorithm itself , but also the design of the interaction under an appropriate model of user behavior .	CL_D14-1078_mn_1	CL_D14-1078_mn_4	CL_D14-1078_mn_1_4
CL	D14-1078	mn	1	5	motivation_background	proposal	info-required	none	secondary	main	none	none	The ability to learn from user interactions can give systems access to unprecedented amounts of knowledge .	To this effect , the talk explores how integrating microeconomic models of human behavior into the learning process leads to new interaction models and their associated learning algorithms , leading to systems that have provable guarantees and that perform robustly in practice .	CL_D14-1078_mn_1	CL_D14-1078_mn_5	CL_D14-1078_mn_1_5
CL	D14-1078	mn	2	3	motivation_background	motivation_problem	info-required	support	secondary	secondary	forw	info-required	This is evident in search engines , recommender systems , and electronic commerce , and it can be the key to solving other knowledge in-tensive tasks .	However , extracting the knowledge conveyed by user interactions is less straightforward than standard machine learning , since it requires learning systems that explicitly account for human decision making , human motivation , and human abilities .	CL_D14-1078_mn_2	CL_D14-1078_mn_3	CL_D14-1078_mn_2_3
CL	D14-1078	mn	3	2	motivation_problem	motivation_background	support	info-required	secondary	secondary	back	info-required	However , extracting the knowledge conveyed by user interactions is less straightforward than standard machine learning , since it requires learning systems that explicitly account for human decision making , human motivation , and human abilities .	This is evident in search engines , recommender systems , and electronic commerce , and it can be the key to solving other knowledge in-tensive tasks .	CL_D14-1078_mn_3	CL_D14-1078_mn_2	CL_D14-1078_mn_2_3
CL	D14-1078	mn	2	4	motivation_background	motivation_hypothesis	info-required	support	secondary	secondary	none	none	This is evident in search engines , recommender systems , and electronic commerce , and it can be the key to solving other knowledge in-tensive tasks .	In this talk , I argue that the design space of such interactive learning systems encompasses not only the machine learning algorithm itself , but also the design of the interaction under an appropriate model of user behavior .	CL_D14-1078_mn_2	CL_D14-1078_mn_4	CL_D14-1078_mn_2_4
CL	D14-1078	mn	2	5	motivation_background	proposal	info-required	none	secondary	main	none	none	This is evident in search engines , recommender systems , and electronic commerce , and it can be the key to solving other knowledge in-tensive tasks .	To this effect , the talk explores how integrating microeconomic models of human behavior into the learning process leads to new interaction models and their associated learning algorithms , leading to systems that have provable guarantees and that perform robustly in practice .	CL_D14-1078_mn_2	CL_D14-1078_mn_5	CL_D14-1078_mn_2_5
CL	D14-1078	mn	3	4	motivation_problem	motivation_hypothesis	support	support	secondary	secondary	forw	support	However , extracting the knowledge conveyed by user interactions is less straightforward than standard machine learning , since it requires learning systems that explicitly account for human decision making , human motivation , and human abilities .	In this talk , I argue that the design space of such interactive learning systems encompasses not only the machine learning algorithm itself , but also the design of the interaction under an appropriate model of user behavior .	CL_D14-1078_mn_3	CL_D14-1078_mn_4	CL_D14-1078_mn_3_4
CL	D14-1078	mn	4	3	motivation_hypothesis	motivation_problem	support	support	secondary	secondary	back	support	In this talk , I argue that the design space of such interactive learning systems encompasses not only the machine learning algorithm itself , but also the design of the interaction under an appropriate model of user behavior .	However , extracting the knowledge conveyed by user interactions is less straightforward than standard machine learning , since it requires learning systems that explicitly account for human decision making , human motivation , and human abilities .	CL_D14-1078_mn_4	CL_D14-1078_mn_3	CL_D14-1078_mn_3_4
CL	D14-1078	mn	3	5	motivation_problem	proposal	support	none	secondary	main	none	none	However , extracting the knowledge conveyed by user interactions is less straightforward than standard machine learning , since it requires learning systems that explicitly account for human decision making , human motivation , and human abilities .	To this effect , the talk explores how integrating microeconomic models of human behavior into the learning process leads to new interaction models and their associated learning algorithms , leading to systems that have provable guarantees and that perform robustly in practice .	CL_D14-1078_mn_3	CL_D14-1078_mn_5	CL_D14-1078_mn_3_5
CL	D14-1078	mn	4	5	motivation_hypothesis	proposal	support	none	secondary	main	forw	support	In this talk , I argue that the design space of such interactive learning systems encompasses not only the machine learning algorithm itself , but also the design of the interaction under an appropriate model of user behavior .	To this effect , the talk explores how integrating microeconomic models of human behavior into the learning process leads to new interaction models and their associated learning algorithms , leading to systems that have provable guarantees and that perform robustly in practice .	CL_D14-1078_mn_4	CL_D14-1078_mn_5	CL_D14-1078_mn_4_5
CL	D14-1078	mn	5	4	proposal	motivation_hypothesis	none	support	main	secondary	back	support	To this effect , the talk explores how integrating microeconomic models of human behavior into the learning process leads to new interaction models and their associated learning algorithms , leading to systems that have provable guarantees and that perform robustly in practice .	In this talk , I argue that the design space of such interactive learning systems encompasses not only the machine learning algorithm itself , but also the design of the interaction under an appropriate model of user behavior .	CL_D14-1078_mn_5	CL_D14-1078_mn_4	CL_D14-1078_mn_4_5
CL	D14-1079	mn	1	2	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	We provide a comparative study between neural word representations and traditional vector spaces based on co-occurrence counts , in a number of compositional tasks .	We use three different semantic spaces and implement seven tensor-based compositional models , which we then test ( together with simpler additive and multiplicative approaches ) in tasks involving verb disambiguation and sentence similarity .	CL_D14-1079_mn_1	CL_D14-1079_mn_2	CL_D14-1079_mn_1_2
CL	D14-1079	mn	2	1	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	We use three different semantic spaces and implement seven tensor-based compositional models , which we then test ( together with simpler additive and multiplicative approaches ) in tasks involving verb disambiguation and sentence similarity .	We provide a comparative study between neural word representations and traditional vector spaces based on co-occurrence counts , in a number of compositional tasks .	CL_D14-1079_mn_2	CL_D14-1079_mn_1	CL_D14-1079_mn_1_2
CL	D14-1079	mn	1	3	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	We provide a comparative study between neural word representations and traditional vector spaces based on co-occurrence counts , in a number of compositional tasks .	To check their scalability , we additionally evaluate the spaces using simple compositional methods on larger-scale tasks with less constrained language : paraphrase detection and dialogue act tagging .	CL_D14-1079_mn_1	CL_D14-1079_mn_3	CL_D14-1079_mn_1_3
CL	D14-1079	mn	1	4	proposal	result_means	none	support	main	secondary	back	support	We provide a comparative study between neural word representations and traditional vector spaces based on co-occurrence counts , in a number of compositional tasks .	In the more constrained tasks , co-occurrence vectors are competitive , although choice of compositional method is important ; on the larger-scale tasks , they are outperformed by neural word embeddings , which show robust , stable performance across the tasks .	CL_D14-1079_mn_1	CL_D14-1079_mn_4	CL_D14-1079_mn_1_4
CL	D14-1079	mn	4	1	result_means	proposal	support	none	secondary	main	forw	support	In the more constrained tasks , co-occurrence vectors are competitive , although choice of compositional method is important ; on the larger-scale tasks , they are outperformed by neural word embeddings , which show robust , stable performance across the tasks .	We provide a comparative study between neural word representations and traditional vector spaces based on co-occurrence counts , in a number of compositional tasks .	CL_D14-1079_mn_4	CL_D14-1079_mn_1	CL_D14-1079_mn_1_4
CL	D14-1079	mn	2	3	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	We use three different semantic spaces and implement seven tensor-based compositional models , which we then test ( together with simpler additive and multiplicative approaches ) in tasks involving verb disambiguation and sentence similarity .	To check their scalability , we additionally evaluate the spaces using simple compositional methods on larger-scale tasks with less constrained language : paraphrase detection and dialogue act tagging .	CL_D14-1079_mn_2	CL_D14-1079_mn_3	CL_D14-1079_mn_2_3
CL	D14-1079	mn	3	2	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	forw	elaboration	To check their scalability , we additionally evaluate the spaces using simple compositional methods on larger-scale tasks with less constrained language : paraphrase detection and dialogue act tagging .	We use three different semantic spaces and implement seven tensor-based compositional models , which we then test ( together with simpler additive and multiplicative approaches ) in tasks involving verb disambiguation and sentence similarity .	CL_D14-1079_mn_3	CL_D14-1079_mn_2	CL_D14-1079_mn_2_3
CL	D14-1079	mn	2	4	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	We use three different semantic spaces and implement seven tensor-based compositional models , which we then test ( together with simpler additive and multiplicative approaches ) in tasks involving verb disambiguation and sentence similarity .	In the more constrained tasks , co-occurrence vectors are competitive , although choice of compositional method is important ; on the larger-scale tasks , they are outperformed by neural word embeddings , which show robust , stable performance across the tasks .	CL_D14-1079_mn_2	CL_D14-1079_mn_4	CL_D14-1079_mn_2_4
CL	D14-1079	mn	3	4	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	To check their scalability , we additionally evaluate the spaces using simple compositional methods on larger-scale tasks with less constrained language : paraphrase detection and dialogue act tagging .	In the more constrained tasks , co-occurrence vectors are competitive , although choice of compositional method is important ; on the larger-scale tasks , they are outperformed by neural word embeddings , which show robust , stable performance across the tasks .	CL_D14-1079_mn_3	CL_D14-1079_mn_4	CL_D14-1079_mn_3_4
CL	D14-1080	mn	1	2	motivation_background	motivation_background	info-required	support	secondary	secondary	forw	info-required	Recurrent neural networks ( RNNs ) are connectionist models of sequential data that are naturally applicable to the analysis of natural language .	Recently , "depth in space"  as an orthogonal notion to "depth in time"  in RNNs has been investigated by stacking multiple layers of RNNs and shown empirically to bring a temporal hierarchy to the architecture .	CL_D14-1080_mn_1	CL_D14-1080_mn_2	CL_D14-1080_mn_1_2
CL	D14-1080	mn	2	1	motivation_background	motivation_background	support	info-required	secondary	secondary	back	info-required	Recently , "depth in space"  as an orthogonal notion to "depth in time"  in RNNs has been investigated by stacking multiple layers of RNNs and shown empirically to bring a temporal hierarchy to the architecture .	Recurrent neural networks ( RNNs ) are connectionist models of sequential data that are naturally applicable to the analysis of natural language .	CL_D14-1080_mn_2	CL_D14-1080_mn_1	CL_D14-1080_mn_1_2
CL	D14-1080	mn	1	3	motivation_background	proposal	info-required	none	secondary	main	none	none	Recurrent neural networks ( RNNs ) are connectionist models of sequential data that are naturally applicable to the analysis of natural language .	In this work we apply these deep RNNs to the task of opinion expression extraction formulated as a token-level sequence-labeling task .	CL_D14-1080_mn_1	CL_D14-1080_mn_3	CL_D14-1080_mn_1_3
CL	D14-1080	mn	1	4	motivation_background	result	info-required	support	secondary	secondary	none	none	Recurrent neural networks ( RNNs ) are connectionist models of sequential data that are naturally applicable to the analysis of natural language .	Experimental results show that deep , narrow RNNs outperform traditional shallow , wide RNNs with the same number of parameters .	CL_D14-1080_mn_1	CL_D14-1080_mn_4	CL_D14-1080_mn_1_4
CL	D14-1080	mn	1	5	motivation_background	result	info-required	elaboration	secondary	secondary	none	none	Recurrent neural networks ( RNNs ) are connectionist models of sequential data that are naturally applicable to the analysis of natural language .	Furthermore , our approach outperforms previous CRF-based baselines , including the state-of-the-art semi-Markov CRF model , and does so without access to the powerful opinion lexicons and syntactic features relied upon by the semi-CRF , as well as without the standard layer-by-layer pre-training typically required of RNN architectures .	CL_D14-1080_mn_1	CL_D14-1080_mn_5	CL_D14-1080_mn_1_5
CL	D14-1080	mn	2	3	motivation_background	proposal	support	none	secondary	main	forw	support	Recently , "depth in space"  as an orthogonal notion to "depth in time"  in RNNs has been investigated by stacking multiple layers of RNNs and shown empirically to bring a temporal hierarchy to the architecture .	In this work we apply these deep RNNs to the task of opinion expression extraction formulated as a token-level sequence-labeling task .	CL_D14-1080_mn_2	CL_D14-1080_mn_3	CL_D14-1080_mn_2_3
CL	D14-1080	mn	3	2	proposal	motivation_background	none	support	main	secondary	back	support	In this work we apply these deep RNNs to the task of opinion expression extraction formulated as a token-level sequence-labeling task .	Recently , "depth in space"  as an orthogonal notion to "depth in time"  in RNNs has been investigated by stacking multiple layers of RNNs and shown empirically to bring a temporal hierarchy to the architecture .	CL_D14-1080_mn_3	CL_D14-1080_mn_2	CL_D14-1080_mn_2_3
CL	D14-1080	mn	2	4	motivation_background	result	support	support	secondary	secondary	none	none	Recently , "depth in space"  as an orthogonal notion to "depth in time"  in RNNs has been investigated by stacking multiple layers of RNNs and shown empirically to bring a temporal hierarchy to the architecture .	Experimental results show that deep , narrow RNNs outperform traditional shallow , wide RNNs with the same number of parameters .	CL_D14-1080_mn_2	CL_D14-1080_mn_4	CL_D14-1080_mn_2_4
CL	D14-1080	mn	2	5	motivation_background	result	support	elaboration	secondary	secondary	none	none	Recently , "depth in space"  as an orthogonal notion to "depth in time"  in RNNs has been investigated by stacking multiple layers of RNNs and shown empirically to bring a temporal hierarchy to the architecture .	Furthermore , our approach outperforms previous CRF-based baselines , including the state-of-the-art semi-Markov CRF model , and does so without access to the powerful opinion lexicons and syntactic features relied upon by the semi-CRF , as well as without the standard layer-by-layer pre-training typically required of RNN architectures .	CL_D14-1080_mn_2	CL_D14-1080_mn_5	CL_D14-1080_mn_2_5
CL	D14-1080	mn	3	4	proposal	result	none	support	main	secondary	back	support	In this work we apply these deep RNNs to the task of opinion expression extraction formulated as a token-level sequence-labeling task .	Experimental results show that deep , narrow RNNs outperform traditional shallow , wide RNNs with the same number of parameters .	CL_D14-1080_mn_3	CL_D14-1080_mn_4	CL_D14-1080_mn_3_4
CL	D14-1080	mn	4	3	result	proposal	support	none	secondary	main	forw	support	Experimental results show that deep , narrow RNNs outperform traditional shallow , wide RNNs with the same number of parameters .	In this work we apply these deep RNNs to the task of opinion expression extraction formulated as a token-level sequence-labeling task .	CL_D14-1080_mn_4	CL_D14-1080_mn_3	CL_D14-1080_mn_3_4
CL	D14-1080	mn	3	5	proposal	result	none	elaboration	main	secondary	none	none	In this work we apply these deep RNNs to the task of opinion expression extraction formulated as a token-level sequence-labeling task .	Furthermore , our approach outperforms previous CRF-based baselines , including the state-of-the-art semi-Markov CRF model , and does so without access to the powerful opinion lexicons and syntactic features relied upon by the semi-CRF , as well as without the standard layer-by-layer pre-training typically required of RNN architectures .	CL_D14-1080_mn_3	CL_D14-1080_mn_5	CL_D14-1080_mn_3_5
CL	D14-1080	mn	4	5	result	result	support	elaboration	secondary	secondary	back	elaboration	Experimental results show that deep , narrow RNNs outperform traditional shallow , wide RNNs with the same number of parameters .	Furthermore , our approach outperforms previous CRF-based baselines , including the state-of-the-art semi-Markov CRF model , and does so without access to the powerful opinion lexicons and syntactic features relied upon by the semi-CRF , as well as without the standard layer-by-layer pre-training typically required of RNN architectures .	CL_D14-1080_mn_4	CL_D14-1080_mn_5	CL_D14-1080_mn_4_5
CL	D14-1080	mn	5	4	result	result	elaboration	support	secondary	secondary	forw	elaboration	Furthermore , our approach outperforms previous CRF-based baselines , including the state-of-the-art semi-Markov CRF model , and does so without access to the powerful opinion lexicons and syntactic features relied upon by the semi-CRF , as well as without the standard layer-by-layer pre-training typically required of RNN architectures .	Experimental results show that deep , narrow RNNs outperform traditional shallow , wide RNNs with the same number of parameters .	CL_D14-1080_mn_5	CL_D14-1080_mn_4	CL_D14-1080_mn_4_5
CL	D14-1081	mn	1	2	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	We propose the first implementation of an infinite-order generative dependency model .	The model is based on a new recursive neural network architecture , the Inside-Outside Recursive Neural Network .	CL_D14-1081_mn_1	CL_D14-1081_mn_2	CL_D14-1081_mn_1_2
CL	D14-1081	mn	2	1	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	The model is based on a new recursive neural network architecture , the Inside-Outside Recursive Neural Network .	We propose the first implementation of an infinite-order generative dependency model .	CL_D14-1081_mn_2	CL_D14-1081_mn_1	CL_D14-1081_mn_1_2
CL	D14-1081	mn	1	3	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	We propose the first implementation of an infinite-order generative dependency model .	This architecture allows information to flow not only bottom-up , as in traditional recursive neural networks , but also top-down .	CL_D14-1081_mn_1	CL_D14-1081_mn_3	CL_D14-1081_mn_1_3
CL	D14-1081	mn	1	4	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	We propose the first implementation of an infinite-order generative dependency model .	This is achieved by computing content as well as context representations for any constituent , and letting these representations interact .	CL_D14-1081_mn_1	CL_D14-1081_mn_4	CL_D14-1081_mn_1_4
CL	D14-1081	mn	1	5	proposal	result_means	none	support	main	secondary	back	support	We propose the first implementation of an infinite-order generative dependency model .	Experimental results on the English section of the Universal Dependency Treebank show that the infinite-order model achieves a perplexity seven times lower than the traditional third-order model using counting , and tends to choose more accurate parses in k-best lists .	CL_D14-1081_mn_1	CL_D14-1081_mn_5	CL_D14-1081_mn_1_5
CL	D14-1081	mn	5	1	result_means	proposal	support	none	secondary	main	forw	support	Experimental results on the English section of the Universal Dependency Treebank show that the infinite-order model achieves a perplexity seven times lower than the traditional third-order model using counting , and tends to choose more accurate parses in k-best lists .	We propose the first implementation of an infinite-order generative dependency model .	CL_D14-1081_mn_5	CL_D14-1081_mn_1	CL_D14-1081_mn_1_5
CL	D14-1081	mn	1	6	proposal	result	none	elaboration	main	secondary	none	none	We propose the first implementation of an infinite-order generative dependency model .	In addition , reranking with this model achieves state-of-the-art unlabelled attachment scores and unlabelled exact match scores .	CL_D14-1081_mn_1	CL_D14-1081_mn_6	CL_D14-1081_mn_1_6
CL	D14-1081	mn	2	3	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	The model is based on a new recursive neural network architecture , the Inside-Outside Recursive Neural Network .	This architecture allows information to flow not only bottom-up , as in traditional recursive neural networks , but also top-down .	CL_D14-1081_mn_2	CL_D14-1081_mn_3	CL_D14-1081_mn_2_3
CL	D14-1081	mn	3	2	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	forw	elaboration	This architecture allows information to flow not only bottom-up , as in traditional recursive neural networks , but also top-down .	The model is based on a new recursive neural network architecture , the Inside-Outside Recursive Neural Network .	CL_D14-1081_mn_3	CL_D14-1081_mn_2	CL_D14-1081_mn_2_3
CL	D14-1081	mn	2	4	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	none	none	The model is based on a new recursive neural network architecture , the Inside-Outside Recursive Neural Network .	This is achieved by computing content as well as context representations for any constituent , and letting these representations interact .	CL_D14-1081_mn_2	CL_D14-1081_mn_4	CL_D14-1081_mn_2_4
CL	D14-1081	mn	2	5	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	The model is based on a new recursive neural network architecture , the Inside-Outside Recursive Neural Network .	Experimental results on the English section of the Universal Dependency Treebank show that the infinite-order model achieves a perplexity seven times lower than the traditional third-order model using counting , and tends to choose more accurate parses in k-best lists .	CL_D14-1081_mn_2	CL_D14-1081_mn_5	CL_D14-1081_mn_2_5
CL	D14-1081	mn	2	6	proposal_implementation	result	elaboration	elaboration	secondary	secondary	none	none	The model is based on a new recursive neural network architecture , the Inside-Outside Recursive Neural Network .	In addition , reranking with this model achieves state-of-the-art unlabelled attachment scores and unlabelled exact match scores .	CL_D14-1081_mn_2	CL_D14-1081_mn_6	CL_D14-1081_mn_2_6
CL	D14-1081	mn	3	4	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	This architecture allows information to flow not only bottom-up , as in traditional recursive neural networks , but also top-down .	This is achieved by computing content as well as context representations for any constituent , and letting these representations interact .	CL_D14-1081_mn_3	CL_D14-1081_mn_4	CL_D14-1081_mn_3_4
CL	D14-1081	mn	4	3	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	forw	elaboration	This is achieved by computing content as well as context representations for any constituent , and letting these representations interact .	This architecture allows information to flow not only bottom-up , as in traditional recursive neural networks , but also top-down .	CL_D14-1081_mn_4	CL_D14-1081_mn_3	CL_D14-1081_mn_3_4
CL	D14-1081	mn	3	5	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	This architecture allows information to flow not only bottom-up , as in traditional recursive neural networks , but also top-down .	Experimental results on the English section of the Universal Dependency Treebank show that the infinite-order model achieves a perplexity seven times lower than the traditional third-order model using counting , and tends to choose more accurate parses in k-best lists .	CL_D14-1081_mn_3	CL_D14-1081_mn_5	CL_D14-1081_mn_3_5
CL	D14-1081	mn	3	6	proposal_implementation	result	elaboration	elaboration	secondary	secondary	none	none	This architecture allows information to flow not only bottom-up , as in traditional recursive neural networks , but also top-down .	In addition , reranking with this model achieves state-of-the-art unlabelled attachment scores and unlabelled exact match scores .	CL_D14-1081_mn_3	CL_D14-1081_mn_6	CL_D14-1081_mn_3_6
CL	D14-1081	mn	4	5	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	This is achieved by computing content as well as context representations for any constituent , and letting these representations interact .	Experimental results on the English section of the Universal Dependency Treebank show that the infinite-order model achieves a perplexity seven times lower than the traditional third-order model using counting , and tends to choose more accurate parses in k-best lists .	CL_D14-1081_mn_4	CL_D14-1081_mn_5	CL_D14-1081_mn_4_5
CL	D14-1081	mn	4	6	proposal_implementation	result	elaboration	elaboration	secondary	secondary	none	none	This is achieved by computing content as well as context representations for any constituent , and letting these representations interact .	In addition , reranking with this model achieves state-of-the-art unlabelled attachment scores and unlabelled exact match scores .	CL_D14-1081_mn_4	CL_D14-1081_mn_6	CL_D14-1081_mn_4_6
CL	D14-1081	mn	5	6	result_means	result	support	elaboration	secondary	secondary	back	elaboration	Experimental results on the English section of the Universal Dependency Treebank show that the infinite-order model achieves a perplexity seven times lower than the traditional third-order model using counting , and tends to choose more accurate parses in k-best lists .	In addition , reranking with this model achieves state-of-the-art unlabelled attachment scores and unlabelled exact match scores .	CL_D14-1081_mn_5	CL_D14-1081_mn_6	CL_D14-1081_mn_5_6
CL	D14-1081	mn	6	5	result	result_means	elaboration	support	secondary	secondary	forw	elaboration	In addition , reranking with this model achieves state-of-the-art unlabelled attachment scores and unlabelled exact match scores .	Experimental results on the English section of the Universal Dependency Treebank show that the infinite-order model achieves a perplexity seven times lower than the traditional third-order model using counting , and tends to choose more accurate parses in k-best lists .	CL_D14-1081_mn_6	CL_D14-1081_mn_5	CL_D14-1081_mn_5_6
CL	D14-1082	mn	1	2	motivation_background	motivation_problem	info-required	support	secondary	secondary	forw	info-required	Almost all current dependency parsers classify based on millions of sparse indicator features .	Not only do these features generalize poorly , but the cost of feature computation restricts parsing speed significantly .	CL_D14-1082_mn_1	CL_D14-1082_mn_2	CL_D14-1082_mn_1_2
CL	D14-1082	mn	2	1	motivation_problem	motivation_background	support	info-required	secondary	secondary	back	info-required	Not only do these features generalize poorly , but the cost of feature computation restricts parsing speed significantly .	Almost all current dependency parsers classify based on millions of sparse indicator features .	CL_D14-1082_mn_2	CL_D14-1082_mn_1	CL_D14-1082_mn_1_2
CL	D14-1082	mn	1	3	motivation_background	proposal	info-required	none	secondary	main	none	none	Almost all current dependency parsers classify based on millions of sparse indicator features .	In this work , we propose a novel way of learning a neural network classifier for use in a greedy , transition-based dependency parser .	CL_D14-1082_mn_1	CL_D14-1082_mn_3	CL_D14-1082_mn_1_3
CL	D14-1082	mn	1	4	motivation_background	result_means	info-required	support	secondary	secondary	none	none	Almost all current dependency parsers classify based on millions of sparse indicator features .	Because this classifier learns and uses just a small number of dense features , it can work very fast , while achiev-ing an about 2 % improvement in unlabeled and labeled attachment scores on both English and Chinese datasets .	CL_D14-1082_mn_1	CL_D14-1082_mn_4	CL_D14-1082_mn_1_4
CL	D14-1082	mn	1	5	motivation_background	observation	info-required	support	secondary	secondary	none	none	Almost all current dependency parsers classify based on millions of sparse indicator features .	Concretely , our parser is able to parse more than 1000 sentences per second at 92.2 % unlabeled attachment score on the English Penn Treebank .	CL_D14-1082_mn_1	CL_D14-1082_mn_5	CL_D14-1082_mn_1_5
CL	D14-1082	mn	2	3	motivation_problem	proposal	support	none	secondary	main	forw	support	Not only do these features generalize poorly , but the cost of feature computation restricts parsing speed significantly .	In this work , we propose a novel way of learning a neural network classifier for use in a greedy , transition-based dependency parser .	CL_D14-1082_mn_2	CL_D14-1082_mn_3	CL_D14-1082_mn_2_3
CL	D14-1082	mn	3	2	proposal	motivation_problem	none	support	main	secondary	back	support	In this work , we propose a novel way of learning a neural network classifier for use in a greedy , transition-based dependency parser .	Not only do these features generalize poorly , but the cost of feature computation restricts parsing speed significantly .	CL_D14-1082_mn_3	CL_D14-1082_mn_2	CL_D14-1082_mn_2_3
CL	D14-1082	mn	2	4	motivation_problem	result_means	support	support	secondary	secondary	none	none	Not only do these features generalize poorly , but the cost of feature computation restricts parsing speed significantly .	Because this classifier learns and uses just a small number of dense features , it can work very fast , while achiev-ing an about 2 % improvement in unlabeled and labeled attachment scores on both English and Chinese datasets .	CL_D14-1082_mn_2	CL_D14-1082_mn_4	CL_D14-1082_mn_2_4
CL	D14-1082	mn	2	5	motivation_problem	observation	support	support	secondary	secondary	none	none	Not only do these features generalize poorly , but the cost of feature computation restricts parsing speed significantly .	Concretely , our parser is able to parse more than 1000 sentences per second at 92.2 % unlabeled attachment score on the English Penn Treebank .	CL_D14-1082_mn_2	CL_D14-1082_mn_5	CL_D14-1082_mn_2_5
CL	D14-1082	mn	3	4	proposal	result_means	none	support	main	secondary	back	support	In this work , we propose a novel way of learning a neural network classifier for use in a greedy , transition-based dependency parser .	Because this classifier learns and uses just a small number of dense features , it can work very fast , while achiev-ing an about 2 % improvement in unlabeled and labeled attachment scores on both English and Chinese datasets .	CL_D14-1082_mn_3	CL_D14-1082_mn_4	CL_D14-1082_mn_3_4
CL	D14-1082	mn	4	3	result_means	proposal	support	none	secondary	main	forw	support	Because this classifier learns and uses just a small number of dense features , it can work very fast , while achiev-ing an about 2 % improvement in unlabeled and labeled attachment scores on both English and Chinese datasets .	In this work , we propose a novel way of learning a neural network classifier for use in a greedy , transition-based dependency parser .	CL_D14-1082_mn_4	CL_D14-1082_mn_3	CL_D14-1082_mn_3_4
CL	D14-1082	mn	3	5	proposal	observation	none	support	main	secondary	none	none	In this work , we propose a novel way of learning a neural network classifier for use in a greedy , transition-based dependency parser .	Concretely , our parser is able to parse more than 1000 sentences per second at 92.2 % unlabeled attachment score on the English Penn Treebank .	CL_D14-1082_mn_3	CL_D14-1082_mn_5	CL_D14-1082_mn_3_5
CL	D14-1082	mn	4	5	result_means	observation	support	support	secondary	secondary	back	support	Because this classifier learns and uses just a small number of dense features , it can work very fast , while achiev-ing an about 2 % improvement in unlabeled and labeled attachment scores on both English and Chinese datasets .	Concretely , our parser is able to parse more than 1000 sentences per second at 92.2 % unlabeled attachment score on the English Penn Treebank .	CL_D14-1082_mn_4	CL_D14-1082_mn_5	CL_D14-1082_mn_4_5
CL	D14-1082	mn	5	4	observation	result_means	support	support	secondary	secondary	forw	support	Concretely , our parser is able to parse more than 1000 sentences per second at 92.2 % unlabeled attachment score on the English Penn Treebank .	Because this classifier learns and uses just a small number of dense features , it can work very fast , while achiev-ing an about 2 % improvement in unlabeled and labeled attachment scores on both English and Chinese datasets .	CL_D14-1082_mn_5	CL_D14-1082_mn_4	CL_D14-1082_mn_4_5
CL	D14-1083	mn	1	2	motivation_background	motivation_problem	info-required	support	secondary	secondary	forw	info-required	Recent years have seen a surge of interest in stance classification in online debates .	Oftentimes , however , it is important to determine not only the stance expressed by an author in her debate posts , but also the reasons behind her supporting or opposing the issue under debate .	CL_D14-1083_mn_1	CL_D14-1083_mn_2	CL_D14-1083_mn_1_2
CL	D14-1083	mn	2	1	motivation_problem	motivation_background	support	info-required	secondary	secondary	back	info-required	Oftentimes , however , it is important to determine not only the stance expressed by an author in her debate posts , but also the reasons behind her supporting or opposing the issue under debate .	Recent years have seen a surge of interest in stance classification in online debates .	CL_D14-1083_mn_2	CL_D14-1083_mn_1	CL_D14-1083_mn_1_2
CL	D14-1083	mn	1	3	motivation_background	proposal	info-required	none	secondary	main	none	none	Recent years have seen a surge of interest in stance classification in online debates .	We therefore examine the new task of reason classification in this paper .	CL_D14-1083_mn_1	CL_D14-1083_mn_3	CL_D14-1083_mn_1_3
CL	D14-1083	mn	1	4	motivation_background	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	Recent years have seen a surge of interest in stance classification in online debates .	Given the close in-terplay between stance classification and reason classification , we design computational models for examining how automatically computed stance information can be profitably exploited for reason classification .	CL_D14-1083_mn_1	CL_D14-1083_mn_4	CL_D14-1083_mn_1_4
CL	D14-1083	mn	1	5	motivation_background	result_means	info-required	support	secondary	secondary	none	none	Recent years have seen a surge of interest in stance classification in online debates .	Experiments on our reason-annotated corpus of ideological debate posts from four domains demonstrate that sophisticated models of stances and reasons can indeed yield more accurate reason and stance classification results than their simpler counterparts .	CL_D14-1083_mn_1	CL_D14-1083_mn_5	CL_D14-1083_mn_1_5
CL	D14-1083	mn	2	3	motivation_problem	proposal	support	none	secondary	main	forw	support	Oftentimes , however , it is important to determine not only the stance expressed by an author in her debate posts , but also the reasons behind her supporting or opposing the issue under debate .	We therefore examine the new task of reason classification in this paper .	CL_D14-1083_mn_2	CL_D14-1083_mn_3	CL_D14-1083_mn_2_3
CL	D14-1083	mn	3	2	proposal	motivation_problem	none	support	main	secondary	back	support	We therefore examine the new task of reason classification in this paper .	Oftentimes , however , it is important to determine not only the stance expressed by an author in her debate posts , but also the reasons behind her supporting or opposing the issue under debate .	CL_D14-1083_mn_3	CL_D14-1083_mn_2	CL_D14-1083_mn_2_3
CL	D14-1083	mn	2	4	motivation_problem	proposal_implementation	support	elaboration	secondary	secondary	none	none	Oftentimes , however , it is important to determine not only the stance expressed by an author in her debate posts , but also the reasons behind her supporting or opposing the issue under debate .	Given the close in-terplay between stance classification and reason classification , we design computational models for examining how automatically computed stance information can be profitably exploited for reason classification .	CL_D14-1083_mn_2	CL_D14-1083_mn_4	CL_D14-1083_mn_2_4
CL	D14-1083	mn	2	5	motivation_problem	result_means	support	support	secondary	secondary	none	none	Oftentimes , however , it is important to determine not only the stance expressed by an author in her debate posts , but also the reasons behind her supporting or opposing the issue under debate .	Experiments on our reason-annotated corpus of ideological debate posts from four domains demonstrate that sophisticated models of stances and reasons can indeed yield more accurate reason and stance classification results than their simpler counterparts .	CL_D14-1083_mn_2	CL_D14-1083_mn_5	CL_D14-1083_mn_2_5
CL	D14-1083	mn	3	4	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	We therefore examine the new task of reason classification in this paper .	Given the close in-terplay between stance classification and reason classification , we design computational models for examining how automatically computed stance information can be profitably exploited for reason classification .	CL_D14-1083_mn_3	CL_D14-1083_mn_4	CL_D14-1083_mn_3_4
CL	D14-1083	mn	4	3	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	Given the close in-terplay between stance classification and reason classification , we design computational models for examining how automatically computed stance information can be profitably exploited for reason classification .	We therefore examine the new task of reason classification in this paper .	CL_D14-1083_mn_4	CL_D14-1083_mn_3	CL_D14-1083_mn_3_4
CL	D14-1083	mn	3	5	proposal	result_means	none	support	main	secondary	back	support	We therefore examine the new task of reason classification in this paper .	Experiments on our reason-annotated corpus of ideological debate posts from four domains demonstrate that sophisticated models of stances and reasons can indeed yield more accurate reason and stance classification results than their simpler counterparts .	CL_D14-1083_mn_3	CL_D14-1083_mn_5	CL_D14-1083_mn_3_5
CL	D14-1083	mn	5	3	result_means	proposal	support	none	secondary	main	forw	support	Experiments on our reason-annotated corpus of ideological debate posts from four domains demonstrate that sophisticated models of stances and reasons can indeed yield more accurate reason and stance classification results than their simpler counterparts .	We therefore examine the new task of reason classification in this paper .	CL_D14-1083_mn_5	CL_D14-1083_mn_3	CL_D14-1083_mn_3_5
CL	D14-1083	mn	4	5	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	Given the close in-terplay between stance classification and reason classification , we design computational models for examining how automatically computed stance information can be profitably exploited for reason classification .	Experiments on our reason-annotated corpus of ideological debate posts from four domains demonstrate that sophisticated models of stances and reasons can indeed yield more accurate reason and stance classification results than their simpler counterparts .	CL_D14-1083_mn_4	CL_D14-1083_mn_5	CL_D14-1083_mn_4_5
CL	D14-1084	mn	1	2	motivation_problem	proposal	support	none	secondary	main	forw	support	State-of-the-art Chinese zero pronoun resolution systems are supervised , thus relying on training data containing manually resolved zero pronouns .	To eliminate the reliance on annotated data , we present a generative model for unsupervised Chinese zero pronoun resolution .	CL_D14-1084_mn_1	CL_D14-1084_mn_2	CL_D14-1084_mn_1_2
CL	D14-1084	mn	2	1	proposal	motivation_problem	none	support	main	secondary	back	support	To eliminate the reliance on annotated data , we present a generative model for unsupervised Chinese zero pronoun resolution .	State-of-the-art Chinese zero pronoun resolution systems are supervised , thus relying on training data containing manually resolved zero pronouns .	CL_D14-1084_mn_2	CL_D14-1084_mn_1	CL_D14-1084_mn_1_2
CL	D14-1084	mn	1	3	motivation_problem	motivation_hypothesis	support	support	secondary	secondary	none	none	State-of-the-art Chinese zero pronoun resolution systems are supervised , thus relying on training data containing manually resolved zero pronouns .	At the core of our model is a novel hypothesis : a probabilistic pronoun resolver trained on overt pronouns in an unsupervised manner can be used to resolve zero pronouns .	CL_D14-1084_mn_1	CL_D14-1084_mn_3	CL_D14-1084_mn_1_3
CL	D14-1084	mn	1	4	motivation_problem	result_means	support	support	secondary	secondary	none	none	State-of-the-art Chinese zero pronoun resolution systems are supervised , thus relying on training data containing manually resolved zero pronouns .	Experiments demonstrate that our unsupervised model rivals its state-of-the-art supervised counterparts in performance when resolving the Chinese zero pronouns in the OntoNotes corpus .	CL_D14-1084_mn_1	CL_D14-1084_mn_4	CL_D14-1084_mn_1_4
CL	D14-1084	mn	2	3	proposal	motivation_hypothesis	none	support	main	secondary	back	support	To eliminate the reliance on annotated data , we present a generative model for unsupervised Chinese zero pronoun resolution .	At the core of our model is a novel hypothesis : a probabilistic pronoun resolver trained on overt pronouns in an unsupervised manner can be used to resolve zero pronouns .	CL_D14-1084_mn_2	CL_D14-1084_mn_3	CL_D14-1084_mn_2_3
CL	D14-1084	mn	3	2	motivation_hypothesis	proposal	support	none	secondary	main	forw	support	At the core of our model is a novel hypothesis : a probabilistic pronoun resolver trained on overt pronouns in an unsupervised manner can be used to resolve zero pronouns .	To eliminate the reliance on annotated data , we present a generative model for unsupervised Chinese zero pronoun resolution .	CL_D14-1084_mn_3	CL_D14-1084_mn_2	CL_D14-1084_mn_2_3
CL	D14-1084	mn	2	4	proposal	result_means	none	support	main	secondary	back	support	To eliminate the reliance on annotated data , we present a generative model for unsupervised Chinese zero pronoun resolution .	Experiments demonstrate that our unsupervised model rivals its state-of-the-art supervised counterparts in performance when resolving the Chinese zero pronouns in the OntoNotes corpus .	CL_D14-1084_mn_2	CL_D14-1084_mn_4	CL_D14-1084_mn_2_4
CL	D14-1084	mn	4	2	result_means	proposal	support	none	secondary	main	forw	support	Experiments demonstrate that our unsupervised model rivals its state-of-the-art supervised counterparts in performance when resolving the Chinese zero pronouns in the OntoNotes corpus .	To eliminate the reliance on annotated data , we present a generative model for unsupervised Chinese zero pronoun resolution .	CL_D14-1084_mn_4	CL_D14-1084_mn_2	CL_D14-1084_mn_2_4
CL	D14-1084	mn	3	4	motivation_hypothesis	result_means	support	support	secondary	secondary	none	none	At the core of our model is a novel hypothesis : a probabilistic pronoun resolver trained on overt pronouns in an unsupervised manner can be used to resolve zero pronouns .	Experiments demonstrate that our unsupervised model rivals its state-of-the-art supervised counterparts in performance when resolving the Chinese zero pronouns in the OntoNotes corpus .	CL_D14-1084_mn_3	CL_D14-1084_mn_4	CL_D14-1084_mn_3_4
CL	D14-1085	mn	1	2	proposal	motivation_hypothesis	none	support	main	secondary	back	support	We present sentence enhancement as a novel technique for text-to-text generation in abstractive summarization .	Compared to extraction or previous approaches to sentence fusion , sentence enhancement increases the range of possible summary sentences by allowing the combination of dependency subtrees from any sentence from the source text .	CL_D14-1085_mn_1	CL_D14-1085_mn_2	CL_D14-1085_mn_1_2
CL	D14-1085	mn	2	1	motivation_hypothesis	proposal	support	none	secondary	main	forw	support	Compared to extraction or previous approaches to sentence fusion , sentence enhancement increases the range of possible summary sentences by allowing the combination of dependency subtrees from any sentence from the source text .	We present sentence enhancement as a novel technique for text-to-text generation in abstractive summarization .	CL_D14-1085_mn_2	CL_D14-1085_mn_1	CL_D14-1085_mn_1_2
CL	D14-1085	mn	1	3	proposal	result	none	support	main	secondary	back	support	We present sentence enhancement as a novel technique for text-to-text generation in abstractive summarization .	Our experiments indicate that our approach yields summary sentences that are competitive with a sentence fusion baseline in terms of content quality , but better in terms of grammaticality , and that the benefit of sentence enhancement relies crucially on an event coreference resolution algorithm using distributional semantics .	CL_D14-1085_mn_1	CL_D14-1085_mn_3	CL_D14-1085_mn_1_3
CL	D14-1085	mn	3	1	result	proposal	support	none	secondary	main	forw	support	Our experiments indicate that our approach yields summary sentences that are competitive with a sentence fusion baseline in terms of content quality , but better in terms of grammaticality , and that the benefit of sentence enhancement relies crucially on an event coreference resolution algorithm using distributional semantics .	We present sentence enhancement as a novel technique for text-to-text generation in abstractive summarization .	CL_D14-1085_mn_3	CL_D14-1085_mn_1	CL_D14-1085_mn_1_3
CL	D14-1085	mn	1	4	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	We present sentence enhancement as a novel technique for text-to-text generation in abstractive summarization .	We also consider how text-to-text generation approaches to summarization can be extended beyond the source text by examining how human summary writers incorporate source-text-external elements into their summary sentences .	CL_D14-1085_mn_1	CL_D14-1085_mn_4	CL_D14-1085_mn_1_4
CL	D14-1085	mn	4	1	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	We also consider how text-to-text generation approaches to summarization can be extended beyond the source text by examining how human summary writers incorporate source-text-external elements into their summary sentences .	We present sentence enhancement as a novel technique for text-to-text generation in abstractive summarization .	CL_D14-1085_mn_4	CL_D14-1085_mn_1	CL_D14-1085_mn_1_4
CL	D14-1085	mn	2	3	motivation_hypothesis	result	support	support	secondary	secondary	none	none	Compared to extraction or previous approaches to sentence fusion , sentence enhancement increases the range of possible summary sentences by allowing the combination of dependency subtrees from any sentence from the source text .	Our experiments indicate that our approach yields summary sentences that are competitive with a sentence fusion baseline in terms of content quality , but better in terms of grammaticality , and that the benefit of sentence enhancement relies crucially on an event coreference resolution algorithm using distributional semantics .	CL_D14-1085_mn_2	CL_D14-1085_mn_3	CL_D14-1085_mn_2_3
CL	D14-1085	mn	2	4	motivation_hypothesis	proposal_implementation	support	elaboration	secondary	secondary	none	none	Compared to extraction or previous approaches to sentence fusion , sentence enhancement increases the range of possible summary sentences by allowing the combination of dependency subtrees from any sentence from the source text .	We also consider how text-to-text generation approaches to summarization can be extended beyond the source text by examining how human summary writers incorporate source-text-external elements into their summary sentences .	CL_D14-1085_mn_2	CL_D14-1085_mn_4	CL_D14-1085_mn_2_4
CL	D14-1085	mn	3	4	result	proposal_implementation	support	elaboration	secondary	secondary	none	none	Our experiments indicate that our approach yields summary sentences that are competitive with a sentence fusion baseline in terms of content quality , but better in terms of grammaticality , and that the benefit of sentence enhancement relies crucially on an event coreference resolution algorithm using distributional semantics .	We also consider how text-to-text generation approaches to summarization can be extended beyond the source text by examining how human summary writers incorporate source-text-external elements into their summary sentences .	CL_D14-1085_mn_3	CL_D14-1085_mn_4	CL_D14-1085_mn_3_4
CL	D14-1086	mn	1	2	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	In this paper we introduce a new game to crowd-source natural language referring expressions .	By designing a two player game , we can both collect and verify referring expressions directly within the game .	CL_D14-1086_mn_1	CL_D14-1086_mn_2	CL_D14-1086_mn_1_2
CL	D14-1086	mn	2	1	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	By designing a two player game , we can both collect and verify referring expressions directly within the game .	In this paper we introduce a new game to crowd-source natural language referring expressions .	CL_D14-1086_mn_2	CL_D14-1086_mn_1	CL_D14-1086_mn_1_2
CL	D14-1086	mn	1	3	proposal	observation	none	support	main	secondary	none	none	In this paper we introduce a new game to crowd-source natural language referring expressions .	To date , the game has produced a dataset containing 130,525 expressions , referring to 96,654 distinct objects , in 19,894 photographs of natural scenes .	CL_D14-1086_mn_1	CL_D14-1086_mn_3	CL_D14-1086_mn_1_3
CL	D14-1086	mn	1	4	proposal	result	none	support	main	secondary	back	support	In this paper we introduce a new game to crowd-source natural language referring expressions .	This dataset is larger and more varied than previous REG datasets and allows us to study referring expressions in real-world scenes .	CL_D14-1086_mn_1	CL_D14-1086_mn_4	CL_D14-1086_mn_1_4
CL	D14-1086	mn	4	1	result	proposal	support	none	secondary	main	forw	support	This dataset is larger and more varied than previous REG datasets and allows us to study referring expressions in real-world scenes .	In this paper we introduce a new game to crowd-source natural language referring expressions .	CL_D14-1086_mn_4	CL_D14-1086_mn_1	CL_D14-1086_mn_1_4
CL	D14-1086	mn	1	5	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	In this paper we introduce a new game to crowd-source natural language referring expressions .	We provide an in depth analysis of the resulting dataset .	CL_D14-1086_mn_1	CL_D14-1086_mn_5	CL_D14-1086_mn_1_5
CL	D14-1086	mn	1	6	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	In this paper we introduce a new game to crowd-source natural language referring expressions .	Based on our findings , we design a new optimization based model for generating referring expressions and perform experimental evaluations on 3 test sets .	CL_D14-1086_mn_1	CL_D14-1086_mn_6	CL_D14-1086_mn_1_6
CL	D14-1086	mn	2	3	proposal_implementation	observation	elaboration	support	secondary	secondary	none	none	By designing a two player game , we can both collect and verify referring expressions directly within the game .	To date , the game has produced a dataset containing 130,525 expressions , referring to 96,654 distinct objects , in 19,894 photographs of natural scenes .	CL_D14-1086_mn_2	CL_D14-1086_mn_3	CL_D14-1086_mn_2_3
CL	D14-1086	mn	2	4	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	By designing a two player game , we can both collect and verify referring expressions directly within the game .	This dataset is larger and more varied than previous REG datasets and allows us to study referring expressions in real-world scenes .	CL_D14-1086_mn_2	CL_D14-1086_mn_4	CL_D14-1086_mn_2_4
CL	D14-1086	mn	2	5	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	By designing a two player game , we can both collect and verify referring expressions directly within the game .	We provide an in depth analysis of the resulting dataset .	CL_D14-1086_mn_2	CL_D14-1086_mn_5	CL_D14-1086_mn_2_5
CL	D14-1086	mn	5	2	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	forw	elaboration	We provide an in depth analysis of the resulting dataset .	By designing a two player game , we can both collect and verify referring expressions directly within the game .	CL_D14-1086_mn_5	CL_D14-1086_mn_2	CL_D14-1086_mn_2_5
CL	D14-1086	mn	2	6	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	none	none	By designing a two player game , we can both collect and verify referring expressions directly within the game .	Based on our findings , we design a new optimization based model for generating referring expressions and perform experimental evaluations on 3 test sets .	CL_D14-1086_mn_2	CL_D14-1086_mn_6	CL_D14-1086_mn_2_6
CL	D14-1086	mn	3	4	observation	result	support	support	secondary	secondary	forw	support	To date , the game has produced a dataset containing 130,525 expressions , referring to 96,654 distinct objects , in 19,894 photographs of natural scenes .	This dataset is larger and more varied than previous REG datasets and allows us to study referring expressions in real-world scenes .	CL_D14-1086_mn_3	CL_D14-1086_mn_4	CL_D14-1086_mn_3_4
CL	D14-1086	mn	4	3	result	observation	support	support	secondary	secondary	back	support	This dataset is larger and more varied than previous REG datasets and allows us to study referring expressions in real-world scenes .	To date , the game has produced a dataset containing 130,525 expressions , referring to 96,654 distinct objects , in 19,894 photographs of natural scenes .	CL_D14-1086_mn_4	CL_D14-1086_mn_3	CL_D14-1086_mn_3_4
CL	D14-1086	mn	3	5	observation	proposal_implementation	support	elaboration	secondary	secondary	none	none	To date , the game has produced a dataset containing 130,525 expressions , referring to 96,654 distinct objects , in 19,894 photographs of natural scenes .	We provide an in depth analysis of the resulting dataset .	CL_D14-1086_mn_3	CL_D14-1086_mn_5	CL_D14-1086_mn_3_5
CL	D14-1086	mn	3	6	observation	proposal_implementation	support	elaboration	secondary	secondary	none	none	To date , the game has produced a dataset containing 130,525 expressions , referring to 96,654 distinct objects , in 19,894 photographs of natural scenes .	Based on our findings , we design a new optimization based model for generating referring expressions and perform experimental evaluations on 3 test sets .	CL_D14-1086_mn_3	CL_D14-1086_mn_6	CL_D14-1086_mn_3_6
CL	D14-1086	mn	4	5	result	proposal_implementation	support	elaboration	secondary	secondary	none	none	This dataset is larger and more varied than previous REG datasets and allows us to study referring expressions in real-world scenes .	We provide an in depth analysis of the resulting dataset .	CL_D14-1086_mn_4	CL_D14-1086_mn_5	CL_D14-1086_mn_4_5
CL	D14-1086	mn	4	6	result	proposal_implementation	support	elaboration	secondary	secondary	none	none	This dataset is larger and more varied than previous REG datasets and allows us to study referring expressions in real-world scenes .	Based on our findings , we design a new optimization based model for generating referring expressions and perform experimental evaluations on 3 test sets .	CL_D14-1086_mn_4	CL_D14-1086_mn_6	CL_D14-1086_mn_4_6
CL	D14-1086	mn	5	6	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	We provide an in depth analysis of the resulting dataset .	Based on our findings , we design a new optimization based model for generating referring expressions and perform experimental evaluations on 3 test sets .	CL_D14-1086_mn_5	CL_D14-1086_mn_6	CL_D14-1086_mn_5_6
CL	D14-1086	mn	6	5	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	forw	elaboration	Based on our findings , we design a new optimization based model for generating referring expressions and perform experimental evaluations on 3 test sets .	We provide an in depth analysis of the resulting dataset .	CL_D14-1086_mn_6	CL_D14-1086_mn_5	CL_D14-1086_mn_5_6
CL	D14-1087	mn	1	2	proposal	motivation_problem	none	support	main	secondary	back	support	We propose an unsupervised approach to constructing templates from a large collection of semantic category names , and use the templates as the semantic representation of categories .	The main challenge is that many terms have multiple meanings , resulting in a lot of wrong templates .	CL_D14-1087_mn_1	CL_D14-1087_mn_2	CL_D14-1087_mn_1_2
CL	D14-1087	mn	2	1	motivation_problem	proposal	support	none	secondary	main	forw	support	The main challenge is that many terms have multiple meanings , resulting in a lot of wrong templates .	We propose an unsupervised approach to constructing templates from a large collection of semantic category names , and use the templates as the semantic representation of categories .	CL_D14-1087_mn_2	CL_D14-1087_mn_1	CL_D14-1087_mn_1_2
CL	D14-1087	mn	1	3	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	We propose an unsupervised approach to constructing templates from a large collection of semantic category names , and use the templates as the semantic representation of categories .	Statistical data and semantic knowledge are extracted from a web corpus to improve template generation .	CL_D14-1087_mn_1	CL_D14-1087_mn_3	CL_D14-1087_mn_1_3
CL	D14-1087	mn	3	1	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	Statistical data and semantic knowledge are extracted from a web corpus to improve template generation .	We propose an unsupervised approach to constructing templates from a large collection of semantic category names , and use the templates as the semantic representation of categories .	CL_D14-1087_mn_3	CL_D14-1087_mn_1	CL_D14-1087_mn_1_3
CL	D14-1087	mn	1	4	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	We propose an unsupervised approach to constructing templates from a large collection of semantic category names , and use the templates as the semantic representation of categories .	A nonlinear scoring function is proposed and demonstrated to be effective .	CL_D14-1087_mn_1	CL_D14-1087_mn_4	CL_D14-1087_mn_1_4
CL	D14-1087	mn	1	5	proposal	result	none	support	main	secondary	back	support	We propose an unsupervised approach to constructing templates from a large collection of semantic category names , and use the templates as the semantic representation of categories .	Experiments show that our approach achieves significantly better results than baseline methods .	CL_D14-1087_mn_1	CL_D14-1087_mn_5	CL_D14-1087_mn_1_5
CL	D14-1087	mn	5	1	result	proposal	support	none	secondary	main	forw	support	Experiments show that our approach achieves significantly better results than baseline methods .	We propose an unsupervised approach to constructing templates from a large collection of semantic category names , and use the templates as the semantic representation of categories .	CL_D14-1087_mn_5	CL_D14-1087_mn_1	CL_D14-1087_mn_1_5
CL	D14-1087	mn	1	6	proposal	result_means	none	elaboration	main	secondary	none	none	We propose an unsupervised approach to constructing templates from a large collection of semantic category names , and use the templates as the semantic representation of categories .	As an immediate application , we apply the extracted templates to the cleaning of a category collection and see promising results ( precision improved from 81 % to 89 % ) .	CL_D14-1087_mn_1	CL_D14-1087_mn_6	CL_D14-1087_mn_1_6
CL	D14-1087	mn	2	3	motivation_problem	proposal_implementation	support	elaboration	secondary	secondary	none	none	The main challenge is that many terms have multiple meanings , resulting in a lot of wrong templates .	Statistical data and semantic knowledge are extracted from a web corpus to improve template generation .	CL_D14-1087_mn_2	CL_D14-1087_mn_3	CL_D14-1087_mn_2_3
CL	D14-1087	mn	2	4	motivation_problem	proposal_implementation	support	elaboration	secondary	secondary	none	none	The main challenge is that many terms have multiple meanings , resulting in a lot of wrong templates .	A nonlinear scoring function is proposed and demonstrated to be effective .	CL_D14-1087_mn_2	CL_D14-1087_mn_4	CL_D14-1087_mn_2_4
CL	D14-1087	mn	2	5	motivation_problem	result	support	support	secondary	secondary	none	none	The main challenge is that many terms have multiple meanings , resulting in a lot of wrong templates .	Experiments show that our approach achieves significantly better results than baseline methods .	CL_D14-1087_mn_2	CL_D14-1087_mn_5	CL_D14-1087_mn_2_5
CL	D14-1087	mn	2	6	motivation_problem	result_means	support	elaboration	secondary	secondary	none	none	The main challenge is that many terms have multiple meanings , resulting in a lot of wrong templates .	As an immediate application , we apply the extracted templates to the cleaning of a category collection and see promising results ( precision improved from 81 % to 89 % ) .	CL_D14-1087_mn_2	CL_D14-1087_mn_6	CL_D14-1087_mn_2_6
CL	D14-1087	mn	3	4	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	Statistical data and semantic knowledge are extracted from a web corpus to improve template generation .	A nonlinear scoring function is proposed and demonstrated to be effective .	CL_D14-1087_mn_3	CL_D14-1087_mn_4	CL_D14-1087_mn_3_4
CL	D14-1087	mn	4	3	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	forw	elaboration	A nonlinear scoring function is proposed and demonstrated to be effective .	Statistical data and semantic knowledge are extracted from a web corpus to improve template generation .	CL_D14-1087_mn_4	CL_D14-1087_mn_3	CL_D14-1087_mn_3_4
CL	D14-1087	mn	3	5	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	Statistical data and semantic knowledge are extracted from a web corpus to improve template generation .	Experiments show that our approach achieves significantly better results than baseline methods .	CL_D14-1087_mn_3	CL_D14-1087_mn_5	CL_D14-1087_mn_3_5
CL	D14-1087	mn	3	6	proposal_implementation	result_means	elaboration	elaboration	secondary	secondary	none	none	Statistical data and semantic knowledge are extracted from a web corpus to improve template generation .	As an immediate application , we apply the extracted templates to the cleaning of a category collection and see promising results ( precision improved from 81 % to 89 % ) .	CL_D14-1087_mn_3	CL_D14-1087_mn_6	CL_D14-1087_mn_3_6
CL	D14-1087	mn	4	5	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	A nonlinear scoring function is proposed and demonstrated to be effective .	Experiments show that our approach achieves significantly better results than baseline methods .	CL_D14-1087_mn_4	CL_D14-1087_mn_5	CL_D14-1087_mn_4_5
CL	D14-1087	mn	4	6	proposal_implementation	result_means	elaboration	elaboration	secondary	secondary	none	none	A nonlinear scoring function is proposed and demonstrated to be effective .	As an immediate application , we apply the extracted templates to the cleaning of a category collection and see promising results ( precision improved from 81 % to 89 % ) .	CL_D14-1087_mn_4	CL_D14-1087_mn_6	CL_D14-1087_mn_4_6
CL	D14-1087	mn	5	6	result	result_means	support	elaboration	secondary	secondary	back	elaboration	Experiments show that our approach achieves significantly better results than baseline methods .	As an immediate application , we apply the extracted templates to the cleaning of a category collection and see promising results ( precision improved from 81 % to 89 % ) .	CL_D14-1087_mn_5	CL_D14-1087_mn_6	CL_D14-1087_mn_5_6
CL	D14-1087	mn	6	5	result_means	result	elaboration	support	secondary	secondary	forw	elaboration	As an immediate application , we apply the extracted templates to the cleaning of a category collection and see promising results ( precision improved from 81 % to 89 % ) .	Experiments show that our approach achieves significantly better results than baseline methods .	CL_D14-1087_mn_6	CL_D14-1087_mn_5	CL_D14-1087_mn_5_6
CL	D14-1088	mn	1	2	motivation_background	motivation_background	info-required	info-required	secondary	secondary	forw	info-required	Taxonomies are the backbone of many structured , semantic knowledge resources .	Recent works for extracting taxonomic relations from text focused on collecting lexical-syntactic patterns to extract the taxonomic relations by matching the patterns to text .	CL_D14-1088_mn_1	CL_D14-1088_mn_2	CL_D14-1088_mn_1_2
CL	D14-1088	mn	2	1	motivation_background	motivation_background	info-required	info-required	secondary	secondary	back	info-required	Recent works for extracting taxonomic relations from text focused on collecting lexical-syntactic patterns to extract the taxonomic relations by matching the patterns to text .	Taxonomies are the backbone of many structured , semantic knowledge resources .	CL_D14-1088_mn_2	CL_D14-1088_mn_1	CL_D14-1088_mn_1_2
CL	D14-1088	mn	1	3	motivation_background	motivation_problem	info-required	support	secondary	secondary	none	none	Taxonomies are the backbone of many structured , semantic knowledge resources .	These approaches , however , often show low coverage due to the lack of contextual analysis across sentences .	CL_D14-1088_mn_1	CL_D14-1088_mn_3	CL_D14-1088_mn_1_3
CL	D14-1088	mn	1	4	motivation_background	proposal	info-required	none	secondary	main	none	none	Taxonomies are the backbone of many structured , semantic knowledge resources .	To address this issue , we propose a novel approach that collectively utilizes contextual information of terms in syntactic structures such that if the set of contexts of a term includes most of contexts of another term , a subsumption relation between the two terms is inferred .	CL_D14-1088_mn_1	CL_D14-1088_mn_4	CL_D14-1088_mn_1_4
CL	D14-1088	mn	1	5	motivation_background	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	Taxonomies are the backbone of many structured , semantic knowledge resources .	We apply this method to the task of taxonomy construction from scratch , where we introduce another novel graph-based algorithm for taxonomic structure induction .	CL_D14-1088_mn_1	CL_D14-1088_mn_5	CL_D14-1088_mn_1_5
CL	D14-1088	mn	1	6	motivation_background	result	info-required	support	secondary	secondary	none	none	Taxonomies are the backbone of many structured , semantic knowledge resources .	Our experiment results show that the proposed method is well complementary with previous methods of linguistic pattern matching and significantly improves recall and thus F-measure .	CL_D14-1088_mn_1	CL_D14-1088_mn_6	CL_D14-1088_mn_1_6
CL	D14-1088	mn	2	3	motivation_background	motivation_problem	info-required	support	secondary	secondary	forw	info-required	Recent works for extracting taxonomic relations from text focused on collecting lexical-syntactic patterns to extract the taxonomic relations by matching the patterns to text .	These approaches , however , often show low coverage due to the lack of contextual analysis across sentences .	CL_D14-1088_mn_2	CL_D14-1088_mn_3	CL_D14-1088_mn_2_3
CL	D14-1088	mn	3	2	motivation_problem	motivation_background	support	info-required	secondary	secondary	back	info-required	These approaches , however , often show low coverage due to the lack of contextual analysis across sentences .	Recent works for extracting taxonomic relations from text focused on collecting lexical-syntactic patterns to extract the taxonomic relations by matching the patterns to text .	CL_D14-1088_mn_3	CL_D14-1088_mn_2	CL_D14-1088_mn_2_3
CL	D14-1088	mn	2	4	motivation_background	proposal	info-required	none	secondary	main	none	none	Recent works for extracting taxonomic relations from text focused on collecting lexical-syntactic patterns to extract the taxonomic relations by matching the patterns to text .	To address this issue , we propose a novel approach that collectively utilizes contextual information of terms in syntactic structures such that if the set of contexts of a term includes most of contexts of another term , a subsumption relation between the two terms is inferred .	CL_D14-1088_mn_2	CL_D14-1088_mn_4	CL_D14-1088_mn_2_4
CL	D14-1088	mn	2	5	motivation_background	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	Recent works for extracting taxonomic relations from text focused on collecting lexical-syntactic patterns to extract the taxonomic relations by matching the patterns to text .	We apply this method to the task of taxonomy construction from scratch , where we introduce another novel graph-based algorithm for taxonomic structure induction .	CL_D14-1088_mn_2	CL_D14-1088_mn_5	CL_D14-1088_mn_2_5
CL	D14-1088	mn	2	6	motivation_background	result	info-required	support	secondary	secondary	none	none	Recent works for extracting taxonomic relations from text focused on collecting lexical-syntactic patterns to extract the taxonomic relations by matching the patterns to text .	Our experiment results show that the proposed method is well complementary with previous methods of linguistic pattern matching and significantly improves recall and thus F-measure .	CL_D14-1088_mn_2	CL_D14-1088_mn_6	CL_D14-1088_mn_2_6
CL	D14-1088	mn	3	4	motivation_problem	proposal	support	none	secondary	main	forw	support	These approaches , however , often show low coverage due to the lack of contextual analysis across sentences .	To address this issue , we propose a novel approach that collectively utilizes contextual information of terms in syntactic structures such that if the set of contexts of a term includes most of contexts of another term , a subsumption relation between the two terms is inferred .	CL_D14-1088_mn_3	CL_D14-1088_mn_4	CL_D14-1088_mn_3_4
CL	D14-1088	mn	4	3	proposal	motivation_problem	none	support	main	secondary	back	support	To address this issue , we propose a novel approach that collectively utilizes contextual information of terms in syntactic structures such that if the set of contexts of a term includes most of contexts of another term , a subsumption relation between the two terms is inferred .	These approaches , however , often show low coverage due to the lack of contextual analysis across sentences .	CL_D14-1088_mn_4	CL_D14-1088_mn_3	CL_D14-1088_mn_3_4
CL	D14-1088	mn	3	5	motivation_problem	proposal_implementation	support	elaboration	secondary	secondary	none	none	These approaches , however , often show low coverage due to the lack of contextual analysis across sentences .	We apply this method to the task of taxonomy construction from scratch , where we introduce another novel graph-based algorithm for taxonomic structure induction .	CL_D14-1088_mn_3	CL_D14-1088_mn_5	CL_D14-1088_mn_3_5
CL	D14-1088	mn	3	6	motivation_problem	result	support	support	secondary	secondary	none	none	These approaches , however , often show low coverage due to the lack of contextual analysis across sentences .	Our experiment results show that the proposed method is well complementary with previous methods of linguistic pattern matching and significantly improves recall and thus F-measure .	CL_D14-1088_mn_3	CL_D14-1088_mn_6	CL_D14-1088_mn_3_6
CL	D14-1088	mn	4	5	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	To address this issue , we propose a novel approach that collectively utilizes contextual information of terms in syntactic structures such that if the set of contexts of a term includes most of contexts of another term , a subsumption relation between the two terms is inferred .	We apply this method to the task of taxonomy construction from scratch , where we introduce another novel graph-based algorithm for taxonomic structure induction .	CL_D14-1088_mn_4	CL_D14-1088_mn_5	CL_D14-1088_mn_4_5
CL	D14-1088	mn	5	4	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	We apply this method to the task of taxonomy construction from scratch , where we introduce another novel graph-based algorithm for taxonomic structure induction .	To address this issue , we propose a novel approach that collectively utilizes contextual information of terms in syntactic structures such that if the set of contexts of a term includes most of contexts of another term , a subsumption relation between the two terms is inferred .	CL_D14-1088_mn_5	CL_D14-1088_mn_4	CL_D14-1088_mn_4_5
CL	D14-1088	mn	4	6	proposal	result	none	support	main	secondary	back	support	To address this issue , we propose a novel approach that collectively utilizes contextual information of terms in syntactic structures such that if the set of contexts of a term includes most of contexts of another term , a subsumption relation between the two terms is inferred .	Our experiment results show that the proposed method is well complementary with previous methods of linguistic pattern matching and significantly improves recall and thus F-measure .	CL_D14-1088_mn_4	CL_D14-1088_mn_6	CL_D14-1088_mn_4_6
CL	D14-1088	mn	6	4	result	proposal	support	none	secondary	main	forw	support	Our experiment results show that the proposed method is well complementary with previous methods of linguistic pattern matching and significantly improves recall and thus F-measure .	To address this issue , we propose a novel approach that collectively utilizes contextual information of terms in syntactic structures such that if the set of contexts of a term includes most of contexts of another term , a subsumption relation between the two terms is inferred .	CL_D14-1088_mn_6	CL_D14-1088_mn_4	CL_D14-1088_mn_4_6
CL	D14-1088	mn	5	6	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	We apply this method to the task of taxonomy construction from scratch , where we introduce another novel graph-based algorithm for taxonomic structure induction .	Our experiment results show that the proposed method is well complementary with previous methods of linguistic pattern matching and significantly improves recall and thus F-measure .	CL_D14-1088_mn_5	CL_D14-1088_mn_6	CL_D14-1088_mn_5_6
CL	D14-1089	mn	1	2	motivation_problem	proposal	support	none	secondary	main	forw	support	State-of-the-art fact extraction is heavily constrained by recall , as demonstrated by recent performance in TAC Slot Filling .	We isolate this recall loss for NE slots by systematically analysing each stage of the slot filling pipeline as a filter over correct answers .	CL_D14-1089_mn_1	CL_D14-1089_mn_2	CL_D14-1089_mn_1_2
CL	D14-1089	mn	2	1	proposal	motivation_problem	none	support	main	secondary	back	support	We isolate this recall loss for NE slots by systematically analysing each stage of the slot filling pipeline as a filter over correct answers .	State-of-the-art fact extraction is heavily constrained by recall , as demonstrated by recent performance in TAC Slot Filling .	CL_D14-1089_mn_2	CL_D14-1089_mn_1	CL_D14-1089_mn_1_2
CL	D14-1089	mn	1	3	motivation_problem	information_additional	support	info-optional	secondary	secondary	none	none	State-of-the-art fact extraction is heavily constrained by recall , as demonstrated by recent performance in TAC Slot Filling .	Recall is critical as candidates never generated can never be recovered , whereas precision can always be increased in downstream processing .	CL_D14-1089_mn_1	CL_D14-1089_mn_3	CL_D14-1089_mn_1_3
CL	D14-1089	mn	1	4	motivation_problem	result	support	support	secondary	secondary	none	none	State-of-the-art fact extraction is heavily constrained by recall , as demonstrated by recent performance in TAC Slot Filling .	We provide precise , empirical confirmation of previously hypothesised sources of recall loss in slot filling .	CL_D14-1089_mn_1	CL_D14-1089_mn_4	CL_D14-1089_mn_1_4
CL	D14-1089	mn	1	5	motivation_problem	result	support	elaboration	secondary	secondary	none	none	State-of-the-art fact extraction is heavily constrained by recall , as demonstrated by recent performance in TAC Slot Filling .	While NE type constraints substantially reduce the search space with only a minor recall penalty , we find that 10 % to 39 % of slot fills will be entirely ignored by most systems .	CL_D14-1089_mn_1	CL_D14-1089_mn_5	CL_D14-1089_mn_1_5
CL	D14-1089	mn	1	6	motivation_problem	result	support	elaboration	secondary	secondary	none	none	State-of-the-art fact extraction is heavily constrained by recall , as demonstrated by recent performance in TAC Slot Filling .	One in six correct answers are lost if coreference is not used , but this can be mostly retained by simple name matching rules .	CL_D14-1089_mn_1	CL_D14-1089_mn_6	CL_D14-1089_mn_1_6
CL	D14-1089	mn	2	3	proposal	information_additional	none	info-optional	main	secondary	back	info-optional	We isolate this recall loss for NE slots by systematically analysing each stage of the slot filling pipeline as a filter over correct answers .	Recall is critical as candidates never generated can never be recovered , whereas precision can always be increased in downstream processing .	CL_D14-1089_mn_2	CL_D14-1089_mn_3	CL_D14-1089_mn_2_3
CL	D14-1089	mn	3	2	information_additional	proposal	info-optional	none	secondary	main	forw	info-optional	Recall is critical as candidates never generated can never be recovered , whereas precision can always be increased in downstream processing .	We isolate this recall loss for NE slots by systematically analysing each stage of the slot filling pipeline as a filter over correct answers .	CL_D14-1089_mn_3	CL_D14-1089_mn_2	CL_D14-1089_mn_2_3
CL	D14-1089	mn	2	4	proposal	result	none	support	main	secondary	back	support	We isolate this recall loss for NE slots by systematically analysing each stage of the slot filling pipeline as a filter over correct answers .	We provide precise , empirical confirmation of previously hypothesised sources of recall loss in slot filling .	CL_D14-1089_mn_2	CL_D14-1089_mn_4	CL_D14-1089_mn_2_4
CL	D14-1089	mn	4	2	result	proposal	support	none	secondary	main	forw	support	We provide precise , empirical confirmation of previously hypothesised sources of recall loss in slot filling .	We isolate this recall loss for NE slots by systematically analysing each stage of the slot filling pipeline as a filter over correct answers .	CL_D14-1089_mn_4	CL_D14-1089_mn_2	CL_D14-1089_mn_2_4
CL	D14-1089	mn	2	5	proposal	result	none	elaboration	main	secondary	none	none	We isolate this recall loss for NE slots by systematically analysing each stage of the slot filling pipeline as a filter over correct answers .	While NE type constraints substantially reduce the search space with only a minor recall penalty , we find that 10 % to 39 % of slot fills will be entirely ignored by most systems .	CL_D14-1089_mn_2	CL_D14-1089_mn_5	CL_D14-1089_mn_2_5
CL	D14-1089	mn	2	6	proposal	result	none	elaboration	main	secondary	none	none	We isolate this recall loss for NE slots by systematically analysing each stage of the slot filling pipeline as a filter over correct answers .	One in six correct answers are lost if coreference is not used , but this can be mostly retained by simple name matching rules .	CL_D14-1089_mn_2	CL_D14-1089_mn_6	CL_D14-1089_mn_2_6
CL	D14-1089	mn	3	4	information_additional	result	info-optional	support	secondary	secondary	none	none	Recall is critical as candidates never generated can never be recovered , whereas precision can always be increased in downstream processing .	We provide precise , empirical confirmation of previously hypothesised sources of recall loss in slot filling .	CL_D14-1089_mn_3	CL_D14-1089_mn_4	CL_D14-1089_mn_3_4
CL	D14-1089	mn	3	5	information_additional	result	info-optional	elaboration	secondary	secondary	none	none	Recall is critical as candidates never generated can never be recovered , whereas precision can always be increased in downstream processing .	While NE type constraints substantially reduce the search space with only a minor recall penalty , we find that 10 % to 39 % of slot fills will be entirely ignored by most systems .	CL_D14-1089_mn_3	CL_D14-1089_mn_5	CL_D14-1089_mn_3_5
CL	D14-1089	mn	3	6	information_additional	result	info-optional	elaboration	secondary	secondary	none	none	Recall is critical as candidates never generated can never be recovered , whereas precision can always be increased in downstream processing .	One in six correct answers are lost if coreference is not used , but this can be mostly retained by simple name matching rules .	CL_D14-1089_mn_3	CL_D14-1089_mn_6	CL_D14-1089_mn_3_6
CL	D14-1089	mn	4	5	result	result	support	elaboration	secondary	secondary	back	elaboration	We provide precise , empirical confirmation of previously hypothesised sources of recall loss in slot filling .	While NE type constraints substantially reduce the search space with only a minor recall penalty , we find that 10 % to 39 % of slot fills will be entirely ignored by most systems .	CL_D14-1089_mn_4	CL_D14-1089_mn_5	CL_D14-1089_mn_4_5
CL	D14-1089	mn	5	4	result	result	elaboration	support	secondary	secondary	forw	elaboration	While NE type constraints substantially reduce the search space with only a minor recall penalty , we find that 10 % to 39 % of slot fills will be entirely ignored by most systems .	We provide precise , empirical confirmation of previously hypothesised sources of recall loss in slot filling .	CL_D14-1089_mn_5	CL_D14-1089_mn_4	CL_D14-1089_mn_4_5
CL	D14-1089	mn	4	6	result	result	support	elaboration	secondary	secondary	back	elaboration	We provide precise , empirical confirmation of previously hypothesised sources of recall loss in slot filling .	One in six correct answers are lost if coreference is not used , but this can be mostly retained by simple name matching rules .	CL_D14-1089_mn_4	CL_D14-1089_mn_6	CL_D14-1089_mn_4_6
CL	D14-1089	mn	6	4	result	result	elaboration	support	secondary	secondary	forw	elaboration	One in six correct answers are lost if coreference is not used , but this can be mostly retained by simple name matching rules .	We provide precise , empirical confirmation of previously hypothesised sources of recall loss in slot filling .	CL_D14-1089_mn_6	CL_D14-1089_mn_4	CL_D14-1089_mn_4_6
CL	D14-1089	mn	5	6	result	result	elaboration	elaboration	secondary	secondary	none	none	While NE type constraints substantially reduce the search space with only a minor recall penalty , we find that 10 % to 39 % of slot fills will be entirely ignored by most systems .	One in six correct answers are lost if coreference is not used , but this can be mostly retained by simple name matching rules .	CL_D14-1089_mn_5	CL_D14-1089_mn_6	CL_D14-1089_mn_5_6
CL	D14-1090	mn	1	2	motivation_problem	motivation_problem	info-required	support	secondary	secondary	forw	info-required	Several state-of-the-art event extraction systems employ models based on Support Vector Machines ( SVMs ) in a pipeline architecture , which fails to exploit the joint dependencies that typically exist among events and arguments .	While there have been attempts to overcome this limitation using Markov Logic Networks ( MLNs ) , it remains challenging to perform joint inference in MLNs when the model encodes many high-dimensional sophisticated features such as those essential for event extraction .	CL_D14-1090_mn_1	CL_D14-1090_mn_2	CL_D14-1090_mn_1_2
CL	D14-1090	mn	2	1	motivation_problem	motivation_problem	support	info-required	secondary	secondary	back	info-required	While there have been attempts to overcome this limitation using Markov Logic Networks ( MLNs ) , it remains challenging to perform joint inference in MLNs when the model encodes many high-dimensional sophisticated features such as those essential for event extraction .	Several state-of-the-art event extraction systems employ models based on Support Vector Machines ( SVMs ) in a pipeline architecture , which fails to exploit the joint dependencies that typically exist among events and arguments .	CL_D14-1090_mn_2	CL_D14-1090_mn_1	CL_D14-1090_mn_1_2
CL	D14-1090	mn	1	3	motivation_problem	proposal	info-required	none	secondary	main	none	none	Several state-of-the-art event extraction systems employ models based on Support Vector Machines ( SVMs ) in a pipeline architecture , which fails to exploit the joint dependencies that typically exist among events and arguments .	In this paper , we propose a new model for event extraction that combines the power of MLNs and SVMs , dwarfing their limitations .	CL_D14-1090_mn_1	CL_D14-1090_mn_3	CL_D14-1090_mn_1_3
CL	D14-1090	mn	1	4	motivation_problem	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	Several state-of-the-art event extraction systems employ models based on Support Vector Machines ( SVMs ) in a pipeline architecture , which fails to exploit the joint dependencies that typically exist among events and arguments .	The key idea is to reliably learn and process high-dimensional features using SVMs ; encode the output of SVMs as low-dimensional , soft formulas in MLNs ; and use the superior joint inferencing power of MLNs to enforce joint consistency constraints over the soft formulas .	CL_D14-1090_mn_1	CL_D14-1090_mn_4	CL_D14-1090_mn_1_4
CL	D14-1090	mn	1	5	motivation_problem	means	info-required	by-means	secondary	secondary	none	none	Several state-of-the-art event extraction systems employ models based on Support Vector Machines ( SVMs ) in a pipeline architecture , which fails to exploit the joint dependencies that typically exist among events and arguments .	We evaluate our approach for the task of extracting biomedical events on the BioNLP 2013 , 2011 and 2009 Genia shared task datasets .	CL_D14-1090_mn_1	CL_D14-1090_mn_5	CL_D14-1090_mn_1_5
CL	D14-1090	mn	1	6	motivation_problem	observation	info-required	support	secondary	secondary	none	none	Several state-of-the-art event extraction systems employ models based on Support Vector Machines ( SVMs ) in a pipeline architecture , which fails to exploit the joint dependencies that typically exist among events and arguments .	Our approach yields the best F1 score to date on the BioNLP'13 ( 53.61 ) and BioNLP'11 ( 58.07 ) datasets and the second-best F1 score to date on the BioNLP'09 dataset ( 58.16 ) .	CL_D14-1090_mn_1	CL_D14-1090_mn_6	CL_D14-1090_mn_1_6
CL	D14-1090	mn	2	3	motivation_problem	proposal	support	none	secondary	main	forw	support	While there have been attempts to overcome this limitation using Markov Logic Networks ( MLNs ) , it remains challenging to perform joint inference in MLNs when the model encodes many high-dimensional sophisticated features such as those essential for event extraction .	In this paper , we propose a new model for event extraction that combines the power of MLNs and SVMs , dwarfing their limitations .	CL_D14-1090_mn_2	CL_D14-1090_mn_3	CL_D14-1090_mn_2_3
CL	D14-1090	mn	3	2	proposal	motivation_problem	none	support	main	secondary	back	support	In this paper , we propose a new model for event extraction that combines the power of MLNs and SVMs , dwarfing their limitations .	While there have been attempts to overcome this limitation using Markov Logic Networks ( MLNs ) , it remains challenging to perform joint inference in MLNs when the model encodes many high-dimensional sophisticated features such as those essential for event extraction .	CL_D14-1090_mn_3	CL_D14-1090_mn_2	CL_D14-1090_mn_2_3
CL	D14-1090	mn	2	4	motivation_problem	proposal_implementation	support	elaboration	secondary	secondary	none	none	While there have been attempts to overcome this limitation using Markov Logic Networks ( MLNs ) , it remains challenging to perform joint inference in MLNs when the model encodes many high-dimensional sophisticated features such as those essential for event extraction .	The key idea is to reliably learn and process high-dimensional features using SVMs ; encode the output of SVMs as low-dimensional , soft formulas in MLNs ; and use the superior joint inferencing power of MLNs to enforce joint consistency constraints over the soft formulas .	CL_D14-1090_mn_2	CL_D14-1090_mn_4	CL_D14-1090_mn_2_4
CL	D14-1090	mn	2	5	motivation_problem	means	support	by-means	secondary	secondary	none	none	While there have been attempts to overcome this limitation using Markov Logic Networks ( MLNs ) , it remains challenging to perform joint inference in MLNs when the model encodes many high-dimensional sophisticated features such as those essential for event extraction .	We evaluate our approach for the task of extracting biomedical events on the BioNLP 2013 , 2011 and 2009 Genia shared task datasets .	CL_D14-1090_mn_2	CL_D14-1090_mn_5	CL_D14-1090_mn_2_5
CL	D14-1090	mn	2	6	motivation_problem	observation	support	support	secondary	secondary	none	none	While there have been attempts to overcome this limitation using Markov Logic Networks ( MLNs ) , it remains challenging to perform joint inference in MLNs when the model encodes many high-dimensional sophisticated features such as those essential for event extraction .	Our approach yields the best F1 score to date on the BioNLP'13 ( 53.61 ) and BioNLP'11 ( 58.07 ) datasets and the second-best F1 score to date on the BioNLP'09 dataset ( 58.16 ) .	CL_D14-1090_mn_2	CL_D14-1090_mn_6	CL_D14-1090_mn_2_6
CL	D14-1090	mn	3	4	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	In this paper , we propose a new model for event extraction that combines the power of MLNs and SVMs , dwarfing their limitations .	The key idea is to reliably learn and process high-dimensional features using SVMs ; encode the output of SVMs as low-dimensional , soft formulas in MLNs ; and use the superior joint inferencing power of MLNs to enforce joint consistency constraints over the soft formulas .	CL_D14-1090_mn_3	CL_D14-1090_mn_4	CL_D14-1090_mn_3_4
CL	D14-1090	mn	4	3	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	The key idea is to reliably learn and process high-dimensional features using SVMs ; encode the output of SVMs as low-dimensional , soft formulas in MLNs ; and use the superior joint inferencing power of MLNs to enforce joint consistency constraints over the soft formulas .	In this paper , we propose a new model for event extraction that combines the power of MLNs and SVMs , dwarfing their limitations .	CL_D14-1090_mn_4	CL_D14-1090_mn_3	CL_D14-1090_mn_3_4
CL	D14-1090	mn	3	5	proposal	means	none	by-means	main	secondary	none	none	In this paper , we propose a new model for event extraction that combines the power of MLNs and SVMs , dwarfing their limitations .	We evaluate our approach for the task of extracting biomedical events on the BioNLP 2013 , 2011 and 2009 Genia shared task datasets .	CL_D14-1090_mn_3	CL_D14-1090_mn_5	CL_D14-1090_mn_3_5
CL	D14-1090	mn	3	6	proposal	observation	none	support	main	secondary	back	support	In this paper , we propose a new model for event extraction that combines the power of MLNs and SVMs , dwarfing their limitations .	Our approach yields the best F1 score to date on the BioNLP'13 ( 53.61 ) and BioNLP'11 ( 58.07 ) datasets and the second-best F1 score to date on the BioNLP'09 dataset ( 58.16 ) .	CL_D14-1090_mn_3	CL_D14-1090_mn_6	CL_D14-1090_mn_3_6
CL	D14-1090	mn	6	3	observation	proposal	support	none	secondary	main	forw	support	Our approach yields the best F1 score to date on the BioNLP'13 ( 53.61 ) and BioNLP'11 ( 58.07 ) datasets and the second-best F1 score to date on the BioNLP'09 dataset ( 58.16 ) .	In this paper , we propose a new model for event extraction that combines the power of MLNs and SVMs , dwarfing their limitations .	CL_D14-1090_mn_6	CL_D14-1090_mn_3	CL_D14-1090_mn_3_6
CL	D14-1090	mn	4	5	proposal_implementation	means	elaboration	by-means	secondary	secondary	none	none	The key idea is to reliably learn and process high-dimensional features using SVMs ; encode the output of SVMs as low-dimensional , soft formulas in MLNs ; and use the superior joint inferencing power of MLNs to enforce joint consistency constraints over the soft formulas .	We evaluate our approach for the task of extracting biomedical events on the BioNLP 2013 , 2011 and 2009 Genia shared task datasets .	CL_D14-1090_mn_4	CL_D14-1090_mn_5	CL_D14-1090_mn_4_5
CL	D14-1090	mn	4	6	proposal_implementation	observation	elaboration	support	secondary	secondary	none	none	The key idea is to reliably learn and process high-dimensional features using SVMs ; encode the output of SVMs as low-dimensional , soft formulas in MLNs ; and use the superior joint inferencing power of MLNs to enforce joint consistency constraints over the soft formulas .	Our approach yields the best F1 score to date on the BioNLP'13 ( 53.61 ) and BioNLP'11 ( 58.07 ) datasets and the second-best F1 score to date on the BioNLP'09 dataset ( 58.16 ) .	CL_D14-1090_mn_4	CL_D14-1090_mn_6	CL_D14-1090_mn_4_6
CL	D14-1090	mn	5	6	means	observation	by-means	support	secondary	secondary	forw	by-means	We evaluate our approach for the task of extracting biomedical events on the BioNLP 2013 , 2011 and 2009 Genia shared task datasets .	Our approach yields the best F1 score to date on the BioNLP'13 ( 53.61 ) and BioNLP'11 ( 58.07 ) datasets and the second-best F1 score to date on the BioNLP'09 dataset ( 58.16 ) .	CL_D14-1090_mn_5	CL_D14-1090_mn_6	CL_D14-1090_mn_5_6
CL	D14-1090	mn	6	5	observation	means	support	by-means	secondary	secondary	back	by-means	Our approach yields the best F1 score to date on the BioNLP'13 ( 53.61 ) and BioNLP'11 ( 58.07 ) datasets and the second-best F1 score to date on the BioNLP'09 dataset ( 58.16 ) .	We evaluate our approach for the task of extracting biomedical events on the BioNLP 2013 , 2011 and 2009 Genia shared task datasets .	CL_D14-1090_mn_6	CL_D14-1090_mn_5	CL_D14-1090_mn_5_6
CL	D14-1091	mn	1	2	motivation_background	motivation_background	info-required	info-required	secondary	secondary	forw	info-required	Stress is a useful cue for English word segmentation .	A wide range of computational models have found that stress cues enable a 2-10 % improvement in segmentation accuracy , depending on the kind of model , by using input that has been annotated with stress using a pronouncing dictionary .	CL_D14-1091_mn_1	CL_D14-1091_mn_2	CL_D14-1091_mn_1_2
CL	D14-1091	mn	2	1	motivation_background	motivation_background	info-required	info-required	secondary	secondary	back	info-required	A wide range of computational models have found that stress cues enable a 2-10 % improvement in segmentation accuracy , depending on the kind of model , by using input that has been annotated with stress using a pronouncing dictionary .	Stress is a useful cue for English word segmentation .	CL_D14-1091_mn_2	CL_D14-1091_mn_1	CL_D14-1091_mn_1_2
CL	D14-1091	mn	1	3	motivation_background	motivation_problem	info-required	support	secondary	secondary	none	none	Stress is a useful cue for English word segmentation .	However , stress is neither invariably produced nor unambiguously identifiable in real speech .	CL_D14-1091_mn_1	CL_D14-1091_mn_3	CL_D14-1091_mn_1_3
CL	D14-1091	mn	1	4	motivation_background	motivation_background	info-required	info-required	secondary	secondary	none	none	Stress is a useful cue for English word segmentation .	Heavy syllables , i.e. those with long vowels or syllable codas , attract stress in English .	CL_D14-1091_mn_1	CL_D14-1091_mn_4	CL_D14-1091_mn_1_4
CL	D14-1091	mn	1	5	motivation_background	proposal	info-required	none	secondary	main	none	none	Stress is a useful cue for English word segmentation .	We devise Adaptor Grammar word segmentation models that exploit either stress , or syllable weight , or both , and evaluate the utility of syllable weight as a cue to word boundaries .	CL_D14-1091_mn_1	CL_D14-1091_mn_5	CL_D14-1091_mn_1_5
CL	D14-1091	mn	1	6	motivation_background	result	info-required	support	secondary	secondary	none	none	Stress is a useful cue for English word segmentation .	Our results suggest that syllable weight encodes largely the same information for word segmentation in English that annotated dictionary stress does .	CL_D14-1091_mn_1	CL_D14-1091_mn_6	CL_D14-1091_mn_1_6
CL	D14-1091	mn	2	3	motivation_background	motivation_problem	info-required	support	secondary	secondary	forw	info-required	A wide range of computational models have found that stress cues enable a 2-10 % improvement in segmentation accuracy , depending on the kind of model , by using input that has been annotated with stress using a pronouncing dictionary .	However , stress is neither invariably produced nor unambiguously identifiable in real speech .	CL_D14-1091_mn_2	CL_D14-1091_mn_3	CL_D14-1091_mn_2_3
CL	D14-1091	mn	3	2	motivation_problem	motivation_background	support	info-required	secondary	secondary	back	info-required	However , stress is neither invariably produced nor unambiguously identifiable in real speech .	A wide range of computational models have found that stress cues enable a 2-10 % improvement in segmentation accuracy , depending on the kind of model , by using input that has been annotated with stress using a pronouncing dictionary .	CL_D14-1091_mn_3	CL_D14-1091_mn_2	CL_D14-1091_mn_2_3
CL	D14-1091	mn	2	4	motivation_background	motivation_background	info-required	info-required	secondary	secondary	none	none	A wide range of computational models have found that stress cues enable a 2-10 % improvement in segmentation accuracy , depending on the kind of model , by using input that has been annotated with stress using a pronouncing dictionary .	Heavy syllables , i.e. those with long vowels or syllable codas , attract stress in English .	CL_D14-1091_mn_2	CL_D14-1091_mn_4	CL_D14-1091_mn_2_4
CL	D14-1091	mn	2	5	motivation_background	proposal	info-required	none	secondary	main	none	none	A wide range of computational models have found that stress cues enable a 2-10 % improvement in segmentation accuracy , depending on the kind of model , by using input that has been annotated with stress using a pronouncing dictionary .	We devise Adaptor Grammar word segmentation models that exploit either stress , or syllable weight , or both , and evaluate the utility of syllable weight as a cue to word boundaries .	CL_D14-1091_mn_2	CL_D14-1091_mn_5	CL_D14-1091_mn_2_5
CL	D14-1091	mn	2	6	motivation_background	result	info-required	support	secondary	secondary	none	none	A wide range of computational models have found that stress cues enable a 2-10 % improvement in segmentation accuracy , depending on the kind of model , by using input that has been annotated with stress using a pronouncing dictionary .	Our results suggest that syllable weight encodes largely the same information for word segmentation in English that annotated dictionary stress does .	CL_D14-1091_mn_2	CL_D14-1091_mn_6	CL_D14-1091_mn_2_6
CL	D14-1091	mn	3	4	motivation_problem	motivation_background	support	info-required	secondary	secondary	back	info-required	However , stress is neither invariably produced nor unambiguously identifiable in real speech .	Heavy syllables , i.e. those with long vowels or syllable codas , attract stress in English .	CL_D14-1091_mn_3	CL_D14-1091_mn_4	CL_D14-1091_mn_3_4
CL	D14-1091	mn	4	3	motivation_background	motivation_problem	info-required	support	secondary	secondary	forw	info-required	Heavy syllables , i.e. those with long vowels or syllable codas , attract stress in English .	However , stress is neither invariably produced nor unambiguously identifiable in real speech .	CL_D14-1091_mn_4	CL_D14-1091_mn_3	CL_D14-1091_mn_3_4
CL	D14-1091	mn	3	5	motivation_problem	proposal	support	none	secondary	main	forw	support	However , stress is neither invariably produced nor unambiguously identifiable in real speech .	We devise Adaptor Grammar word segmentation models that exploit either stress , or syllable weight , or both , and evaluate the utility of syllable weight as a cue to word boundaries .	CL_D14-1091_mn_3	CL_D14-1091_mn_5	CL_D14-1091_mn_3_5
CL	D14-1091	mn	5	3	proposal	motivation_problem	none	support	main	secondary	back	support	We devise Adaptor Grammar word segmentation models that exploit either stress , or syllable weight , or both , and evaluate the utility of syllable weight as a cue to word boundaries .	However , stress is neither invariably produced nor unambiguously identifiable in real speech .	CL_D14-1091_mn_5	CL_D14-1091_mn_3	CL_D14-1091_mn_3_5
CL	D14-1091	mn	3	6	motivation_problem	result	support	support	secondary	secondary	none	none	However , stress is neither invariably produced nor unambiguously identifiable in real speech .	Our results suggest that syllable weight encodes largely the same information for word segmentation in English that annotated dictionary stress does .	CL_D14-1091_mn_3	CL_D14-1091_mn_6	CL_D14-1091_mn_3_6
CL	D14-1091	mn	4	5	motivation_background	proposal	info-required	none	secondary	main	none	none	Heavy syllables , i.e. those with long vowels or syllable codas , attract stress in English .	We devise Adaptor Grammar word segmentation models that exploit either stress , or syllable weight , or both , and evaluate the utility of syllable weight as a cue to word boundaries .	CL_D14-1091_mn_4	CL_D14-1091_mn_5	CL_D14-1091_mn_4_5
CL	D14-1091	mn	4	6	motivation_background	result	info-required	support	secondary	secondary	none	none	Heavy syllables , i.e. those with long vowels or syllable codas , attract stress in English .	Our results suggest that syllable weight encodes largely the same information for word segmentation in English that annotated dictionary stress does .	CL_D14-1091_mn_4	CL_D14-1091_mn_6	CL_D14-1091_mn_4_6
CL	D14-1091	mn	5	6	proposal	result	none	support	main	secondary	back	support	We devise Adaptor Grammar word segmentation models that exploit either stress , or syllable weight , or both , and evaluate the utility of syllable weight as a cue to word boundaries .	Our results suggest that syllable weight encodes largely the same information for word segmentation in English that annotated dictionary stress does .	CL_D14-1091_mn_5	CL_D14-1091_mn_6	CL_D14-1091_mn_5_6
CL	D14-1091	mn	6	5	result	proposal	support	none	secondary	main	forw	support	Our results suggest that syllable weight encodes largely the same information for word segmentation in English that annotated dictionary stress does .	We devise Adaptor Grammar word segmentation models that exploit either stress , or syllable weight , or both , and evaluate the utility of syllable weight as a cue to word boundaries .	CL_D14-1091_mn_6	CL_D14-1091_mn_5	CL_D14-1091_mn_5_6
CL	D14-1092	mn	1	2	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	In this paper , we propose a joint model for unsupervised Chinese word segmentation ( CWS ) .	Inspired by the "products of experts" idea , our joint model firstly combines two generative models , which are word-based hierarchical Dirichlet process model and character-based hidden Markov model , by simply multiplying their probabilities together .	CL_D14-1092_mn_1	CL_D14-1092_mn_2	CL_D14-1092_mn_1_2
CL	D14-1092	mn	2	1	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	Inspired by the "products of experts" idea , our joint model firstly combines two generative models , which are word-based hierarchical Dirichlet process model and character-based hidden Markov model , by simply multiplying their probabilities together .	In this paper , we propose a joint model for unsupervised Chinese word segmentation ( CWS ) .	CL_D14-1092_mn_2	CL_D14-1092_mn_1	CL_D14-1092_mn_1_2
CL	D14-1092	mn	1	3	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	In this paper , we propose a joint model for unsupervised Chinese word segmentation ( CWS ) .	Gibbs sampling is used for model inference .	CL_D14-1092_mn_1	CL_D14-1092_mn_3	CL_D14-1092_mn_1_3
CL	D14-1092	mn	1	4	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	In this paper , we propose a joint model for unsupervised Chinese word segmentation ( CWS ) .	In order to further combine the strength of goodness-based model , we then integrated nVBE into our joint model by using it to initializing the Gibbs sampler .	CL_D14-1092_mn_1	CL_D14-1092_mn_4	CL_D14-1092_mn_1_4
CL	D14-1092	mn	1	5	proposal	means	none	by-means	main	secondary	none	none	In this paper , we propose a joint model for unsupervised Chinese word segmentation ( CWS ) .	We conduct our experiments on PKU and MSRA datasets provided by the second SIGHAN bakeoff .	CL_D14-1092_mn_1	CL_D14-1092_mn_5	CL_D14-1092_mn_1_5
CL	D14-1092	mn	1	6	proposal	result	none	support	main	secondary	back	support	In this paper , we propose a joint model for unsupervised Chinese word segmentation ( CWS ) .	Test results on these two datasets show that the joint model achieves much better results than all of its component models .	CL_D14-1092_mn_1	CL_D14-1092_mn_6	CL_D14-1092_mn_1_6
CL	D14-1092	mn	6	1	result	proposal	support	none	secondary	main	forw	support	Test results on these two datasets show that the joint model achieves much better results than all of its component models .	In this paper , we propose a joint model for unsupervised Chinese word segmentation ( CWS ) .	CL_D14-1092_mn_6	CL_D14-1092_mn_1	CL_D14-1092_mn_1_6
CL	D14-1092	mn	1	7	proposal	result	none	elaboration	main	secondary	none	none	In this paper , we propose a joint model for unsupervised Chinese word segmentation ( CWS ) .	Statistical significance tests also show that it is significantly better than state-of-the-art systems , achieving the highest F-scores .	CL_D14-1092_mn_1	CL_D14-1092_mn_7	CL_D14-1092_mn_1_7
CL	D14-1092	mn	1	8	proposal	result	none	elaboration	main	secondary	none	none	In this paper , we propose a joint model for unsupervised Chinese word segmentation ( CWS ) .	Finally , analysis indicates that compared with nVBE and HDP , the joint model has a stronger ability to solve both combinational and overlapping ambiguities in Chinese word segmentation .	CL_D14-1092_mn_1	CL_D14-1092_mn_8	CL_D14-1092_mn_1_8
CL	D14-1092	mn	2	3	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	Inspired by the "products of experts" idea , our joint model firstly combines two generative models , which are word-based hierarchical Dirichlet process model and character-based hidden Markov model , by simply multiplying their probabilities together .	Gibbs sampling is used for model inference .	CL_D14-1092_mn_2	CL_D14-1092_mn_3	CL_D14-1092_mn_2_3
CL	D14-1092	mn	3	2	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	forw	elaboration	Gibbs sampling is used for model inference .	Inspired by the "products of experts" idea , our joint model firstly combines two generative models , which are word-based hierarchical Dirichlet process model and character-based hidden Markov model , by simply multiplying their probabilities together .	CL_D14-1092_mn_3	CL_D14-1092_mn_2	CL_D14-1092_mn_2_3
CL	D14-1092	mn	2	4	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	none	none	Inspired by the "products of experts" idea , our joint model firstly combines two generative models , which are word-based hierarchical Dirichlet process model and character-based hidden Markov model , by simply multiplying their probabilities together .	In order to further combine the strength of goodness-based model , we then integrated nVBE into our joint model by using it to initializing the Gibbs sampler .	CL_D14-1092_mn_2	CL_D14-1092_mn_4	CL_D14-1092_mn_2_4
CL	D14-1092	mn	2	5	proposal_implementation	means	elaboration	by-means	secondary	secondary	none	none	Inspired by the "products of experts" idea , our joint model firstly combines two generative models , which are word-based hierarchical Dirichlet process model and character-based hidden Markov model , by simply multiplying their probabilities together .	We conduct our experiments on PKU and MSRA datasets provided by the second SIGHAN bakeoff .	CL_D14-1092_mn_2	CL_D14-1092_mn_5	CL_D14-1092_mn_2_5
CL	D14-1092	mn	2	6	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	Inspired by the "products of experts" idea , our joint model firstly combines two generative models , which are word-based hierarchical Dirichlet process model and character-based hidden Markov model , by simply multiplying their probabilities together .	Test results on these two datasets show that the joint model achieves much better results than all of its component models .	CL_D14-1092_mn_2	CL_D14-1092_mn_6	CL_D14-1092_mn_2_6
CL	D14-1092	mn	2	7	proposal_implementation	result	elaboration	elaboration	secondary	secondary	none	none	Inspired by the "products of experts" idea , our joint model firstly combines two generative models , which are word-based hierarchical Dirichlet process model and character-based hidden Markov model , by simply multiplying their probabilities together .	Statistical significance tests also show that it is significantly better than state-of-the-art systems , achieving the highest F-scores .	CL_D14-1092_mn_2	CL_D14-1092_mn_7	CL_D14-1092_mn_2_7
CL	D14-1092	mn	2	8	proposal_implementation	result	elaboration	elaboration	secondary	secondary	none	none	Inspired by the "products of experts" idea , our joint model firstly combines two generative models , which are word-based hierarchical Dirichlet process model and character-based hidden Markov model , by simply multiplying their probabilities together .	Finally , analysis indicates that compared with nVBE and HDP , the joint model has a stronger ability to solve both combinational and overlapping ambiguities in Chinese word segmentation .	CL_D14-1092_mn_2	CL_D14-1092_mn_8	CL_D14-1092_mn_2_8
CL	D14-1092	mn	3	4	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	Gibbs sampling is used for model inference .	In order to further combine the strength of goodness-based model , we then integrated nVBE into our joint model by using it to initializing the Gibbs sampler .	CL_D14-1092_mn_3	CL_D14-1092_mn_4	CL_D14-1092_mn_3_4
CL	D14-1092	mn	4	3	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	forw	elaboration	In order to further combine the strength of goodness-based model , we then integrated nVBE into our joint model by using it to initializing the Gibbs sampler .	Gibbs sampling is used for model inference .	CL_D14-1092_mn_4	CL_D14-1092_mn_3	CL_D14-1092_mn_3_4
CL	D14-1092	mn	3	5	proposal_implementation	means	elaboration	by-means	secondary	secondary	none	none	Gibbs sampling is used for model inference .	We conduct our experiments on PKU and MSRA datasets provided by the second SIGHAN bakeoff .	CL_D14-1092_mn_3	CL_D14-1092_mn_5	CL_D14-1092_mn_3_5
CL	D14-1092	mn	3	6	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	Gibbs sampling is used for model inference .	Test results on these two datasets show that the joint model achieves much better results than all of its component models .	CL_D14-1092_mn_3	CL_D14-1092_mn_6	CL_D14-1092_mn_3_6
CL	D14-1092	mn	3	7	proposal_implementation	result	elaboration	elaboration	secondary	secondary	none	none	Gibbs sampling is used for model inference .	Statistical significance tests also show that it is significantly better than state-of-the-art systems , achieving the highest F-scores .	CL_D14-1092_mn_3	CL_D14-1092_mn_7	CL_D14-1092_mn_3_7
CL	D14-1092	mn	3	8	proposal_implementation	result	elaboration	elaboration	secondary	secondary	none	none	Gibbs sampling is used for model inference .	Finally , analysis indicates that compared with nVBE and HDP , the joint model has a stronger ability to solve both combinational and overlapping ambiguities in Chinese word segmentation .	CL_D14-1092_mn_3	CL_D14-1092_mn_8	CL_D14-1092_mn_3_8
CL	D14-1092	mn	4	5	proposal_implementation	means	elaboration	by-means	secondary	secondary	none	none	In order to further combine the strength of goodness-based model , we then integrated nVBE into our joint model by using it to initializing the Gibbs sampler .	We conduct our experiments on PKU and MSRA datasets provided by the second SIGHAN bakeoff .	CL_D14-1092_mn_4	CL_D14-1092_mn_5	CL_D14-1092_mn_4_5
CL	D14-1092	mn	4	6	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	In order to further combine the strength of goodness-based model , we then integrated nVBE into our joint model by using it to initializing the Gibbs sampler .	Test results on these two datasets show that the joint model achieves much better results than all of its component models .	CL_D14-1092_mn_4	CL_D14-1092_mn_6	CL_D14-1092_mn_4_6
CL	D14-1092	mn	4	7	proposal_implementation	result	elaboration	elaboration	secondary	secondary	none	none	In order to further combine the strength of goodness-based model , we then integrated nVBE into our joint model by using it to initializing the Gibbs sampler .	Statistical significance tests also show that it is significantly better than state-of-the-art systems , achieving the highest F-scores .	CL_D14-1092_mn_4	CL_D14-1092_mn_7	CL_D14-1092_mn_4_7
CL	D14-1092	mn	4	8	proposal_implementation	result	elaboration	elaboration	secondary	secondary	none	none	In order to further combine the strength of goodness-based model , we then integrated nVBE into our joint model by using it to initializing the Gibbs sampler .	Finally , analysis indicates that compared with nVBE and HDP , the joint model has a stronger ability to solve both combinational and overlapping ambiguities in Chinese word segmentation .	CL_D14-1092_mn_4	CL_D14-1092_mn_8	CL_D14-1092_mn_4_8
CL	D14-1092	mn	5	6	means	result	by-means	support	secondary	secondary	forw	by-means	We conduct our experiments on PKU and MSRA datasets provided by the second SIGHAN bakeoff .	Test results on these two datasets show that the joint model achieves much better results than all of its component models .	CL_D14-1092_mn_5	CL_D14-1092_mn_6	CL_D14-1092_mn_5_6
CL	D14-1092	mn	6	5	result	means	support	by-means	secondary	secondary	back	by-means	Test results on these two datasets show that the joint model achieves much better results than all of its component models .	We conduct our experiments on PKU and MSRA datasets provided by the second SIGHAN bakeoff .	CL_D14-1092_mn_6	CL_D14-1092_mn_5	CL_D14-1092_mn_5_6
CL	D14-1092	mn	5	7	means	result	by-means	elaboration	secondary	secondary	none	none	We conduct our experiments on PKU and MSRA datasets provided by the second SIGHAN bakeoff .	Statistical significance tests also show that it is significantly better than state-of-the-art systems , achieving the highest F-scores .	CL_D14-1092_mn_5	CL_D14-1092_mn_7	CL_D14-1092_mn_5_7
CL	D14-1092	mn	5	8	means	result	by-means	elaboration	secondary	secondary	none	none	We conduct our experiments on PKU and MSRA datasets provided by the second SIGHAN bakeoff .	Finally , analysis indicates that compared with nVBE and HDP , the joint model has a stronger ability to solve both combinational and overlapping ambiguities in Chinese word segmentation .	CL_D14-1092_mn_5	CL_D14-1092_mn_8	CL_D14-1092_mn_5_8
CL	D14-1092	mn	6	7	result	result	support	elaboration	secondary	secondary	back	elaboration	Test results on these two datasets show that the joint model achieves much better results than all of its component models .	Statistical significance tests also show that it is significantly better than state-of-the-art systems , achieving the highest F-scores .	CL_D14-1092_mn_6	CL_D14-1092_mn_7	CL_D14-1092_mn_6_7
CL	D14-1092	mn	7	6	result	result	elaboration	support	secondary	secondary	forw	elaboration	Statistical significance tests also show that it is significantly better than state-of-the-art systems , achieving the highest F-scores .	Test results on these two datasets show that the joint model achieves much better results than all of its component models .	CL_D14-1092_mn_7	CL_D14-1092_mn_6	CL_D14-1092_mn_6_7
CL	D14-1092	mn	6	8	result	result	support	elaboration	secondary	secondary	none	none	Test results on these two datasets show that the joint model achieves much better results than all of its component models .	Finally , analysis indicates that compared with nVBE and HDP , the joint model has a stronger ability to solve both combinational and overlapping ambiguities in Chinese word segmentation .	CL_D14-1092_mn_6	CL_D14-1092_mn_8	CL_D14-1092_mn_6_8
CL	D14-1092	mn	7	8	result	result	elaboration	elaboration	secondary	secondary	back	elaboration	Statistical significance tests also show that it is significantly better than state-of-the-art systems , achieving the highest F-scores .	Finally , analysis indicates that compared with nVBE and HDP , the joint model has a stronger ability to solve both combinational and overlapping ambiguities in Chinese word segmentation .	CL_D14-1092_mn_7	CL_D14-1092_mn_8	CL_D14-1092_mn_7_8
CL	D14-1092	mn	8	7	result	result	elaboration	elaboration	secondary	secondary	forw	elaboration	Finally , analysis indicates that compared with nVBE and HDP , the joint model has a stronger ability to solve both combinational and overlapping ambiguities in Chinese word segmentation .	Statistical significance tests also show that it is significantly better than state-of-the-art systems , achieving the highest F-scores .	CL_D14-1092_mn_8	CL_D14-1092_mn_7	CL_D14-1092_mn_7_8
CL	D14-1093	mn	1	2	motivation_background	motivation_problem	info-required	support	secondary	secondary	forw	info-required	Supervised methods have been the dominant approach for Chinese word segmentation .	The performance can drop significantly when the test domain is different from the training domain .	CL_D14-1093_mn_1	CL_D14-1093_mn_2	CL_D14-1093_mn_1_2
CL	D14-1093	mn	2	1	motivation_problem	motivation_background	support	info-required	secondary	secondary	back	info-required	The performance can drop significantly when the test domain is different from the training domain .	Supervised methods have been the dominant approach for Chinese word segmentation .	CL_D14-1093_mn_2	CL_D14-1093_mn_1	CL_D14-1093_mn_1_2
CL	D14-1093	mn	1	3	motivation_background	proposal	info-required	none	secondary	main	none	none	Supervised methods have been the dominant approach for Chinese word segmentation .	In this paper , we study the problem of obtaining partial annotation from freely available data to help Chinese word segmentation on different domains .	CL_D14-1093_mn_1	CL_D14-1093_mn_3	CL_D14-1093_mn_1_3
CL	D14-1093	mn	1	4	motivation_background	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	Supervised methods have been the dominant approach for Chinese word segmentation .	Different sources of free annotations are transformed into a unified form of partial annotation and a variant CRF model is used to leverage both fully and partially annotated data consistently .	CL_D14-1093_mn_1	CL_D14-1093_mn_4	CL_D14-1093_mn_1_4
CL	D14-1093	mn	1	5	motivation_background	result	info-required	support	secondary	secondary	none	none	Supervised methods have been the dominant approach for Chinese word segmentation .	Experimental results show that the Chinese word segmentation model benefits from free partially annotated data .	CL_D14-1093_mn_1	CL_D14-1093_mn_5	CL_D14-1093_mn_1_5
CL	D14-1093	mn	1	6	motivation_background	result_means	info-required	elaboration	secondary	secondary	none	none	Supervised methods have been the dominant approach for Chinese word segmentation .	On the SIGHAN Bakeoff 2010 data , we achieve results that are competitive to the best reported in the literature .	CL_D14-1093_mn_1	CL_D14-1093_mn_6	CL_D14-1093_mn_1_6
CL	D14-1093	mn	2	3	motivation_problem	proposal	support	none	secondary	main	forw	support	The performance can drop significantly when the test domain is different from the training domain .	In this paper , we study the problem of obtaining partial annotation from freely available data to help Chinese word segmentation on different domains .	CL_D14-1093_mn_2	CL_D14-1093_mn_3	CL_D14-1093_mn_2_3
CL	D14-1093	mn	3	2	proposal	motivation_problem	none	support	main	secondary	back	support	In this paper , we study the problem of obtaining partial annotation from freely available data to help Chinese word segmentation on different domains .	The performance can drop significantly when the test domain is different from the training domain .	CL_D14-1093_mn_3	CL_D14-1093_mn_2	CL_D14-1093_mn_2_3
CL	D14-1093	mn	2	4	motivation_problem	proposal_implementation	support	elaboration	secondary	secondary	none	none	The performance can drop significantly when the test domain is different from the training domain .	Different sources of free annotations are transformed into a unified form of partial annotation and a variant CRF model is used to leverage both fully and partially annotated data consistently .	CL_D14-1093_mn_2	CL_D14-1093_mn_4	CL_D14-1093_mn_2_4
CL	D14-1093	mn	2	5	motivation_problem	result	support	support	secondary	secondary	none	none	The performance can drop significantly when the test domain is different from the training domain .	Experimental results show that the Chinese word segmentation model benefits from free partially annotated data .	CL_D14-1093_mn_2	CL_D14-1093_mn_5	CL_D14-1093_mn_2_5
CL	D14-1093	mn	2	6	motivation_problem	result_means	support	elaboration	secondary	secondary	none	none	The performance can drop significantly when the test domain is different from the training domain .	On the SIGHAN Bakeoff 2010 data , we achieve results that are competitive to the best reported in the literature .	CL_D14-1093_mn_2	CL_D14-1093_mn_6	CL_D14-1093_mn_2_6
CL	D14-1093	mn	3	4	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	In this paper , we study the problem of obtaining partial annotation from freely available data to help Chinese word segmentation on different domains .	Different sources of free annotations are transformed into a unified form of partial annotation and a variant CRF model is used to leverage both fully and partially annotated data consistently .	CL_D14-1093_mn_3	CL_D14-1093_mn_4	CL_D14-1093_mn_3_4
CL	D14-1093	mn	4	3	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	Different sources of free annotations are transformed into a unified form of partial annotation and a variant CRF model is used to leverage both fully and partially annotated data consistently .	In this paper , we study the problem of obtaining partial annotation from freely available data to help Chinese word segmentation on different domains .	CL_D14-1093_mn_4	CL_D14-1093_mn_3	CL_D14-1093_mn_3_4
CL	D14-1093	mn	3	5	proposal	result	none	support	main	secondary	back	support	In this paper , we study the problem of obtaining partial annotation from freely available data to help Chinese word segmentation on different domains .	Experimental results show that the Chinese word segmentation model benefits from free partially annotated data .	CL_D14-1093_mn_3	CL_D14-1093_mn_5	CL_D14-1093_mn_3_5
CL	D14-1093	mn	5	3	result	proposal	support	none	secondary	main	forw	support	Experimental results show that the Chinese word segmentation model benefits from free partially annotated data .	In this paper , we study the problem of obtaining partial annotation from freely available data to help Chinese word segmentation on different domains .	CL_D14-1093_mn_5	CL_D14-1093_mn_3	CL_D14-1093_mn_3_5
CL	D14-1093	mn	3	6	proposal	result_means	none	elaboration	main	secondary	none	none	In this paper , we study the problem of obtaining partial annotation from freely available data to help Chinese word segmentation on different domains .	On the SIGHAN Bakeoff 2010 data , we achieve results that are competitive to the best reported in the literature .	CL_D14-1093_mn_3	CL_D14-1093_mn_6	CL_D14-1093_mn_3_6
CL	D14-1093	mn	4	5	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	Different sources of free annotations are transformed into a unified form of partial annotation and a variant CRF model is used to leverage both fully and partially annotated data consistently .	Experimental results show that the Chinese word segmentation model benefits from free partially annotated data .	CL_D14-1093_mn_4	CL_D14-1093_mn_5	CL_D14-1093_mn_4_5
CL	D14-1093	mn	4	6	proposal_implementation	result_means	elaboration	elaboration	secondary	secondary	none	none	Different sources of free annotations are transformed into a unified form of partial annotation and a variant CRF model is used to leverage both fully and partially annotated data consistently .	On the SIGHAN Bakeoff 2010 data , we achieve results that are competitive to the best reported in the literature .	CL_D14-1093_mn_4	CL_D14-1093_mn_6	CL_D14-1093_mn_4_6
CL	D14-1093	mn	5	6	result	result_means	support	elaboration	secondary	secondary	back	elaboration	Experimental results show that the Chinese word segmentation model benefits from free partially annotated data .	On the SIGHAN Bakeoff 2010 data , we achieve results that are competitive to the best reported in the literature .	CL_D14-1093_mn_5	CL_D14-1093_mn_6	CL_D14-1093_mn_5_6
CL	D14-1093	mn	6	5	result_means	result	elaboration	support	secondary	secondary	forw	elaboration	On the SIGHAN Bakeoff 2010 data , we achieve results that are competitive to the best reported in the literature .	Experimental results show that the Chinese word segmentation model benefits from free partially annotated data .	CL_D14-1093_mn_6	CL_D14-1093_mn_5	CL_D14-1093_mn_5_6
CL	D14-1094	mn	1	2	motivation_background	motivation_problem	info-required	support	secondary	secondary	forw	info-required	Most studies on statistical Korean word spacing do not utilize the information provided by the input sentence and assume that it was completely concatenated .	This makes the word spacer ignore the correct spaced parts of the input sentence and erroneously alter them .	CL_D14-1094_mn_1	CL_D14-1094_mn_2	CL_D14-1094_mn_1_2
CL	D14-1094	mn	2	1	motivation_problem	motivation_background	support	info-required	secondary	secondary	back	info-required	This makes the word spacer ignore the correct spaced parts of the input sentence and erroneously alter them .	Most studies on statistical Korean word spacing do not utilize the information provided by the input sentence and assume that it was completely concatenated .	CL_D14-1094_mn_2	CL_D14-1094_mn_1	CL_D14-1094_mn_1_2
CL	D14-1094	mn	1	3	motivation_background	proposal	info-required	none	secondary	main	none	none	Most studies on statistical Korean word spacing do not utilize the information provided by the input sentence and assume that it was completely concatenated .	To overcome such limit , this paper proposes a structural SVM-based Korean word spacing method that can utilize the space information of the input sentence .	CL_D14-1094_mn_1	CL_D14-1094_mn_3	CL_D14-1094_mn_1_3
CL	D14-1094	mn	1	4	motivation_background	observation	info-required	support	secondary	secondary	none	none	Most studies on statistical Korean word spacing do not utilize the information provided by the input sentence and assume that it was completely concatenated .	The experiment on sentences with 10 % spacing errors showed that our method achieved 96.81 % F-score , while the basic structural SVM method only achieved 92.53 % F-score .	CL_D14-1094_mn_1	CL_D14-1094_mn_4	CL_D14-1094_mn_1_4
CL	D14-1094	mn	1	5	motivation_background	result	info-required	support	secondary	secondary	none	none	Most studies on statistical Korean word spacing do not utilize the information provided by the input sentence and assume that it was completely concatenated .	The more the input sentence was correctly spaced , the more accurately our method performed .	CL_D14-1094_mn_1	CL_D14-1094_mn_5	CL_D14-1094_mn_1_5
CL	D14-1094	mn	2	3	motivation_problem	proposal	support	none	secondary	main	forw	support	This makes the word spacer ignore the correct spaced parts of the input sentence and erroneously alter them .	To overcome such limit , this paper proposes a structural SVM-based Korean word spacing method that can utilize the space information of the input sentence .	CL_D14-1094_mn_2	CL_D14-1094_mn_3	CL_D14-1094_mn_2_3
CL	D14-1094	mn	3	2	proposal	motivation_problem	none	support	main	secondary	back	support	To overcome such limit , this paper proposes a structural SVM-based Korean word spacing method that can utilize the space information of the input sentence .	This makes the word spacer ignore the correct spaced parts of the input sentence and erroneously alter them .	CL_D14-1094_mn_3	CL_D14-1094_mn_2	CL_D14-1094_mn_2_3
CL	D14-1094	mn	2	4	motivation_problem	observation	support	support	secondary	secondary	none	none	This makes the word spacer ignore the correct spaced parts of the input sentence and erroneously alter them .	The experiment on sentences with 10 % spacing errors showed that our method achieved 96.81 % F-score , while the basic structural SVM method only achieved 92.53 % F-score .	CL_D14-1094_mn_2	CL_D14-1094_mn_4	CL_D14-1094_mn_2_4
CL	D14-1094	mn	2	5	motivation_problem	result	support	support	secondary	secondary	none	none	This makes the word spacer ignore the correct spaced parts of the input sentence and erroneously alter them .	The more the input sentence was correctly spaced , the more accurately our method performed .	CL_D14-1094_mn_2	CL_D14-1094_mn_5	CL_D14-1094_mn_2_5
CL	D14-1094	mn	3	4	proposal	observation	none	support	main	secondary	none	none	To overcome such limit , this paper proposes a structural SVM-based Korean word spacing method that can utilize the space information of the input sentence .	The experiment on sentences with 10 % spacing errors showed that our method achieved 96.81 % F-score , while the basic structural SVM method only achieved 92.53 % F-score .	CL_D14-1094_mn_3	CL_D14-1094_mn_4	CL_D14-1094_mn_3_4
CL	D14-1094	mn	3	5	proposal	result	none	support	main	secondary	back	support	To overcome such limit , this paper proposes a structural SVM-based Korean word spacing method that can utilize the space information of the input sentence .	The more the input sentence was correctly spaced , the more accurately our method performed .	CL_D14-1094_mn_3	CL_D14-1094_mn_5	CL_D14-1094_mn_3_5
CL	D14-1094	mn	5	3	result	proposal	support	none	secondary	main	forw	support	The more the input sentence was correctly spaced , the more accurately our method performed .	To overcome such limit , this paper proposes a structural SVM-based Korean word spacing method that can utilize the space information of the input sentence .	CL_D14-1094_mn_5	CL_D14-1094_mn_3	CL_D14-1094_mn_3_5
CL	D14-1094	mn	4	5	observation	result	support	support	secondary	secondary	forw	support	The experiment on sentences with 10 % spacing errors showed that our method achieved 96.81 % F-score , while the basic structural SVM method only achieved 92.53 % F-score .	The more the input sentence was correctly spaced , the more accurately our method performed .	CL_D14-1094_mn_4	CL_D14-1094_mn_5	CL_D14-1094_mn_4_5
CL	D14-1094	mn	5	4	result	observation	support	support	secondary	secondary	back	support	The more the input sentence was correctly spaced , the more accurately our method performed .	The experiment on sentences with 10 % spacing errors showed that our method achieved 96.81 % F-score , while the basic structural SVM method only achieved 92.53 % F-score .	CL_D14-1094_mn_5	CL_D14-1094_mn_4	CL_D14-1094_mn_4_5
CL	D14-1095	mn	1	2	proposal	motivation_problem	none	support	main	secondary	back	support	We explore the impact of morphological segmentation on keyword spotting ( KWS ) .	Despite potential benefits , state-of-the-art KWS systems do not use morphological information .	CL_D14-1095_mn_1	CL_D14-1095_mn_2	CL_D14-1095_mn_1_2
CL	D14-1095	mn	2	1	motivation_problem	proposal	support	none	secondary	main	forw	support	Despite potential benefits , state-of-the-art KWS systems do not use morphological information .	We explore the impact of morphological segmentation on keyword spotting ( KWS ) .	CL_D14-1095_mn_2	CL_D14-1095_mn_1	CL_D14-1095_mn_1_2
CL	D14-1095	mn	1	3	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	We explore the impact of morphological segmentation on keyword spotting ( KWS ) .	In this paper , we augment a state-of-the-art KWS system with sub-word units derived from supervised and unsupervised morphological segmentations , and compare with phonetic and syllabic segmentations .	CL_D14-1095_mn_1	CL_D14-1095_mn_3	CL_D14-1095_mn_1_3
CL	D14-1095	mn	3	1	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	In this paper , we augment a state-of-the-art KWS system with sub-word units derived from supervised and unsupervised morphological segmentations , and compare with phonetic and syllabic segmentations .	We explore the impact of morphological segmentation on keyword spotting ( KWS ) .	CL_D14-1095_mn_3	CL_D14-1095_mn_1	CL_D14-1095_mn_1_3
CL	D14-1095	mn	1	4	proposal	result	none	support	main	secondary	back	support	We explore the impact of morphological segmentation on keyword spotting ( KWS ) .	Our experiments demonstrate that morphemes improve overall performance of KWS systems .	CL_D14-1095_mn_1	CL_D14-1095_mn_4	CL_D14-1095_mn_1_4
CL	D14-1095	mn	4	1	result	proposal	support	none	secondary	main	forw	support	Our experiments demonstrate that morphemes improve overall performance of KWS systems .	We explore the impact of morphological segmentation on keyword spotting ( KWS ) .	CL_D14-1095_mn_4	CL_D14-1095_mn_1	CL_D14-1095_mn_1_4
CL	D14-1095	mn	1	5	proposal	result	none	elaboration	main	secondary	none	none	We explore the impact of morphological segmentation on keyword spotting ( KWS ) .	Syllabic units , however , rival the performance of morphological units when used in KWS .	CL_D14-1095_mn_1	CL_D14-1095_mn_5	CL_D14-1095_mn_1_5
CL	D14-1095	mn	1	6	proposal	result	none	elaboration	main	secondary	none	none	We explore the impact of morphological segmentation on keyword spotting ( KWS ) .	By combining morphological , phonetic and syllabic segmentations , we demonstrate substantial performance gains .	CL_D14-1095_mn_1	CL_D14-1095_mn_6	CL_D14-1095_mn_1_6
CL	D14-1095	mn	2	3	motivation_problem	proposal_implementation	support	elaboration	secondary	secondary	none	none	Despite potential benefits , state-of-the-art KWS systems do not use morphological information .	In this paper , we augment a state-of-the-art KWS system with sub-word units derived from supervised and unsupervised morphological segmentations , and compare with phonetic and syllabic segmentations .	CL_D14-1095_mn_2	CL_D14-1095_mn_3	CL_D14-1095_mn_2_3
CL	D14-1095	mn	2	4	motivation_problem	result	support	support	secondary	secondary	none	none	Despite potential benefits , state-of-the-art KWS systems do not use morphological information .	Our experiments demonstrate that morphemes improve overall performance of KWS systems .	CL_D14-1095_mn_2	CL_D14-1095_mn_4	CL_D14-1095_mn_2_4
CL	D14-1095	mn	2	5	motivation_problem	result	support	elaboration	secondary	secondary	none	none	Despite potential benefits , state-of-the-art KWS systems do not use morphological information .	Syllabic units , however , rival the performance of morphological units when used in KWS .	CL_D14-1095_mn_2	CL_D14-1095_mn_5	CL_D14-1095_mn_2_5
CL	D14-1095	mn	2	6	motivation_problem	result	support	elaboration	secondary	secondary	none	none	Despite potential benefits , state-of-the-art KWS systems do not use morphological information .	By combining morphological , phonetic and syllabic segmentations , we demonstrate substantial performance gains .	CL_D14-1095_mn_2	CL_D14-1095_mn_6	CL_D14-1095_mn_2_6
CL	D14-1095	mn	3	4	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	In this paper , we augment a state-of-the-art KWS system with sub-word units derived from supervised and unsupervised morphological segmentations , and compare with phonetic and syllabic segmentations .	Our experiments demonstrate that morphemes improve overall performance of KWS systems .	CL_D14-1095_mn_3	CL_D14-1095_mn_4	CL_D14-1095_mn_3_4
CL	D14-1095	mn	3	5	proposal_implementation	result	elaboration	elaboration	secondary	secondary	none	none	In this paper , we augment a state-of-the-art KWS system with sub-word units derived from supervised and unsupervised morphological segmentations , and compare with phonetic and syllabic segmentations .	Syllabic units , however , rival the performance of morphological units when used in KWS .	CL_D14-1095_mn_3	CL_D14-1095_mn_5	CL_D14-1095_mn_3_5
CL	D14-1095	mn	3	6	proposal_implementation	result	elaboration	elaboration	secondary	secondary	none	none	In this paper , we augment a state-of-the-art KWS system with sub-word units derived from supervised and unsupervised morphological segmentations , and compare with phonetic and syllabic segmentations .	By combining morphological , phonetic and syllabic segmentations , we demonstrate substantial performance gains .	CL_D14-1095_mn_3	CL_D14-1095_mn_6	CL_D14-1095_mn_3_6
CL	D14-1095	mn	4	5	result	result	support	elaboration	secondary	secondary	back	elaboration	Our experiments demonstrate that morphemes improve overall performance of KWS systems .	Syllabic units , however , rival the performance of morphological units when used in KWS .	CL_D14-1095_mn_4	CL_D14-1095_mn_5	CL_D14-1095_mn_4_5
CL	D14-1095	mn	5	4	result	result	elaboration	support	secondary	secondary	forw	elaboration	Syllabic units , however , rival the performance of morphological units when used in KWS .	Our experiments demonstrate that morphemes improve overall performance of KWS systems .	CL_D14-1095_mn_5	CL_D14-1095_mn_4	CL_D14-1095_mn_4_5
CL	D14-1095	mn	4	6	result	result	support	elaboration	secondary	secondary	none	none	Our experiments demonstrate that morphemes improve overall performance of KWS systems .	By combining morphological , phonetic and syllabic segmentations , we demonstrate substantial performance gains .	CL_D14-1095_mn_4	CL_D14-1095_mn_6	CL_D14-1095_mn_4_6
CL	D14-1095	mn	5	6	result	result	elaboration	elaboration	secondary	secondary	back	elaboration	Syllabic units , however , rival the performance of morphological units when used in KWS .	By combining morphological , phonetic and syllabic segmentations , we demonstrate substantial performance gains .	CL_D14-1095_mn_5	CL_D14-1095_mn_6	CL_D14-1095_mn_5_6
CL	D14-1095	mn	6	5	result	result	elaboration	elaboration	secondary	secondary	forw	elaboration	By combining morphological , phonetic and syllabic segmentations , we demonstrate substantial performance gains .	Syllabic units , however , rival the performance of morphological units when used in KWS .	CL_D14-1095_mn_6	CL_D14-1095_mn_5	CL_D14-1095_mn_5_6
CL	D14-1096	mn	1	2	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	In this paper we address the problem of multilingual part-of-speech tagging for resource-poor languages .	We use parallel data to transfer part-of-speech information from resource-rich to resource-poor languages .	CL_D14-1096_mn_1	CL_D14-1096_mn_2	CL_D14-1096_mn_1_2
CL	D14-1096	mn	2	1	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	We use parallel data to transfer part-of-speech information from resource-rich to resource-poor languages .	In this paper we address the problem of multilingual part-of-speech tagging for resource-poor languages .	CL_D14-1096_mn_2	CL_D14-1096_mn_1	CL_D14-1096_mn_1_2
CL	D14-1096	mn	1	3	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	In this paper we address the problem of multilingual part-of-speech tagging for resource-poor languages .	Additionally , we use a small amount of annotated data to learn to "correct" errors from projected approach such as tagset mismatch between languages , achieving state-of-the-art performance ( 91.3 % ) across 8 languages .	CL_D14-1096_mn_1	CL_D14-1096_mn_3	CL_D14-1096_mn_1_3
CL	D14-1096	mn	1	4	proposal	conclusion	none	support	main	secondary	back	support	In this paper we address the problem of multilingual part-of-speech tagging for resource-poor languages .	Our approach is based on modest data requirements , and uses minimum divergence classification .	CL_D14-1096_mn_1	CL_D14-1096_mn_4	CL_D14-1096_mn_1_4
CL	D14-1096	mn	4	1	conclusion	proposal	support	none	secondary	main	forw	support	Our approach is based on modest data requirements , and uses minimum divergence classification .	In this paper we address the problem of multilingual part-of-speech tagging for resource-poor languages .	CL_D14-1096_mn_4	CL_D14-1096_mn_1	CL_D14-1096_mn_1_4
CL	D14-1096	mn	1	5	proposal	result_means	none	support	main	secondary	back	support	In this paper we address the problem of multilingual part-of-speech tagging for resource-poor languages .	For situations where no universal tagset mapping is available , we propose an alternate method , resulting in state-of-the-art 85.6 % accuracy on the resource-poor language Malagasy .	CL_D14-1096_mn_1	CL_D14-1096_mn_5	CL_D14-1096_mn_1_5
CL	D14-1096	mn	5	1	result_means	proposal	support	none	secondary	main	forw	support	For situations where no universal tagset mapping is available , we propose an alternate method , resulting in state-of-the-art 85.6 % accuracy on the resource-poor language Malagasy .	In this paper we address the problem of multilingual part-of-speech tagging for resource-poor languages .	CL_D14-1096_mn_5	CL_D14-1096_mn_1	CL_D14-1096_mn_1_5
CL	D14-1096	mn	2	3	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	We use parallel data to transfer part-of-speech information from resource-rich to resource-poor languages .	Additionally , we use a small amount of annotated data to learn to "correct" errors from projected approach such as tagset mismatch between languages , achieving state-of-the-art performance ( 91.3 % ) across 8 languages .	CL_D14-1096_mn_2	CL_D14-1096_mn_3	CL_D14-1096_mn_2_3
CL	D14-1096	mn	3	2	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	forw	elaboration	Additionally , we use a small amount of annotated data to learn to "correct" errors from projected approach such as tagset mismatch between languages , achieving state-of-the-art performance ( 91.3 % ) across 8 languages .	We use parallel data to transfer part-of-speech information from resource-rich to resource-poor languages .	CL_D14-1096_mn_3	CL_D14-1096_mn_2	CL_D14-1096_mn_2_3
CL	D14-1096	mn	2	4	proposal_implementation	conclusion	elaboration	support	secondary	secondary	none	none	We use parallel data to transfer part-of-speech information from resource-rich to resource-poor languages .	Our approach is based on modest data requirements , and uses minimum divergence classification .	CL_D14-1096_mn_2	CL_D14-1096_mn_4	CL_D14-1096_mn_2_4
CL	D14-1096	mn	2	5	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	We use parallel data to transfer part-of-speech information from resource-rich to resource-poor languages .	For situations where no universal tagset mapping is available , we propose an alternate method , resulting in state-of-the-art 85.6 % accuracy on the resource-poor language Malagasy .	CL_D14-1096_mn_2	CL_D14-1096_mn_5	CL_D14-1096_mn_2_5
CL	D14-1096	mn	3	4	proposal_implementation	conclusion	elaboration	support	secondary	secondary	none	none	Additionally , we use a small amount of annotated data to learn to "correct" errors from projected approach such as tagset mismatch between languages , achieving state-of-the-art performance ( 91.3 % ) across 8 languages .	Our approach is based on modest data requirements , and uses minimum divergence classification .	CL_D14-1096_mn_3	CL_D14-1096_mn_4	CL_D14-1096_mn_3_4
CL	D14-1096	mn	3	5	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	Additionally , we use a small amount of annotated data to learn to "correct" errors from projected approach such as tagset mismatch between languages , achieving state-of-the-art performance ( 91.3 % ) across 8 languages .	For situations where no universal tagset mapping is available , we propose an alternate method , resulting in state-of-the-art 85.6 % accuracy on the resource-poor language Malagasy .	CL_D14-1096_mn_3	CL_D14-1096_mn_5	CL_D14-1096_mn_3_5
CL	D14-1096	mn	4	5	conclusion	result_means	support	support	secondary	secondary	none	none	Our approach is based on modest data requirements , and uses minimum divergence classification .	For situations where no universal tagset mapping is available , we propose an alternate method , resulting in state-of-the-art 85.6 % accuracy on the resource-poor language Malagasy .	CL_D14-1096_mn_4	CL_D14-1096_mn_5	CL_D14-1096_mn_4_5
CL	D14-1097	mn	1	2	information_additional	motivation_background	info-required	support	secondary	secondary	forw	info-required	Active learning ( AL ) consists of asking human annotators to annotate automatically selected data that are assumed to bring the most benefit in the creation of a classifier .	AL allows to learn accurate systems with much less annotated data than what is required by pure supervised learning algorithms , hence limiting the tedious effort of annotating a large collection of data .	CL_D14-1097_mn_1	CL_D14-1097_mn_2	CL_D14-1097_mn_1_2
CL	D14-1097	mn	2	1	motivation_background	information_additional	support	info-required	secondary	secondary	back	info-required	AL allows to learn accurate systems with much less annotated data than what is required by pure supervised learning algorithms , hence limiting the tedious effort of annotating a large collection of data .	Active learning ( AL ) consists of asking human annotators to annotate automatically selected data that are assumed to bring the most benefit in the creation of a classifier .	CL_D14-1097_mn_2	CL_D14-1097_mn_1	CL_D14-1097_mn_1_2
CL	D14-1097	mn	1	3	information_additional	proposal	info-required	none	secondary	main	none	none	Active learning ( AL ) consists of asking human annotators to annotate automatically selected data that are assumed to bring the most benefit in the creation of a classifier .	We experimentally investigate the behavior of several AL strategies for sequence labeling tasks ( in a partially-labeled scenario ) tailored on Partially-Labeled Conditional Random Fields , on four sequence labeling tasks : phrase chunking , part-of-speech tagging , named-entity recognition , and bio-entity recognition .	CL_D14-1097_mn_1	CL_D14-1097_mn_3	CL_D14-1097_mn_1_3
CL	D14-1097	mn	2	3	motivation_background	proposal	support	none	secondary	main	forw	support	AL allows to learn accurate systems with much less annotated data than what is required by pure supervised learning algorithms , hence limiting the tedious effort of annotating a large collection of data .	We experimentally investigate the behavior of several AL strategies for sequence labeling tasks ( in a partially-labeled scenario ) tailored on Partially-Labeled Conditional Random Fields , on four sequence labeling tasks : phrase chunking , part-of-speech tagging , named-entity recognition , and bio-entity recognition .	CL_D14-1097_mn_2	CL_D14-1097_mn_3	CL_D14-1097_mn_2_3
CL	D14-1097	mn	3	2	proposal	motivation_background	none	support	main	secondary	back	support	We experimentally investigate the behavior of several AL strategies for sequence labeling tasks ( in a partially-labeled scenario ) tailored on Partially-Labeled Conditional Random Fields , on four sequence labeling tasks : phrase chunking , part-of-speech tagging , named-entity recognition , and bio-entity recognition .	AL allows to learn accurate systems with much less annotated data than what is required by pure supervised learning algorithms , hence limiting the tedious effort of annotating a large collection of data .	CL_D14-1097_mn_3	CL_D14-1097_mn_2	CL_D14-1097_mn_2_3
CL	D14-1098	mn	1	2	proposal	information_additional	none	info-required	main	secondary	none	none	In this paper , we propose novel structured language modeling methods for code mixing speech recognition by incorporating a well-known syntactic constraint for switching code , namely the Functional Head Constraint ( FHC ) .	Code mixing data is not abundantly available for training language models .	CL_D14-1098_mn_1	CL_D14-1098_mn_2	CL_D14-1098_mn_1_2
CL	D14-1098	mn	1	3	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	In this paper , we propose novel structured language modeling methods for code mixing speech recognition by incorporating a well-known syntactic constraint for switching code , namely the Functional Head Constraint ( FHC ) .	Our proposed methods successfully alleviate this core problem for code mixing speech recognition by using bilingual data to train a structured language model with syntactic constraint .	CL_D14-1098_mn_1	CL_D14-1098_mn_3	CL_D14-1098_mn_1_3
CL	D14-1098	mn	3	1	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	Our proposed methods successfully alleviate this core problem for code mixing speech recognition by using bilingual data to train a structured language model with syntactic constraint .	In this paper , we propose novel structured language modeling methods for code mixing speech recognition by incorporating a well-known syntactic constraint for switching code , namely the Functional Head Constraint ( FHC ) .	CL_D14-1098_mn_3	CL_D14-1098_mn_1	CL_D14-1098_mn_1_3
CL	D14-1098	mn	1	4	proposal	motivation_problem	none	support	main	secondary	none	none	In this paper , we propose novel structured language modeling methods for code mixing speech recognition by incorporating a well-known syntactic constraint for switching code , namely the Functional Head Constraint ( FHC ) .	Linguists and bilingual speakers found that code switch do not happen between the functional head and its complements .	CL_D14-1098_mn_1	CL_D14-1098_mn_4	CL_D14-1098_mn_1_4
CL	D14-1098	mn	1	5	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	In this paper , we propose novel structured language modeling methods for code mixing speech recognition by incorporating a well-known syntactic constraint for switching code , namely the Functional Head Constraint ( FHC ) .	We propose to learn the code mixing language model from bilingual data with this constraint in a weighted finite state transducer ( WFST ) framework .	CL_D14-1098_mn_1	CL_D14-1098_mn_5	CL_D14-1098_mn_1_5
CL	D14-1098	mn	1	6	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	In this paper , we propose novel structured language modeling methods for code mixing speech recognition by incorporating a well-known syntactic constraint for switching code , namely the Functional Head Constraint ( FHC ) .	The constrained code switch language model is obtained by first expanding the search network with a translation model , and then using parsing to restrict paths to those permissible under the constraint .	CL_D14-1098_mn_1	CL_D14-1098_mn_6	CL_D14-1098_mn_1_6
CL	D14-1098	mn	1	7	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	In this paper , we propose novel structured language modeling methods for code mixing speech recognition by incorporating a well-known syntactic constraint for switching code , namely the Functional Head Constraint ( FHC ) .	We implement and compare two approaches - lattice parsing enables a sequential coupling whereas partial parsing enables a tight coupling between parsing and filtering .	CL_D14-1098_mn_1	CL_D14-1098_mn_7	CL_D14-1098_mn_1_7
CL	D14-1098	mn	1	8	proposal	means	none	by-means	main	secondary	none	none	In this paper , we propose novel structured language modeling methods for code mixing speech recognition by incorporating a well-known syntactic constraint for switching code , namely the Functional Head Constraint ( FHC ) .	We tested our system on a lecture speech dataset with 16 % embedded second language , and on a lunch conversation dataset with 20 % embedded language .	CL_D14-1098_mn_1	CL_D14-1098_mn_8	CL_D14-1098_mn_1_8
CL	D14-1098	mn	1	9	proposal	observation	none	support	main	secondary	none	none	In this paper , we propose novel structured language modeling methods for code mixing speech recognition by incorporating a well-known syntactic constraint for switching code , namely the Functional Head Constraint ( FHC ) .	Our language models with lattice parsing and partial parsing reduce word error rates from a baseline mixed language model by 3.8 % and 3.9 % in terms of word error rate relatively on the average on the first and second tasks respectively .	CL_D14-1098_mn_1	CL_D14-1098_mn_9	CL_D14-1098_mn_1_9
CL	D14-1098	mn	1	10	proposal	observation	none	support	main	secondary	none	none	In this paper , we propose novel structured language modeling methods for code mixing speech recognition by incorporating a well-known syntactic constraint for switching code , namely the Functional Head Constraint ( FHC ) .	It outperforms the interpolated language model by 3.7 % and 5.6 % in terms of word error rate relatively , and outperforms the adapted language model by 2.6 % and 4.6 % relatively .	CL_D14-1098_mn_1	CL_D14-1098_mn_10	CL_D14-1098_mn_1_10
CL	D14-1098	mn	1	11	proposal	result	none	support	main	secondary	back	support	In this paper , we propose novel structured language modeling methods for code mixing speech recognition by incorporating a well-known syntactic constraint for switching code , namely the Functional Head Constraint ( FHC ) .	Our proposed approach avoids making early decisions on code-switch boundaries and is therefore more robust .	CL_D14-1098_mn_1	CL_D14-1098_mn_11	CL_D14-1098_mn_1_11
CL	D14-1098	mn	11	1	result	proposal	support	none	secondary	main	forw	support	Our proposed approach avoids making early decisions on code-switch boundaries and is therefore more robust .	In this paper , we propose novel structured language modeling methods for code mixing speech recognition by incorporating a well-known syntactic constraint for switching code , namely the Functional Head Constraint ( FHC ) .	CL_D14-1098_mn_11	CL_D14-1098_mn_1	CL_D14-1098_mn_1_11
CL	D14-1098	mn	1	12	proposal	conclusion	none	support	main	secondary	back	support	In this paper , we propose novel structured language modeling methods for code mixing speech recognition by incorporating a well-known syntactic constraint for switching code , namely the Functional Head Constraint ( FHC ) .	We address the code switch data scarcity challenge by using bilingual data with syntactic structure .	CL_D14-1098_mn_1	CL_D14-1098_mn_12	CL_D14-1098_mn_1_12
CL	D14-1098	mn	12	1	conclusion	proposal	support	none	secondary	main	forw	support	We address the code switch data scarcity challenge by using bilingual data with syntactic structure .	In this paper , we propose novel structured language modeling methods for code mixing speech recognition by incorporating a well-known syntactic constraint for switching code , namely the Functional Head Constraint ( FHC ) .	CL_D14-1098_mn_12	CL_D14-1098_mn_1	CL_D14-1098_mn_1_12
CL	D14-1098	mn	2	3	information_additional	proposal_implementation	info-required	elaboration	secondary	secondary	forw	info-required	Code mixing data is not abundantly available for training language models .	Our proposed methods successfully alleviate this core problem for code mixing speech recognition by using bilingual data to train a structured language model with syntactic constraint .	CL_D14-1098_mn_2	CL_D14-1098_mn_3	CL_D14-1098_mn_2_3
CL	D14-1098	mn	3	2	proposal_implementation	information_additional	elaboration	info-required	secondary	secondary	back	info-required	Our proposed methods successfully alleviate this core problem for code mixing speech recognition by using bilingual data to train a structured language model with syntactic constraint .	Code mixing data is not abundantly available for training language models .	CL_D14-1098_mn_3	CL_D14-1098_mn_2	CL_D14-1098_mn_2_3
CL	D14-1098	mn	2	4	information_additional	motivation_problem	info-required	support	secondary	secondary	none	none	Code mixing data is not abundantly available for training language models .	Linguists and bilingual speakers found that code switch do not happen between the functional head and its complements .	CL_D14-1098_mn_2	CL_D14-1098_mn_4	CL_D14-1098_mn_2_4
CL	D14-1098	mn	2	5	information_additional	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	Code mixing data is not abundantly available for training language models .	We propose to learn the code mixing language model from bilingual data with this constraint in a weighted finite state transducer ( WFST ) framework .	CL_D14-1098_mn_2	CL_D14-1098_mn_5	CL_D14-1098_mn_2_5
CL	D14-1098	mn	2	6	information_additional	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	Code mixing data is not abundantly available for training language models .	The constrained code switch language model is obtained by first expanding the search network with a translation model , and then using parsing to restrict paths to those permissible under the constraint .	CL_D14-1098_mn_2	CL_D14-1098_mn_6	CL_D14-1098_mn_2_6
CL	D14-1098	mn	2	7	information_additional	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	Code mixing data is not abundantly available for training language models .	We implement and compare two approaches - lattice parsing enables a sequential coupling whereas partial parsing enables a tight coupling between parsing and filtering .	CL_D14-1098_mn_2	CL_D14-1098_mn_7	CL_D14-1098_mn_2_7
CL	D14-1098	mn	2	8	information_additional	means	info-required	by-means	secondary	secondary	none	none	Code mixing data is not abundantly available for training language models .	We tested our system on a lecture speech dataset with 16 % embedded second language , and on a lunch conversation dataset with 20 % embedded language .	CL_D14-1098_mn_2	CL_D14-1098_mn_8	CL_D14-1098_mn_2_8
CL	D14-1098	mn	2	9	information_additional	observation	info-required	support	secondary	secondary	none	none	Code mixing data is not abundantly available for training language models .	Our language models with lattice parsing and partial parsing reduce word error rates from a baseline mixed language model by 3.8 % and 3.9 % in terms of word error rate relatively on the average on the first and second tasks respectively .	CL_D14-1098_mn_2	CL_D14-1098_mn_9	CL_D14-1098_mn_2_9
CL	D14-1098	mn	2	10	information_additional	observation	info-required	support	secondary	secondary	none	none	Code mixing data is not abundantly available for training language models .	It outperforms the interpolated language model by 3.7 % and 5.6 % in terms of word error rate relatively , and outperforms the adapted language model by 2.6 % and 4.6 % relatively .	CL_D14-1098_mn_2	CL_D14-1098_mn_10	CL_D14-1098_mn_2_10
CL	D14-1098	mn	2	11	information_additional	result	info-required	support	secondary	secondary	none	none	Code mixing data is not abundantly available for training language models .	Our proposed approach avoids making early decisions on code-switch boundaries and is therefore more robust .	CL_D14-1098_mn_2	CL_D14-1098_mn_11	CL_D14-1098_mn_2_11
CL	D14-1098	mn	2	12	information_additional	conclusion	info-required	support	secondary	secondary	none	none	Code mixing data is not abundantly available for training language models .	We address the code switch data scarcity challenge by using bilingual data with syntactic structure .	CL_D14-1098_mn_2	CL_D14-1098_mn_12	CL_D14-1098_mn_2_12
CL	D14-1098	mn	3	4	proposal_implementation	motivation_problem	elaboration	support	secondary	secondary	none	none	Our proposed methods successfully alleviate this core problem for code mixing speech recognition by using bilingual data to train a structured language model with syntactic constraint .	Linguists and bilingual speakers found that code switch do not happen between the functional head and its complements .	CL_D14-1098_mn_3	CL_D14-1098_mn_4	CL_D14-1098_mn_3_4
CL	D14-1098	mn	3	5	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	Our proposed methods successfully alleviate this core problem for code mixing speech recognition by using bilingual data to train a structured language model with syntactic constraint .	We propose to learn the code mixing language model from bilingual data with this constraint in a weighted finite state transducer ( WFST ) framework .	CL_D14-1098_mn_3	CL_D14-1098_mn_5	CL_D14-1098_mn_3_5
CL	D14-1098	mn	5	3	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	forw	elaboration	We propose to learn the code mixing language model from bilingual data with this constraint in a weighted finite state transducer ( WFST ) framework .	Our proposed methods successfully alleviate this core problem for code mixing speech recognition by using bilingual data to train a structured language model with syntactic constraint .	CL_D14-1098_mn_5	CL_D14-1098_mn_3	CL_D14-1098_mn_3_5
CL	D14-1098	mn	3	6	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	none	none	Our proposed methods successfully alleviate this core problem for code mixing speech recognition by using bilingual data to train a structured language model with syntactic constraint .	The constrained code switch language model is obtained by first expanding the search network with a translation model , and then using parsing to restrict paths to those permissible under the constraint .	CL_D14-1098_mn_3	CL_D14-1098_mn_6	CL_D14-1098_mn_3_6
CL	D14-1098	mn	3	7	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	none	none	Our proposed methods successfully alleviate this core problem for code mixing speech recognition by using bilingual data to train a structured language model with syntactic constraint .	We implement and compare two approaches - lattice parsing enables a sequential coupling whereas partial parsing enables a tight coupling between parsing and filtering .	CL_D14-1098_mn_3	CL_D14-1098_mn_7	CL_D14-1098_mn_3_7
CL	D14-1098	mn	3	8	proposal_implementation	means	elaboration	by-means	secondary	secondary	none	none	Our proposed methods successfully alleviate this core problem for code mixing speech recognition by using bilingual data to train a structured language model with syntactic constraint .	We tested our system on a lecture speech dataset with 16 % embedded second language , and on a lunch conversation dataset with 20 % embedded language .	CL_D14-1098_mn_3	CL_D14-1098_mn_8	CL_D14-1098_mn_3_8
CL	D14-1098	mn	3	9	proposal_implementation	observation	elaboration	support	secondary	secondary	none	none	Our proposed methods successfully alleviate this core problem for code mixing speech recognition by using bilingual data to train a structured language model with syntactic constraint .	Our language models with lattice parsing and partial parsing reduce word error rates from a baseline mixed language model by 3.8 % and 3.9 % in terms of word error rate relatively on the average on the first and second tasks respectively .	CL_D14-1098_mn_3	CL_D14-1098_mn_9	CL_D14-1098_mn_3_9
CL	D14-1098	mn	3	10	proposal_implementation	observation	elaboration	support	secondary	secondary	none	none	Our proposed methods successfully alleviate this core problem for code mixing speech recognition by using bilingual data to train a structured language model with syntactic constraint .	It outperforms the interpolated language model by 3.7 % and 5.6 % in terms of word error rate relatively , and outperforms the adapted language model by 2.6 % and 4.6 % relatively .	CL_D14-1098_mn_3	CL_D14-1098_mn_10	CL_D14-1098_mn_3_10
CL	D14-1098	mn	3	11	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	Our proposed methods successfully alleviate this core problem for code mixing speech recognition by using bilingual data to train a structured language model with syntactic constraint .	Our proposed approach avoids making early decisions on code-switch boundaries and is therefore more robust .	CL_D14-1098_mn_3	CL_D14-1098_mn_11	CL_D14-1098_mn_3_11
CL	D14-1098	mn	3	12	proposal_implementation	conclusion	elaboration	support	secondary	secondary	none	none	Our proposed methods successfully alleviate this core problem for code mixing speech recognition by using bilingual data to train a structured language model with syntactic constraint .	We address the code switch data scarcity challenge by using bilingual data with syntactic structure .	CL_D14-1098_mn_3	CL_D14-1098_mn_12	CL_D14-1098_mn_3_12
CL	D14-1098	mn	4	5	motivation_problem	proposal_implementation	support	elaboration	secondary	secondary	forw	support	Linguists and bilingual speakers found that code switch do not happen between the functional head and its complements .	We propose to learn the code mixing language model from bilingual data with this constraint in a weighted finite state transducer ( WFST ) framework .	CL_D14-1098_mn_4	CL_D14-1098_mn_5	CL_D14-1098_mn_4_5
CL	D14-1098	mn	5	4	proposal_implementation	motivation_problem	elaboration	support	secondary	secondary	back	support	We propose to learn the code mixing language model from bilingual data with this constraint in a weighted finite state transducer ( WFST ) framework .	Linguists and bilingual speakers found that code switch do not happen between the functional head and its complements .	CL_D14-1098_mn_5	CL_D14-1098_mn_4	CL_D14-1098_mn_4_5
CL	D14-1098	mn	4	6	motivation_problem	proposal_implementation	support	elaboration	secondary	secondary	none	none	Linguists and bilingual speakers found that code switch do not happen between the functional head and its complements .	The constrained code switch language model is obtained by first expanding the search network with a translation model , and then using parsing to restrict paths to those permissible under the constraint .	CL_D14-1098_mn_4	CL_D14-1098_mn_6	CL_D14-1098_mn_4_6
CL	D14-1098	mn	4	7	motivation_problem	proposal_implementation	support	elaboration	secondary	secondary	none	none	Linguists and bilingual speakers found that code switch do not happen between the functional head and its complements .	We implement and compare two approaches - lattice parsing enables a sequential coupling whereas partial parsing enables a tight coupling between parsing and filtering .	CL_D14-1098_mn_4	CL_D14-1098_mn_7	CL_D14-1098_mn_4_7
CL	D14-1098	mn	4	8	motivation_problem	means	support	by-means	secondary	secondary	none	none	Linguists and bilingual speakers found that code switch do not happen between the functional head and its complements .	We tested our system on a lecture speech dataset with 16 % embedded second language , and on a lunch conversation dataset with 20 % embedded language .	CL_D14-1098_mn_4	CL_D14-1098_mn_8	CL_D14-1098_mn_4_8
CL	D14-1098	mn	4	9	motivation_problem	observation	support	support	secondary	secondary	none	none	Linguists and bilingual speakers found that code switch do not happen between the functional head and its complements .	Our language models with lattice parsing and partial parsing reduce word error rates from a baseline mixed language model by 3.8 % and 3.9 % in terms of word error rate relatively on the average on the first and second tasks respectively .	CL_D14-1098_mn_4	CL_D14-1098_mn_9	CL_D14-1098_mn_4_9
CL	D14-1098	mn	4	10	motivation_problem	observation	support	support	secondary	secondary	none	none	Linguists and bilingual speakers found that code switch do not happen between the functional head and its complements .	It outperforms the interpolated language model by 3.7 % and 5.6 % in terms of word error rate relatively , and outperforms the adapted language model by 2.6 % and 4.6 % relatively .	CL_D14-1098_mn_4	CL_D14-1098_mn_10	CL_D14-1098_mn_4_10
CL	D14-1098	mn	4	11	motivation_problem	result	support	support	secondary	secondary	none	none	Linguists and bilingual speakers found that code switch do not happen between the functional head and its complements .	Our proposed approach avoids making early decisions on code-switch boundaries and is therefore more robust .	CL_D14-1098_mn_4	CL_D14-1098_mn_11	CL_D14-1098_mn_4_11
CL	D14-1098	mn	4	12	motivation_problem	conclusion	support	support	secondary	secondary	none	none	Linguists and bilingual speakers found that code switch do not happen between the functional head and its complements .	We address the code switch data scarcity challenge by using bilingual data with syntactic structure .	CL_D14-1098_mn_4	CL_D14-1098_mn_12	CL_D14-1098_mn_4_12
CL	D14-1098	mn	5	6	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	We propose to learn the code mixing language model from bilingual data with this constraint in a weighted finite state transducer ( WFST ) framework .	The constrained code switch language model is obtained by first expanding the search network with a translation model , and then using parsing to restrict paths to those permissible under the constraint .	CL_D14-1098_mn_5	CL_D14-1098_mn_6	CL_D14-1098_mn_5_6
CL	D14-1098	mn	6	5	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	forw	elaboration	The constrained code switch language model is obtained by first expanding the search network with a translation model , and then using parsing to restrict paths to those permissible under the constraint .	We propose to learn the code mixing language model from bilingual data with this constraint in a weighted finite state transducer ( WFST ) framework .	CL_D14-1098_mn_6	CL_D14-1098_mn_5	CL_D14-1098_mn_5_6
CL	D14-1098	mn	5	7	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	none	none	We propose to learn the code mixing language model from bilingual data with this constraint in a weighted finite state transducer ( WFST ) framework .	We implement and compare two approaches - lattice parsing enables a sequential coupling whereas partial parsing enables a tight coupling between parsing and filtering .	CL_D14-1098_mn_5	CL_D14-1098_mn_7	CL_D14-1098_mn_5_7
CL	D14-1098	mn	5	8	proposal_implementation	means	elaboration	by-means	secondary	secondary	none	none	We propose to learn the code mixing language model from bilingual data with this constraint in a weighted finite state transducer ( WFST ) framework .	We tested our system on a lecture speech dataset with 16 % embedded second language , and on a lunch conversation dataset with 20 % embedded language .	CL_D14-1098_mn_5	CL_D14-1098_mn_8	CL_D14-1098_mn_5_8
CL	D14-1098	mn	5	9	proposal_implementation	observation	elaboration	support	secondary	secondary	none	none	We propose to learn the code mixing language model from bilingual data with this constraint in a weighted finite state transducer ( WFST ) framework .	Our language models with lattice parsing and partial parsing reduce word error rates from a baseline mixed language model by 3.8 % and 3.9 % in terms of word error rate relatively on the average on the first and second tasks respectively .	CL_D14-1098_mn_5	CL_D14-1098_mn_9	CL_D14-1098_mn_5_9
CL	D14-1098	mn	5	10	proposal_implementation	observation	elaboration	support	secondary	secondary	none	none	We propose to learn the code mixing language model from bilingual data with this constraint in a weighted finite state transducer ( WFST ) framework .	It outperforms the interpolated language model by 3.7 % and 5.6 % in terms of word error rate relatively , and outperforms the adapted language model by 2.6 % and 4.6 % relatively .	CL_D14-1098_mn_5	CL_D14-1098_mn_10	CL_D14-1098_mn_5_10
CL	D14-1098	mn	5	11	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	We propose to learn the code mixing language model from bilingual data with this constraint in a weighted finite state transducer ( WFST ) framework .	Our proposed approach avoids making early decisions on code-switch boundaries and is therefore more robust .	CL_D14-1098_mn_5	CL_D14-1098_mn_11	CL_D14-1098_mn_5_11
CL	D14-1098	mn	5	12	proposal_implementation	conclusion	elaboration	support	secondary	secondary	none	none	We propose to learn the code mixing language model from bilingual data with this constraint in a weighted finite state transducer ( WFST ) framework .	We address the code switch data scarcity challenge by using bilingual data with syntactic structure .	CL_D14-1098_mn_5	CL_D14-1098_mn_12	CL_D14-1098_mn_5_12
CL	D14-1098	mn	6	7	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	The constrained code switch language model is obtained by first expanding the search network with a translation model , and then using parsing to restrict paths to those permissible under the constraint .	We implement and compare two approaches - lattice parsing enables a sequential coupling whereas partial parsing enables a tight coupling between parsing and filtering .	CL_D14-1098_mn_6	CL_D14-1098_mn_7	CL_D14-1098_mn_6_7
CL	D14-1098	mn	7	6	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	forw	elaboration	We implement and compare two approaches - lattice parsing enables a sequential coupling whereas partial parsing enables a tight coupling between parsing and filtering .	The constrained code switch language model is obtained by first expanding the search network with a translation model , and then using parsing to restrict paths to those permissible under the constraint .	CL_D14-1098_mn_7	CL_D14-1098_mn_6	CL_D14-1098_mn_6_7
CL	D14-1098	mn	6	8	proposal_implementation	means	elaboration	by-means	secondary	secondary	none	none	The constrained code switch language model is obtained by first expanding the search network with a translation model , and then using parsing to restrict paths to those permissible under the constraint .	We tested our system on a lecture speech dataset with 16 % embedded second language , and on a lunch conversation dataset with 20 % embedded language .	CL_D14-1098_mn_6	CL_D14-1098_mn_8	CL_D14-1098_mn_6_8
CL	D14-1098	mn	6	9	proposal_implementation	observation	elaboration	support	secondary	secondary	none	none	The constrained code switch language model is obtained by first expanding the search network with a translation model , and then using parsing to restrict paths to those permissible under the constraint .	Our language models with lattice parsing and partial parsing reduce word error rates from a baseline mixed language model by 3.8 % and 3.9 % in terms of word error rate relatively on the average on the first and second tasks respectively .	CL_D14-1098_mn_6	CL_D14-1098_mn_9	CL_D14-1098_mn_6_9
CL	D14-1098	mn	6	10	proposal_implementation	observation	elaboration	support	secondary	secondary	none	none	The constrained code switch language model is obtained by first expanding the search network with a translation model , and then using parsing to restrict paths to those permissible under the constraint .	It outperforms the interpolated language model by 3.7 % and 5.6 % in terms of word error rate relatively , and outperforms the adapted language model by 2.6 % and 4.6 % relatively .	CL_D14-1098_mn_6	CL_D14-1098_mn_10	CL_D14-1098_mn_6_10
CL	D14-1098	mn	6	11	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	The constrained code switch language model is obtained by first expanding the search network with a translation model , and then using parsing to restrict paths to those permissible under the constraint .	Our proposed approach avoids making early decisions on code-switch boundaries and is therefore more robust .	CL_D14-1098_mn_6	CL_D14-1098_mn_11	CL_D14-1098_mn_6_11
CL	D14-1098	mn	6	12	proposal_implementation	conclusion	elaboration	support	secondary	secondary	none	none	The constrained code switch language model is obtained by first expanding the search network with a translation model , and then using parsing to restrict paths to those permissible under the constraint .	We address the code switch data scarcity challenge by using bilingual data with syntactic structure .	CL_D14-1098_mn_6	CL_D14-1098_mn_12	CL_D14-1098_mn_6_12
CL	D14-1098	mn	7	8	proposal_implementation	means	elaboration	by-means	secondary	secondary	none	none	We implement and compare two approaches - lattice parsing enables a sequential coupling whereas partial parsing enables a tight coupling between parsing and filtering .	We tested our system on a lecture speech dataset with 16 % embedded second language , and on a lunch conversation dataset with 20 % embedded language .	CL_D14-1098_mn_7	CL_D14-1098_mn_8	CL_D14-1098_mn_7_8
CL	D14-1098	mn	7	9	proposal_implementation	observation	elaboration	support	secondary	secondary	none	none	We implement and compare two approaches - lattice parsing enables a sequential coupling whereas partial parsing enables a tight coupling between parsing and filtering .	Our language models with lattice parsing and partial parsing reduce word error rates from a baseline mixed language model by 3.8 % and 3.9 % in terms of word error rate relatively on the average on the first and second tasks respectively .	CL_D14-1098_mn_7	CL_D14-1098_mn_9	CL_D14-1098_mn_7_9
CL	D14-1098	mn	7	10	proposal_implementation	observation	elaboration	support	secondary	secondary	none	none	We implement and compare two approaches - lattice parsing enables a sequential coupling whereas partial parsing enables a tight coupling between parsing and filtering .	It outperforms the interpolated language model by 3.7 % and 5.6 % in terms of word error rate relatively , and outperforms the adapted language model by 2.6 % and 4.6 % relatively .	CL_D14-1098_mn_7	CL_D14-1098_mn_10	CL_D14-1098_mn_7_10
CL	D14-1098	mn	7	11	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	We implement and compare two approaches - lattice parsing enables a sequential coupling whereas partial parsing enables a tight coupling between parsing and filtering .	Our proposed approach avoids making early decisions on code-switch boundaries and is therefore more robust .	CL_D14-1098_mn_7	CL_D14-1098_mn_11	CL_D14-1098_mn_7_11
CL	D14-1098	mn	7	12	proposal_implementation	conclusion	elaboration	support	secondary	secondary	none	none	We implement and compare two approaches - lattice parsing enables a sequential coupling whereas partial parsing enables a tight coupling between parsing and filtering .	We address the code switch data scarcity challenge by using bilingual data with syntactic structure .	CL_D14-1098_mn_7	CL_D14-1098_mn_12	CL_D14-1098_mn_7_12
CL	D14-1098	mn	8	9	means	observation	by-means	support	secondary	secondary	none	none	We tested our system on a lecture speech dataset with 16 % embedded second language , and on a lunch conversation dataset with 20 % embedded language .	Our language models with lattice parsing and partial parsing reduce word error rates from a baseline mixed language model by 3.8 % and 3.9 % in terms of word error rate relatively on the average on the first and second tasks respectively .	CL_D14-1098_mn_8	CL_D14-1098_mn_9	CL_D14-1098_mn_8_9
CL	D14-1098	mn	8	10	means	observation	by-means	support	secondary	secondary	none	none	We tested our system on a lecture speech dataset with 16 % embedded second language , and on a lunch conversation dataset with 20 % embedded language .	It outperforms the interpolated language model by 3.7 % and 5.6 % in terms of word error rate relatively , and outperforms the adapted language model by 2.6 % and 4.6 % relatively .	CL_D14-1098_mn_8	CL_D14-1098_mn_10	CL_D14-1098_mn_8_10
CL	D14-1098	mn	8	11	means	result	by-means	support	secondary	secondary	forw	by-means	We tested our system on a lecture speech dataset with 16 % embedded second language , and on a lunch conversation dataset with 20 % embedded language .	Our proposed approach avoids making early decisions on code-switch boundaries and is therefore more robust .	CL_D14-1098_mn_8	CL_D14-1098_mn_11	CL_D14-1098_mn_8_11
CL	D14-1098	mn	11	8	result	means	support	by-means	secondary	secondary	back	by-means	Our proposed approach avoids making early decisions on code-switch boundaries and is therefore more robust .	We tested our system on a lecture speech dataset with 16 % embedded second language , and on a lunch conversation dataset with 20 % embedded language .	CL_D14-1098_mn_11	CL_D14-1098_mn_8	CL_D14-1098_mn_8_11
CL	D14-1098	mn	8	12	means	conclusion	by-means	support	secondary	secondary	none	none	We tested our system on a lecture speech dataset with 16 % embedded second language , and on a lunch conversation dataset with 20 % embedded language .	We address the code switch data scarcity challenge by using bilingual data with syntactic structure .	CL_D14-1098_mn_8	CL_D14-1098_mn_12	CL_D14-1098_mn_8_12
CL	D14-1098	mn	9	10	observation	observation	support	support	secondary	secondary	none	none	Our language models with lattice parsing and partial parsing reduce word error rates from a baseline mixed language model by 3.8 % and 3.9 % in terms of word error rate relatively on the average on the first and second tasks respectively .	It outperforms the interpolated language model by 3.7 % and 5.6 % in terms of word error rate relatively , and outperforms the adapted language model by 2.6 % and 4.6 % relatively .	CL_D14-1098_mn_9	CL_D14-1098_mn_10	CL_D14-1098_mn_9_10
CL	D14-1098	mn	9	11	observation	result	support	support	secondary	secondary	forw	support	Our language models with lattice parsing and partial parsing reduce word error rates from a baseline mixed language model by 3.8 % and 3.9 % in terms of word error rate relatively on the average on the first and second tasks respectively .	Our proposed approach avoids making early decisions on code-switch boundaries and is therefore more robust .	CL_D14-1098_mn_9	CL_D14-1098_mn_11	CL_D14-1098_mn_9_11
CL	D14-1098	mn	11	9	result	observation	support	support	secondary	secondary	back	support	Our proposed approach avoids making early decisions on code-switch boundaries and is therefore more robust .	Our language models with lattice parsing and partial parsing reduce word error rates from a baseline mixed language model by 3.8 % and 3.9 % in terms of word error rate relatively on the average on the first and second tasks respectively .	CL_D14-1098_mn_11	CL_D14-1098_mn_9	CL_D14-1098_mn_9_11
CL	D14-1098	mn	9	12	observation	conclusion	support	support	secondary	secondary	none	none	Our language models with lattice parsing and partial parsing reduce word error rates from a baseline mixed language model by 3.8 % and 3.9 % in terms of word error rate relatively on the average on the first and second tasks respectively .	We address the code switch data scarcity challenge by using bilingual data with syntactic structure .	CL_D14-1098_mn_9	CL_D14-1098_mn_12	CL_D14-1098_mn_9_12
CL	D14-1098	mn	10	11	observation	result	support	support	secondary	secondary	forw	support	It outperforms the interpolated language model by 3.7 % and 5.6 % in terms of word error rate relatively , and outperforms the adapted language model by 2.6 % and 4.6 % relatively .	Our proposed approach avoids making early decisions on code-switch boundaries and is therefore more robust .	CL_D14-1098_mn_10	CL_D14-1098_mn_11	CL_D14-1098_mn_10_11
CL	D14-1098	mn	11	10	result	observation	support	support	secondary	secondary	back	support	Our proposed approach avoids making early decisions on code-switch boundaries and is therefore more robust .	It outperforms the interpolated language model by 3.7 % and 5.6 % in terms of word error rate relatively , and outperforms the adapted language model by 2.6 % and 4.6 % relatively .	CL_D14-1098_mn_11	CL_D14-1098_mn_10	CL_D14-1098_mn_10_11
CL	D14-1098	mn	10	12	observation	conclusion	support	support	secondary	secondary	none	none	It outperforms the interpolated language model by 3.7 % and 5.6 % in terms of word error rate relatively , and outperforms the adapted language model by 2.6 % and 4.6 % relatively .	We address the code switch data scarcity challenge by using bilingual data with syntactic structure .	CL_D14-1098_mn_10	CL_D14-1098_mn_12	CL_D14-1098_mn_10_12
CL	D14-1098	mn	11	12	result	conclusion	support	support	secondary	secondary	none	none	Our proposed approach avoids making early decisions on code-switch boundaries and is therefore more robust .	We address the code switch data scarcity challenge by using bilingual data with syntactic structure .	CL_D14-1098_mn_11	CL_D14-1098_mn_12	CL_D14-1098_mn_11_12
CL	D14-1099	mn	1	2	motivation_background	motivation_problem	info-required	support	secondary	secondary	forw	info-required	The introduction of dynamic oracles has considerably improved the accuracy of greedy transition-based dependency parsers , without sacrificing parsing efficiency .	However , this enhancement is limited to projective parsing , and dynamic oracles have not yet been implemented for parsers supporting non-projectivity .	CL_D14-1099_mn_1	CL_D14-1099_mn_2	CL_D14-1099_mn_1_2
CL	D14-1099	mn	2	1	motivation_problem	motivation_background	support	info-required	secondary	secondary	back	info-required	However , this enhancement is limited to projective parsing , and dynamic oracles have not yet been implemented for parsers supporting non-projectivity .	The introduction of dynamic oracles has considerably improved the accuracy of greedy transition-based dependency parsers , without sacrificing parsing efficiency .	CL_D14-1099_mn_2	CL_D14-1099_mn_1	CL_D14-1099_mn_1_2
CL	D14-1099	mn	1	3	motivation_background	proposal	info-required	none	secondary	main	none	none	The introduction of dynamic oracles has considerably improved the accuracy of greedy transition-based dependency parsers , without sacrificing parsing efficiency .	In this paper we introduce the first such oracle , for a non-projective parser based on Attardi's parser .	CL_D14-1099_mn_1	CL_D14-1099_mn_3	CL_D14-1099_mn_1_3
CL	D14-1099	mn	1	4	motivation_background	conclusion	info-required	support	secondary	secondary	none	none	The introduction of dynamic oracles has considerably improved the accuracy of greedy transition-based dependency parsers , without sacrificing parsing efficiency .	We show that training with this oracle improves parsing accuracy over a conventional ( static ) oracle on a wide range of datasets .	CL_D14-1099_mn_1	CL_D14-1099_mn_4	CL_D14-1099_mn_1_4
CL	D14-1099	mn	2	3	motivation_problem	proposal	support	none	secondary	main	forw	support	However , this enhancement is limited to projective parsing , and dynamic oracles have not yet been implemented for parsers supporting non-projectivity .	In this paper we introduce the first such oracle , for a non-projective parser based on Attardi's parser .	CL_D14-1099_mn_2	CL_D14-1099_mn_3	CL_D14-1099_mn_2_3
CL	D14-1099	mn	3	2	proposal	motivation_problem	none	support	main	secondary	back	support	In this paper we introduce the first such oracle , for a non-projective parser based on Attardi's parser .	However , this enhancement is limited to projective parsing , and dynamic oracles have not yet been implemented for parsers supporting non-projectivity .	CL_D14-1099_mn_3	CL_D14-1099_mn_2	CL_D14-1099_mn_2_3
CL	D14-1099	mn	2	4	motivation_problem	conclusion	support	support	secondary	secondary	none	none	However , this enhancement is limited to projective parsing , and dynamic oracles have not yet been implemented for parsers supporting non-projectivity .	We show that training with this oracle improves parsing accuracy over a conventional ( static ) oracle on a wide range of datasets .	CL_D14-1099_mn_2	CL_D14-1099_mn_4	CL_D14-1099_mn_2_4
CL	D14-1099	mn	3	4	proposal	conclusion	none	support	main	secondary	back	support	In this paper we introduce the first such oracle , for a non-projective parser based on Attardi's parser .	We show that training with this oracle improves parsing accuracy over a conventional ( static ) oracle on a wide range of datasets .	CL_D14-1099_mn_3	CL_D14-1099_mn_4	CL_D14-1099_mn_3_4
CL	D14-1099	mn	4	3	conclusion	proposal	support	none	secondary	main	forw	support	We show that training with this oracle improves parsing accuracy over a conventional ( static ) oracle on a wide range of datasets .	In this paper we introduce the first such oracle , for a non-projective parser based on Attardi's parser .	CL_D14-1099_mn_4	CL_D14-1099_mn_3	CL_D14-1099_mn_3_4
CL	D14-1100	mn	1	2	motivation_problem	proposal	support	none	secondary	main	forw	support	The syntactic ambiguity of a transitive verb ( Vt ) followed by a noun ( N ) has long been a problem in Chinese parsing .	In this paper , we propose a classifier to resolve the ambiguity of Vt-N structures .	CL_D14-1100_mn_1	CL_D14-1100_mn_2	CL_D14-1100_mn_1_2
CL	D14-1100	mn	2	1	proposal	motivation_problem	none	support	main	secondary	back	support	In this paper , we propose a classifier to resolve the ambiguity of Vt-N structures .	The syntactic ambiguity of a transitive verb ( Vt ) followed by a noun ( N ) has long been a problem in Chinese parsing .	CL_D14-1100_mn_2	CL_D14-1100_mn_1	CL_D14-1100_mn_1_2
CL	D14-1100	mn	1	3	motivation_problem	proposal_implementation	support	elaboration	secondary	secondary	none	none	The syntactic ambiguity of a transitive verb ( Vt ) followed by a noun ( N ) has long been a problem in Chinese parsing .	The design of the classifier is based on three important guidelines , namely , adopting linguistically motivated features , using all available resources , and easy integration into a parsing model .	CL_D14-1100_mn_1	CL_D14-1100_mn_3	CL_D14-1100_mn_1_3
CL	D14-1100	mn	1	4	motivation_problem	proposal_implementation	support	elaboration	secondary	secondary	none	none	The syntactic ambiguity of a transitive verb ( Vt ) followed by a noun ( N ) has long been a problem in Chinese parsing .	The linguistically motivated features include semantic relations , context , and morphological structures ; and the available resources are treebank , thesaurus , affix database , and large corpora .	CL_D14-1100_mn_1	CL_D14-1100_mn_4	CL_D14-1100_mn_1_4
CL	D14-1100	mn	1	5	motivation_problem	proposal_implementation	support	elaboration	secondary	secondary	none	none	The syntactic ambiguity of a transitive verb ( Vt ) followed by a noun ( N ) has long been a problem in Chinese parsing .	We also propose two learning approaches that resolve the problem of data sparseness by auto-parsing and extracting relative knowledge from large-scale unlabeled data .	CL_D14-1100_mn_1	CL_D14-1100_mn_5	CL_D14-1100_mn_1_5
CL	D14-1100	mn	1	6	motivation_problem	result	support	support	secondary	secondary	none	none	The syntactic ambiguity of a transitive verb ( Vt ) followed by a noun ( N ) has long been a problem in Chinese parsing .	Our experiment results show that the Vt-N classifier outperforms the current PCFG parser .	CL_D14-1100_mn_1	CL_D14-1100_mn_6	CL_D14-1100_mn_1_6
CL	D14-1100	mn	1	7	motivation_problem	result	support	elaboration	secondary	secondary	none	none	The syntactic ambiguity of a transitive verb ( Vt ) followed by a noun ( N ) has long been a problem in Chinese parsing .	Furthermore , it can be easily and effectively integrated into the PCFG parser and general statistical parsing models .	CL_D14-1100_mn_1	CL_D14-1100_mn_7	CL_D14-1100_mn_1_7
CL	D14-1100	mn	1	8	motivation_problem	result	support	support	secondary	secondary	none	none	The syntactic ambiguity of a transitive verb ( Vt ) followed by a noun ( N ) has long been a problem in Chinese parsing .	Evaluation of the learning approaches indicates that world knowledge facilitates Vt-N disambiguation through data selection and error correction .	CL_D14-1100_mn_1	CL_D14-1100_mn_8	CL_D14-1100_mn_1_8
CL	D14-1100	mn	2	3	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	In this paper , we propose a classifier to resolve the ambiguity of Vt-N structures .	The design of the classifier is based on three important guidelines , namely , adopting linguistically motivated features , using all available resources , and easy integration into a parsing model .	CL_D14-1100_mn_2	CL_D14-1100_mn_3	CL_D14-1100_mn_2_3
CL	D14-1100	mn	3	2	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	The design of the classifier is based on three important guidelines , namely , adopting linguistically motivated features , using all available resources , and easy integration into a parsing model .	In this paper , we propose a classifier to resolve the ambiguity of Vt-N structures .	CL_D14-1100_mn_3	CL_D14-1100_mn_2	CL_D14-1100_mn_2_3
CL	D14-1100	mn	2	4	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	In this paper , we propose a classifier to resolve the ambiguity of Vt-N structures .	The linguistically motivated features include semantic relations , context , and morphological structures ; and the available resources are treebank , thesaurus , affix database , and large corpora .	CL_D14-1100_mn_2	CL_D14-1100_mn_4	CL_D14-1100_mn_2_4
CL	D14-1100	mn	2	5	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	In this paper , we propose a classifier to resolve the ambiguity of Vt-N structures .	We also propose two learning approaches that resolve the problem of data sparseness by auto-parsing and extracting relative knowledge from large-scale unlabeled data .	CL_D14-1100_mn_2	CL_D14-1100_mn_5	CL_D14-1100_mn_2_5
CL	D14-1100	mn	2	6	proposal	result	none	support	main	secondary	back	support	In this paper , we propose a classifier to resolve the ambiguity of Vt-N structures .	Our experiment results show that the Vt-N classifier outperforms the current PCFG parser .	CL_D14-1100_mn_2	CL_D14-1100_mn_6	CL_D14-1100_mn_2_6
CL	D14-1100	mn	6	2	result	proposal	support	none	secondary	main	forw	support	Our experiment results show that the Vt-N classifier outperforms the current PCFG parser .	In this paper , we propose a classifier to resolve the ambiguity of Vt-N structures .	CL_D14-1100_mn_6	CL_D14-1100_mn_2	CL_D14-1100_mn_2_6
CL	D14-1100	mn	2	7	proposal	result	none	elaboration	main	secondary	none	none	In this paper , we propose a classifier to resolve the ambiguity of Vt-N structures .	Furthermore , it can be easily and effectively integrated into the PCFG parser and general statistical parsing models .	CL_D14-1100_mn_2	CL_D14-1100_mn_7	CL_D14-1100_mn_2_7
CL	D14-1100	mn	2	8	proposal	result	none	support	main	secondary	none	none	In this paper , we propose a classifier to resolve the ambiguity of Vt-N structures .	Evaluation of the learning approaches indicates that world knowledge facilitates Vt-N disambiguation through data selection and error correction .	CL_D14-1100_mn_2	CL_D14-1100_mn_8	CL_D14-1100_mn_2_8
CL	D14-1100	mn	3	4	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	The design of the classifier is based on three important guidelines , namely , adopting linguistically motivated features , using all available resources , and easy integration into a parsing model .	The linguistically motivated features include semantic relations , context , and morphological structures ; and the available resources are treebank , thesaurus , affix database , and large corpora .	CL_D14-1100_mn_3	CL_D14-1100_mn_4	CL_D14-1100_mn_3_4
CL	D14-1100	mn	4	3	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	forw	elaboration	The linguistically motivated features include semantic relations , context , and morphological structures ; and the available resources are treebank , thesaurus , affix database , and large corpora .	The design of the classifier is based on three important guidelines , namely , adopting linguistically motivated features , using all available resources , and easy integration into a parsing model .	CL_D14-1100_mn_4	CL_D14-1100_mn_3	CL_D14-1100_mn_3_4
CL	D14-1100	mn	3	5	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	none	none	The design of the classifier is based on three important guidelines , namely , adopting linguistically motivated features , using all available resources , and easy integration into a parsing model .	We also propose two learning approaches that resolve the problem of data sparseness by auto-parsing and extracting relative knowledge from large-scale unlabeled data .	CL_D14-1100_mn_3	CL_D14-1100_mn_5	CL_D14-1100_mn_3_5
CL	D14-1100	mn	3	6	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	The design of the classifier is based on three important guidelines , namely , adopting linguistically motivated features , using all available resources , and easy integration into a parsing model .	Our experiment results show that the Vt-N classifier outperforms the current PCFG parser .	CL_D14-1100_mn_3	CL_D14-1100_mn_6	CL_D14-1100_mn_3_6
CL	D14-1100	mn	3	7	proposal_implementation	result	elaboration	elaboration	secondary	secondary	none	none	The design of the classifier is based on three important guidelines , namely , adopting linguistically motivated features , using all available resources , and easy integration into a parsing model .	Furthermore , it can be easily and effectively integrated into the PCFG parser and general statistical parsing models .	CL_D14-1100_mn_3	CL_D14-1100_mn_7	CL_D14-1100_mn_3_7
CL	D14-1100	mn	3	8	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	The design of the classifier is based on three important guidelines , namely , adopting linguistically motivated features , using all available resources , and easy integration into a parsing model .	Evaluation of the learning approaches indicates that world knowledge facilitates Vt-N disambiguation through data selection and error correction .	CL_D14-1100_mn_3	CL_D14-1100_mn_8	CL_D14-1100_mn_3_8
CL	D14-1100	mn	4	5	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	The linguistically motivated features include semantic relations , context , and morphological structures ; and the available resources are treebank , thesaurus , affix database , and large corpora .	We also propose two learning approaches that resolve the problem of data sparseness by auto-parsing and extracting relative knowledge from large-scale unlabeled data .	CL_D14-1100_mn_4	CL_D14-1100_mn_5	CL_D14-1100_mn_4_5
CL	D14-1100	mn	5	4	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	forw	elaboration	We also propose two learning approaches that resolve the problem of data sparseness by auto-parsing and extracting relative knowledge from large-scale unlabeled data .	The linguistically motivated features include semantic relations , context , and morphological structures ; and the available resources are treebank , thesaurus , affix database , and large corpora .	CL_D14-1100_mn_5	CL_D14-1100_mn_4	CL_D14-1100_mn_4_5
CL	D14-1100	mn	4	6	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	The linguistically motivated features include semantic relations , context , and morphological structures ; and the available resources are treebank , thesaurus , affix database , and large corpora .	Our experiment results show that the Vt-N classifier outperforms the current PCFG parser .	CL_D14-1100_mn_4	CL_D14-1100_mn_6	CL_D14-1100_mn_4_6
CL	D14-1100	mn	4	7	proposal_implementation	result	elaboration	elaboration	secondary	secondary	none	none	The linguistically motivated features include semantic relations , context , and morphological structures ; and the available resources are treebank , thesaurus , affix database , and large corpora .	Furthermore , it can be easily and effectively integrated into the PCFG parser and general statistical parsing models .	CL_D14-1100_mn_4	CL_D14-1100_mn_7	CL_D14-1100_mn_4_7
CL	D14-1100	mn	4	8	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	The linguistically motivated features include semantic relations , context , and morphological structures ; and the available resources are treebank , thesaurus , affix database , and large corpora .	Evaluation of the learning approaches indicates that world knowledge facilitates Vt-N disambiguation through data selection and error correction .	CL_D14-1100_mn_4	CL_D14-1100_mn_8	CL_D14-1100_mn_4_8
CL	D14-1100	mn	5	6	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	We also propose two learning approaches that resolve the problem of data sparseness by auto-parsing and extracting relative knowledge from large-scale unlabeled data .	Our experiment results show that the Vt-N classifier outperforms the current PCFG parser .	CL_D14-1100_mn_5	CL_D14-1100_mn_6	CL_D14-1100_mn_5_6
CL	D14-1100	mn	5	7	proposal_implementation	result	elaboration	elaboration	secondary	secondary	none	none	We also propose two learning approaches that resolve the problem of data sparseness by auto-parsing and extracting relative knowledge from large-scale unlabeled data .	Furthermore , it can be easily and effectively integrated into the PCFG parser and general statistical parsing models .	CL_D14-1100_mn_5	CL_D14-1100_mn_7	CL_D14-1100_mn_5_7
CL	D14-1100	mn	5	8	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	We also propose two learning approaches that resolve the problem of data sparseness by auto-parsing and extracting relative knowledge from large-scale unlabeled data .	Evaluation of the learning approaches indicates that world knowledge facilitates Vt-N disambiguation through data selection and error correction .	CL_D14-1100_mn_5	CL_D14-1100_mn_8	CL_D14-1100_mn_5_8
CL	D14-1100	mn	6	7	result	result	support	elaboration	secondary	secondary	back	elaboration	Our experiment results show that the Vt-N classifier outperforms the current PCFG parser .	Furthermore , it can be easily and effectively integrated into the PCFG parser and general statistical parsing models .	CL_D14-1100_mn_6	CL_D14-1100_mn_7	CL_D14-1100_mn_6_7
CL	D14-1100	mn	7	6	result	result	elaboration	support	secondary	secondary	forw	elaboration	Furthermore , it can be easily and effectively integrated into the PCFG parser and general statistical parsing models .	Our experiment results show that the Vt-N classifier outperforms the current PCFG parser .	CL_D14-1100_mn_7	CL_D14-1100_mn_6	CL_D14-1100_mn_6_7
CL	D14-1100	mn	6	8	result	result	support	support	secondary	secondary	back	support	Our experiment results show that the Vt-N classifier outperforms the current PCFG parser .	Evaluation of the learning approaches indicates that world knowledge facilitates Vt-N disambiguation through data selection and error correction .	CL_D14-1100_mn_6	CL_D14-1100_mn_8	CL_D14-1100_mn_6_8
CL	D14-1100	mn	8	6	result	result	support	support	secondary	secondary	forw	support	Evaluation of the learning approaches indicates that world knowledge facilitates Vt-N disambiguation through data selection and error correction .	Our experiment results show that the Vt-N classifier outperforms the current PCFG parser .	CL_D14-1100_mn_8	CL_D14-1100_mn_6	CL_D14-1100_mn_6_8
CL	D14-1100	mn	7	8	result	result	elaboration	support	secondary	secondary	none	none	Furthermore , it can be easily and effectively integrated into the PCFG parser and general statistical parsing models .	Evaluation of the learning approaches indicates that world knowledge facilitates Vt-N disambiguation through data selection and error correction .	CL_D14-1100_mn_7	CL_D14-1100_mn_8	CL_D14-1100_mn_7_8
CL	D14-1101	mn	1	2	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	We propose a neural network approach to benefit from the non-linearity of corpus-wide statistics for part-of-speech ( POS ) tagging .	We investigated several types of corpus-wide information for the words , such as word embeddings and POS tag distributions .	CL_D14-1101_mn_1	CL_D14-1101_mn_2	CL_D14-1101_mn_1_2
CL	D14-1101	mn	2	1	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	We investigated several types of corpus-wide information for the words , such as word embeddings and POS tag distributions .	We propose a neural network approach to benefit from the non-linearity of corpus-wide statistics for part-of-speech ( POS ) tagging .	CL_D14-1101_mn_2	CL_D14-1101_mn_1	CL_D14-1101_mn_1_2
CL	D14-1101	mn	1	3	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	We propose a neural network approach to benefit from the non-linearity of corpus-wide statistics for part-of-speech ( POS ) tagging .	Since these statistics are encoded as dense continuous features , it is not trivial to combine these features comparing with sparse discrete features .	CL_D14-1101_mn_1	CL_D14-1101_mn_3	CL_D14-1101_mn_1_3
CL	D14-1101	mn	1	4	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	We propose a neural network approach to benefit from the non-linearity of corpus-wide statistics for part-of-speech ( POS ) tagging .	Our tagger is designed as a combination of a linear model for discrete features and a feed-forward neural network that captures the non-linear interactions among the continuous features .	CL_D14-1101_mn_1	CL_D14-1101_mn_4	CL_D14-1101_mn_1_4
CL	D14-1101	mn	1	5	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	We propose a neural network approach to benefit from the non-linearity of corpus-wide statistics for part-of-speech ( POS ) tagging .	By using several recent advances in the activation functions for neural networks , the proposed method marks new state-of-the-art accuracies for English POS tagging tasks .	CL_D14-1101_mn_1	CL_D14-1101_mn_5	CL_D14-1101_mn_1_5
CL	D14-1101	mn	2	3	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	We investigated several types of corpus-wide information for the words , such as word embeddings and POS tag distributions .	Since these statistics are encoded as dense continuous features , it is not trivial to combine these features comparing with sparse discrete features .	CL_D14-1101_mn_2	CL_D14-1101_mn_3	CL_D14-1101_mn_2_3
CL	D14-1101	mn	3	2	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	forw	elaboration	Since these statistics are encoded as dense continuous features , it is not trivial to combine these features comparing with sparse discrete features .	We investigated several types of corpus-wide information for the words , such as word embeddings and POS tag distributions .	CL_D14-1101_mn_3	CL_D14-1101_mn_2	CL_D14-1101_mn_2_3
CL	D14-1101	mn	2	4	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	none	none	We investigated several types of corpus-wide information for the words , such as word embeddings and POS tag distributions .	Our tagger is designed as a combination of a linear model for discrete features and a feed-forward neural network that captures the non-linear interactions among the continuous features .	CL_D14-1101_mn_2	CL_D14-1101_mn_4	CL_D14-1101_mn_2_4
CL	D14-1101	mn	2	5	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	none	none	We investigated several types of corpus-wide information for the words , such as word embeddings and POS tag distributions .	By using several recent advances in the activation functions for neural networks , the proposed method marks new state-of-the-art accuracies for English POS tagging tasks .	CL_D14-1101_mn_2	CL_D14-1101_mn_5	CL_D14-1101_mn_2_5
CL	D14-1101	mn	3	4	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	Since these statistics are encoded as dense continuous features , it is not trivial to combine these features comparing with sparse discrete features .	Our tagger is designed as a combination of a linear model for discrete features and a feed-forward neural network that captures the non-linear interactions among the continuous features .	CL_D14-1101_mn_3	CL_D14-1101_mn_4	CL_D14-1101_mn_3_4
CL	D14-1101	mn	4	3	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	forw	elaboration	Our tagger is designed as a combination of a linear model for discrete features and a feed-forward neural network that captures the non-linear interactions among the continuous features .	Since these statistics are encoded as dense continuous features , it is not trivial to combine these features comparing with sparse discrete features .	CL_D14-1101_mn_4	CL_D14-1101_mn_3	CL_D14-1101_mn_3_4
CL	D14-1101	mn	3	5	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	none	none	Since these statistics are encoded as dense continuous features , it is not trivial to combine these features comparing with sparse discrete features .	By using several recent advances in the activation functions for neural networks , the proposed method marks new state-of-the-art accuracies for English POS tagging tasks .	CL_D14-1101_mn_3	CL_D14-1101_mn_5	CL_D14-1101_mn_3_5
CL	D14-1101	mn	4	5	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	Our tagger is designed as a combination of a linear model for discrete features and a feed-forward neural network that captures the non-linear interactions among the continuous features .	By using several recent advances in the activation functions for neural networks , the proposed method marks new state-of-the-art accuracies for English POS tagging tasks .	CL_D14-1101_mn_4	CL_D14-1101_mn_5	CL_D14-1101_mn_4_5
CL	D14-1101	mn	5	4	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	forw	elaboration	By using several recent advances in the activation functions for neural networks , the proposed method marks new state-of-the-art accuracies for English POS tagging tasks .	Our tagger is designed as a combination of a linear model for discrete features and a feed-forward neural network that captures the non-linear interactions among the continuous features .	CL_D14-1101_mn_5	CL_D14-1101_mn_4	CL_D14-1101_mn_4_5
CL	D14-1102	mn	1	2	motivation_background	motivation_background	info-required	support	secondary	secondary	forw	info-required	Different approaches to high-quality grammatical error correction have been proposed recently , many of which have their own strengths and weaknesses .	Most of these approaches are based on classification or statistical machine translation ( SMT ) .	CL_D14-1102_mn_1	CL_D14-1102_mn_2	CL_D14-1102_mn_1_2
CL	D14-1102	mn	2	1	motivation_background	motivation_background	support	info-required	secondary	secondary	back	info-required	Most of these approaches are based on classification or statistical machine translation ( SMT ) .	Different approaches to high-quality grammatical error correction have been proposed recently , many of which have their own strengths and weaknesses .	CL_D14-1102_mn_2	CL_D14-1102_mn_1	CL_D14-1102_mn_1_2
CL	D14-1102	mn	1	3	motivation_background	proposal	info-required	none	secondary	main	none	none	Different approaches to high-quality grammatical error correction have been proposed recently , many of which have their own strengths and weaknesses .	In this paper , we propose to combine the output from a classification-based system and an SMT-based system to improve the correction quality .	CL_D14-1102_mn_1	CL_D14-1102_mn_3	CL_D14-1102_mn_1_3
CL	D14-1102	mn	1	4	motivation_background	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	Different approaches to high-quality grammatical error correction have been proposed recently , many of which have their own strengths and weaknesses .	We adopt the system combination technique of Heafield and Lavie ( 2010 ) .	CL_D14-1102_mn_1	CL_D14-1102_mn_4	CL_D14-1102_mn_1_4
CL	D14-1102	mn	1	5	motivation_background	result_means	info-required	support	secondary	secondary	none	none	Different approaches to high-quality grammatical error correction have been proposed recently , many of which have their own strengths and weaknesses .	We achieve an F0.5 score of 39.39 % on the test set of the CoNLL-2014 shared task , outperforming the best system in the shared task .	CL_D14-1102_mn_1	CL_D14-1102_mn_5	CL_D14-1102_mn_1_5
CL	D14-1102	mn	2	3	motivation_background	proposal	support	none	secondary	main	forw	support	Most of these approaches are based on classification or statistical machine translation ( SMT ) .	In this paper , we propose to combine the output from a classification-based system and an SMT-based system to improve the correction quality .	CL_D14-1102_mn_2	CL_D14-1102_mn_3	CL_D14-1102_mn_2_3
CL	D14-1102	mn	3	2	proposal	motivation_background	none	support	main	secondary	back	support	In this paper , we propose to combine the output from a classification-based system and an SMT-based system to improve the correction quality .	Most of these approaches are based on classification or statistical machine translation ( SMT ) .	CL_D14-1102_mn_3	CL_D14-1102_mn_2	CL_D14-1102_mn_2_3
CL	D14-1102	mn	2	4	motivation_background	proposal_implementation	support	elaboration	secondary	secondary	none	none	Most of these approaches are based on classification or statistical machine translation ( SMT ) .	We adopt the system combination technique of Heafield and Lavie ( 2010 ) .	CL_D14-1102_mn_2	CL_D14-1102_mn_4	CL_D14-1102_mn_2_4
CL	D14-1102	mn	2	5	motivation_background	result_means	support	support	secondary	secondary	none	none	Most of these approaches are based on classification or statistical machine translation ( SMT ) .	We achieve an F0.5 score of 39.39 % on the test set of the CoNLL-2014 shared task , outperforming the best system in the shared task .	CL_D14-1102_mn_2	CL_D14-1102_mn_5	CL_D14-1102_mn_2_5
CL	D14-1102	mn	3	4	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	In this paper , we propose to combine the output from a classification-based system and an SMT-based system to improve the correction quality .	We adopt the system combination technique of Heafield and Lavie ( 2010 ) .	CL_D14-1102_mn_3	CL_D14-1102_mn_4	CL_D14-1102_mn_3_4
CL	D14-1102	mn	4	3	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	We adopt the system combination technique of Heafield and Lavie ( 2010 ) .	In this paper , we propose to combine the output from a classification-based system and an SMT-based system to improve the correction quality .	CL_D14-1102_mn_4	CL_D14-1102_mn_3	CL_D14-1102_mn_3_4
CL	D14-1102	mn	3	5	proposal	result_means	none	support	main	secondary	back	support	In this paper , we propose to combine the output from a classification-based system and an SMT-based system to improve the correction quality .	We achieve an F0.5 score of 39.39 % on the test set of the CoNLL-2014 shared task , outperforming the best system in the shared task .	CL_D14-1102_mn_3	CL_D14-1102_mn_5	CL_D14-1102_mn_3_5
CL	D14-1102	mn	5	3	result_means	proposal	support	none	secondary	main	forw	support	We achieve an F0.5 score of 39.39 % on the test set of the CoNLL-2014 shared task , outperforming the best system in the shared task .	In this paper , we propose to combine the output from a classification-based system and an SMT-based system to improve the correction quality .	CL_D14-1102_mn_5	CL_D14-1102_mn_3	CL_D14-1102_mn_3_5
CL	D14-1102	mn	4	5	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	We adopt the system combination technique of Heafield and Lavie ( 2010 ) .	We achieve an F0.5 score of 39.39 % on the test set of the CoNLL-2014 shared task , outperforming the best system in the shared task .	CL_D14-1102_mn_4	CL_D14-1102_mn_5	CL_D14-1102_mn_4_5
CL	D14-1103	mn	1	2	proposal	result_means	none	support	main	secondary	back	support	In this paper we propose a method to increase dependency parser performance without using additional labeled or unlabeled data by refining the layer of predicted part-of-speech ( POS ) tags .	We perform experiments on English and German and show significant improvements for both languages .	CL_D14-1103_mn_1	CL_D14-1103_mn_2	CL_D14-1103_mn_1_2
CL	D14-1103	mn	2	1	result_means	proposal	support	none	secondary	main	forw	support	We perform experiments on English and German and show significant improvements for both languages .	In this paper we propose a method to increase dependency parser performance without using additional labeled or unlabeled data by refining the layer of predicted part-of-speech ( POS ) tags .	CL_D14-1103_mn_2	CL_D14-1103_mn_1	CL_D14-1103_mn_1_2
CL	D14-1103	mn	1	3	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	In this paper we propose a method to increase dependency parser performance without using additional labeled or unlabeled data by refining the layer of predicted part-of-speech ( POS ) tags .	The refinement is based on generative split-merge training for Hidden Markov models ( HMMs ) .	CL_D14-1103_mn_1	CL_D14-1103_mn_3	CL_D14-1103_mn_1_3
CL	D14-1103	mn	3	1	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	The refinement is based on generative split-merge training for Hidden Markov models ( HMMs ) .	In this paper we propose a method to increase dependency parser performance without using additional labeled or unlabeled data by refining the layer of predicted part-of-speech ( POS ) tags .	CL_D14-1103_mn_3	CL_D14-1103_mn_1	CL_D14-1103_mn_1_3
CL	D14-1103	mn	2	3	result_means	proposal_implementation	support	elaboration	secondary	secondary	none	none	We perform experiments on English and German and show significant improvements for both languages .	The refinement is based on generative split-merge training for Hidden Markov models ( HMMs ) .	CL_D14-1103_mn_2	CL_D14-1103_mn_3	CL_D14-1103_mn_2_3
CL	D14-1104	mn	1	2	motivation_background	motivation_background	info-required	info-required	secondary	secondary	forw	info-required	Importance weighting is a generalization of various statistical bias correction techniques .	While our labeled data in NLP is heavily biased , importance weighting has seen only few applications in NLP , most of them relying on a small amount of labeled target data .	CL_D14-1104_mn_1	CL_D14-1104_mn_2	CL_D14-1104_mn_1_2
CL	D14-1104	mn	2	1	motivation_background	motivation_background	info-required	info-required	secondary	secondary	back	info-required	While our labeled data in NLP is heavily biased , importance weighting has seen only few applications in NLP , most of them relying on a small amount of labeled target data .	Importance weighting is a generalization of various statistical bias correction techniques .	CL_D14-1104_mn_2	CL_D14-1104_mn_1	CL_D14-1104_mn_1_2
CL	D14-1104	mn	1	3	motivation_background	motivation_problem	info-required	support	secondary	secondary	none	none	Importance weighting is a generalization of various statistical bias correction techniques .	The publication bias toward reporting positive results makes it hard to say whether researchers have tried .	CL_D14-1104_mn_1	CL_D14-1104_mn_3	CL_D14-1104_mn_1_3
CL	D14-1104	mn	1	4	motivation_background	proposal	info-required	none	secondary	main	none	none	Importance weighting is a generalization of various statistical bias correction techniques .	This paper presents a negative result on unsupervised domain adaptation for POS tagging .	CL_D14-1104_mn_1	CL_D14-1104_mn_4	CL_D14-1104_mn_1_4
CL	D14-1104	mn	1	5	motivation_background	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	Importance weighting is a generalization of various statistical bias correction techniques .	In this setup , we only have unlabeled data and thus only indirect access to the bias in emission and transition probabilities .	CL_D14-1104_mn_1	CL_D14-1104_mn_5	CL_D14-1104_mn_1_5
CL	D14-1104	mn	1	6	motivation_background	information_additional	info-required	info-required	secondary	secondary	none	none	Importance weighting is a generalization of various statistical bias correction techniques .	Moreover , most errors in POS tagging are due to unseen words , and there , importance weighting cannot help .	CL_D14-1104_mn_1	CL_D14-1104_mn_6	CL_D14-1104_mn_1_6
CL	D14-1104	mn	1	7	motivation_background	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	Importance weighting is a generalization of various statistical bias correction techniques .	We present experiments with a wide variety of weight functions , quantilizations , as well as with randomly generated weights , to support these claims .	CL_D14-1104_mn_1	CL_D14-1104_mn_7	CL_D14-1104_mn_1_7
CL	D14-1104	mn	2	3	motivation_background	motivation_problem	info-required	support	secondary	secondary	forw	info-required	While our labeled data in NLP is heavily biased , importance weighting has seen only few applications in NLP , most of them relying on a small amount of labeled target data .	The publication bias toward reporting positive results makes it hard to say whether researchers have tried .	CL_D14-1104_mn_2	CL_D14-1104_mn_3	CL_D14-1104_mn_2_3
CL	D14-1104	mn	3	2	motivation_problem	motivation_background	support	info-required	secondary	secondary	back	info-required	The publication bias toward reporting positive results makes it hard to say whether researchers have tried .	While our labeled data in NLP is heavily biased , importance weighting has seen only few applications in NLP , most of them relying on a small amount of labeled target data .	CL_D14-1104_mn_3	CL_D14-1104_mn_2	CL_D14-1104_mn_2_3
CL	D14-1104	mn	2	4	motivation_background	proposal	info-required	none	secondary	main	none	none	While our labeled data in NLP is heavily biased , importance weighting has seen only few applications in NLP , most of them relying on a small amount of labeled target data .	This paper presents a negative result on unsupervised domain adaptation for POS tagging .	CL_D14-1104_mn_2	CL_D14-1104_mn_4	CL_D14-1104_mn_2_4
CL	D14-1104	mn	2	5	motivation_background	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	While our labeled data in NLP is heavily biased , importance weighting has seen only few applications in NLP , most of them relying on a small amount of labeled target data .	In this setup , we only have unlabeled data and thus only indirect access to the bias in emission and transition probabilities .	CL_D14-1104_mn_2	CL_D14-1104_mn_5	CL_D14-1104_mn_2_5
CL	D14-1104	mn	2	6	motivation_background	information_additional	info-required	info-required	secondary	secondary	none	none	While our labeled data in NLP is heavily biased , importance weighting has seen only few applications in NLP , most of them relying on a small amount of labeled target data .	Moreover , most errors in POS tagging are due to unseen words , and there , importance weighting cannot help .	CL_D14-1104_mn_2	CL_D14-1104_mn_6	CL_D14-1104_mn_2_6
CL	D14-1104	mn	2	7	motivation_background	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	While our labeled data in NLP is heavily biased , importance weighting has seen only few applications in NLP , most of them relying on a small amount of labeled target data .	We present experiments with a wide variety of weight functions , quantilizations , as well as with randomly generated weights , to support these claims .	CL_D14-1104_mn_2	CL_D14-1104_mn_7	CL_D14-1104_mn_2_7
CL	D14-1104	mn	3	4	motivation_problem	proposal	support	none	secondary	main	forw	support	The publication bias toward reporting positive results makes it hard to say whether researchers have tried .	This paper presents a negative result on unsupervised domain adaptation for POS tagging .	CL_D14-1104_mn_3	CL_D14-1104_mn_4	CL_D14-1104_mn_3_4
CL	D14-1104	mn	4	3	proposal	motivation_problem	none	support	main	secondary	back	support	This paper presents a negative result on unsupervised domain adaptation for POS tagging .	The publication bias toward reporting positive results makes it hard to say whether researchers have tried .	CL_D14-1104_mn_4	CL_D14-1104_mn_3	CL_D14-1104_mn_3_4
CL	D14-1104	mn	3	5	motivation_problem	proposal_implementation	support	elaboration	secondary	secondary	none	none	The publication bias toward reporting positive results makes it hard to say whether researchers have tried .	In this setup , we only have unlabeled data and thus only indirect access to the bias in emission and transition probabilities .	CL_D14-1104_mn_3	CL_D14-1104_mn_5	CL_D14-1104_mn_3_5
CL	D14-1104	mn	3	6	motivation_problem	information_additional	support	info-required	secondary	secondary	none	none	The publication bias toward reporting positive results makes it hard to say whether researchers have tried .	Moreover , most errors in POS tagging are due to unseen words , and there , importance weighting cannot help .	CL_D14-1104_mn_3	CL_D14-1104_mn_6	CL_D14-1104_mn_3_6
CL	D14-1104	mn	3	7	motivation_problem	proposal_implementation	support	elaboration	secondary	secondary	none	none	The publication bias toward reporting positive results makes it hard to say whether researchers have tried .	We present experiments with a wide variety of weight functions , quantilizations , as well as with randomly generated weights , to support these claims .	CL_D14-1104_mn_3	CL_D14-1104_mn_7	CL_D14-1104_mn_3_7
CL	D14-1104	mn	4	5	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	This paper presents a negative result on unsupervised domain adaptation for POS tagging .	In this setup , we only have unlabeled data and thus only indirect access to the bias in emission and transition probabilities .	CL_D14-1104_mn_4	CL_D14-1104_mn_5	CL_D14-1104_mn_4_5
CL	D14-1104	mn	5	4	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	In this setup , we only have unlabeled data and thus only indirect access to the bias in emission and transition probabilities .	This paper presents a negative result on unsupervised domain adaptation for POS tagging .	CL_D14-1104_mn_5	CL_D14-1104_mn_4	CL_D14-1104_mn_4_5
CL	D14-1104	mn	4	6	proposal	information_additional	none	info-required	main	secondary	none	none	This paper presents a negative result on unsupervised domain adaptation for POS tagging .	Moreover , most errors in POS tagging are due to unseen words , and there , importance weighting cannot help .	CL_D14-1104_mn_4	CL_D14-1104_mn_6	CL_D14-1104_mn_4_6
CL	D14-1104	mn	4	7	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	This paper presents a negative result on unsupervised domain adaptation for POS tagging .	We present experiments with a wide variety of weight functions , quantilizations , as well as with randomly generated weights , to support these claims .	CL_D14-1104_mn_4	CL_D14-1104_mn_7	CL_D14-1104_mn_4_7
CL	D14-1104	mn	5	6	proposal_implementation	information_additional	elaboration	info-required	secondary	secondary	none	none	In this setup , we only have unlabeled data and thus only indirect access to the bias in emission and transition probabilities .	Moreover , most errors in POS tagging are due to unseen words , and there , importance weighting cannot help .	CL_D14-1104_mn_5	CL_D14-1104_mn_6	CL_D14-1104_mn_5_6
CL	D14-1104	mn	5	7	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	In this setup , we only have unlabeled data and thus only indirect access to the bias in emission and transition probabilities .	We present experiments with a wide variety of weight functions , quantilizations , as well as with randomly generated weights , to support these claims .	CL_D14-1104_mn_5	CL_D14-1104_mn_7	CL_D14-1104_mn_5_7
CL	D14-1104	mn	7	5	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	forw	elaboration	We present experiments with a wide variety of weight functions , quantilizations , as well as with randomly generated weights , to support these claims .	In this setup , we only have unlabeled data and thus only indirect access to the bias in emission and transition probabilities .	CL_D14-1104_mn_7	CL_D14-1104_mn_5	CL_D14-1104_mn_5_7
CL	D14-1104	mn	6	7	information_additional	proposal_implementation	info-required	elaboration	secondary	secondary	forw	info-required	Moreover , most errors in POS tagging are due to unseen words , and there , importance weighting cannot help .	We present experiments with a wide variety of weight functions , quantilizations , as well as with randomly generated weights , to support these claims .	CL_D14-1104_mn_6	CL_D14-1104_mn_7	CL_D14-1104_mn_6_7
CL	D14-1104	mn	7	6	proposal_implementation	information_additional	elaboration	info-required	secondary	secondary	back	info-required	We present experiments with a wide variety of weight functions , quantilizations , as well as with randomly generated weights , to support these claims .	Moreover , most errors in POS tagging are due to unseen words , and there , importance weighting cannot help .	CL_D14-1104_mn_7	CL_D14-1104_mn_6	CL_D14-1104_mn_6_7
CL	D14-1105	mn	1	2	motivation_background	motivation_problem	info-required	support	secondary	secondary	forw	info-required	Code-mixing is frequently observed in user generated content on social media , especially from multilingual users .	The linguistic complexity of such content is compounded by presence of spelling variations , transliteration and non-adherance to formal grammar .	CL_D14-1105_mn_1	CL_D14-1105_mn_2	CL_D14-1105_mn_1_2
CL	D14-1105	mn	2	1	motivation_problem	motivation_background	support	info-required	secondary	secondary	back	info-required	The linguistic complexity of such content is compounded by presence of spelling variations , transliteration and non-adherance to formal grammar .	Code-mixing is frequently observed in user generated content on social media , especially from multilingual users .	CL_D14-1105_mn_2	CL_D14-1105_mn_1	CL_D14-1105_mn_1_2
CL	D14-1105	mn	1	3	motivation_background	proposal	info-required	none	secondary	main	none	none	Code-mixing is frequently observed in user generated content on social media , especially from multilingual users .	We describe our initial efforts to create a multi-level annotated corpus of Hindi-English code-mixed text collated from Facebook forums , and explore language identification , back-transliteration , normalization and POS tagging of this data .	CL_D14-1105_mn_1	CL_D14-1105_mn_3	CL_D14-1105_mn_1_3
CL	D14-1105	mn	1	4	motivation_background	conclusion	info-required	support	secondary	secondary	none	none	Code-mixing is frequently observed in user generated content on social media , especially from multilingual users .	Our results show that language identification and transliteration for Hindi are two major challenges that impact POS tagging accuracy .	CL_D14-1105_mn_1	CL_D14-1105_mn_4	CL_D14-1105_mn_1_4
CL	D14-1105	mn	2	3	motivation_problem	proposal	support	none	secondary	main	forw	support	The linguistic complexity of such content is compounded by presence of spelling variations , transliteration and non-adherance to formal grammar .	We describe our initial efforts to create a multi-level annotated corpus of Hindi-English code-mixed text collated from Facebook forums , and explore language identification , back-transliteration , normalization and POS tagging of this data .	CL_D14-1105_mn_2	CL_D14-1105_mn_3	CL_D14-1105_mn_2_3
CL	D14-1105	mn	3	2	proposal	motivation_problem	none	support	main	secondary	back	support	We describe our initial efforts to create a multi-level annotated corpus of Hindi-English code-mixed text collated from Facebook forums , and explore language identification , back-transliteration , normalization and POS tagging of this data .	The linguistic complexity of such content is compounded by presence of spelling variations , transliteration and non-adherance to formal grammar .	CL_D14-1105_mn_3	CL_D14-1105_mn_2	CL_D14-1105_mn_2_3
CL	D14-1105	mn	2	4	motivation_problem	conclusion	support	support	secondary	secondary	none	none	The linguistic complexity of such content is compounded by presence of spelling variations , transliteration and non-adherance to formal grammar .	Our results show that language identification and transliteration for Hindi are two major challenges that impact POS tagging accuracy .	CL_D14-1105_mn_2	CL_D14-1105_mn_4	CL_D14-1105_mn_2_4
CL	D14-1105	mn	3	4	proposal	conclusion	none	support	main	secondary	back	support	We describe our initial efforts to create a multi-level annotated corpus of Hindi-English code-mixed text collated from Facebook forums , and explore language identification , back-transliteration , normalization and POS tagging of this data .	Our results show that language identification and transliteration for Hindi are two major challenges that impact POS tagging accuracy .	CL_D14-1105_mn_3	CL_D14-1105_mn_4	CL_D14-1105_mn_3_4
CL	D14-1105	mn	4	3	conclusion	proposal	support	none	secondary	main	forw	support	Our results show that language identification and transliteration for Hindi are two major challenges that impact POS tagging accuracy .	We describe our initial efforts to create a multi-level annotated corpus of Hindi-English code-mixed text collated from Facebook forums , and explore language identification , back-transliteration , normalization and POS tagging of this data .	CL_D14-1105_mn_4	CL_D14-1105_mn_3	CL_D14-1105_mn_3_4
CL	D14-1106	mn	1	2	proposal	information_additional	none	info-optional	main	secondary	back	info-optional	We investigate grammatical error detection in spoken language , and present a data-driven method to train a dependency parser to automatically identify and label grammatical errors .	This method is agnostic to the label set used , and the only manual annotations needed for training are grammatical error labels .	CL_D14-1106_mn_1	CL_D14-1106_mn_2	CL_D14-1106_mn_1_2
CL	D14-1106	mn	2	1	information_additional	proposal	info-optional	none	secondary	main	forw	info-optional	This method is agnostic to the label set used , and the only manual annotations needed for training are grammatical error labels .	We investigate grammatical error detection in spoken language , and present a data-driven method to train a dependency parser to automatically identify and label grammatical errors .	CL_D14-1106_mn_2	CL_D14-1106_mn_1	CL_D14-1106_mn_1_2
CL	D14-1106	mn	1	3	proposal	result	none	support	main	secondary	back	support	We investigate grammatical error detection in spoken language , and present a data-driven method to train a dependency parser to automatically identify and label grammatical errors .	We find that the proposed system is robust to disfluencies , so that a separate stage to elide disfluencies is not required .	CL_D14-1106_mn_1	CL_D14-1106_mn_3	CL_D14-1106_mn_1_3
CL	D14-1106	mn	3	1	result	proposal	support	none	secondary	main	forw	support	We find that the proposed system is robust to disfluencies , so that a separate stage to elide disfluencies is not required .	We investigate grammatical error detection in spoken language , and present a data-driven method to train a dependency parser to automatically identify and label grammatical errors .	CL_D14-1106_mn_3	CL_D14-1106_mn_1	CL_D14-1106_mn_1_3
CL	D14-1106	mn	1	4	proposal	result_means	none	elaboration	main	secondary	none	none	We investigate grammatical error detection in spoken language , and present a data-driven method to train a dependency parser to automatically identify and label grammatical errors .	The proposed system outperforms two baseline systems on two different corpora that use different sets of error tags .	CL_D14-1106_mn_1	CL_D14-1106_mn_4	CL_D14-1106_mn_1_4
CL	D14-1106	mn	1	5	proposal	observation	none	support	main	secondary	none	none	We investigate grammatical error detection in spoken language , and present a data-driven method to train a dependency parser to automatically identify and label grammatical errors .	It is able to identify utterances with grammatical errors with an F1-score as high as 0.623 , as compared to a baseline F1 of 0.350 on the same data .	CL_D14-1106_mn_1	CL_D14-1106_mn_5	CL_D14-1106_mn_1_5
CL	D14-1106	mn	2	3	information_additional	result	info-optional	support	secondary	secondary	none	none	This method is agnostic to the label set used , and the only manual annotations needed for training are grammatical error labels .	We find that the proposed system is robust to disfluencies , so that a separate stage to elide disfluencies is not required .	CL_D14-1106_mn_2	CL_D14-1106_mn_3	CL_D14-1106_mn_2_3
CL	D14-1106	mn	2	4	information_additional	result_means	info-optional	elaboration	secondary	secondary	none	none	This method is agnostic to the label set used , and the only manual annotations needed for training are grammatical error labels .	The proposed system outperforms two baseline systems on two different corpora that use different sets of error tags .	CL_D14-1106_mn_2	CL_D14-1106_mn_4	CL_D14-1106_mn_2_4
CL	D14-1106	mn	2	5	information_additional	observation	info-optional	support	secondary	secondary	none	none	This method is agnostic to the label set used , and the only manual annotations needed for training are grammatical error labels .	It is able to identify utterances with grammatical errors with an F1-score as high as 0.623 , as compared to a baseline F1 of 0.350 on the same data .	CL_D14-1106_mn_2	CL_D14-1106_mn_5	CL_D14-1106_mn_2_5
CL	D14-1106	mn	3	4	result	result_means	support	elaboration	secondary	secondary	back	elaboration	We find that the proposed system is robust to disfluencies , so that a separate stage to elide disfluencies is not required .	The proposed system outperforms two baseline systems on two different corpora that use different sets of error tags .	CL_D14-1106_mn_3	CL_D14-1106_mn_4	CL_D14-1106_mn_3_4
CL	D14-1106	mn	4	3	result_means	result	elaboration	support	secondary	secondary	forw	elaboration	The proposed system outperforms two baseline systems on two different corpora that use different sets of error tags .	We find that the proposed system is robust to disfluencies , so that a separate stage to elide disfluencies is not required .	CL_D14-1106_mn_4	CL_D14-1106_mn_3	CL_D14-1106_mn_3_4
CL	D14-1106	mn	3	5	result	observation	support	support	secondary	secondary	back	support	We find that the proposed system is robust to disfluencies , so that a separate stage to elide disfluencies is not required .	It is able to identify utterances with grammatical errors with an F1-score as high as 0.623 , as compared to a baseline F1 of 0.350 on the same data .	CL_D14-1106_mn_3	CL_D14-1106_mn_5	CL_D14-1106_mn_3_5
CL	D14-1106	mn	5	3	observation	result	support	support	secondary	secondary	forw	support	It is able to identify utterances with grammatical errors with an F1-score as high as 0.623 , as compared to a baseline F1 of 0.350 on the same data .	We find that the proposed system is robust to disfluencies , so that a separate stage to elide disfluencies is not required .	CL_D14-1106_mn_5	CL_D14-1106_mn_3	CL_D14-1106_mn_3_5
CL	D14-1106	mn	4	5	result_means	observation	elaboration	support	secondary	secondary	none	none	The proposed system outperforms two baseline systems on two different corpora that use different sets of error tags .	It is able to identify utterances with grammatical errors with an F1-score as high as 0.623 , as compared to a baseline F1 of 0.350 on the same data .	CL_D14-1106_mn_4	CL_D14-1106_mn_5	CL_D14-1106_mn_4_5
CL	D14-1107	mn	1	2	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	We introduce a new CCG parsing model which is factored on lexical category assignments .	Parsing is then simply a deterministic search for the most probable category sequence that supports a CCG derivation .	CL_D14-1107_mn_1	CL_D14-1107_mn_2	CL_D14-1107_mn_1_2
CL	D14-1107	mn	2	1	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	Parsing is then simply a deterministic search for the most probable category sequence that supports a CCG derivation .	We introduce a new CCG parsing model which is factored on lexical category assignments .	CL_D14-1107_mn_2	CL_D14-1107_mn_1	CL_D14-1107_mn_1_2
CL	D14-1107	mn	1	3	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	We introduce a new CCG parsing model which is factored on lexical category assignments .	The parser is extremely simple , with a tiny feature set , no POS tagger , and no statistical model of the derivation or dependencies .	CL_D14-1107_mn_1	CL_D14-1107_mn_3	CL_D14-1107_mn_1_3
CL	D14-1107	mn	1	4	proposal	information_additional	none	info-optional	main	secondary	none	none	We introduce a new CCG parsing model which is factored on lexical category assignments .	Formulating the model in this way allows a highly effective heuristic for Aparsing , which makes parsing extremely fast .	CL_D14-1107_mn_1	CL_D14-1107_mn_4	CL_D14-1107_mn_1_4
CL	D14-1107	mn	1	5	proposal	result	none	support	main	secondary	back	support	We introduce a new CCG parsing model which is factored on lexical category assignments .	Compared to the standard C&C CCG parser , our model is more accurate out-of-domain , is four times faster , has higher coverage , and is greatly simplified .	CL_D14-1107_mn_1	CL_D14-1107_mn_5	CL_D14-1107_mn_1_5
CL	D14-1107	mn	5	1	result	proposal	support	none	secondary	main	forw	support	Compared to the standard C&C CCG parser , our model is more accurate out-of-domain , is four times faster , has higher coverage , and is greatly simplified .	We introduce a new CCG parsing model which is factored on lexical category assignments .	CL_D14-1107_mn_5	CL_D14-1107_mn_1	CL_D14-1107_mn_1_5
CL	D14-1107	mn	1	6	proposal	result_means	none	elaboration	main	secondary	none	none	We introduce a new CCG parsing model which is factored on lexical category assignments .	We also show that using our parser improves the performance of a state-of-the-art question answering system .	CL_D14-1107_mn_1	CL_D14-1107_mn_6	CL_D14-1107_mn_1_6
CL	D14-1107	mn	2	3	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	Parsing is then simply a deterministic search for the most probable category sequence that supports a CCG derivation .	The parser is extremely simple , with a tiny feature set , no POS tagger , and no statistical model of the derivation or dependencies .	CL_D14-1107_mn_2	CL_D14-1107_mn_3	CL_D14-1107_mn_2_3
CL	D14-1107	mn	3	2	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	forw	elaboration	The parser is extremely simple , with a tiny feature set , no POS tagger , and no statistical model of the derivation or dependencies .	Parsing is then simply a deterministic search for the most probable category sequence that supports a CCG derivation .	CL_D14-1107_mn_3	CL_D14-1107_mn_2	CL_D14-1107_mn_2_3
CL	D14-1107	mn	2	4	proposal_implementation	information_additional	elaboration	info-optional	secondary	secondary	none	none	Parsing is then simply a deterministic search for the most probable category sequence that supports a CCG derivation .	Formulating the model in this way allows a highly effective heuristic for Aparsing , which makes parsing extremely fast .	CL_D14-1107_mn_2	CL_D14-1107_mn_4	CL_D14-1107_mn_2_4
CL	D14-1107	mn	2	5	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	Parsing is then simply a deterministic search for the most probable category sequence that supports a CCG derivation .	Compared to the standard C&C CCG parser , our model is more accurate out-of-domain , is four times faster , has higher coverage , and is greatly simplified .	CL_D14-1107_mn_2	CL_D14-1107_mn_5	CL_D14-1107_mn_2_5
CL	D14-1107	mn	2	6	proposal_implementation	result_means	elaboration	elaboration	secondary	secondary	none	none	Parsing is then simply a deterministic search for the most probable category sequence that supports a CCG derivation .	We also show that using our parser improves the performance of a state-of-the-art question answering system .	CL_D14-1107_mn_2	CL_D14-1107_mn_6	CL_D14-1107_mn_2_6
CL	D14-1107	mn	3	4	proposal_implementation	information_additional	elaboration	info-optional	secondary	secondary	back	info-optional	The parser is extremely simple , with a tiny feature set , no POS tagger , and no statistical model of the derivation or dependencies .	Formulating the model in this way allows a highly effective heuristic for Aparsing , which makes parsing extremely fast .	CL_D14-1107_mn_3	CL_D14-1107_mn_4	CL_D14-1107_mn_3_4
CL	D14-1107	mn	4	3	information_additional	proposal_implementation	info-optional	elaboration	secondary	secondary	forw	info-optional	Formulating the model in this way allows a highly effective heuristic for Aparsing , which makes parsing extremely fast .	The parser is extremely simple , with a tiny feature set , no POS tagger , and no statistical model of the derivation or dependencies .	CL_D14-1107_mn_4	CL_D14-1107_mn_3	CL_D14-1107_mn_3_4
CL	D14-1107	mn	3	5	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	The parser is extremely simple , with a tiny feature set , no POS tagger , and no statistical model of the derivation or dependencies .	Compared to the standard C&C CCG parser , our model is more accurate out-of-domain , is four times faster , has higher coverage , and is greatly simplified .	CL_D14-1107_mn_3	CL_D14-1107_mn_5	CL_D14-1107_mn_3_5
CL	D14-1107	mn	3	6	proposal_implementation	result_means	elaboration	elaboration	secondary	secondary	none	none	The parser is extremely simple , with a tiny feature set , no POS tagger , and no statistical model of the derivation or dependencies .	We also show that using our parser improves the performance of a state-of-the-art question answering system .	CL_D14-1107_mn_3	CL_D14-1107_mn_6	CL_D14-1107_mn_3_6
CL	D14-1107	mn	4	5	information_additional	result	info-optional	support	secondary	secondary	none	none	Formulating the model in this way allows a highly effective heuristic for Aparsing , which makes parsing extremely fast .	Compared to the standard C&C CCG parser , our model is more accurate out-of-domain , is four times faster , has higher coverage , and is greatly simplified .	CL_D14-1107_mn_4	CL_D14-1107_mn_5	CL_D14-1107_mn_4_5
CL	D14-1107	mn	4	6	information_additional	result_means	info-optional	elaboration	secondary	secondary	none	none	Formulating the model in this way allows a highly effective heuristic for Aparsing , which makes parsing extremely fast .	We also show that using our parser improves the performance of a state-of-the-art question answering system .	CL_D14-1107_mn_4	CL_D14-1107_mn_6	CL_D14-1107_mn_4_6
CL	D14-1107	mn	5	6	result	result_means	support	elaboration	secondary	secondary	back	elaboration	Compared to the standard C&C CCG parser , our model is more accurate out-of-domain , is four times faster , has higher coverage , and is greatly simplified .	We also show that using our parser improves the performance of a state-of-the-art question answering system .	CL_D14-1107_mn_5	CL_D14-1107_mn_6	CL_D14-1107_mn_5_6
CL	D14-1107	mn	6	5	result_means	result	elaboration	support	secondary	secondary	forw	elaboration	We also show that using our parser improves the performance of a state-of-the-art question answering system .	Compared to the standard C&C CCG parser , our model is more accurate out-of-domain , is four times faster , has higher coverage , and is greatly simplified .	CL_D14-1107_mn_6	CL_D14-1107_mn_5	CL_D14-1107_mn_5_6
CL	D14-1108	mn	1	2	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	We describe a new dependency parser for English tweets , TWEEBOPARSER .	The parser builds on several contributions : new syntactic annotations for a corpus of tweets ( TWEEBANK ) , with conventions informed by the domain ; adaptations to a statistical parsing algorithm ; and a new approach to exploiting out-of-domain Penn Treebank data .	CL_D14-1108_mn_1	CL_D14-1108_mn_2	CL_D14-1108_mn_1_2
CL	D14-1108	mn	2	1	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	The parser builds on several contributions : new syntactic annotations for a corpus of tweets ( TWEEBANK ) , with conventions informed by the domain ; adaptations to a statistical parsing algorithm ; and a new approach to exploiting out-of-domain Penn Treebank data .	We describe a new dependency parser for English tweets , TWEEBOPARSER .	CL_D14-1108_mn_2	CL_D14-1108_mn_1	CL_D14-1108_mn_1_2
CL	D14-1108	mn	1	3	proposal	result_means	none	support	main	secondary	back	support	We describe a new dependency parser for English tweets , TWEEBOPARSER .	Our experiments show that the parser achieves over 80 % unlabeled attachment accuracy on our new , high-quality test set and measure the benefit of our contributions .	CL_D14-1108_mn_1	CL_D14-1108_mn_3	CL_D14-1108_mn_1_3
CL	D14-1108	mn	3	1	result_means	proposal	support	none	secondary	main	forw	support	Our experiments show that the parser achieves over 80 % unlabeled attachment accuracy on our new , high-quality test set and measure the benefit of our contributions .	We describe a new dependency parser for English tweets , TWEEBOPARSER .	CL_D14-1108_mn_3	CL_D14-1108_mn_1	CL_D14-1108_mn_1_3
CL	D14-1108	mn	1	4	proposal	information_additional	none	info-optional	main	secondary	none	none	We describe a new dependency parser for English tweets , TWEEBOPARSER .	Our dataset and parser can be found at http : //www.ark.cs.cmu.edu/TweetNLP .	CL_D14-1108_mn_1	CL_D14-1108_mn_4	CL_D14-1108_mn_1_4
CL	D14-1108	mn	2	3	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	The parser builds on several contributions : new syntactic annotations for a corpus of tweets ( TWEEBANK ) , with conventions informed by the domain ; adaptations to a statistical parsing algorithm ; and a new approach to exploiting out-of-domain Penn Treebank data .	Our experiments show that the parser achieves over 80 % unlabeled attachment accuracy on our new , high-quality test set and measure the benefit of our contributions .	CL_D14-1108_mn_2	CL_D14-1108_mn_3	CL_D14-1108_mn_2_3
CL	D14-1108	mn	2	4	proposal_implementation	information_additional	elaboration	info-optional	secondary	secondary	none	none	The parser builds on several contributions : new syntactic annotations for a corpus of tweets ( TWEEBANK ) , with conventions informed by the domain ; adaptations to a statistical parsing algorithm ; and a new approach to exploiting out-of-domain Penn Treebank data .	Our dataset and parser can be found at http : //www.ark.cs.cmu.edu/TweetNLP .	CL_D14-1108_mn_2	CL_D14-1108_mn_4	CL_D14-1108_mn_2_4
CL	D14-1108	mn	3	4	result_means	information_additional	support	info-optional	secondary	secondary	back	info-optional	Our experiments show that the parser achieves over 80 % unlabeled attachment accuracy on our new , high-quality test set and measure the benefit of our contributions .	Our dataset and parser can be found at http : //www.ark.cs.cmu.edu/TweetNLP .	CL_D14-1108_mn_3	CL_D14-1108_mn_4	CL_D14-1108_mn_3_4
CL	D14-1108	mn	4	3	information_additional	result_means	info-optional	support	secondary	secondary	forw	info-optional	Our dataset and parser can be found at http : //www.ark.cs.cmu.edu/TweetNLP .	Our experiments show that the parser achieves over 80 % unlabeled attachment accuracy on our new , high-quality test set and measure the benefit of our contributions .	CL_D14-1108_mn_4	CL_D14-1108_mn_3	CL_D14-1108_mn_3_4
CL	D14-1109	mn	1	2	motivation_problem	motivation_background	info-required	support	secondary	secondary	forw	info-required	Dependency parsing with high-order features results in a provably hard decoding problem .	A lot of work has gone into developing powerful optimization methods for solving these combinatorial problems .	CL_D14-1109_mn_1	CL_D14-1109_mn_2	CL_D14-1109_mn_1_2
CL	D14-1109	mn	2	1	motivation_background	motivation_problem	support	info-required	secondary	secondary	back	info-required	A lot of work has gone into developing powerful optimization methods for solving these combinatorial problems .	Dependency parsing with high-order features results in a provably hard decoding problem .	CL_D14-1109_mn_2	CL_D14-1109_mn_1	CL_D14-1109_mn_1_2
CL	D14-1109	mn	1	3	motivation_problem	proposal	info-required	none	secondary	main	none	none	Dependency parsing with high-order features results in a provably hard decoding problem .	In contrast , we explore , analyze , and demonstrate that a substantially simpler randomized greedy inference algorithm already suffices for near optimal parsing : a) we analytically quantify the number of local optima that the greedy method has to overcome in the context of first-order parsing ; b) we show that , as a decoding algorithm , the greedy method surpasses dual decomposition in second-order parsing ; c) we empirically demonstrate that our approach with up to third-order and global features outperforms the state-of-the-art dual decomposition and MCMC sampling methods when evaluated on 14 languages of non-projective CoNLL datasets .	CL_D14-1109_mn_1	CL_D14-1109_mn_3	CL_D14-1109_mn_1_3
CL	D14-1109	mn	2	3	motivation_background	proposal	support	none	secondary	main	forw	support	A lot of work has gone into developing powerful optimization methods for solving these combinatorial problems .	In contrast , we explore , analyze , and demonstrate that a substantially simpler randomized greedy inference algorithm already suffices for near optimal parsing : a) we analytically quantify the number of local optima that the greedy method has to overcome in the context of first-order parsing ; b) we show that , as a decoding algorithm , the greedy method surpasses dual decomposition in second-order parsing ; c) we empirically demonstrate that our approach with up to third-order and global features outperforms the state-of-the-art dual decomposition and MCMC sampling methods when evaluated on 14 languages of non-projective CoNLL datasets .	CL_D14-1109_mn_2	CL_D14-1109_mn_3	CL_D14-1109_mn_2_3
CL	D14-1109	mn	3	2	proposal	motivation_background	none	support	main	secondary	back	support	In contrast , we explore , analyze , and demonstrate that a substantially simpler randomized greedy inference algorithm already suffices for near optimal parsing : a) we analytically quantify the number of local optima that the greedy method has to overcome in the context of first-order parsing ; b) we show that , as a decoding algorithm , the greedy method surpasses dual decomposition in second-order parsing ; c) we empirically demonstrate that our approach with up to third-order and global features outperforms the state-of-the-art dual decomposition and MCMC sampling methods when evaluated on 14 languages of non-projective CoNLL datasets .	A lot of work has gone into developing powerful optimization methods for solving these combinatorial problems .	CL_D14-1109_mn_3	CL_D14-1109_mn_2	CL_D14-1109_mn_2_3
CL	D14-1110	mn	1	2	motivation_background	motivation_problem	info-required	support	secondary	secondary	forw	info-required	Most word representation methods assume that each word owns a single semantic vector .	This is usually problematic because lexical ambiguity is ubiquitous , which is also the problem to be resolved by word sense disambiguation .	CL_D14-1110_mn_1	CL_D14-1110_mn_2	CL_D14-1110_mn_1_2
CL	D14-1110	mn	2	1	motivation_problem	motivation_background	support	info-required	secondary	secondary	back	info-required	This is usually problematic because lexical ambiguity is ubiquitous , which is also the problem to be resolved by word sense disambiguation .	Most word representation methods assume that each word owns a single semantic vector .	CL_D14-1110_mn_2	CL_D14-1110_mn_1	CL_D14-1110_mn_1_2
CL	D14-1110	mn	1	3	motivation_background	proposal	info-required	none	secondary	main	none	none	Most word representation methods assume that each word owns a single semantic vector .	In this paper , we present a unified model for joint word sense representation and disambiguation , which will assign distinct representations for each word sense .	CL_D14-1110_mn_1	CL_D14-1110_mn_3	CL_D14-1110_mn_1_3
CL	D14-1110	mn	1	4	motivation_background	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	Most word representation methods assume that each word owns a single semantic vector .	The basic idea is that both word sense representation ( WSR ) and word sense disambiguation ( WSD ) will benefit from each other : ( 1 ) high-quality WSR will capture rich information about words and senses , which should be helpful for WSD , and ( 2 ) high-quality WSD will provide reliable disambiguated corpora for learning better sense representations .	CL_D14-1110_mn_1	CL_D14-1110_mn_4	CL_D14-1110_mn_1_4
CL	D14-1110	mn	1	5	motivation_background	result	info-required	support	secondary	secondary	none	none	Most word representation methods assume that each word owns a single semantic vector .	Experimental results show that , our model improves the performance of contextual word similarity compared to existing WSR methods , outperforms state-of-the-art supervised methods on domain-specific WSD , and achieves competitive performance on coarse-grained all-words WSD .	CL_D14-1110_mn_1	CL_D14-1110_mn_5	CL_D14-1110_mn_1_5
CL	D14-1110	mn	2	3	motivation_problem	proposal	support	none	secondary	main	forw	support	This is usually problematic because lexical ambiguity is ubiquitous , which is also the problem to be resolved by word sense disambiguation .	In this paper , we present a unified model for joint word sense representation and disambiguation , which will assign distinct representations for each word sense .	CL_D14-1110_mn_2	CL_D14-1110_mn_3	CL_D14-1110_mn_2_3
CL	D14-1110	mn	3	2	proposal	motivation_problem	none	support	main	secondary	back	support	In this paper , we present a unified model for joint word sense representation and disambiguation , which will assign distinct representations for each word sense .	This is usually problematic because lexical ambiguity is ubiquitous , which is also the problem to be resolved by word sense disambiguation .	CL_D14-1110_mn_3	CL_D14-1110_mn_2	CL_D14-1110_mn_2_3
CL	D14-1110	mn	2	4	motivation_problem	proposal_implementation	support	elaboration	secondary	secondary	none	none	This is usually problematic because lexical ambiguity is ubiquitous , which is also the problem to be resolved by word sense disambiguation .	The basic idea is that both word sense representation ( WSR ) and word sense disambiguation ( WSD ) will benefit from each other : ( 1 ) high-quality WSR will capture rich information about words and senses , which should be helpful for WSD , and ( 2 ) high-quality WSD will provide reliable disambiguated corpora for learning better sense representations .	CL_D14-1110_mn_2	CL_D14-1110_mn_4	CL_D14-1110_mn_2_4
CL	D14-1110	mn	2	5	motivation_problem	result	support	support	secondary	secondary	none	none	This is usually problematic because lexical ambiguity is ubiquitous , which is also the problem to be resolved by word sense disambiguation .	Experimental results show that , our model improves the performance of contextual word similarity compared to existing WSR methods , outperforms state-of-the-art supervised methods on domain-specific WSD , and achieves competitive performance on coarse-grained all-words WSD .	CL_D14-1110_mn_2	CL_D14-1110_mn_5	CL_D14-1110_mn_2_5
CL	D14-1110	mn	3	4	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	In this paper , we present a unified model for joint word sense representation and disambiguation , which will assign distinct representations for each word sense .	The basic idea is that both word sense representation ( WSR ) and word sense disambiguation ( WSD ) will benefit from each other : ( 1 ) high-quality WSR will capture rich information about words and senses , which should be helpful for WSD , and ( 2 ) high-quality WSD will provide reliable disambiguated corpora for learning better sense representations .	CL_D14-1110_mn_3	CL_D14-1110_mn_4	CL_D14-1110_mn_3_4
CL	D14-1110	mn	4	3	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	The basic idea is that both word sense representation ( WSR ) and word sense disambiguation ( WSD ) will benefit from each other : ( 1 ) high-quality WSR will capture rich information about words and senses , which should be helpful for WSD , and ( 2 ) high-quality WSD will provide reliable disambiguated corpora for learning better sense representations .	In this paper , we present a unified model for joint word sense representation and disambiguation , which will assign distinct representations for each word sense .	CL_D14-1110_mn_4	CL_D14-1110_mn_3	CL_D14-1110_mn_3_4
CL	D14-1110	mn	3	5	proposal	result	none	support	main	secondary	back	support	In this paper , we present a unified model for joint word sense representation and disambiguation , which will assign distinct representations for each word sense .	Experimental results show that , our model improves the performance of contextual word similarity compared to existing WSR methods , outperforms state-of-the-art supervised methods on domain-specific WSD , and achieves competitive performance on coarse-grained all-words WSD .	CL_D14-1110_mn_3	CL_D14-1110_mn_5	CL_D14-1110_mn_3_5
CL	D14-1110	mn	5	3	result	proposal	support	none	secondary	main	forw	support	Experimental results show that , our model improves the performance of contextual word similarity compared to existing WSR methods , outperforms state-of-the-art supervised methods on domain-specific WSD , and achieves competitive performance on coarse-grained all-words WSD .	In this paper , we present a unified model for joint word sense representation and disambiguation , which will assign distinct representations for each word sense .	CL_D14-1110_mn_5	CL_D14-1110_mn_3	CL_D14-1110_mn_3_5
CL	D14-1110	mn	4	5	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	The basic idea is that both word sense representation ( WSR ) and word sense disambiguation ( WSD ) will benefit from each other : ( 1 ) high-quality WSR will capture rich information about words and senses , which should be helpful for WSD , and ( 2 ) high-quality WSD will provide reliable disambiguated corpora for learning better sense representations .	Experimental results show that , our model improves the performance of contextual word similarity compared to existing WSR methods , outperforms state-of-the-art supervised methods on domain-specific WSD , and achieves competitive performance on coarse-grained all-words WSD .	CL_D14-1110_mn_4	CL_D14-1110_mn_5	CL_D14-1110_mn_4_5
CL	D14-1111	mn	1	2	motivation_background	proposal	support	none	secondary	main	forw	support	Compositional distributional semantics is a subfield of Computational Linguistics which investigates methods for representing the meanings of phrases and sentences .	In this paper , we explore implementations of a framework based on Combinatory Categorial Grammar ( CCG ) , in which words with certain grammatical types have meanings represented by multi-linear maps ( i.e. multi-dimensional arrays , or tensors ) .	CL_D14-1111_mn_1	CL_D14-1111_mn_2	CL_D14-1111_mn_1_2
CL	D14-1111	mn	2	1	proposal	motivation_background	none	support	main	secondary	back	support	In this paper , we explore implementations of a framework based on Combinatory Categorial Grammar ( CCG ) , in which words with certain grammatical types have meanings represented by multi-linear maps ( i.e. multi-dimensional arrays , or tensors ) .	Compositional distributional semantics is a subfield of Computational Linguistics which investigates methods for representing the meanings of phrases and sentences .	CL_D14-1111_mn_2	CL_D14-1111_mn_1	CL_D14-1111_mn_1_2
CL	D14-1111	mn	1	3	motivation_background	motivation_problem	support	support	secondary	secondary	none	none	Compositional distributional semantics is a subfield of Computational Linguistics which investigates methods for representing the meanings of phrases and sentences .	An obstacle to full implemen-tation of the framework is the size of these tensors .	CL_D14-1111_mn_1	CL_D14-1111_mn_3	CL_D14-1111_mn_1_3
CL	D14-1111	mn	1	4	motivation_background	proposal_implementation	support	elaboration	secondary	secondary	none	none	Compositional distributional semantics is a subfield of Computational Linguistics which investigates methods for representing the meanings of phrases and sentences .	We examine the performance of lower dimensional approximations of transitive verb tensors on a sentence plausibility/selectional preference task .	CL_D14-1111_mn_1	CL_D14-1111_mn_4	CL_D14-1111_mn_1_4
CL	D14-1111	mn	1	5	motivation_background	result	support	support	secondary	secondary	none	none	Compositional distributional semantics is a subfield of Computational Linguistics which investigates methods for representing the meanings of phrases and sentences .	We find that the matrices perform as well as , and sometimes even better than , full tensors , allowing a reduction in the number of parameters needed to model the framework .	CL_D14-1111_mn_1	CL_D14-1111_mn_5	CL_D14-1111_mn_1_5
CL	D14-1111	mn	2	3	proposal	motivation_problem	none	support	main	secondary	none	none	In this paper , we explore implementations of a framework based on Combinatory Categorial Grammar ( CCG ) , in which words with certain grammatical types have meanings represented by multi-linear maps ( i.e. multi-dimensional arrays , or tensors ) .	An obstacle to full implemen-tation of the framework is the size of these tensors .	CL_D14-1111_mn_2	CL_D14-1111_mn_3	CL_D14-1111_mn_2_3
CL	D14-1111	mn	2	4	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	In this paper , we explore implementations of a framework based on Combinatory Categorial Grammar ( CCG ) , in which words with certain grammatical types have meanings represented by multi-linear maps ( i.e. multi-dimensional arrays , or tensors ) .	We examine the performance of lower dimensional approximations of transitive verb tensors on a sentence plausibility/selectional preference task .	CL_D14-1111_mn_2	CL_D14-1111_mn_4	CL_D14-1111_mn_2_4
CL	D14-1111	mn	4	2	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	We examine the performance of lower dimensional approximations of transitive verb tensors on a sentence plausibility/selectional preference task .	In this paper , we explore implementations of a framework based on Combinatory Categorial Grammar ( CCG ) , in which words with certain grammatical types have meanings represented by multi-linear maps ( i.e. multi-dimensional arrays , or tensors ) .	CL_D14-1111_mn_4	CL_D14-1111_mn_2	CL_D14-1111_mn_2_4
CL	D14-1111	mn	2	5	proposal	result	none	support	main	secondary	back	support	In this paper , we explore implementations of a framework based on Combinatory Categorial Grammar ( CCG ) , in which words with certain grammatical types have meanings represented by multi-linear maps ( i.e. multi-dimensional arrays , or tensors ) .	We find that the matrices perform as well as , and sometimes even better than , full tensors , allowing a reduction in the number of parameters needed to model the framework .	CL_D14-1111_mn_2	CL_D14-1111_mn_5	CL_D14-1111_mn_2_5
CL	D14-1111	mn	5	2	result	proposal	support	none	secondary	main	forw	support	We find that the matrices perform as well as , and sometimes even better than , full tensors , allowing a reduction in the number of parameters needed to model the framework .	In this paper , we explore implementations of a framework based on Combinatory Categorial Grammar ( CCG ) , in which words with certain grammatical types have meanings represented by multi-linear maps ( i.e. multi-dimensional arrays , or tensors ) .	CL_D14-1111_mn_5	CL_D14-1111_mn_2	CL_D14-1111_mn_2_5
CL	D14-1111	mn	3	4	motivation_problem	proposal_implementation	support	elaboration	secondary	secondary	forw	support	An obstacle to full implemen-tation of the framework is the size of these tensors .	We examine the performance of lower dimensional approximations of transitive verb tensors on a sentence plausibility/selectional preference task .	CL_D14-1111_mn_3	CL_D14-1111_mn_4	CL_D14-1111_mn_3_4
CL	D14-1111	mn	4	3	proposal_implementation	motivation_problem	elaboration	support	secondary	secondary	back	support	We examine the performance of lower dimensional approximations of transitive verb tensors on a sentence plausibility/selectional preference task .	An obstacle to full implemen-tation of the framework is the size of these tensors .	CL_D14-1111_mn_4	CL_D14-1111_mn_3	CL_D14-1111_mn_3_4
CL	D14-1111	mn	3	5	motivation_problem	result	support	support	secondary	secondary	none	none	An obstacle to full implemen-tation of the framework is the size of these tensors .	We find that the matrices perform as well as , and sometimes even better than , full tensors , allowing a reduction in the number of parameters needed to model the framework .	CL_D14-1111_mn_3	CL_D14-1111_mn_5	CL_D14-1111_mn_3_5
CL	D14-1111	mn	4	5	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	We examine the performance of lower dimensional approximations of transitive verb tensors on a sentence plausibility/selectional preference task .	We find that the matrices perform as well as , and sometimes even better than , full tensors , allowing a reduction in the number of parameters needed to model the framework .	CL_D14-1111_mn_4	CL_D14-1111_mn_5	CL_D14-1111_mn_4_5
CL	D14-1112	mn	1	2	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	In this paper we propose a computational method for determining the orthographic similarity between Romanian and related languages .	We account for etymons and cognates and we investigate not only the number of related words , but also their forms , quantifying orthographic similarities .	CL_D14-1112_mn_1	CL_D14-1112_mn_2	CL_D14-1112_mn_1_2
CL	D14-1112	mn	2	1	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	We account for etymons and cognates and we investigate not only the number of related words , but also their forms , quantifying orthographic similarities .	In this paper we propose a computational method for determining the orthographic similarity between Romanian and related languages .	CL_D14-1112_mn_2	CL_D14-1112_mn_1	CL_D14-1112_mn_1_2
CL	D14-1112	mn	1	3	proposal	conclusion	none	support	main	secondary	back	support	In this paper we propose a computational method for determining the orthographic similarity between Romanian and related languages .	The method we propose is adaptable to any language , as far as resources are available .	CL_D14-1112_mn_1	CL_D14-1112_mn_3	CL_D14-1112_mn_1_3
CL	D14-1112	mn	3	1	conclusion	proposal	support	none	secondary	main	forw	support	The method we propose is adaptable to any language , as far as resources are available .	In this paper we propose a computational method for determining the orthographic similarity between Romanian and related languages .	CL_D14-1112_mn_3	CL_D14-1112_mn_1	CL_D14-1112_mn_1_3
CL	D14-1112	mn	2	3	proposal_implementation	conclusion	elaboration	support	secondary	secondary	none	none	We account for etymons and cognates and we investigate not only the number of related words , but also their forms , quantifying orthographic similarities .	The method we propose is adaptable to any language , as far as resources are available .	CL_D14-1112_mn_2	CL_D14-1112_mn_3	CL_D14-1112_mn_2_3
CL	D14-1113	mn	1	2	motivation_background	motivation_problem	info-required	support	secondary	secondary	forw	info-required	There is rising interest in vector-space word embeddings and their use in NLP , especially given recent methods for their fast estimation at very large scale .	Nearly all this work , however , assumes a single vector per word typeignoring polysemy and thus jeopardizing their useful-ness for downstream tasks .	CL_D14-1113_mn_1	CL_D14-1113_mn_2	CL_D14-1113_mn_1_2
CL	D14-1113	mn	2	1	motivation_problem	motivation_background	support	info-required	secondary	secondary	back	info-required	Nearly all this work , however , assumes a single vector per word typeignoring polysemy and thus jeopardizing their useful-ness for downstream tasks .	There is rising interest in vector-space word embeddings and their use in NLP , especially given recent methods for their fast estimation at very large scale .	CL_D14-1113_mn_2	CL_D14-1113_mn_1	CL_D14-1113_mn_1_2
CL	D14-1113	mn	1	3	motivation_background	proposal	info-required	none	secondary	main	none	none	There is rising interest in vector-space word embeddings and their use in NLP , especially given recent methods for their fast estimation at very large scale .	We present an extension to the Skip-gram model that efficiently learns multiple embeddings per word type .	CL_D14-1113_mn_1	CL_D14-1113_mn_3	CL_D14-1113_mn_1_3
CL	D14-1113	mn	1	4	motivation_background	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	There is rising interest in vector-space word embeddings and their use in NLP , especially given recent methods for their fast estimation at very large scale .	It differs from recent related work by jointly performing word sense discrimination and embedding learning , by non-parametrically estimating the number of senses per word type , and by its efficiency and scalability .	CL_D14-1113_mn_1	CL_D14-1113_mn_4	CL_D14-1113_mn_1_4
CL	D14-1113	mn	1	5	motivation_background	result_means	info-required	support	secondary	secondary	none	none	There is rising interest in vector-space word embeddings and their use in NLP , especially given recent methods for their fast estimation at very large scale .	We present new state-of-the-art results in the word similarity in context task and demonstrate its scalability by training with one machine on a corpus of nearly 1 billion tokens in less than 6 hours .	CL_D14-1113_mn_1	CL_D14-1113_mn_5	CL_D14-1113_mn_1_5
CL	D14-1113	mn	2	3	motivation_problem	proposal	support	none	secondary	main	forw	support	Nearly all this work , however , assumes a single vector per word typeignoring polysemy and thus jeopardizing their useful-ness for downstream tasks .	We present an extension to the Skip-gram model that efficiently learns multiple embeddings per word type .	CL_D14-1113_mn_2	CL_D14-1113_mn_3	CL_D14-1113_mn_2_3
CL	D14-1113	mn	3	2	proposal	motivation_problem	none	support	main	secondary	back	support	We present an extension to the Skip-gram model that efficiently learns multiple embeddings per word type .	Nearly all this work , however , assumes a single vector per word typeignoring polysemy and thus jeopardizing their useful-ness for downstream tasks .	CL_D14-1113_mn_3	CL_D14-1113_mn_2	CL_D14-1113_mn_2_3
CL	D14-1113	mn	2	4	motivation_problem	proposal_implementation	support	elaboration	secondary	secondary	none	none	Nearly all this work , however , assumes a single vector per word typeignoring polysemy and thus jeopardizing their useful-ness for downstream tasks .	It differs from recent related work by jointly performing word sense discrimination and embedding learning , by non-parametrically estimating the number of senses per word type , and by its efficiency and scalability .	CL_D14-1113_mn_2	CL_D14-1113_mn_4	CL_D14-1113_mn_2_4
CL	D14-1113	mn	2	5	motivation_problem	result_means	support	support	secondary	secondary	none	none	Nearly all this work , however , assumes a single vector per word typeignoring polysemy and thus jeopardizing their useful-ness for downstream tasks .	We present new state-of-the-art results in the word similarity in context task and demonstrate its scalability by training with one machine on a corpus of nearly 1 billion tokens in less than 6 hours .	CL_D14-1113_mn_2	CL_D14-1113_mn_5	CL_D14-1113_mn_2_5
CL	D14-1113	mn	3	4	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	We present an extension to the Skip-gram model that efficiently learns multiple embeddings per word type .	It differs from recent related work by jointly performing word sense discrimination and embedding learning , by non-parametrically estimating the number of senses per word type , and by its efficiency and scalability .	CL_D14-1113_mn_3	CL_D14-1113_mn_4	CL_D14-1113_mn_3_4
CL	D14-1113	mn	4	3	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	It differs from recent related work by jointly performing word sense discrimination and embedding learning , by non-parametrically estimating the number of senses per word type , and by its efficiency and scalability .	We present an extension to the Skip-gram model that efficiently learns multiple embeddings per word type .	CL_D14-1113_mn_4	CL_D14-1113_mn_3	CL_D14-1113_mn_3_4
CL	D14-1113	mn	3	5	proposal	result_means	none	support	main	secondary	back	support	We present an extension to the Skip-gram model that efficiently learns multiple embeddings per word type .	We present new state-of-the-art results in the word similarity in context task and demonstrate its scalability by training with one machine on a corpus of nearly 1 billion tokens in less than 6 hours .	CL_D14-1113_mn_3	CL_D14-1113_mn_5	CL_D14-1113_mn_3_5
CL	D14-1113	mn	5	3	result_means	proposal	support	none	secondary	main	forw	support	We present new state-of-the-art results in the word similarity in context task and demonstrate its scalability by training with one machine on a corpus of nearly 1 billion tokens in less than 6 hours .	We present an extension to the Skip-gram model that efficiently learns multiple embeddings per word type .	CL_D14-1113_mn_5	CL_D14-1113_mn_3	CL_D14-1113_mn_3_5
CL	D14-1113	mn	4	5	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	It differs from recent related work by jointly performing word sense discrimination and embedding learning , by non-parametrically estimating the number of senses per word type , and by its efficiency and scalability .	We present new state-of-the-art results in the word similarity in context task and demonstrate its scalability by training with one machine on a corpus of nearly 1 billion tokens in less than 6 hours .	CL_D14-1113_mn_4	CL_D14-1113_mn_5	CL_D14-1113_mn_4_5
CL	D14-1114	mn	1	2	motivation_background	motivation_problem	info-required	support	secondary	secondary	forw	info-required	Knowledge graphs are recently used for enriching query representations in an entity-aware way for the rich facts organized around entities in it .	However , few of the methods pay attention to non-entity words and clicked websites in queries , which also help conveying user intent .	CL_D14-1114_mn_1	CL_D14-1114_mn_2	CL_D14-1114_mn_1_2
CL	D14-1114	mn	2	1	motivation_problem	motivation_background	support	info-required	secondary	secondary	back	info-required	However , few of the methods pay attention to non-entity words and clicked websites in queries , which also help conveying user intent .	Knowledge graphs are recently used for enriching query representations in an entity-aware way for the rich facts organized around entities in it .	CL_D14-1114_mn_2	CL_D14-1114_mn_1	CL_D14-1114_mn_1_2
CL	D14-1114	mn	1	3	motivation_background	proposal	info-required	none	secondary	main	none	none	Knowledge graphs are recently used for enriching query representations in an entity-aware way for the rich facts organized around entities in it .	In this paper , we tackle the problem of intent understanding with innovatively representing entity words , refiners and clicked urls as intent topics in a unified knowledge graph based framework , in a way to exploit and expand knowledge graph which we call "tailor" .	CL_D14-1114_mn_1	CL_D14-1114_mn_3	CL_D14-1114_mn_1_3
CL	D14-1114	mn	1	4	motivation_background	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	Knowledge graphs are recently used for enriching query representations in an entity-aware way for the rich facts organized around entities in it .	We collaboratively exploit global knowledge in knowledge graphs and local contexts in query log to initialize intent representation , then propagate the enriched features in a graph consisting of intent topics using an unsupervised algorithm .	CL_D14-1114_mn_1	CL_D14-1114_mn_4	CL_D14-1114_mn_1_4
CL	D14-1114	mn	1	5	motivation_background	conclusion	info-required	support	secondary	secondary	none	none	Knowledge graphs are recently used for enriching query representations in an entity-aware way for the rich facts organized around entities in it .	The experiments prove intent topics with knowledge graph enriched features significantly enhance intent understanding .	CL_D14-1114_mn_1	CL_D14-1114_mn_5	CL_D14-1114_mn_1_5
CL	D14-1114	mn	2	3	motivation_problem	proposal	support	none	secondary	main	forw	support	However , few of the methods pay attention to non-entity words and clicked websites in queries , which also help conveying user intent .	In this paper , we tackle the problem of intent understanding with innovatively representing entity words , refiners and clicked urls as intent topics in a unified knowledge graph based framework , in a way to exploit and expand knowledge graph which we call "tailor" .	CL_D14-1114_mn_2	CL_D14-1114_mn_3	CL_D14-1114_mn_2_3
CL	D14-1114	mn	3	2	proposal	motivation_problem	none	support	main	secondary	back	support	In this paper , we tackle the problem of intent understanding with innovatively representing entity words , refiners and clicked urls as intent topics in a unified knowledge graph based framework , in a way to exploit and expand knowledge graph which we call "tailor" .	However , few of the methods pay attention to non-entity words and clicked websites in queries , which also help conveying user intent .	CL_D14-1114_mn_3	CL_D14-1114_mn_2	CL_D14-1114_mn_2_3
CL	D14-1114	mn	2	4	motivation_problem	proposal_implementation	support	elaboration	secondary	secondary	none	none	However , few of the methods pay attention to non-entity words and clicked websites in queries , which also help conveying user intent .	We collaboratively exploit global knowledge in knowledge graphs and local contexts in query log to initialize intent representation , then propagate the enriched features in a graph consisting of intent topics using an unsupervised algorithm .	CL_D14-1114_mn_2	CL_D14-1114_mn_4	CL_D14-1114_mn_2_4
CL	D14-1114	mn	2	5	motivation_problem	conclusion	support	support	secondary	secondary	none	none	However , few of the methods pay attention to non-entity words and clicked websites in queries , which also help conveying user intent .	The experiments prove intent topics with knowledge graph enriched features significantly enhance intent understanding .	CL_D14-1114_mn_2	CL_D14-1114_mn_5	CL_D14-1114_mn_2_5
CL	D14-1114	mn	3	4	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	In this paper , we tackle the problem of intent understanding with innovatively representing entity words , refiners and clicked urls as intent topics in a unified knowledge graph based framework , in a way to exploit and expand knowledge graph which we call "tailor" .	We collaboratively exploit global knowledge in knowledge graphs and local contexts in query log to initialize intent representation , then propagate the enriched features in a graph consisting of intent topics using an unsupervised algorithm .	CL_D14-1114_mn_3	CL_D14-1114_mn_4	CL_D14-1114_mn_3_4
CL	D14-1114	mn	4	3	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	We collaboratively exploit global knowledge in knowledge graphs and local contexts in query log to initialize intent representation , then propagate the enriched features in a graph consisting of intent topics using an unsupervised algorithm .	In this paper , we tackle the problem of intent understanding with innovatively representing entity words , refiners and clicked urls as intent topics in a unified knowledge graph based framework , in a way to exploit and expand knowledge graph which we call "tailor" .	CL_D14-1114_mn_4	CL_D14-1114_mn_3	CL_D14-1114_mn_3_4
CL	D14-1114	mn	3	5	proposal	conclusion	none	support	main	secondary	back	support	In this paper , we tackle the problem of intent understanding with innovatively representing entity words , refiners and clicked urls as intent topics in a unified knowledge graph based framework , in a way to exploit and expand knowledge graph which we call "tailor" .	The experiments prove intent topics with knowledge graph enriched features significantly enhance intent understanding .	CL_D14-1114_mn_3	CL_D14-1114_mn_5	CL_D14-1114_mn_3_5
CL	D14-1114	mn	5	3	conclusion	proposal	support	none	secondary	main	forw	support	The experiments prove intent topics with knowledge graph enriched features significantly enhance intent understanding .	In this paper , we tackle the problem of intent understanding with innovatively representing entity words , refiners and clicked urls as intent topics in a unified knowledge graph based framework , in a way to exploit and expand knowledge graph which we call "tailor" .	CL_D14-1114_mn_5	CL_D14-1114_mn_3	CL_D14-1114_mn_3_5
CL	D14-1114	mn	4	5	proposal_implementation	conclusion	elaboration	support	secondary	secondary	none	none	We collaboratively exploit global knowledge in knowledge graphs and local contexts in query log to initialize intent representation , then propagate the enriched features in a graph consisting of intent topics using an unsupervised algorithm .	The experiments prove intent topics with knowledge graph enriched features significantly enhance intent understanding .	CL_D14-1114_mn_4	CL_D14-1114_mn_5	CL_D14-1114_mn_4_5
CL	D14-1115	mn	1	2	motivation_background	proposal	support	none	secondary	main	forw	support	The role of Web search queries has been demonstrated in the extraction of attributes of instances and classes , or of sets of related instances and their class labels .	This paper explores the acquisition of open-domain commonsense knowledge , usually available as factual knowledge , from Web search queries .	CL_D14-1115_mn_1	CL_D14-1115_mn_2	CL_D14-1115_mn_1_2
CL	D14-1115	mn	2	1	proposal	motivation_background	none	support	main	secondary	back	support	This paper explores the acquisition of open-domain commonsense knowledge , usually available as factual knowledge , from Web search queries .	The role of Web search queries has been demonstrated in the extraction of attributes of instances and classes , or of sets of related instances and their class labels .	CL_D14-1115_mn_2	CL_D14-1115_mn_1	CL_D14-1115_mn_1_2
CL	D14-1115	mn	1	3	motivation_background	proposal_implementation	support	elaboration	secondary	secondary	none	none	The role of Web search queries has been demonstrated in the extraction of attributes of instances and classes , or of sets of related instances and their class labels .	Similarly to previous work in open-domain information extraction , knowledge extracted from text - in this case , from queries - takes the form of lexicalized assertions associated with open-domain classes .	CL_D14-1115_mn_1	CL_D14-1115_mn_3	CL_D14-1115_mn_1_3
CL	D14-1115	mn	1	4	motivation_background	result	support	support	secondary	secondary	none	none	The role of Web search queries has been demonstrated in the extraction of attributes of instances and classes , or of sets of related instances and their class labels .	Experimental results indicate that facts extracted from queries complement , and have competitive accuracy levels relative to , facts extracted from Web documents by previous methods .	CL_D14-1115_mn_1	CL_D14-1115_mn_4	CL_D14-1115_mn_1_4
CL	D14-1115	mn	2	3	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	This paper explores the acquisition of open-domain commonsense knowledge , usually available as factual knowledge , from Web search queries .	Similarly to previous work in open-domain information extraction , knowledge extracted from text - in this case , from queries - takes the form of lexicalized assertions associated with open-domain classes .	CL_D14-1115_mn_2	CL_D14-1115_mn_3	CL_D14-1115_mn_2_3
CL	D14-1115	mn	3	2	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	Similarly to previous work in open-domain information extraction , knowledge extracted from text - in this case , from queries - takes the form of lexicalized assertions associated with open-domain classes .	This paper explores the acquisition of open-domain commonsense knowledge , usually available as factual knowledge , from Web search queries .	CL_D14-1115_mn_3	CL_D14-1115_mn_2	CL_D14-1115_mn_2_3
CL	D14-1115	mn	2	4	proposal	result	none	support	main	secondary	back	support	This paper explores the acquisition of open-domain commonsense knowledge , usually available as factual knowledge , from Web search queries .	Experimental results indicate that facts extracted from queries complement , and have competitive accuracy levels relative to , facts extracted from Web documents by previous methods .	CL_D14-1115_mn_2	CL_D14-1115_mn_4	CL_D14-1115_mn_2_4
CL	D14-1115	mn	4	2	result	proposal	support	none	secondary	main	forw	support	Experimental results indicate that facts extracted from queries complement , and have competitive accuracy levels relative to , facts extracted from Web documents by previous methods .	This paper explores the acquisition of open-domain commonsense knowledge , usually available as factual knowledge , from Web search queries .	CL_D14-1115_mn_4	CL_D14-1115_mn_2	CL_D14-1115_mn_2_4
CL	D14-1115	mn	3	4	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	Similarly to previous work in open-domain information extraction , knowledge extracted from text - in this case , from queries - takes the form of lexicalized assertions associated with open-domain classes .	Experimental results indicate that facts extracted from queries complement , and have competitive accuracy levels relative to , facts extracted from Web documents by previous methods .	CL_D14-1115_mn_3	CL_D14-1115_mn_4	CL_D14-1115_mn_3_4
CL	D14-1116	mn	1	2	motivation_background	motivation_background	info-required	support	secondary	secondary	forw	info-required	Question Answering over Linked Data ( QALD ) aims to evaluate a question answering system over structured data , the key objective of which is to translate questions posed using natural language into structured queries .	This technique can help common users to directly access open-structured knowledge on the Web and , accordingly , has attracted much attention .	CL_D14-1116_mn_1	CL_D14-1116_mn_2	CL_D14-1116_mn_1_2
CL	D14-1116	mn	2	1	motivation_background	motivation_background	support	info-required	secondary	secondary	back	info-required	This technique can help common users to directly access open-structured knowledge on the Web and , accordingly , has attracted much attention .	Question Answering over Linked Data ( QALD ) aims to evaluate a question answering system over structured data , the key objective of which is to translate questions posed using natural language into structured queries .	CL_D14-1116_mn_2	CL_D14-1116_mn_1	CL_D14-1116_mn_1_2
CL	D14-1116	mn	1	3	motivation_background	proposal	info-required	elaboration	secondary	secondary	none	none	Question Answering over Linked Data ( QALD ) aims to evaluate a question answering system over structured data , the key objective of which is to translate questions posed using natural language into structured queries .	To this end , we propose a novel method using first-order logic .	CL_D14-1116_mn_1	CL_D14-1116_mn_3	CL_D14-1116_mn_1_3
CL	D14-1116	mn	1	4	motivation_background	proposal	info-required	none	secondary	main	none	none	Question Answering over Linked Data ( QALD ) aims to evaluate a question answering system over structured data , the key objective of which is to translate questions posed using natural language into structured queries .	We formulate the knowledge for resolving the ambiguities in the main three steps of QALD ( phrase detection , phrase-to-semantic-item mapping and semantic item grouping ) as first-order logic clauses in a Markov Logic Network .	CL_D14-1116_mn_1	CL_D14-1116_mn_4	CL_D14-1116_mn_1_4
CL	D14-1116	mn	1	5	motivation_background	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	Question Answering over Linked Data ( QALD ) aims to evaluate a question answering system over structured data , the key objective of which is to translate questions posed using natural language into structured queries .	All clauses can then produce interacted effects in a unified framework and can jointly resolve all ambiguities .	CL_D14-1116_mn_1	CL_D14-1116_mn_5	CL_D14-1116_mn_1_5
CL	D14-1116	mn	1	6	motivation_background	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	Question Answering over Linked Data ( QALD ) aims to evaluate a question answering system over structured data , the key objective of which is to translate questions posed using natural language into structured queries .	Moreover , our method adopts a pattern-learning strategy for semantic item grouping .	CL_D14-1116_mn_1	CL_D14-1116_mn_6	CL_D14-1116_mn_1_6
CL	D14-1116	mn	1	7	motivation_background	conclusion	info-required	support	secondary	secondary	none	none	Question Answering over Linked Data ( QALD ) aims to evaluate a question answering system over structured data , the key objective of which is to translate questions posed using natural language into structured queries .	In this way , our method can cover more text expressions and answer more questions than previous methods using manually designed patterns .	CL_D14-1116_mn_1	CL_D14-1116_mn_7	CL_D14-1116_mn_1_7
CL	D14-1116	mn	1	8	motivation_background	result	info-required	support	secondary	secondary	none	none	Question Answering over Linked Data ( QALD ) aims to evaluate a question answering system over structured data , the key objective of which is to translate questions posed using natural language into structured queries .	The experimental results using open benchmarks demonstrate the effectiveness of the proposed method .	CL_D14-1116_mn_1	CL_D14-1116_mn_8	CL_D14-1116_mn_1_8
CL	D14-1116	mn	2	3	motivation_background	proposal	support	elaboration	secondary	secondary	forw	support	This technique can help common users to directly access open-structured knowledge on the Web and , accordingly , has attracted much attention .	To this end , we propose a novel method using first-order logic .	CL_D14-1116_mn_2	CL_D14-1116_mn_3	CL_D14-1116_mn_2_3
CL	D14-1116	mn	3	2	proposal	motivation_background	elaboration	support	secondary	secondary	back	support	To this end , we propose a novel method using first-order logic .	This technique can help common users to directly access open-structured knowledge on the Web and , accordingly , has attracted much attention .	CL_D14-1116_mn_3	CL_D14-1116_mn_2	CL_D14-1116_mn_2_3
CL	D14-1116	mn	2	4	motivation_background	proposal	support	none	secondary	main	none	none	This technique can help common users to directly access open-structured knowledge on the Web and , accordingly , has attracted much attention .	We formulate the knowledge for resolving the ambiguities in the main three steps of QALD ( phrase detection , phrase-to-semantic-item mapping and semantic item grouping ) as first-order logic clauses in a Markov Logic Network .	CL_D14-1116_mn_2	CL_D14-1116_mn_4	CL_D14-1116_mn_2_4
CL	D14-1116	mn	2	5	motivation_background	proposal_implementation	support	elaboration	secondary	secondary	none	none	This technique can help common users to directly access open-structured knowledge on the Web and , accordingly , has attracted much attention .	All clauses can then produce interacted effects in a unified framework and can jointly resolve all ambiguities .	CL_D14-1116_mn_2	CL_D14-1116_mn_5	CL_D14-1116_mn_2_5
CL	D14-1116	mn	2	6	motivation_background	proposal_implementation	support	elaboration	secondary	secondary	none	none	This technique can help common users to directly access open-structured knowledge on the Web and , accordingly , has attracted much attention .	Moreover , our method adopts a pattern-learning strategy for semantic item grouping .	CL_D14-1116_mn_2	CL_D14-1116_mn_6	CL_D14-1116_mn_2_6
CL	D14-1116	mn	2	7	motivation_background	conclusion	support	support	secondary	secondary	none	none	This technique can help common users to directly access open-structured knowledge on the Web and , accordingly , has attracted much attention .	In this way , our method can cover more text expressions and answer more questions than previous methods using manually designed patterns .	CL_D14-1116_mn_2	CL_D14-1116_mn_7	CL_D14-1116_mn_2_7
CL	D14-1116	mn	2	8	motivation_background	result	support	support	secondary	secondary	none	none	This technique can help common users to directly access open-structured knowledge on the Web and , accordingly , has attracted much attention .	The experimental results using open benchmarks demonstrate the effectiveness of the proposed method .	CL_D14-1116_mn_2	CL_D14-1116_mn_8	CL_D14-1116_mn_2_8
CL	D14-1116	mn	3	4	proposal	proposal	elaboration	none	secondary	main	forw	elaboration	To this end , we propose a novel method using first-order logic .	We formulate the knowledge for resolving the ambiguities in the main three steps of QALD ( phrase detection , phrase-to-semantic-item mapping and semantic item grouping ) as first-order logic clauses in a Markov Logic Network .	CL_D14-1116_mn_3	CL_D14-1116_mn_4	CL_D14-1116_mn_3_4
CL	D14-1116	mn	4	3	proposal	proposal	none	elaboration	main	secondary	back	elaboration	We formulate the knowledge for resolving the ambiguities in the main three steps of QALD ( phrase detection , phrase-to-semantic-item mapping and semantic item grouping ) as first-order logic clauses in a Markov Logic Network .	To this end , we propose a novel method using first-order logic .	CL_D14-1116_mn_4	CL_D14-1116_mn_3	CL_D14-1116_mn_3_4
CL	D14-1116	mn	3	5	proposal	proposal_implementation	elaboration	elaboration	secondary	secondary	none	none	To this end , we propose a novel method using first-order logic .	All clauses can then produce interacted effects in a unified framework and can jointly resolve all ambiguities .	CL_D14-1116_mn_3	CL_D14-1116_mn_5	CL_D14-1116_mn_3_5
CL	D14-1116	mn	3	6	proposal	proposal_implementation	elaboration	elaboration	secondary	secondary	none	none	To this end , we propose a novel method using first-order logic .	Moreover , our method adopts a pattern-learning strategy for semantic item grouping .	CL_D14-1116_mn_3	CL_D14-1116_mn_6	CL_D14-1116_mn_3_6
CL	D14-1116	mn	3	7	proposal	conclusion	elaboration	support	secondary	secondary	none	none	To this end , we propose a novel method using first-order logic .	In this way , our method can cover more text expressions and answer more questions than previous methods using manually designed patterns .	CL_D14-1116_mn_3	CL_D14-1116_mn_7	CL_D14-1116_mn_3_7
CL	D14-1116	mn	3	8	proposal	result	elaboration	support	secondary	secondary	none	none	To this end , we propose a novel method using first-order logic .	The experimental results using open benchmarks demonstrate the effectiveness of the proposed method .	CL_D14-1116_mn_3	CL_D14-1116_mn_8	CL_D14-1116_mn_3_8
CL	D14-1116	mn	4	5	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	We formulate the knowledge for resolving the ambiguities in the main three steps of QALD ( phrase detection , phrase-to-semantic-item mapping and semantic item grouping ) as first-order logic clauses in a Markov Logic Network .	All clauses can then produce interacted effects in a unified framework and can jointly resolve all ambiguities .	CL_D14-1116_mn_4	CL_D14-1116_mn_5	CL_D14-1116_mn_4_5
CL	D14-1116	mn	5	4	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	All clauses can then produce interacted effects in a unified framework and can jointly resolve all ambiguities .	We formulate the knowledge for resolving the ambiguities in the main three steps of QALD ( phrase detection , phrase-to-semantic-item mapping and semantic item grouping ) as first-order logic clauses in a Markov Logic Network .	CL_D14-1116_mn_5	CL_D14-1116_mn_4	CL_D14-1116_mn_4_5
CL	D14-1116	mn	4	6	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	We formulate the knowledge for resolving the ambiguities in the main three steps of QALD ( phrase detection , phrase-to-semantic-item mapping and semantic item grouping ) as first-order logic clauses in a Markov Logic Network .	Moreover , our method adopts a pattern-learning strategy for semantic item grouping .	CL_D14-1116_mn_4	CL_D14-1116_mn_6	CL_D14-1116_mn_4_6
CL	D14-1116	mn	4	7	proposal	conclusion	none	support	main	secondary	back	support	We formulate the knowledge for resolving the ambiguities in the main three steps of QALD ( phrase detection , phrase-to-semantic-item mapping and semantic item grouping ) as first-order logic clauses in a Markov Logic Network .	In this way , our method can cover more text expressions and answer more questions than previous methods using manually designed patterns .	CL_D14-1116_mn_4	CL_D14-1116_mn_7	CL_D14-1116_mn_4_7
CL	D14-1116	mn	7	4	conclusion	proposal	support	none	secondary	main	forw	support	In this way , our method can cover more text expressions and answer more questions than previous methods using manually designed patterns .	We formulate the knowledge for resolving the ambiguities in the main three steps of QALD ( phrase detection , phrase-to-semantic-item mapping and semantic item grouping ) as first-order logic clauses in a Markov Logic Network .	CL_D14-1116_mn_7	CL_D14-1116_mn_4	CL_D14-1116_mn_4_7
CL	D14-1116	mn	4	8	proposal	result	none	support	main	secondary	none	none	We formulate the knowledge for resolving the ambiguities in the main three steps of QALD ( phrase detection , phrase-to-semantic-item mapping and semantic item grouping ) as first-order logic clauses in a Markov Logic Network .	The experimental results using open benchmarks demonstrate the effectiveness of the proposed method .	CL_D14-1116_mn_4	CL_D14-1116_mn_8	CL_D14-1116_mn_4_8
CL	D14-1116	mn	5	6	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	All clauses can then produce interacted effects in a unified framework and can jointly resolve all ambiguities .	Moreover , our method adopts a pattern-learning strategy for semantic item grouping .	CL_D14-1116_mn_5	CL_D14-1116_mn_6	CL_D14-1116_mn_5_6
CL	D14-1116	mn	6	5	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	forw	elaboration	Moreover , our method adopts a pattern-learning strategy for semantic item grouping .	All clauses can then produce interacted effects in a unified framework and can jointly resolve all ambiguities .	CL_D14-1116_mn_6	CL_D14-1116_mn_5	CL_D14-1116_mn_5_6
CL	D14-1116	mn	5	7	proposal_implementation	conclusion	elaboration	support	secondary	secondary	none	none	All clauses can then produce interacted effects in a unified framework and can jointly resolve all ambiguities .	In this way , our method can cover more text expressions and answer more questions than previous methods using manually designed patterns .	CL_D14-1116_mn_5	CL_D14-1116_mn_7	CL_D14-1116_mn_5_7
CL	D14-1116	mn	5	8	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	All clauses can then produce interacted effects in a unified framework and can jointly resolve all ambiguities .	The experimental results using open benchmarks demonstrate the effectiveness of the proposed method .	CL_D14-1116_mn_5	CL_D14-1116_mn_8	CL_D14-1116_mn_5_8
CL	D14-1116	mn	6	7	proposal_implementation	conclusion	elaboration	support	secondary	secondary	none	none	Moreover , our method adopts a pattern-learning strategy for semantic item grouping .	In this way , our method can cover more text expressions and answer more questions than previous methods using manually designed patterns .	CL_D14-1116_mn_6	CL_D14-1116_mn_7	CL_D14-1116_mn_6_7
CL	D14-1116	mn	6	8	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	Moreover , our method adopts a pattern-learning strategy for semantic item grouping .	The experimental results using open benchmarks demonstrate the effectiveness of the proposed method .	CL_D14-1116_mn_6	CL_D14-1116_mn_8	CL_D14-1116_mn_6_8
CL	D14-1116	mn	7	8	conclusion	result	support	support	secondary	secondary	back	support	In this way , our method can cover more text expressions and answer more questions than previous methods using manually designed patterns .	The experimental results using open benchmarks demonstrate the effectiveness of the proposed method .	CL_D14-1116_mn_7	CL_D14-1116_mn_8	CL_D14-1116_mn_7_8
CL	D14-1116	mn	8	7	result	conclusion	support	support	secondary	secondary	forw	support	The experimental results using open benchmarks demonstrate the effectiveness of the proposed method .	In this way , our method can cover more text expressions and answer more questions than previous methods using manually designed patterns .	CL_D14-1116_mn_8	CL_D14-1116_mn_7	CL_D14-1116_mn_7_8
CL	D14-1117	mn	1	2	motivation_background	proposal	support	elaboration	secondary	secondary	forw	support	Much recent work focuses on formal interpretation of natural question utterances , with the goal of executing the resulting structured queries on knowledge graphs ( KGs ) such as Freebase .	Here we address two limitations of this approach when applied to open-domain , entity-oriented Web queries .	CL_D14-1117_mn_1	CL_D14-1117_mn_2	CL_D14-1117_mn_1_2
CL	D14-1117	mn	2	1	proposal	motivation_background	elaboration	support	secondary	secondary	back	support	Here we address two limitations of this approach when applied to open-domain , entity-oriented Web queries .	Much recent work focuses on formal interpretation of natural question utterances , with the goal of executing the resulting structured queries on knowledge graphs ( KGs ) such as Freebase .	CL_D14-1117_mn_2	CL_D14-1117_mn_1	CL_D14-1117_mn_1_2
CL	D14-1117	mn	1	3	motivation_background	motivation_problem	support	support	secondary	secondary	none	none	Much recent work focuses on formal interpretation of natural question utterances , with the goal of executing the resulting structured queries on knowledge graphs ( KGs ) such as Freebase .	First , Web queries are rarely well-formed questions .	CL_D14-1117_mn_1	CL_D14-1117_mn_3	CL_D14-1117_mn_1_3
CL	D14-1117	mn	1	4	motivation_background	motivation_problem	support	info-required	secondary	secondary	none	none	Much recent work focuses on formal interpretation of natural question utterances , with the goal of executing the resulting structured queries on knowledge graphs ( KGs ) such as Freebase .	They are "telegraphic" , with missing verbs , prepositions , clauses , case and phrase clues .	CL_D14-1117_mn_1	CL_D14-1117_mn_4	CL_D14-1117_mn_1_4
CL	D14-1117	mn	1	5	motivation_background	motivation_problem	support	elaboration	secondary	secondary	none	none	Much recent work focuses on formal interpretation of natural question utterances , with the goal of executing the resulting structured queries on knowledge graphs ( KGs ) such as Freebase .	Second , the KG is always incomplete , unable to directly answer many queries .	CL_D14-1117_mn_1	CL_D14-1117_mn_5	CL_D14-1117_mn_1_5
CL	D14-1117	mn	1	6	motivation_background	proposal	support	none	secondary	main	none	none	Much recent work focuses on formal interpretation of natural question utterances , with the goal of executing the resulting structured queries on knowledge graphs ( KGs ) such as Freebase .	We propose a novel technique to segment a telegraphic query and assign a coarse-grained purpose to each segment : a base entity e1 , a relation type r , a target entity type t2 , and contextual words s .	CL_D14-1117_mn_1	CL_D14-1117_mn_6	CL_D14-1117_mn_1_6
CL	D14-1117	mn	1	7	motivation_background	proposal_implementation	support	elaboration	secondary	secondary	none	none	Much recent work focuses on formal interpretation of natural question utterances , with the goal of executing the resulting structured queries on knowledge graphs ( KGs ) such as Freebase .	The query seeks entity e2  t2 where r ( e1 , e2 ) holds , further evidenced by schema-agnostic words s .	CL_D14-1117_mn_1	CL_D14-1117_mn_7	CL_D14-1117_mn_1_7
CL	D14-1117	mn	1	8	motivation_background	proposal_implementation	support	elaboration	secondary	secondary	none	none	Much recent work focuses on formal interpretation of natural question utterances , with the goal of executing the resulting structured queries on knowledge graphs ( KGs ) such as Freebase .	Query segmentation is integrated with the KG and an unstructured corpus where mentions of entities have been linked to the KG .	CL_D14-1117_mn_1	CL_D14-1117_mn_8	CL_D14-1117_mn_1_8
CL	D14-1117	mn	1	9	motivation_background	proposal_implementation	support	info-optional	secondary	secondary	none	none	Much recent work focuses on formal interpretation of natural question utterances , with the goal of executing the resulting structured queries on knowledge graphs ( KGs ) such as Freebase .	We do not trust the best or any specific query segmentation .	CL_D14-1117_mn_1	CL_D14-1117_mn_9	CL_D14-1117_mn_1_9
CL	D14-1117	mn	1	10	motivation_background	proposal_implementation	support	elaboration	secondary	secondary	none	none	Much recent work focuses on formal interpretation of natural question utterances , with the goal of executing the resulting structured queries on knowledge graphs ( KGs ) such as Freebase .	Instead , evidence in favor of candidate e2s are aggregated across several segmentations .	CL_D14-1117_mn_1	CL_D14-1117_mn_10	CL_D14-1117_mn_1_10
CL	D14-1117	mn	1	11	motivation_background	result_means	support	support	secondary	secondary	none	none	Much recent work focuses on formal interpretation of natural question utterances , with the goal of executing the resulting structured queries on knowledge graphs ( KGs ) such as Freebase .	Extensive experiments on the ClueWeb corpus and parts of Freebase as our KG , using over a thousand telegraphic queries adapted from TREC , INEX , and WebQuestions , show the efficacy of our approach .	CL_D14-1117_mn_1	CL_D14-1117_mn_11	CL_D14-1117_mn_1_11
CL	D14-1117	mn	1	12	motivation_background	observation	support	support	secondary	secondary	none	none	Much recent work focuses on formal interpretation of natural question utterances , with the goal of executing the resulting structured queries on knowledge graphs ( KGs ) such as Freebase .	For one benchmark , MAP improves from 0.2-0.29 ( competitive baselines ) to 0.42 ( our system ) .	CL_D14-1117_mn_1	CL_D14-1117_mn_12	CL_D14-1117_mn_1_12
CL	D14-1117	mn	1	13	motivation_background	observation	support	elaboration	secondary	secondary	none	none	Much recent work focuses on formal interpretation of natural question utterances , with the goal of executing the resulting structured queries on knowledge graphs ( KGs ) such as Freebase .	NDCG @ 10 improves from 0.29-0.36 to 0.54 .	CL_D14-1117_mn_1	CL_D14-1117_mn_13	CL_D14-1117_mn_1_13
CL	D14-1117	mn	2	3	proposal	motivation_problem	elaboration	support	secondary	secondary	back	support	Here we address two limitations of this approach when applied to open-domain , entity-oriented Web queries .	First , Web queries are rarely well-formed questions .	CL_D14-1117_mn_2	CL_D14-1117_mn_3	CL_D14-1117_mn_2_3
CL	D14-1117	mn	3	2	motivation_problem	proposal	support	elaboration	secondary	secondary	forw	support	First , Web queries are rarely well-formed questions .	Here we address two limitations of this approach when applied to open-domain , entity-oriented Web queries .	CL_D14-1117_mn_3	CL_D14-1117_mn_2	CL_D14-1117_mn_2_3
CL	D14-1117	mn	2	4	proposal	motivation_problem	elaboration	info-required	secondary	secondary	none	none	Here we address two limitations of this approach when applied to open-domain , entity-oriented Web queries .	They are "telegraphic" , with missing verbs , prepositions , clauses , case and phrase clues .	CL_D14-1117_mn_2	CL_D14-1117_mn_4	CL_D14-1117_mn_2_4
CL	D14-1117	mn	2	5	proposal	motivation_problem	elaboration	elaboration	secondary	secondary	none	none	Here we address two limitations of this approach when applied to open-domain , entity-oriented Web queries .	Second , the KG is always incomplete , unable to directly answer many queries .	CL_D14-1117_mn_2	CL_D14-1117_mn_5	CL_D14-1117_mn_2_5
CL	D14-1117	mn	2	6	proposal	proposal	elaboration	none	secondary	main	forw	elaboration	Here we address two limitations of this approach when applied to open-domain , entity-oriented Web queries .	We propose a novel technique to segment a telegraphic query and assign a coarse-grained purpose to each segment : a base entity e1 , a relation type r , a target entity type t2 , and contextual words s .	CL_D14-1117_mn_2	CL_D14-1117_mn_6	CL_D14-1117_mn_2_6
CL	D14-1117	mn	6	2	proposal	proposal	none	elaboration	main	secondary	back	elaboration	We propose a novel technique to segment a telegraphic query and assign a coarse-grained purpose to each segment : a base entity e1 , a relation type r , a target entity type t2 , and contextual words s .	Here we address two limitations of this approach when applied to open-domain , entity-oriented Web queries .	CL_D14-1117_mn_6	CL_D14-1117_mn_2	CL_D14-1117_mn_2_6
CL	D14-1117	mn	2	7	proposal	proposal_implementation	elaboration	elaboration	secondary	secondary	none	none	Here we address two limitations of this approach when applied to open-domain , entity-oriented Web queries .	The query seeks entity e2  t2 where r ( e1 , e2 ) holds , further evidenced by schema-agnostic words s .	CL_D14-1117_mn_2	CL_D14-1117_mn_7	CL_D14-1117_mn_2_7
CL	D14-1117	mn	2	8	proposal	proposal_implementation	elaboration	elaboration	secondary	secondary	none	none	Here we address two limitations of this approach when applied to open-domain , entity-oriented Web queries .	Query segmentation is integrated with the KG and an unstructured corpus where mentions of entities have been linked to the KG .	CL_D14-1117_mn_2	CL_D14-1117_mn_8	CL_D14-1117_mn_2_8
CL	D14-1117	mn	2	9	proposal	proposal_implementation	elaboration	info-optional	secondary	secondary	none	none	Here we address two limitations of this approach when applied to open-domain , entity-oriented Web queries .	We do not trust the best or any specific query segmentation .	CL_D14-1117_mn_2	CL_D14-1117_mn_9	CL_D14-1117_mn_2_9
CL	D14-1117	mn	2	10	proposal	proposal_implementation	elaboration	elaboration	secondary	secondary	none	none	Here we address two limitations of this approach when applied to open-domain , entity-oriented Web queries .	Instead , evidence in favor of candidate e2s are aggregated across several segmentations .	CL_D14-1117_mn_2	CL_D14-1117_mn_10	CL_D14-1117_mn_2_10
CL	D14-1117	mn	2	11	proposal	result_means	elaboration	support	secondary	secondary	none	none	Here we address two limitations of this approach when applied to open-domain , entity-oriented Web queries .	Extensive experiments on the ClueWeb corpus and parts of Freebase as our KG , using over a thousand telegraphic queries adapted from TREC , INEX , and WebQuestions , show the efficacy of our approach .	CL_D14-1117_mn_2	CL_D14-1117_mn_11	CL_D14-1117_mn_2_11
CL	D14-1117	mn	2	12	proposal	observation	elaboration	support	secondary	secondary	none	none	Here we address two limitations of this approach when applied to open-domain , entity-oriented Web queries .	For one benchmark , MAP improves from 0.2-0.29 ( competitive baselines ) to 0.42 ( our system ) .	CL_D14-1117_mn_2	CL_D14-1117_mn_12	CL_D14-1117_mn_2_12
CL	D14-1117	mn	2	13	proposal	observation	elaboration	elaboration	secondary	secondary	none	none	Here we address two limitations of this approach when applied to open-domain , entity-oriented Web queries .	NDCG @ 10 improves from 0.29-0.36 to 0.54 .	CL_D14-1117_mn_2	CL_D14-1117_mn_13	CL_D14-1117_mn_2_13
CL	D14-1117	mn	3	4	motivation_problem	motivation_problem	support	info-required	secondary	secondary	back	info-required	First , Web queries are rarely well-formed questions .	They are "telegraphic" , with missing verbs , prepositions , clauses , case and phrase clues .	CL_D14-1117_mn_3	CL_D14-1117_mn_4	CL_D14-1117_mn_3_4
CL	D14-1117	mn	4	3	motivation_problem	motivation_problem	info-required	support	secondary	secondary	forw	info-required	They are "telegraphic" , with missing verbs , prepositions , clauses , case and phrase clues .	First , Web queries are rarely well-formed questions .	CL_D14-1117_mn_4	CL_D14-1117_mn_3	CL_D14-1117_mn_3_4
CL	D14-1117	mn	3	5	motivation_problem	motivation_problem	support	elaboration	secondary	secondary	back	elaboration	First , Web queries are rarely well-formed questions .	Second , the KG is always incomplete , unable to directly answer many queries .	CL_D14-1117_mn_3	CL_D14-1117_mn_5	CL_D14-1117_mn_3_5
CL	D14-1117	mn	5	3	motivation_problem	motivation_problem	elaboration	support	secondary	secondary	forw	elaboration	Second , the KG is always incomplete , unable to directly answer many queries .	First , Web queries are rarely well-formed questions .	CL_D14-1117_mn_5	CL_D14-1117_mn_3	CL_D14-1117_mn_3_5
CL	D14-1117	mn	3	6	motivation_problem	proposal	support	none	secondary	main	none	none	First , Web queries are rarely well-formed questions .	We propose a novel technique to segment a telegraphic query and assign a coarse-grained purpose to each segment : a base entity e1 , a relation type r , a target entity type t2 , and contextual words s .	CL_D14-1117_mn_3	CL_D14-1117_mn_6	CL_D14-1117_mn_3_6
CL	D14-1117	mn	3	7	motivation_problem	proposal_implementation	support	elaboration	secondary	secondary	none	none	First , Web queries are rarely well-formed questions .	The query seeks entity e2  t2 where r ( e1 , e2 ) holds , further evidenced by schema-agnostic words s .	CL_D14-1117_mn_3	CL_D14-1117_mn_7	CL_D14-1117_mn_3_7
CL	D14-1117	mn	3	8	motivation_problem	proposal_implementation	support	elaboration	secondary	secondary	none	none	First , Web queries are rarely well-formed questions .	Query segmentation is integrated with the KG and an unstructured corpus where mentions of entities have been linked to the KG .	CL_D14-1117_mn_3	CL_D14-1117_mn_8	CL_D14-1117_mn_3_8
CL	D14-1117	mn	3	9	motivation_problem	proposal_implementation	support	info-optional	secondary	secondary	none	none	First , Web queries are rarely well-formed questions .	We do not trust the best or any specific query segmentation .	CL_D14-1117_mn_3	CL_D14-1117_mn_9	CL_D14-1117_mn_3_9
CL	D14-1117	mn	3	10	motivation_problem	proposal_implementation	support	elaboration	secondary	secondary	none	none	First , Web queries are rarely well-formed questions .	Instead , evidence in favor of candidate e2s are aggregated across several segmentations .	CL_D14-1117_mn_3	CL_D14-1117_mn_10	CL_D14-1117_mn_3_10
CL	D14-1117	mn	3	11	motivation_problem	result_means	support	support	secondary	secondary	none	none	First , Web queries are rarely well-formed questions .	Extensive experiments on the ClueWeb corpus and parts of Freebase as our KG , using over a thousand telegraphic queries adapted from TREC , INEX , and WebQuestions , show the efficacy of our approach .	CL_D14-1117_mn_3	CL_D14-1117_mn_11	CL_D14-1117_mn_3_11
CL	D14-1117	mn	3	12	motivation_problem	observation	support	support	secondary	secondary	none	none	First , Web queries are rarely well-formed questions .	For one benchmark , MAP improves from 0.2-0.29 ( competitive baselines ) to 0.42 ( our system ) .	CL_D14-1117_mn_3	CL_D14-1117_mn_12	CL_D14-1117_mn_3_12
CL	D14-1117	mn	3	13	motivation_problem	observation	support	elaboration	secondary	secondary	none	none	First , Web queries are rarely well-formed questions .	NDCG @ 10 improves from 0.29-0.36 to 0.54 .	CL_D14-1117_mn_3	CL_D14-1117_mn_13	CL_D14-1117_mn_3_13
CL	D14-1117	mn	4	5	motivation_problem	motivation_problem	info-required	elaboration	secondary	secondary	none	none	They are "telegraphic" , with missing verbs , prepositions , clauses , case and phrase clues .	Second , the KG is always incomplete , unable to directly answer many queries .	CL_D14-1117_mn_4	CL_D14-1117_mn_5	CL_D14-1117_mn_4_5
CL	D14-1117	mn	4	6	motivation_problem	proposal	info-required	none	secondary	main	none	none	They are "telegraphic" , with missing verbs , prepositions , clauses , case and phrase clues .	We propose a novel technique to segment a telegraphic query and assign a coarse-grained purpose to each segment : a base entity e1 , a relation type r , a target entity type t2 , and contextual words s .	CL_D14-1117_mn_4	CL_D14-1117_mn_6	CL_D14-1117_mn_4_6
CL	D14-1117	mn	4	7	motivation_problem	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	They are "telegraphic" , with missing verbs , prepositions , clauses , case and phrase clues .	The query seeks entity e2  t2 where r ( e1 , e2 ) holds , further evidenced by schema-agnostic words s .	CL_D14-1117_mn_4	CL_D14-1117_mn_7	CL_D14-1117_mn_4_7
CL	D14-1117	mn	4	8	motivation_problem	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	They are "telegraphic" , with missing verbs , prepositions , clauses , case and phrase clues .	Query segmentation is integrated with the KG and an unstructured corpus where mentions of entities have been linked to the KG .	CL_D14-1117_mn_4	CL_D14-1117_mn_8	CL_D14-1117_mn_4_8
CL	D14-1117	mn	4	9	motivation_problem	proposal_implementation	info-required	info-optional	secondary	secondary	none	none	They are "telegraphic" , with missing verbs , prepositions , clauses , case and phrase clues .	We do not trust the best or any specific query segmentation .	CL_D14-1117_mn_4	CL_D14-1117_mn_9	CL_D14-1117_mn_4_9
CL	D14-1117	mn	4	10	motivation_problem	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	They are "telegraphic" , with missing verbs , prepositions , clauses , case and phrase clues .	Instead , evidence in favor of candidate e2s are aggregated across several segmentations .	CL_D14-1117_mn_4	CL_D14-1117_mn_10	CL_D14-1117_mn_4_10
CL	D14-1117	mn	4	11	motivation_problem	result_means	info-required	support	secondary	secondary	none	none	They are "telegraphic" , with missing verbs , prepositions , clauses , case and phrase clues .	Extensive experiments on the ClueWeb corpus and parts of Freebase as our KG , using over a thousand telegraphic queries adapted from TREC , INEX , and WebQuestions , show the efficacy of our approach .	CL_D14-1117_mn_4	CL_D14-1117_mn_11	CL_D14-1117_mn_4_11
CL	D14-1117	mn	4	12	motivation_problem	observation	info-required	support	secondary	secondary	none	none	They are "telegraphic" , with missing verbs , prepositions , clauses , case and phrase clues .	For one benchmark , MAP improves from 0.2-0.29 ( competitive baselines ) to 0.42 ( our system ) .	CL_D14-1117_mn_4	CL_D14-1117_mn_12	CL_D14-1117_mn_4_12
CL	D14-1117	mn	4	13	motivation_problem	observation	info-required	elaboration	secondary	secondary	none	none	They are "telegraphic" , with missing verbs , prepositions , clauses , case and phrase clues .	NDCG @ 10 improves from 0.29-0.36 to 0.54 .	CL_D14-1117_mn_4	CL_D14-1117_mn_13	CL_D14-1117_mn_4_13
CL	D14-1117	mn	5	6	motivation_problem	proposal	elaboration	none	secondary	main	none	none	Second , the KG is always incomplete , unable to directly answer many queries .	We propose a novel technique to segment a telegraphic query and assign a coarse-grained purpose to each segment : a base entity e1 , a relation type r , a target entity type t2 , and contextual words s .	CL_D14-1117_mn_5	CL_D14-1117_mn_6	CL_D14-1117_mn_5_6
CL	D14-1117	mn	5	7	motivation_problem	proposal_implementation	elaboration	elaboration	secondary	secondary	none	none	Second , the KG is always incomplete , unable to directly answer many queries .	The query seeks entity e2  t2 where r ( e1 , e2 ) holds , further evidenced by schema-agnostic words s .	CL_D14-1117_mn_5	CL_D14-1117_mn_7	CL_D14-1117_mn_5_7
CL	D14-1117	mn	5	8	motivation_problem	proposal_implementation	elaboration	elaboration	secondary	secondary	none	none	Second , the KG is always incomplete , unable to directly answer many queries .	Query segmentation is integrated with the KG and an unstructured corpus where mentions of entities have been linked to the KG .	CL_D14-1117_mn_5	CL_D14-1117_mn_8	CL_D14-1117_mn_5_8
CL	D14-1117	mn	5	9	motivation_problem	proposal_implementation	elaboration	info-optional	secondary	secondary	none	none	Second , the KG is always incomplete , unable to directly answer many queries .	We do not trust the best or any specific query segmentation .	CL_D14-1117_mn_5	CL_D14-1117_mn_9	CL_D14-1117_mn_5_9
CL	D14-1117	mn	5	10	motivation_problem	proposal_implementation	elaboration	elaboration	secondary	secondary	none	none	Second , the KG is always incomplete , unable to directly answer many queries .	Instead , evidence in favor of candidate e2s are aggregated across several segmentations .	CL_D14-1117_mn_5	CL_D14-1117_mn_10	CL_D14-1117_mn_5_10
CL	D14-1117	mn	5	11	motivation_problem	result_means	elaboration	support	secondary	secondary	none	none	Second , the KG is always incomplete , unable to directly answer many queries .	Extensive experiments on the ClueWeb corpus and parts of Freebase as our KG , using over a thousand telegraphic queries adapted from TREC , INEX , and WebQuestions , show the efficacy of our approach .	CL_D14-1117_mn_5	CL_D14-1117_mn_11	CL_D14-1117_mn_5_11
CL	D14-1117	mn	5	12	motivation_problem	observation	elaboration	support	secondary	secondary	none	none	Second , the KG is always incomplete , unable to directly answer many queries .	For one benchmark , MAP improves from 0.2-0.29 ( competitive baselines ) to 0.42 ( our system ) .	CL_D14-1117_mn_5	CL_D14-1117_mn_12	CL_D14-1117_mn_5_12
CL	D14-1117	mn	5	13	motivation_problem	observation	elaboration	elaboration	secondary	secondary	none	none	Second , the KG is always incomplete , unable to directly answer many queries .	NDCG @ 10 improves from 0.29-0.36 to 0.54 .	CL_D14-1117_mn_5	CL_D14-1117_mn_13	CL_D14-1117_mn_5_13
CL	D14-1117	mn	6	7	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	We propose a novel technique to segment a telegraphic query and assign a coarse-grained purpose to each segment : a base entity e1 , a relation type r , a target entity type t2 , and contextual words s .	The query seeks entity e2  t2 where r ( e1 , e2 ) holds , further evidenced by schema-agnostic words s .	CL_D14-1117_mn_6	CL_D14-1117_mn_7	CL_D14-1117_mn_6_7
CL	D14-1117	mn	7	6	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	The query seeks entity e2  t2 where r ( e1 , e2 ) holds , further evidenced by schema-agnostic words s .	We propose a novel technique to segment a telegraphic query and assign a coarse-grained purpose to each segment : a base entity e1 , a relation type r , a target entity type t2 , and contextual words s .	CL_D14-1117_mn_7	CL_D14-1117_mn_6	CL_D14-1117_mn_6_7
CL	D14-1117	mn	6	8	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	We propose a novel technique to segment a telegraphic query and assign a coarse-grained purpose to each segment : a base entity e1 , a relation type r , a target entity type t2 , and contextual words s .	Query segmentation is integrated with the KG and an unstructured corpus where mentions of entities have been linked to the KG .	CL_D14-1117_mn_6	CL_D14-1117_mn_8	CL_D14-1117_mn_6_8
CL	D14-1117	mn	6	9	proposal	proposal_implementation	none	info-optional	main	secondary	none	none	We propose a novel technique to segment a telegraphic query and assign a coarse-grained purpose to each segment : a base entity e1 , a relation type r , a target entity type t2 , and contextual words s .	We do not trust the best or any specific query segmentation .	CL_D14-1117_mn_6	CL_D14-1117_mn_9	CL_D14-1117_mn_6_9
CL	D14-1117	mn	6	10	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	We propose a novel technique to segment a telegraphic query and assign a coarse-grained purpose to each segment : a base entity e1 , a relation type r , a target entity type t2 , and contextual words s .	Instead , evidence in favor of candidate e2s are aggregated across several segmentations .	CL_D14-1117_mn_6	CL_D14-1117_mn_10	CL_D14-1117_mn_6_10
CL	D14-1117	mn	6	11	proposal	result_means	none	support	main	secondary	back	support	We propose a novel technique to segment a telegraphic query and assign a coarse-grained purpose to each segment : a base entity e1 , a relation type r , a target entity type t2 , and contextual words s .	Extensive experiments on the ClueWeb corpus and parts of Freebase as our KG , using over a thousand telegraphic queries adapted from TREC , INEX , and WebQuestions , show the efficacy of our approach .	CL_D14-1117_mn_6	CL_D14-1117_mn_11	CL_D14-1117_mn_6_11
CL	D14-1117	mn	11	6	result_means	proposal	support	none	secondary	main	forw	support	Extensive experiments on the ClueWeb corpus and parts of Freebase as our KG , using over a thousand telegraphic queries adapted from TREC , INEX , and WebQuestions , show the efficacy of our approach .	We propose a novel technique to segment a telegraphic query and assign a coarse-grained purpose to each segment : a base entity e1 , a relation type r , a target entity type t2 , and contextual words s .	CL_D14-1117_mn_11	CL_D14-1117_mn_6	CL_D14-1117_mn_6_11
CL	D14-1117	mn	6	12	proposal	observation	none	support	main	secondary	none	none	We propose a novel technique to segment a telegraphic query and assign a coarse-grained purpose to each segment : a base entity e1 , a relation type r , a target entity type t2 , and contextual words s .	For one benchmark , MAP improves from 0.2-0.29 ( competitive baselines ) to 0.42 ( our system ) .	CL_D14-1117_mn_6	CL_D14-1117_mn_12	CL_D14-1117_mn_6_12
CL	D14-1117	mn	6	13	proposal	observation	none	elaboration	main	secondary	none	none	We propose a novel technique to segment a telegraphic query and assign a coarse-grained purpose to each segment : a base entity e1 , a relation type r , a target entity type t2 , and contextual words s .	NDCG @ 10 improves from 0.29-0.36 to 0.54 .	CL_D14-1117_mn_6	CL_D14-1117_mn_13	CL_D14-1117_mn_6_13
CL	D14-1117	mn	7	8	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	The query seeks entity e2  t2 where r ( e1 , e2 ) holds , further evidenced by schema-agnostic words s .	Query segmentation is integrated with the KG and an unstructured corpus where mentions of entities have been linked to the KG .	CL_D14-1117_mn_7	CL_D14-1117_mn_8	CL_D14-1117_mn_7_8
CL	D14-1117	mn	8	7	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	forw	elaboration	Query segmentation is integrated with the KG and an unstructured corpus where mentions of entities have been linked to the KG .	The query seeks entity e2  t2 where r ( e1 , e2 ) holds , further evidenced by schema-agnostic words s .	CL_D14-1117_mn_8	CL_D14-1117_mn_7	CL_D14-1117_mn_7_8
CL	D14-1117	mn	7	9	proposal_implementation	proposal_implementation	elaboration	info-optional	secondary	secondary	none	none	The query seeks entity e2  t2 where r ( e1 , e2 ) holds , further evidenced by schema-agnostic words s .	We do not trust the best or any specific query segmentation .	CL_D14-1117_mn_7	CL_D14-1117_mn_9	CL_D14-1117_mn_7_9
CL	D14-1117	mn	7	10	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	none	none	The query seeks entity e2  t2 where r ( e1 , e2 ) holds , further evidenced by schema-agnostic words s .	Instead , evidence in favor of candidate e2s are aggregated across several segmentations .	CL_D14-1117_mn_7	CL_D14-1117_mn_10	CL_D14-1117_mn_7_10
CL	D14-1117	mn	7	11	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	The query seeks entity e2  t2 where r ( e1 , e2 ) holds , further evidenced by schema-agnostic words s .	Extensive experiments on the ClueWeb corpus and parts of Freebase as our KG , using over a thousand telegraphic queries adapted from TREC , INEX , and WebQuestions , show the efficacy of our approach .	CL_D14-1117_mn_7	CL_D14-1117_mn_11	CL_D14-1117_mn_7_11
CL	D14-1117	mn	7	12	proposal_implementation	observation	elaboration	support	secondary	secondary	none	none	The query seeks entity e2  t2 where r ( e1 , e2 ) holds , further evidenced by schema-agnostic words s .	For one benchmark , MAP improves from 0.2-0.29 ( competitive baselines ) to 0.42 ( our system ) .	CL_D14-1117_mn_7	CL_D14-1117_mn_12	CL_D14-1117_mn_7_12
CL	D14-1117	mn	7	13	proposal_implementation	observation	elaboration	elaboration	secondary	secondary	none	none	The query seeks entity e2  t2 where r ( e1 , e2 ) holds , further evidenced by schema-agnostic words s .	NDCG @ 10 improves from 0.29-0.36 to 0.54 .	CL_D14-1117_mn_7	CL_D14-1117_mn_13	CL_D14-1117_mn_7_13
CL	D14-1117	mn	8	9	proposal_implementation	proposal_implementation	elaboration	info-optional	secondary	secondary	back	info-optional	Query segmentation is integrated with the KG and an unstructured corpus where mentions of entities have been linked to the KG .	We do not trust the best or any specific query segmentation .	CL_D14-1117_mn_8	CL_D14-1117_mn_9	CL_D14-1117_mn_8_9
CL	D14-1117	mn	9	8	proposal_implementation	proposal_implementation	info-optional	elaboration	secondary	secondary	forw	info-optional	We do not trust the best or any specific query segmentation .	Query segmentation is integrated with the KG and an unstructured corpus where mentions of entities have been linked to the KG .	CL_D14-1117_mn_9	CL_D14-1117_mn_8	CL_D14-1117_mn_8_9
CL	D14-1117	mn	8	10	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	none	none	Query segmentation is integrated with the KG and an unstructured corpus where mentions of entities have been linked to the KG .	Instead , evidence in favor of candidate e2s are aggregated across several segmentations .	CL_D14-1117_mn_8	CL_D14-1117_mn_10	CL_D14-1117_mn_8_10
CL	D14-1117	mn	8	11	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	Query segmentation is integrated with the KG and an unstructured corpus where mentions of entities have been linked to the KG .	Extensive experiments on the ClueWeb corpus and parts of Freebase as our KG , using over a thousand telegraphic queries adapted from TREC , INEX , and WebQuestions , show the efficacy of our approach .	CL_D14-1117_mn_8	CL_D14-1117_mn_11	CL_D14-1117_mn_8_11
CL	D14-1117	mn	8	12	proposal_implementation	observation	elaboration	support	secondary	secondary	none	none	Query segmentation is integrated with the KG and an unstructured corpus where mentions of entities have been linked to the KG .	For one benchmark , MAP improves from 0.2-0.29 ( competitive baselines ) to 0.42 ( our system ) .	CL_D14-1117_mn_8	CL_D14-1117_mn_12	CL_D14-1117_mn_8_12
CL	D14-1117	mn	8	13	proposal_implementation	observation	elaboration	elaboration	secondary	secondary	none	none	Query segmentation is integrated with the KG and an unstructured corpus where mentions of entities have been linked to the KG .	NDCG @ 10 improves from 0.29-0.36 to 0.54 .	CL_D14-1117_mn_8	CL_D14-1117_mn_13	CL_D14-1117_mn_8_13
CL	D14-1117	mn	9	10	proposal_implementation	proposal_implementation	info-optional	elaboration	secondary	secondary	back	elaboration	We do not trust the best or any specific query segmentation .	Instead , evidence in favor of candidate e2s are aggregated across several segmentations .	CL_D14-1117_mn_9	CL_D14-1117_mn_10	CL_D14-1117_mn_9_10
CL	D14-1117	mn	10	9	proposal_implementation	proposal_implementation	elaboration	info-optional	secondary	secondary	forw	elaboration	Instead , evidence in favor of candidate e2s are aggregated across several segmentations .	We do not trust the best or any specific query segmentation .	CL_D14-1117_mn_10	CL_D14-1117_mn_9	CL_D14-1117_mn_9_10
CL	D14-1117	mn	9	11	proposal_implementation	result_means	info-optional	support	secondary	secondary	none	none	We do not trust the best or any specific query segmentation .	Extensive experiments on the ClueWeb corpus and parts of Freebase as our KG , using over a thousand telegraphic queries adapted from TREC , INEX , and WebQuestions , show the efficacy of our approach .	CL_D14-1117_mn_9	CL_D14-1117_mn_11	CL_D14-1117_mn_9_11
CL	D14-1117	mn	9	12	proposal_implementation	observation	info-optional	support	secondary	secondary	none	none	We do not trust the best or any specific query segmentation .	For one benchmark , MAP improves from 0.2-0.29 ( competitive baselines ) to 0.42 ( our system ) .	CL_D14-1117_mn_9	CL_D14-1117_mn_12	CL_D14-1117_mn_9_12
CL	D14-1117	mn	9	13	proposal_implementation	observation	info-optional	elaboration	secondary	secondary	none	none	We do not trust the best or any specific query segmentation .	NDCG @ 10 improves from 0.29-0.36 to 0.54 .	CL_D14-1117_mn_9	CL_D14-1117_mn_13	CL_D14-1117_mn_9_13
CL	D14-1117	mn	10	11	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	Instead , evidence in favor of candidate e2s are aggregated across several segmentations .	Extensive experiments on the ClueWeb corpus and parts of Freebase as our KG , using over a thousand telegraphic queries adapted from TREC , INEX , and WebQuestions , show the efficacy of our approach .	CL_D14-1117_mn_10	CL_D14-1117_mn_11	CL_D14-1117_mn_10_11
CL	D14-1117	mn	10	12	proposal_implementation	observation	elaboration	support	secondary	secondary	none	none	Instead , evidence in favor of candidate e2s are aggregated across several segmentations .	For one benchmark , MAP improves from 0.2-0.29 ( competitive baselines ) to 0.42 ( our system ) .	CL_D14-1117_mn_10	CL_D14-1117_mn_12	CL_D14-1117_mn_10_12
CL	D14-1117	mn	10	13	proposal_implementation	observation	elaboration	elaboration	secondary	secondary	none	none	Instead , evidence in favor of candidate e2s are aggregated across several segmentations .	NDCG @ 10 improves from 0.29-0.36 to 0.54 .	CL_D14-1117_mn_10	CL_D14-1117_mn_13	CL_D14-1117_mn_10_13
CL	D14-1117	mn	11	12	result_means	observation	support	support	secondary	secondary	back	support	Extensive experiments on the ClueWeb corpus and parts of Freebase as our KG , using over a thousand telegraphic queries adapted from TREC , INEX , and WebQuestions , show the efficacy of our approach .	For one benchmark , MAP improves from 0.2-0.29 ( competitive baselines ) to 0.42 ( our system ) .	CL_D14-1117_mn_11	CL_D14-1117_mn_12	CL_D14-1117_mn_11_12
CL	D14-1117	mn	12	11	observation	result_means	support	support	secondary	secondary	forw	support	For one benchmark , MAP improves from 0.2-0.29 ( competitive baselines ) to 0.42 ( our system ) .	Extensive experiments on the ClueWeb corpus and parts of Freebase as our KG , using over a thousand telegraphic queries adapted from TREC , INEX , and WebQuestions , show the efficacy of our approach .	CL_D14-1117_mn_12	CL_D14-1117_mn_11	CL_D14-1117_mn_11_12
CL	D14-1117	mn	11	13	result_means	observation	support	elaboration	secondary	secondary	none	none	Extensive experiments on the ClueWeb corpus and parts of Freebase as our KG , using over a thousand telegraphic queries adapted from TREC , INEX , and WebQuestions , show the efficacy of our approach .	NDCG @ 10 improves from 0.29-0.36 to 0.54 .	CL_D14-1117_mn_11	CL_D14-1117_mn_13	CL_D14-1117_mn_11_13
CL	D14-1117	mn	12	13	observation	observation	support	elaboration	secondary	secondary	back	elaboration	For one benchmark , MAP improves from 0.2-0.29 ( competitive baselines ) to 0.42 ( our system ) .	NDCG @ 10 improves from 0.29-0.36 to 0.54 .	CL_D14-1117_mn_12	CL_D14-1117_mn_13	CL_D14-1117_mn_12_13
CL	D14-1117	mn	13	12	observation	observation	elaboration	support	secondary	secondary	forw	elaboration	NDCG @ 10 improves from 0.29-0.36 to 0.54 .	For one benchmark , MAP improves from 0.2-0.29 ( competitive baselines ) to 0.42 ( our system ) .	CL_D14-1117_mn_13	CL_D14-1117_mn_12	CL_D14-1117_mn_12_13
CL	D14-1118	mn	1	2	motivation_background	motivation_background	info-required	info-required	secondary	secondary	forw	info-required	Estimating questions' difficulty levels is an important task in community question answering ( CQA ) services .	Previous studies propose to solve this problem based on the question-user comparisons extracted from the question answering threads .	CL_D14-1118_mn_1	CL_D14-1118_mn_2	CL_D14-1118_mn_1_2
CL	D14-1118	mn	2	1	motivation_background	motivation_background	info-required	info-required	secondary	secondary	back	info-required	Previous studies propose to solve this problem based on the question-user comparisons extracted from the question answering threads .	Estimating questions' difficulty levels is an important task in community question answering ( CQA ) services .	CL_D14-1118_mn_2	CL_D14-1118_mn_1	CL_D14-1118_mn_1_2
CL	D14-1118	mn	1	3	motivation_background	motivation_problem	info-required	support	secondary	secondary	none	none	Estimating questions' difficulty levels is an important task in community question answering ( CQA ) services .	However , they suffer from data sparseness problem as each question only gets a limited number of comparisons .	CL_D14-1118_mn_1	CL_D14-1118_mn_3	CL_D14-1118_mn_1_3
CL	D14-1118	mn	1	4	motivation_background	motivation_problem	info-required	elaboration	secondary	secondary	none	none	Estimating questions' difficulty levels is an important task in community question answering ( CQA ) services .	Moreover , they cannot handle newly posted questions which get no comparisons .	CL_D14-1118_mn_1	CL_D14-1118_mn_4	CL_D14-1118_mn_1_4
CL	D14-1118	mn	1	5	motivation_background	proposal	info-required	none	secondary	main	none	none	Estimating questions' difficulty levels is an important task in community question answering ( CQA ) services .	In this paper , we propose a novel question difficulty estimation approach called Regularized Competition Model ( RCM ) , which naturally combines question-user comparisons and questions' textual descriptions into a unified framework .	CL_D14-1118_mn_1	CL_D14-1118_mn_5	CL_D14-1118_mn_1_5
CL	D14-1118	mn	1	6	motivation_background	motivation_hypothesis	info-required	support	secondary	secondary	none	none	Estimating questions' difficulty levels is an important task in community question answering ( CQA ) services .	By incorporating textual information , RCM can effectively deal with data sparseness problem .	CL_D14-1118_mn_1	CL_D14-1118_mn_6	CL_D14-1118_mn_1_6
CL	D14-1118	mn	1	7	motivation_background	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	Estimating questions' difficulty levels is an important task in community question answering ( CQA ) services .	We further employ a K-Nearest Neighbor approach to estimate difficulty levels of newly posted questions , again by leveraging textual similarities .	CL_D14-1118_mn_1	CL_D14-1118_mn_7	CL_D14-1118_mn_1_7
CL	D14-1118	mn	1	8	motivation_background	result_means	info-required	support	secondary	secondary	none	none	Estimating questions' difficulty levels is an important task in community question answering ( CQA ) services .	Experiments on two publicly available data sets show that for both well-resolved and newly-posted questions , RCM performs the estimation task significantly better than existing methods , demonstrating the advantage of incorporating textual information .	CL_D14-1118_mn_1	CL_D14-1118_mn_8	CL_D14-1118_mn_1_8
CL	D14-1118	mn	1	9	motivation_background	conclusion	info-required	support	secondary	secondary	none	none	Estimating questions' difficulty levels is an important task in community question answering ( CQA ) services .	More interestingly , we observe that RCM might provide an automatic way to quantitatively measure the knowledge levels of words .	CL_D14-1118_mn_1	CL_D14-1118_mn_9	CL_D14-1118_mn_1_9
CL	D14-1118	mn	2	3	motivation_background	motivation_problem	info-required	support	secondary	secondary	forw	info-required	Previous studies propose to solve this problem based on the question-user comparisons extracted from the question answering threads .	However , they suffer from data sparseness problem as each question only gets a limited number of comparisons .	CL_D14-1118_mn_2	CL_D14-1118_mn_3	CL_D14-1118_mn_2_3
CL	D14-1118	mn	3	2	motivation_problem	motivation_background	support	info-required	secondary	secondary	back	info-required	However , they suffer from data sparseness problem as each question only gets a limited number of comparisons .	Previous studies propose to solve this problem based on the question-user comparisons extracted from the question answering threads .	CL_D14-1118_mn_3	CL_D14-1118_mn_2	CL_D14-1118_mn_2_3
CL	D14-1118	mn	2	4	motivation_background	motivation_problem	info-required	elaboration	secondary	secondary	none	none	Previous studies propose to solve this problem based on the question-user comparisons extracted from the question answering threads .	Moreover , they cannot handle newly posted questions which get no comparisons .	CL_D14-1118_mn_2	CL_D14-1118_mn_4	CL_D14-1118_mn_2_4
CL	D14-1118	mn	2	5	motivation_background	proposal	info-required	none	secondary	main	none	none	Previous studies propose to solve this problem based on the question-user comparisons extracted from the question answering threads .	In this paper , we propose a novel question difficulty estimation approach called Regularized Competition Model ( RCM ) , which naturally combines question-user comparisons and questions' textual descriptions into a unified framework .	CL_D14-1118_mn_2	CL_D14-1118_mn_5	CL_D14-1118_mn_2_5
CL	D14-1118	mn	2	6	motivation_background	motivation_hypothesis	info-required	support	secondary	secondary	none	none	Previous studies propose to solve this problem based on the question-user comparisons extracted from the question answering threads .	By incorporating textual information , RCM can effectively deal with data sparseness problem .	CL_D14-1118_mn_2	CL_D14-1118_mn_6	CL_D14-1118_mn_2_6
CL	D14-1118	mn	2	7	motivation_background	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	Previous studies propose to solve this problem based on the question-user comparisons extracted from the question answering threads .	We further employ a K-Nearest Neighbor approach to estimate difficulty levels of newly posted questions , again by leveraging textual similarities .	CL_D14-1118_mn_2	CL_D14-1118_mn_7	CL_D14-1118_mn_2_7
CL	D14-1118	mn	2	8	motivation_background	result_means	info-required	support	secondary	secondary	none	none	Previous studies propose to solve this problem based on the question-user comparisons extracted from the question answering threads .	Experiments on two publicly available data sets show that for both well-resolved and newly-posted questions , RCM performs the estimation task significantly better than existing methods , demonstrating the advantage of incorporating textual information .	CL_D14-1118_mn_2	CL_D14-1118_mn_8	CL_D14-1118_mn_2_8
CL	D14-1118	mn	2	9	motivation_background	conclusion	info-required	support	secondary	secondary	none	none	Previous studies propose to solve this problem based on the question-user comparisons extracted from the question answering threads .	More interestingly , we observe that RCM might provide an automatic way to quantitatively measure the knowledge levels of words .	CL_D14-1118_mn_2	CL_D14-1118_mn_9	CL_D14-1118_mn_2_9
CL	D14-1118	mn	3	4	motivation_problem	motivation_problem	support	elaboration	secondary	secondary	back	elaboration	However , they suffer from data sparseness problem as each question only gets a limited number of comparisons .	Moreover , they cannot handle newly posted questions which get no comparisons .	CL_D14-1118_mn_3	CL_D14-1118_mn_4	CL_D14-1118_mn_3_4
CL	D14-1118	mn	4	3	motivation_problem	motivation_problem	elaboration	support	secondary	secondary	forw	elaboration	Moreover , they cannot handle newly posted questions which get no comparisons .	However , they suffer from data sparseness problem as each question only gets a limited number of comparisons .	CL_D14-1118_mn_4	CL_D14-1118_mn_3	CL_D14-1118_mn_3_4
CL	D14-1118	mn	3	5	motivation_problem	proposal	support	none	secondary	main	forw	support	However , they suffer from data sparseness problem as each question only gets a limited number of comparisons .	In this paper , we propose a novel question difficulty estimation approach called Regularized Competition Model ( RCM ) , which naturally combines question-user comparisons and questions' textual descriptions into a unified framework .	CL_D14-1118_mn_3	CL_D14-1118_mn_5	CL_D14-1118_mn_3_5
CL	D14-1118	mn	5	3	proposal	motivation_problem	none	support	main	secondary	back	support	In this paper , we propose a novel question difficulty estimation approach called Regularized Competition Model ( RCM ) , which naturally combines question-user comparisons and questions' textual descriptions into a unified framework .	However , they suffer from data sparseness problem as each question only gets a limited number of comparisons .	CL_D14-1118_mn_5	CL_D14-1118_mn_3	CL_D14-1118_mn_3_5
CL	D14-1118	mn	3	6	motivation_problem	motivation_hypothesis	support	support	secondary	secondary	none	none	However , they suffer from data sparseness problem as each question only gets a limited number of comparisons .	By incorporating textual information , RCM can effectively deal with data sparseness problem .	CL_D14-1118_mn_3	CL_D14-1118_mn_6	CL_D14-1118_mn_3_6
CL	D14-1118	mn	3	7	motivation_problem	proposal_implementation	support	elaboration	secondary	secondary	none	none	However , they suffer from data sparseness problem as each question only gets a limited number of comparisons .	We further employ a K-Nearest Neighbor approach to estimate difficulty levels of newly posted questions , again by leveraging textual similarities .	CL_D14-1118_mn_3	CL_D14-1118_mn_7	CL_D14-1118_mn_3_7
CL	D14-1118	mn	3	8	motivation_problem	result_means	support	support	secondary	secondary	none	none	However , they suffer from data sparseness problem as each question only gets a limited number of comparisons .	Experiments on two publicly available data sets show that for both well-resolved and newly-posted questions , RCM performs the estimation task significantly better than existing methods , demonstrating the advantage of incorporating textual information .	CL_D14-1118_mn_3	CL_D14-1118_mn_8	CL_D14-1118_mn_3_8
CL	D14-1118	mn	3	9	motivation_problem	conclusion	support	support	secondary	secondary	none	none	However , they suffer from data sparseness problem as each question only gets a limited number of comparisons .	More interestingly , we observe that RCM might provide an automatic way to quantitatively measure the knowledge levels of words .	CL_D14-1118_mn_3	CL_D14-1118_mn_9	CL_D14-1118_mn_3_9
CL	D14-1118	mn	4	5	motivation_problem	proposal	elaboration	none	secondary	main	none	none	Moreover , they cannot handle newly posted questions which get no comparisons .	In this paper , we propose a novel question difficulty estimation approach called Regularized Competition Model ( RCM ) , which naturally combines question-user comparisons and questions' textual descriptions into a unified framework .	CL_D14-1118_mn_4	CL_D14-1118_mn_5	CL_D14-1118_mn_4_5
CL	D14-1118	mn	4	6	motivation_problem	motivation_hypothesis	elaboration	support	secondary	secondary	none	none	Moreover , they cannot handle newly posted questions which get no comparisons .	By incorporating textual information , RCM can effectively deal with data sparseness problem .	CL_D14-1118_mn_4	CL_D14-1118_mn_6	CL_D14-1118_mn_4_6
CL	D14-1118	mn	4	7	motivation_problem	proposal_implementation	elaboration	elaboration	secondary	secondary	none	none	Moreover , they cannot handle newly posted questions which get no comparisons .	We further employ a K-Nearest Neighbor approach to estimate difficulty levels of newly posted questions , again by leveraging textual similarities .	CL_D14-1118_mn_4	CL_D14-1118_mn_7	CL_D14-1118_mn_4_7
CL	D14-1118	mn	4	8	motivation_problem	result_means	elaboration	support	secondary	secondary	none	none	Moreover , they cannot handle newly posted questions which get no comparisons .	Experiments on two publicly available data sets show that for both well-resolved and newly-posted questions , RCM performs the estimation task significantly better than existing methods , demonstrating the advantage of incorporating textual information .	CL_D14-1118_mn_4	CL_D14-1118_mn_8	CL_D14-1118_mn_4_8
CL	D14-1118	mn	4	9	motivation_problem	conclusion	elaboration	support	secondary	secondary	none	none	Moreover , they cannot handle newly posted questions which get no comparisons .	More interestingly , we observe that RCM might provide an automatic way to quantitatively measure the knowledge levels of words .	CL_D14-1118_mn_4	CL_D14-1118_mn_9	CL_D14-1118_mn_4_9
CL	D14-1118	mn	5	6	proposal	motivation_hypothesis	none	support	main	secondary	back	support	In this paper , we propose a novel question difficulty estimation approach called Regularized Competition Model ( RCM ) , which naturally combines question-user comparisons and questions' textual descriptions into a unified framework .	By incorporating textual information , RCM can effectively deal with data sparseness problem .	CL_D14-1118_mn_5	CL_D14-1118_mn_6	CL_D14-1118_mn_5_6
CL	D14-1118	mn	6	5	motivation_hypothesis	proposal	support	none	secondary	main	forw	support	By incorporating textual information , RCM can effectively deal with data sparseness problem .	In this paper , we propose a novel question difficulty estimation approach called Regularized Competition Model ( RCM ) , which naturally combines question-user comparisons and questions' textual descriptions into a unified framework .	CL_D14-1118_mn_6	CL_D14-1118_mn_5	CL_D14-1118_mn_5_6
CL	D14-1118	mn	5	7	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	In this paper , we propose a novel question difficulty estimation approach called Regularized Competition Model ( RCM ) , which naturally combines question-user comparisons and questions' textual descriptions into a unified framework .	We further employ a K-Nearest Neighbor approach to estimate difficulty levels of newly posted questions , again by leveraging textual similarities .	CL_D14-1118_mn_5	CL_D14-1118_mn_7	CL_D14-1118_mn_5_7
CL	D14-1118	mn	7	5	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	We further employ a K-Nearest Neighbor approach to estimate difficulty levels of newly posted questions , again by leveraging textual similarities .	In this paper , we propose a novel question difficulty estimation approach called Regularized Competition Model ( RCM ) , which naturally combines question-user comparisons and questions' textual descriptions into a unified framework .	CL_D14-1118_mn_7	CL_D14-1118_mn_5	CL_D14-1118_mn_5_7
CL	D14-1118	mn	5	8	proposal	result_means	none	support	main	secondary	none	none	In this paper , we propose a novel question difficulty estimation approach called Regularized Competition Model ( RCM ) , which naturally combines question-user comparisons and questions' textual descriptions into a unified framework .	Experiments on two publicly available data sets show that for both well-resolved and newly-posted questions , RCM performs the estimation task significantly better than existing methods , demonstrating the advantage of incorporating textual information .	CL_D14-1118_mn_5	CL_D14-1118_mn_8	CL_D14-1118_mn_5_8
CL	D14-1118	mn	5	9	proposal	conclusion	none	support	main	secondary	back	support	In this paper , we propose a novel question difficulty estimation approach called Regularized Competition Model ( RCM ) , which naturally combines question-user comparisons and questions' textual descriptions into a unified framework .	More interestingly , we observe that RCM might provide an automatic way to quantitatively measure the knowledge levels of words .	CL_D14-1118_mn_5	CL_D14-1118_mn_9	CL_D14-1118_mn_5_9
CL	D14-1118	mn	9	5	conclusion	proposal	support	none	secondary	main	forw	support	More interestingly , we observe that RCM might provide an automatic way to quantitatively measure the knowledge levels of words .	In this paper , we propose a novel question difficulty estimation approach called Regularized Competition Model ( RCM ) , which naturally combines question-user comparisons and questions' textual descriptions into a unified framework .	CL_D14-1118_mn_9	CL_D14-1118_mn_5	CL_D14-1118_mn_5_9
CL	D14-1118	mn	6	7	motivation_hypothesis	proposal_implementation	support	elaboration	secondary	secondary	none	none	By incorporating textual information , RCM can effectively deal with data sparseness problem .	We further employ a K-Nearest Neighbor approach to estimate difficulty levels of newly posted questions , again by leveraging textual similarities .	CL_D14-1118_mn_6	CL_D14-1118_mn_7	CL_D14-1118_mn_6_7
CL	D14-1118	mn	6	8	motivation_hypothesis	result_means	support	support	secondary	secondary	none	none	By incorporating textual information , RCM can effectively deal with data sparseness problem .	Experiments on two publicly available data sets show that for both well-resolved and newly-posted questions , RCM performs the estimation task significantly better than existing methods , demonstrating the advantage of incorporating textual information .	CL_D14-1118_mn_6	CL_D14-1118_mn_8	CL_D14-1118_mn_6_8
CL	D14-1118	mn	6	9	motivation_hypothesis	conclusion	support	support	secondary	secondary	none	none	By incorporating textual information , RCM can effectively deal with data sparseness problem .	More interestingly , we observe that RCM might provide an automatic way to quantitatively measure the knowledge levels of words .	CL_D14-1118_mn_6	CL_D14-1118_mn_9	CL_D14-1118_mn_6_9
CL	D14-1118	mn	7	8	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	We further employ a K-Nearest Neighbor approach to estimate difficulty levels of newly posted questions , again by leveraging textual similarities .	Experiments on two publicly available data sets show that for both well-resolved and newly-posted questions , RCM performs the estimation task significantly better than existing methods , demonstrating the advantage of incorporating textual information .	CL_D14-1118_mn_7	CL_D14-1118_mn_8	CL_D14-1118_mn_7_8
CL	D14-1118	mn	7	9	proposal_implementation	conclusion	elaboration	support	secondary	secondary	none	none	We further employ a K-Nearest Neighbor approach to estimate difficulty levels of newly posted questions , again by leveraging textual similarities .	More interestingly , we observe that RCM might provide an automatic way to quantitatively measure the knowledge levels of words .	CL_D14-1118_mn_7	CL_D14-1118_mn_9	CL_D14-1118_mn_7_9
CL	D14-1118	mn	8	9	result_means	conclusion	support	support	secondary	secondary	forw	support	Experiments on two publicly available data sets show that for both well-resolved and newly-posted questions , RCM performs the estimation task significantly better than existing methods , demonstrating the advantage of incorporating textual information .	More interestingly , we observe that RCM might provide an automatic way to quantitatively measure the knowledge levels of words .	CL_D14-1118_mn_8	CL_D14-1118_mn_9	CL_D14-1118_mn_8_9
CL	D14-1118	mn	9	8	conclusion	result_means	support	support	secondary	secondary	back	support	More interestingly , we observe that RCM might provide an automatic way to quantitatively measure the knowledge levels of words .	Experiments on two publicly available data sets show that for both well-resolved and newly-posted questions , RCM performs the estimation task significantly better than existing methods , demonstrating the advantage of incorporating textual information .	CL_D14-1118_mn_9	CL_D14-1118_mn_8	CL_D14-1118_mn_8_9
CL	D14-1119	mn	1	2	information_additional	proposal	info-optional	none	secondary	main	forw	info-optional	A poll consists of a question and a set of predefined answers from which voters can select .	We present the new problem of vote prediction on comments , which involves determining which of these answers a voter selected given a comment she wrote after voting .	CL_D14-1119_mn_1	CL_D14-1119_mn_2	CL_D14-1119_mn_1_2
CL	D14-1119	mn	2	1	proposal	information_additional	none	info-optional	main	secondary	back	info-optional	We present the new problem of vote prediction on comments , which involves determining which of these answers a voter selected given a comment she wrote after voting .	A poll consists of a question and a set of predefined answers from which voters can select .	CL_D14-1119_mn_2	CL_D14-1119_mn_1	CL_D14-1119_mn_1_2
CL	D14-1119	mn	1	3	information_additional	proposal_implementation	info-optional	elaboration	secondary	secondary	none	none	A poll consists of a question and a set of predefined answers from which voters can select .	To address this task , we exploit not only the information extracted from the comments but also extra-textual information such as user demographic information and inter-comment constraints .	CL_D14-1119_mn_1	CL_D14-1119_mn_3	CL_D14-1119_mn_1_3
CL	D14-1119	mn	1	4	information_additional	result_means	info-optional	support	secondary	secondary	none	none	A poll consists of a question and a set of predefined answers from which voters can select .	In an evaluation involving nearly one million comments collected from the popular SodaHead social polling website , we show that a vote prediction system that exploits only textual information can be improved significantly when extended with extra-textual information .	CL_D14-1119_mn_1	CL_D14-1119_mn_4	CL_D14-1119_mn_1_4
CL	D14-1119	mn	2	3	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	We present the new problem of vote prediction on comments , which involves determining which of these answers a voter selected given a comment she wrote after voting .	To address this task , we exploit not only the information extracted from the comments but also extra-textual information such as user demographic information and inter-comment constraints .	CL_D14-1119_mn_2	CL_D14-1119_mn_3	CL_D14-1119_mn_2_3
CL	D14-1119	mn	3	2	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	To address this task , we exploit not only the information extracted from the comments but also extra-textual information such as user demographic information and inter-comment constraints .	We present the new problem of vote prediction on comments , which involves determining which of these answers a voter selected given a comment she wrote after voting .	CL_D14-1119_mn_3	CL_D14-1119_mn_2	CL_D14-1119_mn_2_3
CL	D14-1119	mn	2	4	proposal	result_means	none	support	main	secondary	back	support	We present the new problem of vote prediction on comments , which involves determining which of these answers a voter selected given a comment she wrote after voting .	In an evaluation involving nearly one million comments collected from the popular SodaHead social polling website , we show that a vote prediction system that exploits only textual information can be improved significantly when extended with extra-textual information .	CL_D14-1119_mn_2	CL_D14-1119_mn_4	CL_D14-1119_mn_2_4
CL	D14-1119	mn	4	2	result_means	proposal	support	none	secondary	main	forw	support	In an evaluation involving nearly one million comments collected from the popular SodaHead social polling website , we show that a vote prediction system that exploits only textual information can be improved significantly when extended with extra-textual information .	We present the new problem of vote prediction on comments , which involves determining which of these answers a voter selected given a comment she wrote after voting .	CL_D14-1119_mn_4	CL_D14-1119_mn_2	CL_D14-1119_mn_2_4
CL	D14-1119	mn	3	4	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	To address this task , we exploit not only the information extracted from the comments but also extra-textual information such as user demographic information and inter-comment constraints .	In an evaluation involving nearly one million comments collected from the popular SodaHead social polling website , we show that a vote prediction system that exploits only textual information can be improved significantly when extended with extra-textual information .	CL_D14-1119_mn_3	CL_D14-1119_mn_4	CL_D14-1119_mn_3_4
CL	D14-1120	mn	1	2	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	In this paper we first exploit cash-tags ( " $ " followed by stocks' ticker symbols ) in Twitter to build a stock network , where nodes are stocks connected by edges when two stocks co-occur frequently in tweets .	We then employ a labeled topic model to jointly model both the tweets and the network structure to assign each node and each edge a topic respectively .	CL_D14-1120_mn_1	CL_D14-1120_mn_2	CL_D14-1120_mn_1_2
CL	D14-1120	mn	2	1	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	We then employ a labeled topic model to jointly model both the tweets and the network structure to assign each node and each edge a topic respectively .	In this paper we first exploit cash-tags ( " $ " followed by stocks' ticker symbols ) in Twitter to build a stock network , where nodes are stocks connected by edges when two stocks co-occur frequently in tweets .	CL_D14-1120_mn_2	CL_D14-1120_mn_1	CL_D14-1120_mn_1_2
CL	D14-1120	mn	1	3	proposal	conclusion	none	support	main	secondary	back	support	In this paper we first exploit cash-tags ( " $ " followed by stocks' ticker symbols ) in Twitter to build a stock network , where nodes are stocks connected by edges when two stocks co-occur frequently in tweets .	This Semantic Stock Network ( SSN ) summarizes discussion topics about stocks and stock relations .	CL_D14-1120_mn_1	CL_D14-1120_mn_3	CL_D14-1120_mn_1_3
CL	D14-1120	mn	3	1	conclusion	proposal	support	none	secondary	main	forw	support	This Semantic Stock Network ( SSN ) summarizes discussion topics about stocks and stock relations .	In this paper we first exploit cash-tags ( " $ " followed by stocks' ticker symbols ) in Twitter to build a stock network , where nodes are stocks connected by edges when two stocks co-occur frequently in tweets .	CL_D14-1120_mn_3	CL_D14-1120_mn_1	CL_D14-1120_mn_1_3
CL	D14-1120	mn	1	4	proposal	conclusion	none	support	main	secondary	none	none	In this paper we first exploit cash-tags ( " $ " followed by stocks' ticker symbols ) in Twitter to build a stock network , where nodes are stocks connected by edges when two stocks co-occur frequently in tweets .	We further show that social sentiment about stock ( node ) topics and stock relationship ( edge ) topics are predictive of each stock's market .	CL_D14-1120_mn_1	CL_D14-1120_mn_4	CL_D14-1120_mn_1_4
CL	D14-1120	mn	1	5	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	In this paper we first exploit cash-tags ( " $ " followed by stocks' ticker symbols ) in Twitter to build a stock network , where nodes are stocks connected by edges when two stocks co-occur frequently in tweets .	For prediction , we propose to regress the topic-sentiment time-series and the stock's price time series .	CL_D14-1120_mn_1	CL_D14-1120_mn_5	CL_D14-1120_mn_1_5
CL	D14-1120	mn	1	6	proposal	result	none	support	main	secondary	back	support	In this paper we first exploit cash-tags ( " $ " followed by stocks' ticker symbols ) in Twitter to build a stock network , where nodes are stocks connected by edges when two stocks co-occur frequently in tweets .	Experimental results demonstrate that topic sentiments from close neighbors are able to help improve the prediction of a stock markedly .	CL_D14-1120_mn_1	CL_D14-1120_mn_6	CL_D14-1120_mn_1_6
CL	D14-1120	mn	6	1	result	proposal	support	none	secondary	main	forw	support	Experimental results demonstrate that topic sentiments from close neighbors are able to help improve the prediction of a stock markedly .	In this paper we first exploit cash-tags ( " $ " followed by stocks' ticker symbols ) in Twitter to build a stock network , where nodes are stocks connected by edges when two stocks co-occur frequently in tweets .	CL_D14-1120_mn_6	CL_D14-1120_mn_1	CL_D14-1120_mn_1_6
CL	D14-1120	mn	2	3	proposal_implementation	conclusion	elaboration	support	secondary	secondary	none	none	We then employ a labeled topic model to jointly model both the tweets and the network structure to assign each node and each edge a topic respectively .	This Semantic Stock Network ( SSN ) summarizes discussion topics about stocks and stock relations .	CL_D14-1120_mn_2	CL_D14-1120_mn_3	CL_D14-1120_mn_2_3
CL	D14-1120	mn	2	4	proposal_implementation	conclusion	elaboration	support	secondary	secondary	none	none	We then employ a labeled topic model to jointly model both the tweets and the network structure to assign each node and each edge a topic respectively .	We further show that social sentiment about stock ( node ) topics and stock relationship ( edge ) topics are predictive of each stock's market .	CL_D14-1120_mn_2	CL_D14-1120_mn_4	CL_D14-1120_mn_2_4
CL	D14-1120	mn	2	5	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	We then employ a labeled topic model to jointly model both the tweets and the network structure to assign each node and each edge a topic respectively .	For prediction , we propose to regress the topic-sentiment time-series and the stock's price time series .	CL_D14-1120_mn_2	CL_D14-1120_mn_5	CL_D14-1120_mn_2_5
CL	D14-1120	mn	5	2	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	forw	elaboration	For prediction , we propose to regress the topic-sentiment time-series and the stock's price time series .	We then employ a labeled topic model to jointly model both the tweets and the network structure to assign each node and each edge a topic respectively .	CL_D14-1120_mn_5	CL_D14-1120_mn_2	CL_D14-1120_mn_2_5
CL	D14-1120	mn	2	6	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	We then employ a labeled topic model to jointly model both the tweets and the network structure to assign each node and each edge a topic respectively .	Experimental results demonstrate that topic sentiments from close neighbors are able to help improve the prediction of a stock markedly .	CL_D14-1120_mn_2	CL_D14-1120_mn_6	CL_D14-1120_mn_2_6
CL	D14-1120	mn	3	4	conclusion	conclusion	support	support	secondary	secondary	back	support	This Semantic Stock Network ( SSN ) summarizes discussion topics about stocks and stock relations .	We further show that social sentiment about stock ( node ) topics and stock relationship ( edge ) topics are predictive of each stock's market .	CL_D14-1120_mn_3	CL_D14-1120_mn_4	CL_D14-1120_mn_3_4
CL	D14-1120	mn	4	3	conclusion	conclusion	support	support	secondary	secondary	forw	support	We further show that social sentiment about stock ( node ) topics and stock relationship ( edge ) topics are predictive of each stock's market .	This Semantic Stock Network ( SSN ) summarizes discussion topics about stocks and stock relations .	CL_D14-1120_mn_4	CL_D14-1120_mn_3	CL_D14-1120_mn_3_4
CL	D14-1120	mn	3	5	conclusion	proposal_implementation	support	elaboration	secondary	secondary	none	none	This Semantic Stock Network ( SSN ) summarizes discussion topics about stocks and stock relations .	For prediction , we propose to regress the topic-sentiment time-series and the stock's price time series .	CL_D14-1120_mn_3	CL_D14-1120_mn_5	CL_D14-1120_mn_3_5
CL	D14-1120	mn	3	6	conclusion	result	support	support	secondary	secondary	none	none	This Semantic Stock Network ( SSN ) summarizes discussion topics about stocks and stock relations .	Experimental results demonstrate that topic sentiments from close neighbors are able to help improve the prediction of a stock markedly .	CL_D14-1120_mn_3	CL_D14-1120_mn_6	CL_D14-1120_mn_3_6
CL	D14-1120	mn	4	5	conclusion	proposal_implementation	support	elaboration	secondary	secondary	none	none	We further show that social sentiment about stock ( node ) topics and stock relationship ( edge ) topics are predictive of each stock's market .	For prediction , we propose to regress the topic-sentiment time-series and the stock's price time series .	CL_D14-1120_mn_4	CL_D14-1120_mn_5	CL_D14-1120_mn_4_5
CL	D14-1120	mn	4	6	conclusion	result	support	support	secondary	secondary	none	none	We further show that social sentiment about stock ( node ) topics and stock relationship ( edge ) topics are predictive of each stock's market .	Experimental results demonstrate that topic sentiments from close neighbors are able to help improve the prediction of a stock markedly .	CL_D14-1120_mn_4	CL_D14-1120_mn_6	CL_D14-1120_mn_4_6
CL	D14-1120	mn	5	6	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	For prediction , we propose to regress the topic-sentiment time-series and the stock's price time series .	Experimental results demonstrate that topic sentiments from close neighbors are able to help improve the prediction of a stock markedly .	CL_D14-1120_mn_5	CL_D14-1120_mn_6	CL_D14-1120_mn_5_6
CL	D14-1121	mn	1	2	motivation_background	proposal	support	none	secondary	main	forw	support	Demographic lexica have potential for widespread use in social science , economic , and business applications .	We derive predictive lexica ( words and weights ) for age and gender using regression and classification models from word usage in Facebook , blog , and Twitter data with associated demographic labels .	CL_D14-1121_mn_1	CL_D14-1121_mn_2	CL_D14-1121_mn_1_2
CL	D14-1121	mn	2	1	proposal	motivation_background	none	support	main	secondary	back	support	We derive predictive lexica ( words and weights ) for age and gender using regression and classification models from word usage in Facebook , blog , and Twitter data with associated demographic labels .	Demographic lexica have potential for widespread use in social science , economic , and business applications .	CL_D14-1121_mn_2	CL_D14-1121_mn_1	CL_D14-1121_mn_1_2
CL	D14-1121	mn	1	3	motivation_background	result_means	support	support	secondary	secondary	none	none	Demographic lexica have potential for widespread use in social science , economic , and business applications .	The lexica , made publicly available, achieved state-of-the-art accuracy in language based age and gender prediction over Facebook and Twitter , and were evaluated for generalization across social media genres as well as in limited message situations .	CL_D14-1121_mn_1	CL_D14-1121_mn_3	CL_D14-1121_mn_1_3
CL	D14-1121	mn	2	3	proposal	result_means	none	support	main	secondary	back	support	We derive predictive lexica ( words and weights ) for age and gender using regression and classification models from word usage in Facebook , blog , and Twitter data with associated demographic labels .	The lexica , made publicly available, achieved state-of-the-art accuracy in language based age and gender prediction over Facebook and Twitter , and were evaluated for generalization across social media genres as well as in limited message situations .	CL_D14-1121_mn_2	CL_D14-1121_mn_3	CL_D14-1121_mn_2_3
CL	D14-1121	mn	3	2	result_means	proposal	support	none	secondary	main	forw	support	The lexica , made publicly available, achieved state-of-the-art accuracy in language based age and gender prediction over Facebook and Twitter , and were evaluated for generalization across social media genres as well as in limited message situations .	We derive predictive lexica ( words and weights ) for age and gender using regression and classification models from word usage in Facebook , blog , and Twitter data with associated demographic labels .	CL_D14-1121_mn_3	CL_D14-1121_mn_2	CL_D14-1121_mn_2_3
CL	D14-1122	mn	1	2	motivation_background	motivation_problem	info-required	support	secondary	secondary	forw	info-required	Dependency parsing is a core task in NLP , and it is widely used by many applications such as information extraction , ques-tion answering , and machine translation .	In the era of social media , a big challenge is that parsers trained on traditional newswire corpora typically suffer from the domain mismatch issue , and thus perform poorly on social media data .	CL_D14-1122_mn_1	CL_D14-1122_mn_2	CL_D14-1122_mn_1_2
CL	D14-1122	mn	2	1	motivation_problem	motivation_background	support	info-required	secondary	secondary	back	info-required	In the era of social media , a big challenge is that parsers trained on traditional newswire corpora typically suffer from the domain mismatch issue , and thus perform poorly on social media data .	Dependency parsing is a core task in NLP , and it is widely used by many applications such as information extraction , ques-tion answering , and machine translation .	CL_D14-1122_mn_2	CL_D14-1122_mn_1	CL_D14-1122_mn_1_2
CL	D14-1122	mn	1	3	motivation_background	proposal	info-required	none	secondary	main	none	none	Dependency parsing is a core task in NLP , and it is widely used by many applications such as information extraction , ques-tion answering , and machine translation .	We present a new GFL/FUDG-annotated Chinese treebank with more than 18K tokens from Sina Weibo ( the Chinese equivalent of Twitter ) .	CL_D14-1122_mn_1	CL_D14-1122_mn_3	CL_D14-1122_mn_1_3
CL	D14-1122	mn	1	4	motivation_background	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	Dependency parsing is a core task in NLP , and it is widely used by many applications such as information extraction , ques-tion answering , and machine translation .	We formulate the dependency parsing problem as many small and parallelizable arc prediction tasks : for each task , we use a programmable probabilistic first-order logic to infer the dependency arc of a token in the sentence .	CL_D14-1122_mn_1	CL_D14-1122_mn_4	CL_D14-1122_mn_1_4
CL	D14-1122	mn	1	5	motivation_background	result	info-required	support	secondary	secondary	none	none	Dependency parsing is a core task in NLP , and it is widely used by many applications such as information extraction , ques-tion answering , and machine translation .	In experiments , we show that the proposed model outperforms an off-the-shelf Stanford Chinese parser , as well as a strong MaltParser baseline that is trained on the same in-domain data .	CL_D14-1122_mn_1	CL_D14-1122_mn_5	CL_D14-1122_mn_1_5
CL	D14-1122	mn	2	3	motivation_problem	proposal	support	none	secondary	main	forw	support	In the era of social media , a big challenge is that parsers trained on traditional newswire corpora typically suffer from the domain mismatch issue , and thus perform poorly on social media data .	We present a new GFL/FUDG-annotated Chinese treebank with more than 18K tokens from Sina Weibo ( the Chinese equivalent of Twitter ) .	CL_D14-1122_mn_2	CL_D14-1122_mn_3	CL_D14-1122_mn_2_3
CL	D14-1122	mn	3	2	proposal	motivation_problem	none	support	main	secondary	back	support	We present a new GFL/FUDG-annotated Chinese treebank with more than 18K tokens from Sina Weibo ( the Chinese equivalent of Twitter ) .	In the era of social media , a big challenge is that parsers trained on traditional newswire corpora typically suffer from the domain mismatch issue , and thus perform poorly on social media data .	CL_D14-1122_mn_3	CL_D14-1122_mn_2	CL_D14-1122_mn_2_3
CL	D14-1122	mn	2	4	motivation_problem	proposal_implementation	support	elaboration	secondary	secondary	none	none	In the era of social media , a big challenge is that parsers trained on traditional newswire corpora typically suffer from the domain mismatch issue , and thus perform poorly on social media data .	We formulate the dependency parsing problem as many small and parallelizable arc prediction tasks : for each task , we use a programmable probabilistic first-order logic to infer the dependency arc of a token in the sentence .	CL_D14-1122_mn_2	CL_D14-1122_mn_4	CL_D14-1122_mn_2_4
CL	D14-1122	mn	2	5	motivation_problem	result	support	support	secondary	secondary	none	none	In the era of social media , a big challenge is that parsers trained on traditional newswire corpora typically suffer from the domain mismatch issue , and thus perform poorly on social media data .	In experiments , we show that the proposed model outperforms an off-the-shelf Stanford Chinese parser , as well as a strong MaltParser baseline that is trained on the same in-domain data .	CL_D14-1122_mn_2	CL_D14-1122_mn_5	CL_D14-1122_mn_2_5
CL	D14-1122	mn	3	4	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	We present a new GFL/FUDG-annotated Chinese treebank with more than 18K tokens from Sina Weibo ( the Chinese equivalent of Twitter ) .	We formulate the dependency parsing problem as many small and parallelizable arc prediction tasks : for each task , we use a programmable probabilistic first-order logic to infer the dependency arc of a token in the sentence .	CL_D14-1122_mn_3	CL_D14-1122_mn_4	CL_D14-1122_mn_3_4
CL	D14-1122	mn	4	3	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	We formulate the dependency parsing problem as many small and parallelizable arc prediction tasks : for each task , we use a programmable probabilistic first-order logic to infer the dependency arc of a token in the sentence .	We present a new GFL/FUDG-annotated Chinese treebank with more than 18K tokens from Sina Weibo ( the Chinese equivalent of Twitter ) .	CL_D14-1122_mn_4	CL_D14-1122_mn_3	CL_D14-1122_mn_3_4
CL	D14-1122	mn	3	5	proposal	result	none	support	main	secondary	back	support	We present a new GFL/FUDG-annotated Chinese treebank with more than 18K tokens from Sina Weibo ( the Chinese equivalent of Twitter ) .	In experiments , we show that the proposed model outperforms an off-the-shelf Stanford Chinese parser , as well as a strong MaltParser baseline that is trained on the same in-domain data .	CL_D14-1122_mn_3	CL_D14-1122_mn_5	CL_D14-1122_mn_3_5
CL	D14-1122	mn	5	3	result	proposal	support	none	secondary	main	forw	support	In experiments , we show that the proposed model outperforms an off-the-shelf Stanford Chinese parser , as well as a strong MaltParser baseline that is trained on the same in-domain data .	We present a new GFL/FUDG-annotated Chinese treebank with more than 18K tokens from Sina Weibo ( the Chinese equivalent of Twitter ) .	CL_D14-1122_mn_5	CL_D14-1122_mn_3	CL_D14-1122_mn_3_5
CL	D14-1122	mn	4	5	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	We formulate the dependency parsing problem as many small and parallelizable arc prediction tasks : for each task , we use a programmable probabilistic first-order logic to infer the dependency arc of a token in the sentence .	In experiments , we show that the proposed model outperforms an off-the-shelf Stanford Chinese parser , as well as a strong MaltParser baseline that is trained on the same in-domain data .	CL_D14-1122_mn_4	CL_D14-1122_mn_5	CL_D14-1122_mn_4_5
CL	D14-1123	mn	1	2	motivation_background	motivation_background	info-optional	info-required	secondary	secondary	forw	info-optional	Microblog has become a major platform for information about real-world events .	Automatically discovering real-world events from microblog has attracted the attention of many researchers .	CL_D14-1123_mn_1	CL_D14-1123_mn_2	CL_D14-1123_mn_1_2
CL	D14-1123	mn	2	1	motivation_background	motivation_background	info-required	info-optional	secondary	secondary	back	info-optional	Automatically discovering real-world events from microblog has attracted the attention of many researchers .	Microblog has become a major platform for information about real-world events .	CL_D14-1123_mn_2	CL_D14-1123_mn_1	CL_D14-1123_mn_1_2
CL	D14-1123	mn	1	3	motivation_background	motivation_problem	info-optional	support	secondary	secondary	none	none	Microblog has become a major platform for information about real-world events .	However , most of existing work ignore the importance of emotion information for event detection .	CL_D14-1123_mn_1	CL_D14-1123_mn_3	CL_D14-1123_mn_1_3
CL	D14-1123	mn	1	4	motivation_background	motivation_hypothesis	info-optional	support	secondary	secondary	none	none	Microblog has become a major platform for information about real-world events .	We argue that people's emotional reactions immediately reflect the occurring of real-world events and should be important for event detection .	CL_D14-1123_mn_1	CL_D14-1123_mn_4	CL_D14-1123_mn_1_4
CL	D14-1123	mn	1	5	motivation_background	proposal	info-optional	none	secondary	main	none	none	Microblog has become a major platform for information about real-world events .	In this study , we focus on the problem of community-related event detection by community emotions .	CL_D14-1123_mn_1	CL_D14-1123_mn_5	CL_D14-1123_mn_1_5
CL	D14-1123	mn	1	6	motivation_background	proposal_implementation	info-optional	elaboration	secondary	secondary	none	none	Microblog has become a major platform for information about real-world events .	To address the problem , we propose a novel framework which include the following three key components : microblog emotion classification , community emotion aggregation and community emotion burst detection .	CL_D14-1123_mn_1	CL_D14-1123_mn_6	CL_D14-1123_mn_1_6
CL	D14-1123	mn	1	7	motivation_background	means	info-optional	by-means	secondary	secondary	none	none	Microblog has become a major platform for information about real-world events .	We evaluate our approach on real microblog data sets .	CL_D14-1123_mn_1	CL_D14-1123_mn_7	CL_D14-1123_mn_1_7
CL	D14-1123	mn	1	8	motivation_background	conclusion	info-optional	support	secondary	secondary	none	none	Microblog has become a major platform for information about real-world events .	Experimental results demonstrate the effectiveness of the proposed framework .	CL_D14-1123_mn_1	CL_D14-1123_mn_8	CL_D14-1123_mn_1_8
CL	D14-1123	mn	2	3	motivation_background	motivation_problem	info-required	support	secondary	secondary	forw	info-required	Automatically discovering real-world events from microblog has attracted the attention of many researchers .	However , most of existing work ignore the importance of emotion information for event detection .	CL_D14-1123_mn_2	CL_D14-1123_mn_3	CL_D14-1123_mn_2_3
CL	D14-1123	mn	3	2	motivation_problem	motivation_background	support	info-required	secondary	secondary	back	info-required	However , most of existing work ignore the importance of emotion information for event detection .	Automatically discovering real-world events from microblog has attracted the attention of many researchers .	CL_D14-1123_mn_3	CL_D14-1123_mn_2	CL_D14-1123_mn_2_3
CL	D14-1123	mn	2	4	motivation_background	motivation_hypothesis	info-required	support	secondary	secondary	none	none	Automatically discovering real-world events from microblog has attracted the attention of many researchers .	We argue that people's emotional reactions immediately reflect the occurring of real-world events and should be important for event detection .	CL_D14-1123_mn_2	CL_D14-1123_mn_4	CL_D14-1123_mn_2_4
CL	D14-1123	mn	2	5	motivation_background	proposal	info-required	none	secondary	main	none	none	Automatically discovering real-world events from microblog has attracted the attention of many researchers .	In this study , we focus on the problem of community-related event detection by community emotions .	CL_D14-1123_mn_2	CL_D14-1123_mn_5	CL_D14-1123_mn_2_5
CL	D14-1123	mn	2	6	motivation_background	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	Automatically discovering real-world events from microblog has attracted the attention of many researchers .	To address the problem , we propose a novel framework which include the following three key components : microblog emotion classification , community emotion aggregation and community emotion burst detection .	CL_D14-1123_mn_2	CL_D14-1123_mn_6	CL_D14-1123_mn_2_6
CL	D14-1123	mn	2	7	motivation_background	means	info-required	by-means	secondary	secondary	none	none	Automatically discovering real-world events from microblog has attracted the attention of many researchers .	We evaluate our approach on real microblog data sets .	CL_D14-1123_mn_2	CL_D14-1123_mn_7	CL_D14-1123_mn_2_7
CL	D14-1123	mn	2	8	motivation_background	conclusion	info-required	support	secondary	secondary	none	none	Automatically discovering real-world events from microblog has attracted the attention of many researchers .	Experimental results demonstrate the effectiveness of the proposed framework .	CL_D14-1123_mn_2	CL_D14-1123_mn_8	CL_D14-1123_mn_2_8
CL	D14-1123	mn	3	4	motivation_problem	motivation_hypothesis	support	support	secondary	secondary	forw	support	However , most of existing work ignore the importance of emotion information for event detection .	We argue that people's emotional reactions immediately reflect the occurring of real-world events and should be important for event detection .	CL_D14-1123_mn_3	CL_D14-1123_mn_4	CL_D14-1123_mn_3_4
CL	D14-1123	mn	4	3	motivation_hypothesis	motivation_problem	support	support	secondary	secondary	back	support	We argue that people's emotional reactions immediately reflect the occurring of real-world events and should be important for event detection .	However , most of existing work ignore the importance of emotion information for event detection .	CL_D14-1123_mn_4	CL_D14-1123_mn_3	CL_D14-1123_mn_3_4
CL	D14-1123	mn	3	5	motivation_problem	proposal	support	none	secondary	main	none	none	However , most of existing work ignore the importance of emotion information for event detection .	In this study , we focus on the problem of community-related event detection by community emotions .	CL_D14-1123_mn_3	CL_D14-1123_mn_5	CL_D14-1123_mn_3_5
CL	D14-1123	mn	3	6	motivation_problem	proposal_implementation	support	elaboration	secondary	secondary	none	none	However , most of existing work ignore the importance of emotion information for event detection .	To address the problem , we propose a novel framework which include the following three key components : microblog emotion classification , community emotion aggregation and community emotion burst detection .	CL_D14-1123_mn_3	CL_D14-1123_mn_6	CL_D14-1123_mn_3_6
CL	D14-1123	mn	3	7	motivation_problem	means	support	by-means	secondary	secondary	none	none	However , most of existing work ignore the importance of emotion information for event detection .	We evaluate our approach on real microblog data sets .	CL_D14-1123_mn_3	CL_D14-1123_mn_7	CL_D14-1123_mn_3_7
CL	D14-1123	mn	3	8	motivation_problem	conclusion	support	support	secondary	secondary	none	none	However , most of existing work ignore the importance of emotion information for event detection .	Experimental results demonstrate the effectiveness of the proposed framework .	CL_D14-1123_mn_3	CL_D14-1123_mn_8	CL_D14-1123_mn_3_8
CL	D14-1123	mn	4	5	motivation_hypothesis	proposal	support	none	secondary	main	forw	support	We argue that people's emotional reactions immediately reflect the occurring of real-world events and should be important for event detection .	In this study , we focus on the problem of community-related event detection by community emotions .	CL_D14-1123_mn_4	CL_D14-1123_mn_5	CL_D14-1123_mn_4_5
CL	D14-1123	mn	5	4	proposal	motivation_hypothesis	none	support	main	secondary	back	support	In this study , we focus on the problem of community-related event detection by community emotions .	We argue that people's emotional reactions immediately reflect the occurring of real-world events and should be important for event detection .	CL_D14-1123_mn_5	CL_D14-1123_mn_4	CL_D14-1123_mn_4_5
CL	D14-1123	mn	4	6	motivation_hypothesis	proposal_implementation	support	elaboration	secondary	secondary	none	none	We argue that people's emotional reactions immediately reflect the occurring of real-world events and should be important for event detection .	To address the problem , we propose a novel framework which include the following three key components : microblog emotion classification , community emotion aggregation and community emotion burst detection .	CL_D14-1123_mn_4	CL_D14-1123_mn_6	CL_D14-1123_mn_4_6
CL	D14-1123	mn	4	7	motivation_hypothesis	means	support	by-means	secondary	secondary	none	none	We argue that people's emotional reactions immediately reflect the occurring of real-world events and should be important for event detection .	We evaluate our approach on real microblog data sets .	CL_D14-1123_mn_4	CL_D14-1123_mn_7	CL_D14-1123_mn_4_7
CL	D14-1123	mn	4	8	motivation_hypothesis	conclusion	support	support	secondary	secondary	none	none	We argue that people's emotional reactions immediately reflect the occurring of real-world events and should be important for event detection .	Experimental results demonstrate the effectiveness of the proposed framework .	CL_D14-1123_mn_4	CL_D14-1123_mn_8	CL_D14-1123_mn_4_8
CL	D14-1123	mn	5	6	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	In this study , we focus on the problem of community-related event detection by community emotions .	To address the problem , we propose a novel framework which include the following three key components : microblog emotion classification , community emotion aggregation and community emotion burst detection .	CL_D14-1123_mn_5	CL_D14-1123_mn_6	CL_D14-1123_mn_5_6
CL	D14-1123	mn	6	5	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	To address the problem , we propose a novel framework which include the following three key components : microblog emotion classification , community emotion aggregation and community emotion burst detection .	In this study , we focus on the problem of community-related event detection by community emotions .	CL_D14-1123_mn_6	CL_D14-1123_mn_5	CL_D14-1123_mn_5_6
CL	D14-1123	mn	5	7	proposal	means	none	by-means	main	secondary	none	none	In this study , we focus on the problem of community-related event detection by community emotions .	We evaluate our approach on real microblog data sets .	CL_D14-1123_mn_5	CL_D14-1123_mn_7	CL_D14-1123_mn_5_7
CL	D14-1123	mn	5	8	proposal	conclusion	none	support	main	secondary	back	support	In this study , we focus on the problem of community-related event detection by community emotions .	Experimental results demonstrate the effectiveness of the proposed framework .	CL_D14-1123_mn_5	CL_D14-1123_mn_8	CL_D14-1123_mn_5_8
CL	D14-1123	mn	8	5	conclusion	proposal	support	none	secondary	main	forw	support	Experimental results demonstrate the effectiveness of the proposed framework .	In this study , we focus on the problem of community-related event detection by community emotions .	CL_D14-1123_mn_8	CL_D14-1123_mn_5	CL_D14-1123_mn_5_8
CL	D14-1123	mn	6	7	proposal_implementation	means	elaboration	by-means	secondary	secondary	none	none	To address the problem , we propose a novel framework which include the following three key components : microblog emotion classification , community emotion aggregation and community emotion burst detection .	We evaluate our approach on real microblog data sets .	CL_D14-1123_mn_6	CL_D14-1123_mn_7	CL_D14-1123_mn_6_7
CL	D14-1123	mn	6	8	proposal_implementation	conclusion	elaboration	support	secondary	secondary	none	none	To address the problem , we propose a novel framework which include the following three key components : microblog emotion classification , community emotion aggregation and community emotion burst detection .	Experimental results demonstrate the effectiveness of the proposed framework .	CL_D14-1123_mn_6	CL_D14-1123_mn_8	CL_D14-1123_mn_6_8
CL	D14-1123	mn	7	8	means	conclusion	by-means	support	secondary	secondary	forw	by-means	We evaluate our approach on real microblog data sets .	Experimental results demonstrate the effectiveness of the proposed framework .	CL_D14-1123_mn_7	CL_D14-1123_mn_8	CL_D14-1123_mn_7_8
CL	D14-1123	mn	8	7	conclusion	means	support	by-means	secondary	secondary	back	by-means	Experimental results demonstrate the effectiveness of the proposed framework .	We evaluate our approach on real microblog data sets .	CL_D14-1123_mn_8	CL_D14-1123_mn_7	CL_D14-1123_mn_7_8
CL	D14-1124	mn	1	2	motivation_background	motivation_problem	info-required	support	secondary	secondary	forw	info-required	Casual online forums such as Reddit , Slashdot and Digg , are continuing to increase in popularity as a means of communication .	Detecting disagreement in this domain is a considerable challenge .	CL_D14-1124_mn_1	CL_D14-1124_mn_2	CL_D14-1124_mn_1_2
CL	D14-1124	mn	2	1	motivation_problem	motivation_background	support	info-required	secondary	secondary	back	info-required	Detecting disagreement in this domain is a considerable challenge .	Casual online forums such as Reddit , Slashdot and Digg , are continuing to increase in popularity as a means of communication .	CL_D14-1124_mn_2	CL_D14-1124_mn_1	CL_D14-1124_mn_1_2
CL	D14-1124	mn	1	3	motivation_background	motivation_problem	info-required	support	secondary	secondary	none	none	Casual online forums such as Reddit , Slashdot and Digg , are continuing to increase in popularity as a means of communication .	Many topics are unique to the conversation on the forum , and the appearance of disagreement may be much more subtle than on political blogs or social media sites such as twitter .	CL_D14-1124_mn_1	CL_D14-1124_mn_3	CL_D14-1124_mn_1_3
CL	D14-1124	mn	1	4	motivation_background	proposal	info-required	none	secondary	secondary	none	none	Casual online forums such as Reddit , Slashdot and Digg , are continuing to increase in popularity as a means of communication .	In this analysis we present a crowd-sourced annotated corpus for topic level disagreement detection in Slashdot , showing that disagreement detection in this domain is difficult even for humans .	CL_D14-1124_mn_1	CL_D14-1124_mn_4	CL_D14-1124_mn_1_4
CL	D14-1124	mn	1	5	motivation_background	result	info-required	support	secondary	secondary	none	none	Casual online forums such as Reddit , Slashdot and Digg , are continuing to increase in popularity as a means of communication .	We then proceed to show that a new set of features determined from the rhetorical structure of the conversation significantly improves the performance on disagreement detection over a baseline consisting of unigram/bigram features , discourse markers , structural features and meta-post features .	CL_D14-1124_mn_1	CL_D14-1124_mn_5	CL_D14-1124_mn_1_5
CL	D14-1124	mn	2	3	motivation_problem	motivation_problem	support	support	secondary	secondary	forw	support	Detecting disagreement in this domain is a considerable challenge .	Many topics are unique to the conversation on the forum , and the appearance of disagreement may be much more subtle than on political blogs or social media sites such as twitter .	CL_D14-1124_mn_2	CL_D14-1124_mn_3	CL_D14-1124_mn_2_3
CL	D14-1124	mn	3	2	motivation_problem	motivation_problem	support	support	secondary	secondary	back	support	Many topics are unique to the conversation on the forum , and the appearance of disagreement may be much more subtle than on political blogs or social media sites such as twitter .	Detecting disagreement in this domain is a considerable challenge .	CL_D14-1124_mn_3	CL_D14-1124_mn_2	CL_D14-1124_mn_2_3
CL	D14-1124	mn	2	4	motivation_problem	proposal	support	none	secondary	secondary	none	none	Detecting disagreement in this domain is a considerable challenge .	In this analysis we present a crowd-sourced annotated corpus for topic level disagreement detection in Slashdot , showing that disagreement detection in this domain is difficult even for humans .	CL_D14-1124_mn_2	CL_D14-1124_mn_4	CL_D14-1124_mn_2_4
CL	D14-1124	mn	2	5	motivation_problem	result	support	support	secondary	secondary	none	none	Detecting disagreement in this domain is a considerable challenge .	We then proceed to show that a new set of features determined from the rhetorical structure of the conversation significantly improves the performance on disagreement detection over a baseline consisting of unigram/bigram features , discourse markers , structural features and meta-post features .	CL_D14-1124_mn_2	CL_D14-1124_mn_5	CL_D14-1124_mn_2_5
CL	D14-1124	mn	3	4	motivation_problem	proposal	support	none	secondary	secondary	forw	support	Many topics are unique to the conversation on the forum , and the appearance of disagreement may be much more subtle than on political blogs or social media sites such as twitter .	In this analysis we present a crowd-sourced annotated corpus for topic level disagreement detection in Slashdot , showing that disagreement detection in this domain is difficult even for humans .	CL_D14-1124_mn_3	CL_D14-1124_mn_4	CL_D14-1124_mn_3_4
CL	D14-1124	mn	4	3	proposal	motivation_problem	none	support	secondary	secondary	back	support	In this analysis we present a crowd-sourced annotated corpus for topic level disagreement detection in Slashdot , showing that disagreement detection in this domain is difficult even for humans .	Many topics are unique to the conversation on the forum , and the appearance of disagreement may be much more subtle than on political blogs or social media sites such as twitter .	CL_D14-1124_mn_4	CL_D14-1124_mn_3	CL_D14-1124_mn_3_4
CL	D14-1124	mn	3	5	motivation_problem	result	support	support	secondary	secondary	none	none	Many topics are unique to the conversation on the forum , and the appearance of disagreement may be much more subtle than on political blogs or social media sites such as twitter .	We then proceed to show that a new set of features determined from the rhetorical structure of the conversation significantly improves the performance on disagreement detection over a baseline consisting of unigram/bigram features , discourse markers , structural features and meta-post features .	CL_D14-1124_mn_3	CL_D14-1124_mn_5	CL_D14-1124_mn_3_5
CL	D14-1124	mn	4	5	proposal	result	none	support	secondary	secondary	back	support	In this analysis we present a crowd-sourced annotated corpus for topic level disagreement detection in Slashdot , showing that disagreement detection in this domain is difficult even for humans .	We then proceed to show that a new set of features determined from the rhetorical structure of the conversation significantly improves the performance on disagreement detection over a baseline consisting of unigram/bigram features , discourse markers , structural features and meta-post features .	CL_D14-1124_mn_4	CL_D14-1124_mn_5	CL_D14-1124_mn_4_5
CL	D14-1124	mn	5	4	result	proposal	support	none	secondary	secondary	forw	support	We then proceed to show that a new set of features determined from the rhetorical structure of the conversation significantly improves the performance on disagreement detection over a baseline consisting of unigram/bigram features , discourse markers , structural features and meta-post features .	In this analysis we present a crowd-sourced annotated corpus for topic level disagreement detection in Slashdot , showing that disagreement detection in this domain is difficult even for humans .	CL_D14-1124_mn_5	CL_D14-1124_mn_4	CL_D14-1124_mn_4_5
CL	D14-1125	mn	1	2	motivation_background	proposal	support	none	secondary	main	forw	support	Recently , work in NLP was initiated on a type of opinion inference that arises when opinions are expressed toward events which have positive or negative effects on entities ( +/-effect events ) .	This paper addresses methods for creating a lexicon of such events , to support such work on opinion inference .	CL_D14-1125_mn_1	CL_D14-1125_mn_2	CL_D14-1125_mn_1_2
CL	D14-1125	mn	2	1	proposal	motivation_background	none	support	main	secondary	back	support	This paper addresses methods for creating a lexicon of such events , to support such work on opinion inference .	Recently , work in NLP was initiated on a type of opinion inference that arises when opinions are expressed toward events which have positive or negative effects on entities ( +/-effect events ) .	CL_D14-1125_mn_2	CL_D14-1125_mn_1	CL_D14-1125_mn_1_2
CL	D14-1125	mn	1	3	motivation_background	proposal_implementation	support	elaboration	secondary	secondary	none	none	Recently , work in NLP was initiated on a type of opinion inference that arises when opinions are expressed toward events which have positive or negative effects on entities ( +/-effect events ) .	Due to significant sense ambiguity , our goal is to develop a sense-level rather than word-level lexicon .	CL_D14-1125_mn_1	CL_D14-1125_mn_3	CL_D14-1125_mn_1_3
CL	D14-1125	mn	1	4	motivation_background	proposal_implementation	support	elaboration	secondary	secondary	none	none	Recently , work in NLP was initiated on a type of opinion inference that arises when opinions are expressed toward events which have positive or negative effects on entities ( +/-effect events ) .	To maximize the effectiveness of different types of information , we combine a graph-based method using WordNet1 relations and a standard classifier using gloss information .	CL_D14-1125_mn_1	CL_D14-1125_mn_4	CL_D14-1125_mn_1_4
CL	D14-1125	mn	1	5	motivation_background	proposal_implementation	support	elaboration	secondary	secondary	none	none	Recently , work in NLP was initiated on a type of opinion inference that arises when opinions are expressed toward events which have positive or negative effects on entities ( +/-effect events ) .	A hybrid between the two gives the best results .	CL_D14-1125_mn_1	CL_D14-1125_mn_5	CL_D14-1125_mn_1_5
CL	D14-1125	mn	1	6	motivation_background	result	support	support	secondary	secondary	none	none	Recently , work in NLP was initiated on a type of opinion inference that arises when opinions are expressed toward events which have positive or negative effects on entities ( +/-effect events ) .	Further , we provide evidence that the model is an effective way to guide manual annotation to find +/-effect senses that are not in the seed set .	CL_D14-1125_mn_1	CL_D14-1125_mn_6	CL_D14-1125_mn_1_6
CL	D14-1125	mn	2	3	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	This paper addresses methods for creating a lexicon of such events , to support such work on opinion inference .	Due to significant sense ambiguity , our goal is to develop a sense-level rather than word-level lexicon .	CL_D14-1125_mn_2	CL_D14-1125_mn_3	CL_D14-1125_mn_2_3
CL	D14-1125	mn	3	2	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	Due to significant sense ambiguity , our goal is to develop a sense-level rather than word-level lexicon .	This paper addresses methods for creating a lexicon of such events , to support such work on opinion inference .	CL_D14-1125_mn_3	CL_D14-1125_mn_2	CL_D14-1125_mn_2_3
CL	D14-1125	mn	2	4	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	This paper addresses methods for creating a lexicon of such events , to support such work on opinion inference .	To maximize the effectiveness of different types of information , we combine a graph-based method using WordNet1 relations and a standard classifier using gloss information .	CL_D14-1125_mn_2	CL_D14-1125_mn_4	CL_D14-1125_mn_2_4
CL	D14-1125	mn	2	5	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	This paper addresses methods for creating a lexicon of such events , to support such work on opinion inference .	A hybrid between the two gives the best results .	CL_D14-1125_mn_2	CL_D14-1125_mn_5	CL_D14-1125_mn_2_5
CL	D14-1125	mn	2	6	proposal	result	none	support	main	secondary	none	none	This paper addresses methods for creating a lexicon of such events , to support such work on opinion inference .	Further , we provide evidence that the model is an effective way to guide manual annotation to find +/-effect senses that are not in the seed set .	CL_D14-1125_mn_2	CL_D14-1125_mn_6	CL_D14-1125_mn_2_6
CL	D14-1125	mn	3	4	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	Due to significant sense ambiguity , our goal is to develop a sense-level rather than word-level lexicon .	To maximize the effectiveness of different types of information , we combine a graph-based method using WordNet1 relations and a standard classifier using gloss information .	CL_D14-1125_mn_3	CL_D14-1125_mn_4	CL_D14-1125_mn_3_4
CL	D14-1125	mn	4	3	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	forw	elaboration	To maximize the effectiveness of different types of information , we combine a graph-based method using WordNet1 relations and a standard classifier using gloss information .	Due to significant sense ambiguity , our goal is to develop a sense-level rather than word-level lexicon .	CL_D14-1125_mn_4	CL_D14-1125_mn_3	CL_D14-1125_mn_3_4
CL	D14-1125	mn	3	5	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	none	none	Due to significant sense ambiguity , our goal is to develop a sense-level rather than word-level lexicon .	A hybrid between the two gives the best results .	CL_D14-1125_mn_3	CL_D14-1125_mn_5	CL_D14-1125_mn_3_5
CL	D14-1125	mn	3	6	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	Due to significant sense ambiguity , our goal is to develop a sense-level rather than word-level lexicon .	Further , we provide evidence that the model is an effective way to guide manual annotation to find +/-effect senses that are not in the seed set .	CL_D14-1125_mn_3	CL_D14-1125_mn_6	CL_D14-1125_mn_3_6
CL	D14-1125	mn	4	5	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	To maximize the effectiveness of different types of information , we combine a graph-based method using WordNet1 relations and a standard classifier using gloss information .	A hybrid between the two gives the best results .	CL_D14-1125_mn_4	CL_D14-1125_mn_5	CL_D14-1125_mn_4_5
CL	D14-1125	mn	5	4	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	forw	elaboration	A hybrid between the two gives the best results .	To maximize the effectiveness of different types of information , we combine a graph-based method using WordNet1 relations and a standard classifier using gloss information .	CL_D14-1125_mn_5	CL_D14-1125_mn_4	CL_D14-1125_mn_4_5
CL	D14-1125	mn	4	6	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	To maximize the effectiveness of different types of information , we combine a graph-based method using WordNet1 relations and a standard classifier using gloss information .	Further , we provide evidence that the model is an effective way to guide manual annotation to find +/-effect senses that are not in the seed set .	CL_D14-1125_mn_4	CL_D14-1125_mn_6	CL_D14-1125_mn_4_6
CL	D14-1125	mn	5	6	proposal_implementation	result	elaboration	support	secondary	secondary	back	support	A hybrid between the two gives the best results .	Further , we provide evidence that the model is an effective way to guide manual annotation to find +/-effect senses that are not in the seed set .	CL_D14-1125_mn_5	CL_D14-1125_mn_6	CL_D14-1125_mn_5_6
CL	D14-1125	mn	6	5	result	proposal_implementation	support	elaboration	secondary	secondary	forw	support	Further , we provide evidence that the model is an effective way to guide manual annotation to find +/-effect senses that are not in the seed set .	A hybrid between the two gives the best results .	CL_D14-1125_mn_6	CL_D14-1125_mn_5	CL_D14-1125_mn_5_6
CL	D14-1126	mn	1	2	motivation_background	proposal	support	none	secondary	main	forw	support	Aspect-based opinion mining has attracted lots of attention today .	In this paper , we address the problem of product aspect rating prediction , where we would like to extract the product aspects , and predict aspect ratings simultaneously .	CL_D14-1126_mn_1	CL_D14-1126_mn_2	CL_D14-1126_mn_1_2
CL	D14-1126	mn	2	1	proposal	motivation_background	none	support	main	secondary	back	support	In this paper , we address the problem of product aspect rating prediction , where we would like to extract the product aspects , and predict aspect ratings simultaneously .	Aspect-based opinion mining has attracted lots of attention today .	CL_D14-1126_mn_2	CL_D14-1126_mn_1	CL_D14-1126_mn_1_2
CL	D14-1126	mn	1	3	motivation_background	motivation_problem	support	info-required	secondary	secondary	none	none	Aspect-based opinion mining has attracted lots of attention today .	Topic models have been widely adapted to jointly model aspects and sentiments , but existing models may not do the prediction task well due to their weakness in sentiment extraction .	CL_D14-1126_mn_1	CL_D14-1126_mn_3	CL_D14-1126_mn_1_3
CL	D14-1126	mn	1	4	motivation_background	motivation_problem	support	support	secondary	secondary	none	none	Aspect-based opinion mining has attracted lots of attention today .	The sentiment topics usually do not have clear correspondence to commonly used ratings , and the model may fail to extract certain kinds of sentiments due to skewed data .	CL_D14-1126_mn_1	CL_D14-1126_mn_4	CL_D14-1126_mn_1_4
CL	D14-1126	mn	1	5	motivation_background	proposal_implementation	support	elaboration	secondary	secondary	none	none	Aspect-based opinion mining has attracted lots of attention today .	To tackle this problem , we propose a sentiment-aligned topic model ( SATM ) , where we incorporate two types of external knowledge : product-level overall rating distribution and word-level sentiment lexicon .	CL_D14-1126_mn_1	CL_D14-1126_mn_5	CL_D14-1126_mn_1_5
CL	D14-1126	mn	1	6	motivation_background	result	support	support	secondary	secondary	none	none	Aspect-based opinion mining has attracted lots of attention today .	Experiments on real dataset demonstrate that SATM is effective on product aspect rating prediction , and it achieves better performance compared to the existing approaches .	CL_D14-1126_mn_1	CL_D14-1126_mn_6	CL_D14-1126_mn_1_6
CL	D14-1126	mn	2	3	proposal	motivation_problem	none	info-required	main	secondary	none	none	In this paper , we address the problem of product aspect rating prediction , where we would like to extract the product aspects , and predict aspect ratings simultaneously .	Topic models have been widely adapted to jointly model aspects and sentiments , but existing models may not do the prediction task well due to their weakness in sentiment extraction .	CL_D14-1126_mn_2	CL_D14-1126_mn_3	CL_D14-1126_mn_2_3
CL	D14-1126	mn	2	4	proposal	motivation_problem	none	support	main	secondary	none	none	In this paper , we address the problem of product aspect rating prediction , where we would like to extract the product aspects , and predict aspect ratings simultaneously .	The sentiment topics usually do not have clear correspondence to commonly used ratings , and the model may fail to extract certain kinds of sentiments due to skewed data .	CL_D14-1126_mn_2	CL_D14-1126_mn_4	CL_D14-1126_mn_2_4
CL	D14-1126	mn	2	5	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	In this paper , we address the problem of product aspect rating prediction , where we would like to extract the product aspects , and predict aspect ratings simultaneously .	To tackle this problem , we propose a sentiment-aligned topic model ( SATM ) , where we incorporate two types of external knowledge : product-level overall rating distribution and word-level sentiment lexicon .	CL_D14-1126_mn_2	CL_D14-1126_mn_5	CL_D14-1126_mn_2_5
CL	D14-1126	mn	5	2	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	To tackle this problem , we propose a sentiment-aligned topic model ( SATM ) , where we incorporate two types of external knowledge : product-level overall rating distribution and word-level sentiment lexicon .	In this paper , we address the problem of product aspect rating prediction , where we would like to extract the product aspects , and predict aspect ratings simultaneously .	CL_D14-1126_mn_5	CL_D14-1126_mn_2	CL_D14-1126_mn_2_5
CL	D14-1126	mn	2	6	proposal	result	none	support	main	secondary	back	support	In this paper , we address the problem of product aspect rating prediction , where we would like to extract the product aspects , and predict aspect ratings simultaneously .	Experiments on real dataset demonstrate that SATM is effective on product aspect rating prediction , and it achieves better performance compared to the existing approaches .	CL_D14-1126_mn_2	CL_D14-1126_mn_6	CL_D14-1126_mn_2_6
CL	D14-1126	mn	6	2	result	proposal	support	none	secondary	main	forw	support	Experiments on real dataset demonstrate that SATM is effective on product aspect rating prediction , and it achieves better performance compared to the existing approaches .	In this paper , we address the problem of product aspect rating prediction , where we would like to extract the product aspects , and predict aspect ratings simultaneously .	CL_D14-1126_mn_6	CL_D14-1126_mn_2	CL_D14-1126_mn_2_6
CL	D14-1126	mn	3	4	motivation_problem	motivation_problem	info-required	support	secondary	secondary	back	support	Topic models have been widely adapted to jointly model aspects and sentiments , but existing models may not do the prediction task well due to their weakness in sentiment extraction .	The sentiment topics usually do not have clear correspondence to commonly used ratings , and the model may fail to extract certain kinds of sentiments due to skewed data .	CL_D14-1126_mn_3	CL_D14-1126_mn_4	CL_D14-1126_mn_3_4
CL	D14-1126	mn	4	3	motivation_problem	motivation_problem	support	info-required	secondary	secondary	forw	support	The sentiment topics usually do not have clear correspondence to commonly used ratings , and the model may fail to extract certain kinds of sentiments due to skewed data .	Topic models have been widely adapted to jointly model aspects and sentiments , but existing models may not do the prediction task well due to their weakness in sentiment extraction .	CL_D14-1126_mn_4	CL_D14-1126_mn_3	CL_D14-1126_mn_3_4
CL	D14-1126	mn	3	5	motivation_problem	proposal_implementation	info-required	elaboration	secondary	secondary	forw	info-required	Topic models have been widely adapted to jointly model aspects and sentiments , but existing models may not do the prediction task well due to their weakness in sentiment extraction .	To tackle this problem , we propose a sentiment-aligned topic model ( SATM ) , where we incorporate two types of external knowledge : product-level overall rating distribution and word-level sentiment lexicon .	CL_D14-1126_mn_3	CL_D14-1126_mn_5	CL_D14-1126_mn_3_5
CL	D14-1126	mn	5	3	proposal_implementation	motivation_problem	elaboration	info-required	secondary	secondary	back	info-required	To tackle this problem , we propose a sentiment-aligned topic model ( SATM ) , where we incorporate two types of external knowledge : product-level overall rating distribution and word-level sentiment lexicon .	Topic models have been widely adapted to jointly model aspects and sentiments , but existing models may not do the prediction task well due to their weakness in sentiment extraction .	CL_D14-1126_mn_5	CL_D14-1126_mn_3	CL_D14-1126_mn_3_5
CL	D14-1126	mn	3	6	motivation_problem	result	info-required	support	secondary	secondary	none	none	Topic models have been widely adapted to jointly model aspects and sentiments , but existing models may not do the prediction task well due to their weakness in sentiment extraction .	Experiments on real dataset demonstrate that SATM is effective on product aspect rating prediction , and it achieves better performance compared to the existing approaches .	CL_D14-1126_mn_3	CL_D14-1126_mn_6	CL_D14-1126_mn_3_6
CL	D14-1126	mn	4	5	motivation_problem	proposal_implementation	support	elaboration	secondary	secondary	none	none	The sentiment topics usually do not have clear correspondence to commonly used ratings , and the model may fail to extract certain kinds of sentiments due to skewed data .	To tackle this problem , we propose a sentiment-aligned topic model ( SATM ) , where we incorporate two types of external knowledge : product-level overall rating distribution and word-level sentiment lexicon .	CL_D14-1126_mn_4	CL_D14-1126_mn_5	CL_D14-1126_mn_4_5
CL	D14-1126	mn	4	6	motivation_problem	result	support	support	secondary	secondary	none	none	The sentiment topics usually do not have clear correspondence to commonly used ratings , and the model may fail to extract certain kinds of sentiments due to skewed data .	Experiments on real dataset demonstrate that SATM is effective on product aspect rating prediction , and it achieves better performance compared to the existing approaches .	CL_D14-1126_mn_4	CL_D14-1126_mn_6	CL_D14-1126_mn_4_6
CL	D14-1126	mn	5	6	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	To tackle this problem , we propose a sentiment-aligned topic model ( SATM ) , where we incorporate two types of external knowledge : product-level overall rating distribution and word-level sentiment lexicon .	Experiments on real dataset demonstrate that SATM is effective on product aspect rating prediction , and it achieves better performance compared to the existing approaches .	CL_D14-1126_mn_5	CL_D14-1126_mn_6	CL_D14-1126_mn_5_6
CL	D14-1127	mn	1	2	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	We present a weakly supervised approach for learning hashtags , hashtag patterns , and phrases associated with five emotions : AFFECTION , ANGER/RAGE , FEAR/ANXIETY , JOY , and SADNESS/DISAPPOINTMENT .	Starting with seed hashtags to label an initial set of tweets , we train emotion classifiers and use them to learn new emotion hashtags and hashtag patterns .	CL_D14-1127_mn_1	CL_D14-1127_mn_2	CL_D14-1127_mn_1_2
CL	D14-1127	mn	2	1	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	Starting with seed hashtags to label an initial set of tweets , we train emotion classifiers and use them to learn new emotion hashtags and hashtag patterns .	We present a weakly supervised approach for learning hashtags , hashtag patterns , and phrases associated with five emotions : AFFECTION , ANGER/RAGE , FEAR/ANXIETY , JOY , and SADNESS/DISAPPOINTMENT .	CL_D14-1127_mn_2	CL_D14-1127_mn_1	CL_D14-1127_mn_1_2
CL	D14-1127	mn	1	3	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	We present a weakly supervised approach for learning hashtags , hashtag patterns , and phrases associated with five emotions : AFFECTION , ANGER/RAGE , FEAR/ANXIETY , JOY , and SADNESS/DISAPPOINTMENT .	This process then repeats in a bootstrapping framework .	CL_D14-1127_mn_1	CL_D14-1127_mn_3	CL_D14-1127_mn_1_3
CL	D14-1127	mn	1	4	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	We present a weakly supervised approach for learning hashtags , hashtag patterns , and phrases associated with five emotions : AFFECTION , ANGER/RAGE , FEAR/ANXIETY , JOY , and SADNESS/DISAPPOINTMENT .	Emotion phrases are also extracted from the learned hashtags and used to create phrase-based emotion classifiers .	CL_D14-1127_mn_1	CL_D14-1127_mn_4	CL_D14-1127_mn_1_4
CL	D14-1127	mn	1	5	proposal	observation	none	support	main	secondary	back	support	We present a weakly supervised approach for learning hashtags , hashtag patterns , and phrases associated with five emotions : AFFECTION , ANGER/RAGE , FEAR/ANXIETY , JOY , and SADNESS/DISAPPOINTMENT .	We show that the learned set of emotion indicators yields a substantial improve-ment in F-scores , ranging from + % 5 to + % 18 over baseline classifiers .	CL_D14-1127_mn_1	CL_D14-1127_mn_5	CL_D14-1127_mn_1_5
CL	D14-1127	mn	5	1	observation	proposal	support	none	secondary	main	forw	support	We show that the learned set of emotion indicators yields a substantial improve-ment in F-scores , ranging from + % 5 to + % 18 over baseline classifiers .	We present a weakly supervised approach for learning hashtags , hashtag patterns , and phrases associated with five emotions : AFFECTION , ANGER/RAGE , FEAR/ANXIETY , JOY , and SADNESS/DISAPPOINTMENT .	CL_D14-1127_mn_5	CL_D14-1127_mn_1	CL_D14-1127_mn_1_5
CL	D14-1127	mn	2	3	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	Starting with seed hashtags to label an initial set of tweets , we train emotion classifiers and use them to learn new emotion hashtags and hashtag patterns .	This process then repeats in a bootstrapping framework .	CL_D14-1127_mn_2	CL_D14-1127_mn_3	CL_D14-1127_mn_2_3
CL	D14-1127	mn	3	2	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	forw	elaboration	This process then repeats in a bootstrapping framework .	Starting with seed hashtags to label an initial set of tweets , we train emotion classifiers and use them to learn new emotion hashtags and hashtag patterns .	CL_D14-1127_mn_3	CL_D14-1127_mn_2	CL_D14-1127_mn_2_3
CL	D14-1127	mn	2	4	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	Starting with seed hashtags to label an initial set of tweets , we train emotion classifiers and use them to learn new emotion hashtags and hashtag patterns .	Emotion phrases are also extracted from the learned hashtags and used to create phrase-based emotion classifiers .	CL_D14-1127_mn_2	CL_D14-1127_mn_4	CL_D14-1127_mn_2_4
CL	D14-1127	mn	4	2	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	forw	elaboration	Emotion phrases are also extracted from the learned hashtags and used to create phrase-based emotion classifiers .	Starting with seed hashtags to label an initial set of tweets , we train emotion classifiers and use them to learn new emotion hashtags and hashtag patterns .	CL_D14-1127_mn_4	CL_D14-1127_mn_2	CL_D14-1127_mn_2_4
CL	D14-1127	mn	2	5	proposal_implementation	observation	elaboration	support	secondary	secondary	none	none	Starting with seed hashtags to label an initial set of tweets , we train emotion classifiers and use them to learn new emotion hashtags and hashtag patterns .	We show that the learned set of emotion indicators yields a substantial improve-ment in F-scores , ranging from + % 5 to + % 18 over baseline classifiers .	CL_D14-1127_mn_2	CL_D14-1127_mn_5	CL_D14-1127_mn_2_5
CL	D14-1127	mn	3	4	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	none	none	This process then repeats in a bootstrapping framework .	Emotion phrases are also extracted from the learned hashtags and used to create phrase-based emotion classifiers .	CL_D14-1127_mn_3	CL_D14-1127_mn_4	CL_D14-1127_mn_3_4
CL	D14-1127	mn	3	5	proposal_implementation	observation	elaboration	support	secondary	secondary	none	none	This process then repeats in a bootstrapping framework .	We show that the learned set of emotion indicators yields a substantial improve-ment in F-scores , ranging from + % 5 to + % 18 over baseline classifiers .	CL_D14-1127_mn_3	CL_D14-1127_mn_5	CL_D14-1127_mn_3_5
CL	D14-1127	mn	4	5	proposal_implementation	observation	elaboration	support	secondary	secondary	none	none	Emotion phrases are also extracted from the learned hashtags and used to create phrase-based emotion classifiers .	We show that the learned set of emotion indicators yields a substantial improve-ment in F-scores , ranging from + % 5 to + % 18 over baseline classifiers .	CL_D14-1127_mn_4	CL_D14-1127_mn_5	CL_D14-1127_mn_4_5
CL	D14-1128	mn	1	2	motivation_hypothesis	proposal	support	none	secondary	main	forw	support	We put forward the hypothesis that high-accuracy sentiment analysis is only possible if word senses with different polarity are accurately recognized .	We provide evidence for this hypothesis in a case study for the adjective "hard" and propose contextually enhanced sentiment lexicons that contain the information necessary for sentiment-relevant sense disambiguation .	CL_D14-1128_mn_1	CL_D14-1128_mn_2	CL_D14-1128_mn_1_2
CL	D14-1128	mn	2	1	proposal	motivation_hypothesis	none	support	main	secondary	back	support	We provide evidence for this hypothesis in a case study for the adjective "hard" and propose contextually enhanced sentiment lexicons that contain the information necessary for sentiment-relevant sense disambiguation .	We put forward the hypothesis that high-accuracy sentiment analysis is only possible if word senses with different polarity are accurately recognized .	CL_D14-1128_mn_2	CL_D14-1128_mn_1	CL_D14-1128_mn_1_2
CL	D14-1128	mn	1	3	motivation_hypothesis	result	support	support	secondary	secondary	none	none	We put forward the hypothesis that high-accuracy sentiment analysis is only possible if word senses with different polarity are accurately recognized .	An experimental evaluation demonstrates that senses with different polarity can be distinguished well using a combination of standard and novel features .	CL_D14-1128_mn_1	CL_D14-1128_mn_3	CL_D14-1128_mn_1_3
CL	D14-1128	mn	2	3	proposal	result	none	support	main	secondary	back	support	We provide evidence for this hypothesis in a case study for the adjective "hard" and propose contextually enhanced sentiment lexicons that contain the information necessary for sentiment-relevant sense disambiguation .	An experimental evaluation demonstrates that senses with different polarity can be distinguished well using a combination of standard and novel features .	CL_D14-1128_mn_2	CL_D14-1128_mn_3	CL_D14-1128_mn_2_3
CL	D14-1128	mn	3	2	result	proposal	support	none	secondary	main	forw	support	An experimental evaluation demonstrates that senses with different polarity can be distinguished well using a combination of standard and novel features .	We provide evidence for this hypothesis in a case study for the adjective "hard" and propose contextually enhanced sentiment lexicons that contain the information necessary for sentiment-relevant sense disambiguation .	CL_D14-1128_mn_3	CL_D14-1128_mn_2	CL_D14-1128_mn_2_3
CL	D14-1129	mn	1	2	motivation_background	proposal	support	none	secondary	main	forw	support	Identifying parallel web pages from bilingual web sites is a crucial step of bilingual resource construction for cross-lingual information processing .	In this paper , we propose a link-based approach to distinguish parallel web pages from bilingual web sites .	CL_D14-1129_mn_1	CL_D14-1129_mn_2	CL_D14-1129_mn_1_2
CL	D14-1129	mn	2	1	proposal	motivation_background	none	support	main	secondary	back	support	In this paper , we propose a link-based approach to distinguish parallel web pages from bilingual web sites .	Identifying parallel web pages from bilingual web sites is a crucial step of bilingual resource construction for cross-lingual information processing .	CL_D14-1129_mn_2	CL_D14-1129_mn_1	CL_D14-1129_mn_1_2
CL	D14-1129	mn	1	3	motivation_background	motivation_hypothesis	support	support	secondary	secondary	none	none	Identifying parallel web pages from bilingual web sites is a crucial step of bilingual resource construction for cross-lingual information processing .	Compared with the existing methods , which only employ the internal translation similarity ( such as content-based similarity and page structural similarity ) , we hypothesize that the external translation similarity is an effective feature to identify parallel web pages .	CL_D14-1129_mn_1	CL_D14-1129_mn_3	CL_D14-1129_mn_1_3
CL	D14-1129	mn	1	4	motivation_background	information_additional	support	info-optional	secondary	secondary	none	none	Identifying parallel web pages from bilingual web sites is a crucial step of bilingual resource construction for cross-lingual information processing .	Within a bilingual web site , web pages are interconnected by hyperlinks .	CL_D14-1129_mn_1	CL_D14-1129_mn_4	CL_D14-1129_mn_1_4
CL	D14-1129	mn	1	5	motivation_background	proposal_implementation	support	elaboration	secondary	secondary	none	none	Identifying parallel web pages from bilingual web sites is a crucial step of bilingual resource construction for cross-lingual information processing .	The basic idea of our method is that the translation similarity of two pages can be inferred from their neighbor pages , which can be adopted as an important source of external similarity .	CL_D14-1129_mn_1	CL_D14-1129_mn_5	CL_D14-1129_mn_1_5
CL	D14-1129	mn	1	6	motivation_background	proposal_implementation	support	elaboration	secondary	secondary	none	none	Identifying parallel web pages from bilingual web sites is a crucial step of bilingual resource construction for cross-lingual information processing .	Thus , the translation similarity of page pairs will influence each other .	CL_D14-1129_mn_1	CL_D14-1129_mn_6	CL_D14-1129_mn_1_6
CL	D14-1129	mn	1	7	motivation_background	proposal_implementation	support	elaboration	secondary	secondary	none	none	Identifying parallel web pages from bilingual web sites is a crucial step of bilingual resource construction for cross-lingual information processing .	An iterative algorithm is developed to estimate the external translation similarity and the final translation similarity .	CL_D14-1129_mn_1	CL_D14-1129_mn_7	CL_D14-1129_mn_1_7
CL	D14-1129	mn	1	8	motivation_background	proposal_implementation	support	elaboration	secondary	secondary	none	none	Identifying parallel web pages from bilingual web sites is a crucial step of bilingual resource construction for cross-lingual information processing .	Both internal and external similarity measures are combined in the iterative algorithm .	CL_D14-1129_mn_1	CL_D14-1129_mn_8	CL_D14-1129_mn_1_8
CL	D14-1129	mn	1	9	motivation_background	result_means	support	support	secondary	secondary	none	none	Identifying parallel web pages from bilingual web sites is a crucial step of bilingual resource construction for cross-lingual information processing .	Experiments on six bilingual websites demonstrate that our method is effective and obtains significant improvement ( 6.2 % F-Score ) over the baseline which only utilizes internal translation similarity .	CL_D14-1129_mn_1	CL_D14-1129_mn_9	CL_D14-1129_mn_1_9
CL	D14-1129	mn	2	3	proposal	motivation_hypothesis	none	support	main	secondary	back	support	In this paper , we propose a link-based approach to distinguish parallel web pages from bilingual web sites .	Compared with the existing methods , which only employ the internal translation similarity ( such as content-based similarity and page structural similarity ) , we hypothesize that the external translation similarity is an effective feature to identify parallel web pages .	CL_D14-1129_mn_2	CL_D14-1129_mn_3	CL_D14-1129_mn_2_3
CL	D14-1129	mn	3	2	motivation_hypothesis	proposal	support	none	secondary	main	forw	support	Compared with the existing methods , which only employ the internal translation similarity ( such as content-based similarity and page structural similarity ) , we hypothesize that the external translation similarity is an effective feature to identify parallel web pages .	In this paper , we propose a link-based approach to distinguish parallel web pages from bilingual web sites .	CL_D14-1129_mn_3	CL_D14-1129_mn_2	CL_D14-1129_mn_2_3
CL	D14-1129	mn	2	4	proposal	information_additional	none	info-optional	main	secondary	none	none	In this paper , we propose a link-based approach to distinguish parallel web pages from bilingual web sites .	Within a bilingual web site , web pages are interconnected by hyperlinks .	CL_D14-1129_mn_2	CL_D14-1129_mn_4	CL_D14-1129_mn_2_4
CL	D14-1129	mn	2	5	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	In this paper , we propose a link-based approach to distinguish parallel web pages from bilingual web sites .	The basic idea of our method is that the translation similarity of two pages can be inferred from their neighbor pages , which can be adopted as an important source of external similarity .	CL_D14-1129_mn_2	CL_D14-1129_mn_5	CL_D14-1129_mn_2_5
CL	D14-1129	mn	5	2	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	The basic idea of our method is that the translation similarity of two pages can be inferred from their neighbor pages , which can be adopted as an important source of external similarity .	In this paper , we propose a link-based approach to distinguish parallel web pages from bilingual web sites .	CL_D14-1129_mn_5	CL_D14-1129_mn_2	CL_D14-1129_mn_2_5
CL	D14-1129	mn	2	6	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	In this paper , we propose a link-based approach to distinguish parallel web pages from bilingual web sites .	Thus , the translation similarity of page pairs will influence each other .	CL_D14-1129_mn_2	CL_D14-1129_mn_6	CL_D14-1129_mn_2_6
CL	D14-1129	mn	2	7	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	In this paper , we propose a link-based approach to distinguish parallel web pages from bilingual web sites .	An iterative algorithm is developed to estimate the external translation similarity and the final translation similarity .	CL_D14-1129_mn_2	CL_D14-1129_mn_7	CL_D14-1129_mn_2_7
CL	D14-1129	mn	7	2	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	An iterative algorithm is developed to estimate the external translation similarity and the final translation similarity .	In this paper , we propose a link-based approach to distinguish parallel web pages from bilingual web sites .	CL_D14-1129_mn_7	CL_D14-1129_mn_2	CL_D14-1129_mn_2_7
CL	D14-1129	mn	2	8	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	In this paper , we propose a link-based approach to distinguish parallel web pages from bilingual web sites .	Both internal and external similarity measures are combined in the iterative algorithm .	CL_D14-1129_mn_2	CL_D14-1129_mn_8	CL_D14-1129_mn_2_8
CL	D14-1129	mn	2	9	proposal	result_means	none	support	main	secondary	back	support	In this paper , we propose a link-based approach to distinguish parallel web pages from bilingual web sites .	Experiments on six bilingual websites demonstrate that our method is effective and obtains significant improvement ( 6.2 % F-Score ) over the baseline which only utilizes internal translation similarity .	CL_D14-1129_mn_2	CL_D14-1129_mn_9	CL_D14-1129_mn_2_9
CL	D14-1129	mn	9	2	result_means	proposal	support	none	secondary	main	forw	support	Experiments on six bilingual websites demonstrate that our method is effective and obtains significant improvement ( 6.2 % F-Score ) over the baseline which only utilizes internal translation similarity .	In this paper , we propose a link-based approach to distinguish parallel web pages from bilingual web sites .	CL_D14-1129_mn_9	CL_D14-1129_mn_2	CL_D14-1129_mn_2_9
CL	D14-1129	mn	3	4	motivation_hypothesis	information_additional	support	info-optional	secondary	secondary	none	none	Compared with the existing methods , which only employ the internal translation similarity ( such as content-based similarity and page structural similarity ) , we hypothesize that the external translation similarity is an effective feature to identify parallel web pages .	Within a bilingual web site , web pages are interconnected by hyperlinks .	CL_D14-1129_mn_3	CL_D14-1129_mn_4	CL_D14-1129_mn_3_4
CL	D14-1129	mn	3	5	motivation_hypothesis	proposal_implementation	support	elaboration	secondary	secondary	none	none	Compared with the existing methods , which only employ the internal translation similarity ( such as content-based similarity and page structural similarity ) , we hypothesize that the external translation similarity is an effective feature to identify parallel web pages .	The basic idea of our method is that the translation similarity of two pages can be inferred from their neighbor pages , which can be adopted as an important source of external similarity .	CL_D14-1129_mn_3	CL_D14-1129_mn_5	CL_D14-1129_mn_3_5
CL	D14-1129	mn	3	6	motivation_hypothesis	proposal_implementation	support	elaboration	secondary	secondary	none	none	Compared with the existing methods , which only employ the internal translation similarity ( such as content-based similarity and page structural similarity ) , we hypothesize that the external translation similarity is an effective feature to identify parallel web pages .	Thus , the translation similarity of page pairs will influence each other .	CL_D14-1129_mn_3	CL_D14-1129_mn_6	CL_D14-1129_mn_3_6
CL	D14-1129	mn	3	7	motivation_hypothesis	proposal_implementation	support	elaboration	secondary	secondary	none	none	Compared with the existing methods , which only employ the internal translation similarity ( such as content-based similarity and page structural similarity ) , we hypothesize that the external translation similarity is an effective feature to identify parallel web pages .	An iterative algorithm is developed to estimate the external translation similarity and the final translation similarity .	CL_D14-1129_mn_3	CL_D14-1129_mn_7	CL_D14-1129_mn_3_7
CL	D14-1129	mn	3	8	motivation_hypothesis	proposal_implementation	support	elaboration	secondary	secondary	none	none	Compared with the existing methods , which only employ the internal translation similarity ( such as content-based similarity and page structural similarity ) , we hypothesize that the external translation similarity is an effective feature to identify parallel web pages .	Both internal and external similarity measures are combined in the iterative algorithm .	CL_D14-1129_mn_3	CL_D14-1129_mn_8	CL_D14-1129_mn_3_8
CL	D14-1129	mn	3	9	motivation_hypothesis	result_means	support	support	secondary	secondary	none	none	Compared with the existing methods , which only employ the internal translation similarity ( such as content-based similarity and page structural similarity ) , we hypothesize that the external translation similarity is an effective feature to identify parallel web pages .	Experiments on six bilingual websites demonstrate that our method is effective and obtains significant improvement ( 6.2 % F-Score ) over the baseline which only utilizes internal translation similarity .	CL_D14-1129_mn_3	CL_D14-1129_mn_9	CL_D14-1129_mn_3_9
CL	D14-1129	mn	4	5	information_additional	proposal_implementation	info-optional	elaboration	secondary	secondary	forw	info-optional	Within a bilingual web site , web pages are interconnected by hyperlinks .	The basic idea of our method is that the translation similarity of two pages can be inferred from their neighbor pages , which can be adopted as an important source of external similarity .	CL_D14-1129_mn_4	CL_D14-1129_mn_5	CL_D14-1129_mn_4_5
CL	D14-1129	mn	5	4	proposal_implementation	information_additional	elaboration	info-optional	secondary	secondary	back	info-optional	The basic idea of our method is that the translation similarity of two pages can be inferred from their neighbor pages , which can be adopted as an important source of external similarity .	Within a bilingual web site , web pages are interconnected by hyperlinks .	CL_D14-1129_mn_5	CL_D14-1129_mn_4	CL_D14-1129_mn_4_5
CL	D14-1129	mn	4	6	information_additional	proposal_implementation	info-optional	elaboration	secondary	secondary	none	none	Within a bilingual web site , web pages are interconnected by hyperlinks .	Thus , the translation similarity of page pairs will influence each other .	CL_D14-1129_mn_4	CL_D14-1129_mn_6	CL_D14-1129_mn_4_6
CL	D14-1129	mn	4	7	information_additional	proposal_implementation	info-optional	elaboration	secondary	secondary	none	none	Within a bilingual web site , web pages are interconnected by hyperlinks .	An iterative algorithm is developed to estimate the external translation similarity and the final translation similarity .	CL_D14-1129_mn_4	CL_D14-1129_mn_7	CL_D14-1129_mn_4_7
CL	D14-1129	mn	4	8	information_additional	proposal_implementation	info-optional	elaboration	secondary	secondary	none	none	Within a bilingual web site , web pages are interconnected by hyperlinks .	Both internal and external similarity measures are combined in the iterative algorithm .	CL_D14-1129_mn_4	CL_D14-1129_mn_8	CL_D14-1129_mn_4_8
CL	D14-1129	mn	4	9	information_additional	result_means	info-optional	support	secondary	secondary	none	none	Within a bilingual web site , web pages are interconnected by hyperlinks .	Experiments on six bilingual websites demonstrate that our method is effective and obtains significant improvement ( 6.2 % F-Score ) over the baseline which only utilizes internal translation similarity .	CL_D14-1129_mn_4	CL_D14-1129_mn_9	CL_D14-1129_mn_4_9
CL	D14-1129	mn	5	6	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	The basic idea of our method is that the translation similarity of two pages can be inferred from their neighbor pages , which can be adopted as an important source of external similarity .	Thus , the translation similarity of page pairs will influence each other .	CL_D14-1129_mn_5	CL_D14-1129_mn_6	CL_D14-1129_mn_5_6
CL	D14-1129	mn	6	5	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	forw	elaboration	Thus , the translation similarity of page pairs will influence each other .	The basic idea of our method is that the translation similarity of two pages can be inferred from their neighbor pages , which can be adopted as an important source of external similarity .	CL_D14-1129_mn_6	CL_D14-1129_mn_5	CL_D14-1129_mn_5_6
CL	D14-1129	mn	5	7	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	none	none	The basic idea of our method is that the translation similarity of two pages can be inferred from their neighbor pages , which can be adopted as an important source of external similarity .	An iterative algorithm is developed to estimate the external translation similarity and the final translation similarity .	CL_D14-1129_mn_5	CL_D14-1129_mn_7	CL_D14-1129_mn_5_7
CL	D14-1129	mn	5	8	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	none	none	The basic idea of our method is that the translation similarity of two pages can be inferred from their neighbor pages , which can be adopted as an important source of external similarity .	Both internal and external similarity measures are combined in the iterative algorithm .	CL_D14-1129_mn_5	CL_D14-1129_mn_8	CL_D14-1129_mn_5_8
CL	D14-1129	mn	5	9	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	The basic idea of our method is that the translation similarity of two pages can be inferred from their neighbor pages , which can be adopted as an important source of external similarity .	Experiments on six bilingual websites demonstrate that our method is effective and obtains significant improvement ( 6.2 % F-Score ) over the baseline which only utilizes internal translation similarity .	CL_D14-1129_mn_5	CL_D14-1129_mn_9	CL_D14-1129_mn_5_9
CL	D14-1129	mn	6	7	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	none	none	Thus , the translation similarity of page pairs will influence each other .	An iterative algorithm is developed to estimate the external translation similarity and the final translation similarity .	CL_D14-1129_mn_6	CL_D14-1129_mn_7	CL_D14-1129_mn_6_7
CL	D14-1129	mn	6	8	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	none	none	Thus , the translation similarity of page pairs will influence each other .	Both internal and external similarity measures are combined in the iterative algorithm .	CL_D14-1129_mn_6	CL_D14-1129_mn_8	CL_D14-1129_mn_6_8
CL	D14-1129	mn	6	9	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	Thus , the translation similarity of page pairs will influence each other .	Experiments on six bilingual websites demonstrate that our method is effective and obtains significant improvement ( 6.2 % F-Score ) over the baseline which only utilizes internal translation similarity .	CL_D14-1129_mn_6	CL_D14-1129_mn_9	CL_D14-1129_mn_6_9
CL	D14-1129	mn	7	8	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	An iterative algorithm is developed to estimate the external translation similarity and the final translation similarity .	Both internal and external similarity measures are combined in the iterative algorithm .	CL_D14-1129_mn_7	CL_D14-1129_mn_8	CL_D14-1129_mn_7_8
CL	D14-1129	mn	8	7	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	forw	elaboration	Both internal and external similarity measures are combined in the iterative algorithm .	An iterative algorithm is developed to estimate the external translation similarity and the final translation similarity .	CL_D14-1129_mn_8	CL_D14-1129_mn_7	CL_D14-1129_mn_7_8
CL	D14-1129	mn	7	9	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	An iterative algorithm is developed to estimate the external translation similarity and the final translation similarity .	Experiments on six bilingual websites demonstrate that our method is effective and obtains significant improvement ( 6.2 % F-Score ) over the baseline which only utilizes internal translation similarity .	CL_D14-1129_mn_7	CL_D14-1129_mn_9	CL_D14-1129_mn_7_9
CL	D14-1129	mn	8	9	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	Both internal and external similarity measures are combined in the iterative algorithm .	Experiments on six bilingual websites demonstrate that our method is effective and obtains significant improvement ( 6.2 % F-Score ) over the baseline which only utilizes internal translation similarity .	CL_D14-1129_mn_8	CL_D14-1129_mn_9	CL_D14-1129_mn_8_9
CL	D14-1130	mn	1	2	motivation_background	motivation_problem	info-required	support	secondary	secondary	forw	info-required	Analyses of computer aided translation typically focus on either frontend interfaces and human effort , or backend translation and machine learnability of corrections .	However , this distinction is artificial in practice since the frontend and backend must work in concert .	CL_D14-1130_mn_1	CL_D14-1130_mn_2	CL_D14-1130_mn_1_2
CL	D14-1130	mn	2	1	motivation_problem	motivation_background	support	info-required	secondary	secondary	back	info-required	However , this distinction is artificial in practice since the frontend and backend must work in concert .	Analyses of computer aided translation typically focus on either frontend interfaces and human effort , or backend translation and machine learnability of corrections .	CL_D14-1130_mn_2	CL_D14-1130_mn_1	CL_D14-1130_mn_1_2
CL	D14-1130	mn	1	3	motivation_background	proposal	info-required	none	secondary	main	none	none	Analyses of computer aided translation typically focus on either frontend interfaces and human effort , or backend translation and machine learnability of corrections .	We present the first holistic , quantitative evaluation of these issues by contrasting two assistive modes : post-editing and interactive machine translation ( MT ) .	CL_D14-1130_mn_1	CL_D14-1130_mn_3	CL_D14-1130_mn_1_3
CL	D14-1130	mn	1	4	motivation_background	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	Analyses of computer aided translation typically focus on either frontend interfaces and human effort , or backend translation and machine learnability of corrections .	We describe a new translator interface , extensive modifications to a phrase-based MT system , and a novel objective function for re-tuning to human corrections .	CL_D14-1130_mn_1	CL_D14-1130_mn_4	CL_D14-1130_mn_1_4
CL	D14-1130	mn	1	5	motivation_background	result_means	info-required	support	secondary	secondary	none	none	Analyses of computer aided translation typically focus on either frontend interfaces and human effort , or backend translation and machine learnability of corrections .	Evaluation with professional bilingual translators shows that post-edit is faster than interactive at the cost of translation quality for French-English and English-German .	CL_D14-1130_mn_1	CL_D14-1130_mn_5	CL_D14-1130_mn_1_5
CL	D14-1130	mn	1	6	motivation_background	result	info-required	elaboration	secondary	secondary	none	none	Analyses of computer aided translation typically focus on either frontend interfaces and human effort , or backend translation and machine learnability of corrections .	However , re-tuning the MT system to interactive output leads to larger , statistically significant reductions in HTER versus re-tuning to post-edit .	CL_D14-1130_mn_1	CL_D14-1130_mn_6	CL_D14-1130_mn_1_6
CL	D14-1130	mn	1	7	motivation_background	result	info-required	elaboration	secondary	secondary	none	none	Analyses of computer aided translation typically focus on either frontend interfaces and human effort , or backend translation and machine learnability of corrections .	Analysis shows that tuning directly to HTER results in fine-grained corrections to subsequent machine output .	CL_D14-1130_mn_1	CL_D14-1130_mn_7	CL_D14-1130_mn_1_7
CL	D14-1130	mn	2	3	motivation_problem	proposal	support	none	secondary	main	forw	support	However , this distinction is artificial in practice since the frontend and backend must work in concert .	We present the first holistic , quantitative evaluation of these issues by contrasting two assistive modes : post-editing and interactive machine translation ( MT ) .	CL_D14-1130_mn_2	CL_D14-1130_mn_3	CL_D14-1130_mn_2_3
CL	D14-1130	mn	3	2	proposal	motivation_problem	none	support	main	secondary	back	support	We present the first holistic , quantitative evaluation of these issues by contrasting two assistive modes : post-editing and interactive machine translation ( MT ) .	However , this distinction is artificial in practice since the frontend and backend must work in concert .	CL_D14-1130_mn_3	CL_D14-1130_mn_2	CL_D14-1130_mn_2_3
CL	D14-1130	mn	2	4	motivation_problem	proposal_implementation	support	elaboration	secondary	secondary	none	none	However , this distinction is artificial in practice since the frontend and backend must work in concert .	We describe a new translator interface , extensive modifications to a phrase-based MT system , and a novel objective function for re-tuning to human corrections .	CL_D14-1130_mn_2	CL_D14-1130_mn_4	CL_D14-1130_mn_2_4
CL	D14-1130	mn	2	5	motivation_problem	result_means	support	support	secondary	secondary	none	none	However , this distinction is artificial in practice since the frontend and backend must work in concert .	Evaluation with professional bilingual translators shows that post-edit is faster than interactive at the cost of translation quality for French-English and English-German .	CL_D14-1130_mn_2	CL_D14-1130_mn_5	CL_D14-1130_mn_2_5
CL	D14-1130	mn	2	6	motivation_problem	result	support	elaboration	secondary	secondary	none	none	However , this distinction is artificial in practice since the frontend and backend must work in concert .	However , re-tuning the MT system to interactive output leads to larger , statistically significant reductions in HTER versus re-tuning to post-edit .	CL_D14-1130_mn_2	CL_D14-1130_mn_6	CL_D14-1130_mn_2_6
CL	D14-1130	mn	2	7	motivation_problem	result	support	elaboration	secondary	secondary	none	none	However , this distinction is artificial in practice since the frontend and backend must work in concert .	Analysis shows that tuning directly to HTER results in fine-grained corrections to subsequent machine output .	CL_D14-1130_mn_2	CL_D14-1130_mn_7	CL_D14-1130_mn_2_7
CL	D14-1130	mn	3	4	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	We present the first holistic , quantitative evaluation of these issues by contrasting two assistive modes : post-editing and interactive machine translation ( MT ) .	We describe a new translator interface , extensive modifications to a phrase-based MT system , and a novel objective function for re-tuning to human corrections .	CL_D14-1130_mn_3	CL_D14-1130_mn_4	CL_D14-1130_mn_3_4
CL	D14-1130	mn	4	3	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	We describe a new translator interface , extensive modifications to a phrase-based MT system , and a novel objective function for re-tuning to human corrections .	We present the first holistic , quantitative evaluation of these issues by contrasting two assistive modes : post-editing and interactive machine translation ( MT ) .	CL_D14-1130_mn_4	CL_D14-1130_mn_3	CL_D14-1130_mn_3_4
CL	D14-1130	mn	3	5	proposal	result_means	none	support	main	secondary	back	support	We present the first holistic , quantitative evaluation of these issues by contrasting two assistive modes : post-editing and interactive machine translation ( MT ) .	Evaluation with professional bilingual translators shows that post-edit is faster than interactive at the cost of translation quality for French-English and English-German .	CL_D14-1130_mn_3	CL_D14-1130_mn_5	CL_D14-1130_mn_3_5
CL	D14-1130	mn	5	3	result_means	proposal	support	none	secondary	main	forw	support	Evaluation with professional bilingual translators shows that post-edit is faster than interactive at the cost of translation quality for French-English and English-German .	We present the first holistic , quantitative evaluation of these issues by contrasting two assistive modes : post-editing and interactive machine translation ( MT ) .	CL_D14-1130_mn_5	CL_D14-1130_mn_3	CL_D14-1130_mn_3_5
CL	D14-1130	mn	3	6	proposal	result	none	elaboration	main	secondary	none	none	We present the first holistic , quantitative evaluation of these issues by contrasting two assistive modes : post-editing and interactive machine translation ( MT ) .	However , re-tuning the MT system to interactive output leads to larger , statistically significant reductions in HTER versus re-tuning to post-edit .	CL_D14-1130_mn_3	CL_D14-1130_mn_6	CL_D14-1130_mn_3_6
CL	D14-1130	mn	3	7	proposal	result	none	elaboration	main	secondary	none	none	We present the first holistic , quantitative evaluation of these issues by contrasting two assistive modes : post-editing and interactive machine translation ( MT ) .	Analysis shows that tuning directly to HTER results in fine-grained corrections to subsequent machine output .	CL_D14-1130_mn_3	CL_D14-1130_mn_7	CL_D14-1130_mn_3_7
CL	D14-1130	mn	4	5	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	We describe a new translator interface , extensive modifications to a phrase-based MT system , and a novel objective function for re-tuning to human corrections .	Evaluation with professional bilingual translators shows that post-edit is faster than interactive at the cost of translation quality for French-English and English-German .	CL_D14-1130_mn_4	CL_D14-1130_mn_5	CL_D14-1130_mn_4_5
CL	D14-1130	mn	4	6	proposal_implementation	result	elaboration	elaboration	secondary	secondary	none	none	We describe a new translator interface , extensive modifications to a phrase-based MT system , and a novel objective function for re-tuning to human corrections .	However , re-tuning the MT system to interactive output leads to larger , statistically significant reductions in HTER versus re-tuning to post-edit .	CL_D14-1130_mn_4	CL_D14-1130_mn_6	CL_D14-1130_mn_4_6
CL	D14-1130	mn	4	7	proposal_implementation	result	elaboration	elaboration	secondary	secondary	none	none	We describe a new translator interface , extensive modifications to a phrase-based MT system , and a novel objective function for re-tuning to human corrections .	Analysis shows that tuning directly to HTER results in fine-grained corrections to subsequent machine output .	CL_D14-1130_mn_4	CL_D14-1130_mn_7	CL_D14-1130_mn_4_7
CL	D14-1130	mn	5	6	result_means	result	support	elaboration	secondary	secondary	back	elaboration	Evaluation with professional bilingual translators shows that post-edit is faster than interactive at the cost of translation quality for French-English and English-German .	However , re-tuning the MT system to interactive output leads to larger , statistically significant reductions in HTER versus re-tuning to post-edit .	CL_D14-1130_mn_5	CL_D14-1130_mn_6	CL_D14-1130_mn_5_6
CL	D14-1130	mn	6	5	result	result_means	elaboration	support	secondary	secondary	forw	elaboration	However , re-tuning the MT system to interactive output leads to larger , statistically significant reductions in HTER versus re-tuning to post-edit .	Evaluation with professional bilingual translators shows that post-edit is faster than interactive at the cost of translation quality for French-English and English-German .	CL_D14-1130_mn_6	CL_D14-1130_mn_5	CL_D14-1130_mn_5_6
CL	D14-1130	mn	5	7	result_means	result	support	elaboration	secondary	secondary	none	none	Evaluation with professional bilingual translators shows that post-edit is faster than interactive at the cost of translation quality for French-English and English-German .	Analysis shows that tuning directly to HTER results in fine-grained corrections to subsequent machine output .	CL_D14-1130_mn_5	CL_D14-1130_mn_7	CL_D14-1130_mn_5_7
CL	D14-1130	mn	6	7	result	result	elaboration	elaboration	secondary	secondary	back	elaboration	However , re-tuning the MT system to interactive output leads to larger , statistically significant reductions in HTER versus re-tuning to post-edit .	Analysis shows that tuning directly to HTER results in fine-grained corrections to subsequent machine output .	CL_D14-1130_mn_6	CL_D14-1130_mn_7	CL_D14-1130_mn_6_7
CL	D14-1130	mn	7	6	result	result	elaboration	elaboration	secondary	secondary	forw	elaboration	Analysis shows that tuning directly to HTER results in fine-grained corrections to subsequent machine output .	However , re-tuning the MT system to interactive output leads to larger , statistically significant reductions in HTER versus re-tuning to post-edit .	CL_D14-1130_mn_7	CL_D14-1130_mn_6	CL_D14-1130_mn_6_7
CL	D14-1131	mn	1	2	motivation_background	proposal	support	none	secondary	main	forw	support	The combinatorial space of translation derivations in phrase-based statistical machine translation is given by the intersection between a translation lattice and a target language model .	We replace this in-tractable intersection by a tractable relaxation which incorporates a low-order upperbound on the language model .	CL_D14-1131_mn_1	CL_D14-1131_mn_2	CL_D14-1131_mn_1_2
CL	D14-1131	mn	2	1	proposal	motivation_background	none	support	main	secondary	back	support	We replace this in-tractable intersection by a tractable relaxation which incorporates a low-order upperbound on the language model .	The combinatorial space of translation derivations in phrase-based statistical machine translation is given by the intersection between a translation lattice and a target language model .	CL_D14-1131_mn_2	CL_D14-1131_mn_1	CL_D14-1131_mn_1_2
CL	D14-1131	mn	1	3	motivation_background	proposal_implementation	support	elaboration	secondary	secondary	none	none	The combinatorial space of translation derivations in phrase-based statistical machine translation is given by the intersection between a translation lattice and a target language model .	Exact optimisation is achieved through a coarse-to-fine strategy with connections to adaptive rejection sampling .	CL_D14-1131_mn_1	CL_D14-1131_mn_3	CL_D14-1131_mn_1_3
CL	D14-1131	mn	1	4	motivation_background	proposal_implementation	support	elaboration	secondary	secondary	none	none	The combinatorial space of translation derivations in phrase-based statistical machine translation is given by the intersection between a translation lattice and a target language model .	We perform exact optimisation with unpruned language models of order 3 to 5 and show search-error curves for beam search and cube pruning on standard test sets .	CL_D14-1131_mn_1	CL_D14-1131_mn_4	CL_D14-1131_mn_1_4
CL	D14-1131	mn	1	5	motivation_background	conclusion	support	support	secondary	secondary	none	none	The combinatorial space of translation derivations in phrase-based statistical machine translation is given by the intersection between a translation lattice and a target language model .	This is the first work to tractably tackle exact optimisation with language models of orders higher than 3 .	CL_D14-1131_mn_1	CL_D14-1131_mn_5	CL_D14-1131_mn_1_5
CL	D14-1131	mn	2	3	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	We replace this in-tractable intersection by a tractable relaxation which incorporates a low-order upperbound on the language model .	Exact optimisation is achieved through a coarse-to-fine strategy with connections to adaptive rejection sampling .	CL_D14-1131_mn_2	CL_D14-1131_mn_3	CL_D14-1131_mn_2_3
CL	D14-1131	mn	3	2	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	Exact optimisation is achieved through a coarse-to-fine strategy with connections to adaptive rejection sampling .	We replace this in-tractable intersection by a tractable relaxation which incorporates a low-order upperbound on the language model .	CL_D14-1131_mn_3	CL_D14-1131_mn_2	CL_D14-1131_mn_2_3
CL	D14-1131	mn	2	4	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	We replace this in-tractable intersection by a tractable relaxation which incorporates a low-order upperbound on the language model .	We perform exact optimisation with unpruned language models of order 3 to 5 and show search-error curves for beam search and cube pruning on standard test sets .	CL_D14-1131_mn_2	CL_D14-1131_mn_4	CL_D14-1131_mn_2_4
CL	D14-1131	mn	2	5	proposal	conclusion	none	support	main	secondary	back	support	We replace this in-tractable intersection by a tractable relaxation which incorporates a low-order upperbound on the language model .	This is the first work to tractably tackle exact optimisation with language models of orders higher than 3 .	CL_D14-1131_mn_2	CL_D14-1131_mn_5	CL_D14-1131_mn_2_5
CL	D14-1131	mn	5	2	conclusion	proposal	support	none	secondary	main	forw	support	This is the first work to tractably tackle exact optimisation with language models of orders higher than 3 .	We replace this in-tractable intersection by a tractable relaxation which incorporates a low-order upperbound on the language model .	CL_D14-1131_mn_5	CL_D14-1131_mn_2	CL_D14-1131_mn_2_5
CL	D14-1131	mn	3	4	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	Exact optimisation is achieved through a coarse-to-fine strategy with connections to adaptive rejection sampling .	We perform exact optimisation with unpruned language models of order 3 to 5 and show search-error curves for beam search and cube pruning on standard test sets .	CL_D14-1131_mn_3	CL_D14-1131_mn_4	CL_D14-1131_mn_3_4
CL	D14-1131	mn	4	3	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	forw	elaboration	We perform exact optimisation with unpruned language models of order 3 to 5 and show search-error curves for beam search and cube pruning on standard test sets .	Exact optimisation is achieved through a coarse-to-fine strategy with connections to adaptive rejection sampling .	CL_D14-1131_mn_4	CL_D14-1131_mn_3	CL_D14-1131_mn_3_4
CL	D14-1131	mn	3	5	proposal_implementation	conclusion	elaboration	support	secondary	secondary	none	none	Exact optimisation is achieved through a coarse-to-fine strategy with connections to adaptive rejection sampling .	This is the first work to tractably tackle exact optimisation with language models of orders higher than 3 .	CL_D14-1131_mn_3	CL_D14-1131_mn_5	CL_D14-1131_mn_3_5
CL	D14-1131	mn	4	5	proposal_implementation	conclusion	elaboration	support	secondary	secondary	none	none	We perform exact optimisation with unpruned language models of order 3 to 5 and show search-error curves for beam search and cube pruning on standard test sets .	This is the first work to tractably tackle exact optimisation with language models of orders higher than 3 .	CL_D14-1131_mn_4	CL_D14-1131_mn_5	CL_D14-1131_mn_4_5
CL	D14-1132	mn	1	2	motivation_background	motivation_problem	info-required	support	secondary	secondary	forw	info-required	Recent work by Cherry ( 2013 ) has shown that directly optimizing phrase-based reordering models towards BLEU can lead to significant gains .	Their approach is limited to small training sets of a few thousand sentences and a similar number of sparse features .	CL_D14-1132_mn_1	CL_D14-1132_mn_2	CL_D14-1132_mn_1_2
CL	D14-1132	mn	2	1	motivation_problem	motivation_background	support	info-required	secondary	secondary	back	info-required	Their approach is limited to small training sets of a few thousand sentences and a similar number of sparse features .	Recent work by Cherry ( 2013 ) has shown that directly optimizing phrase-based reordering models towards BLEU can lead to significant gains .	CL_D14-1132_mn_2	CL_D14-1132_mn_1	CL_D14-1132_mn_1_2
CL	D14-1132	mn	1	3	motivation_background	proposal	info-required	none	secondary	main	none	none	Recent work by Cherry ( 2013 ) has shown that directly optimizing phrase-based reordering models towards BLEU can lead to significant gains .	We show how the expected BLEU objective allows us to train a simple linear discriminative reordering model with millions of sparse features on hundreds of thousands of sentences resulting in significant improvements .	CL_D14-1132_mn_1	CL_D14-1132_mn_3	CL_D14-1132_mn_1_3
CL	D14-1132	mn	1	4	motivation_background	result	info-required	support	secondary	secondary	none	none	Recent work by Cherry ( 2013 ) has shown that directly optimizing phrase-based reordering models towards BLEU can lead to significant gains .	A comparison to likelihood training demonstrates that expected BLEU is vastly more effective .	CL_D14-1132_mn_1	CL_D14-1132_mn_4	CL_D14-1132_mn_1_4
CL	D14-1132	mn	1	5	motivation_background	observation	info-required	support	secondary	secondary	none	none	Recent work by Cherry ( 2013 ) has shown that directly optimizing phrase-based reordering models towards BLEU can lead to significant gains .	Our best results improve a hierarchical lexicalized reordering baseline by up to 2.0 BLEU in a single-reference setting on a French-English WMT 2012 setup .	CL_D14-1132_mn_1	CL_D14-1132_mn_5	CL_D14-1132_mn_1_5
CL	D14-1132	mn	2	3	motivation_problem	proposal	support	none	secondary	main	forw	support	Their approach is limited to small training sets of a few thousand sentences and a similar number of sparse features .	We show how the expected BLEU objective allows us to train a simple linear discriminative reordering model with millions of sparse features on hundreds of thousands of sentences resulting in significant improvements .	CL_D14-1132_mn_2	CL_D14-1132_mn_3	CL_D14-1132_mn_2_3
CL	D14-1132	mn	3	2	proposal	motivation_problem	none	support	main	secondary	back	support	We show how the expected BLEU objective allows us to train a simple linear discriminative reordering model with millions of sparse features on hundreds of thousands of sentences resulting in significant improvements .	Their approach is limited to small training sets of a few thousand sentences and a similar number of sparse features .	CL_D14-1132_mn_3	CL_D14-1132_mn_2	CL_D14-1132_mn_2_3
CL	D14-1132	mn	2	4	motivation_problem	result	support	support	secondary	secondary	none	none	Their approach is limited to small training sets of a few thousand sentences and a similar number of sparse features .	A comparison to likelihood training demonstrates that expected BLEU is vastly more effective .	CL_D14-1132_mn_2	CL_D14-1132_mn_4	CL_D14-1132_mn_2_4
CL	D14-1132	mn	2	5	motivation_problem	observation	support	support	secondary	secondary	none	none	Their approach is limited to small training sets of a few thousand sentences and a similar number of sparse features .	Our best results improve a hierarchical lexicalized reordering baseline by up to 2.0 BLEU in a single-reference setting on a French-English WMT 2012 setup .	CL_D14-1132_mn_2	CL_D14-1132_mn_5	CL_D14-1132_mn_2_5
CL	D14-1132	mn	3	4	proposal	result	none	support	main	secondary	back	support	We show how the expected BLEU objective allows us to train a simple linear discriminative reordering model with millions of sparse features on hundreds of thousands of sentences resulting in significant improvements .	A comparison to likelihood training demonstrates that expected BLEU is vastly more effective .	CL_D14-1132_mn_3	CL_D14-1132_mn_4	CL_D14-1132_mn_3_4
CL	D14-1132	mn	4	3	result	proposal	support	none	secondary	main	forw	support	A comparison to likelihood training demonstrates that expected BLEU is vastly more effective .	We show how the expected BLEU objective allows us to train a simple linear discriminative reordering model with millions of sparse features on hundreds of thousands of sentences resulting in significant improvements .	CL_D14-1132_mn_4	CL_D14-1132_mn_3	CL_D14-1132_mn_3_4
CL	D14-1132	mn	3	5	proposal	observation	none	support	main	secondary	back	support	We show how the expected BLEU objective allows us to train a simple linear discriminative reordering model with millions of sparse features on hundreds of thousands of sentences resulting in significant improvements .	Our best results improve a hierarchical lexicalized reordering baseline by up to 2.0 BLEU in a single-reference setting on a French-English WMT 2012 setup .	CL_D14-1132_mn_3	CL_D14-1132_mn_5	CL_D14-1132_mn_3_5
CL	D14-1132	mn	5	3	observation	proposal	support	none	secondary	main	forw	support	Our best results improve a hierarchical lexicalized reordering baseline by up to 2.0 BLEU in a single-reference setting on a French-English WMT 2012 setup .	We show how the expected BLEU objective allows us to train a simple linear discriminative reordering model with millions of sparse features on hundreds of thousands of sentences resulting in significant improvements .	CL_D14-1132_mn_5	CL_D14-1132_mn_3	CL_D14-1132_mn_3_5
CL	D14-1132	mn	4	5	result	observation	support	support	secondary	secondary	none	none	A comparison to likelihood training demonstrates that expected BLEU is vastly more effective .	Our best results improve a hierarchical lexicalized reordering baseline by up to 2.0 BLEU in a single-reference setting on a French-English WMT 2012 setup .	CL_D14-1132_mn_4	CL_D14-1132_mn_5	CL_D14-1132_mn_4_5
CL	D14-1133	mn	1	2	motivation_background	proposal	support	none	secondary	main	forw	support	Numerous works in Statistical Machine Translation ( SMT ) have attempted to identify better translation hypotheses obtained by an initial decoding using an improved , but more costly scoring function .	In this work , we introduce an approach that takes the hypotheses produced by a state-of-the-art , reranked phrase-based SMT system , and explores new parts of the search space by applying rewriting rules selected on the basis of posterior phrase-level confidence .	CL_D14-1133_mn_1	CL_D14-1133_mn_2	CL_D14-1133_mn_1_2
CL	D14-1133	mn	2	1	proposal	motivation_background	none	support	main	secondary	back	support	In this work , we introduce an approach that takes the hypotheses produced by a state-of-the-art , reranked phrase-based SMT system , and explores new parts of the search space by applying rewriting rules selected on the basis of posterior phrase-level confidence .	Numerous works in Statistical Machine Translation ( SMT ) have attempted to identify better translation hypotheses obtained by an initial decoding using an improved , but more costly scoring function .	CL_D14-1133_mn_2	CL_D14-1133_mn_1	CL_D14-1133_mn_1_2
CL	D14-1133	mn	1	3	motivation_background	result_means	support	support	secondary	secondary	none	none	Numerous works in Statistical Machine Translation ( SMT ) have attempted to identify better translation hypotheses obtained by an initial decoding using an improved , but more costly scoring function .	In the medical domain , we obtain a 1.9 BLEU improvement over a reranked baseline exploiting the same scoring function , corresponding to a 5.4 BLEU improvement over the original Moses baseline .	CL_D14-1133_mn_1	CL_D14-1133_mn_3	CL_D14-1133_mn_1_3
CL	D14-1133	mn	1	4	motivation_background	result	support	elaboration	secondary	secondary	none	none	Numerous works in Statistical Machine Translation ( SMT ) have attempted to identify better translation hypotheses obtained by an initial decoding using an improved , but more costly scoring function .	We show that if an indication of which phrases require rewriting is provided , our automatic rewriting procedure yields an additional improvement of 1.5 BLEU .	CL_D14-1133_mn_1	CL_D14-1133_mn_4	CL_D14-1133_mn_1_4
CL	D14-1133	mn	1	5	motivation_background	result	support	elaboration	secondary	secondary	none	none	Numerous works in Statistical Machine Translation ( SMT ) have attempted to identify better translation hypotheses obtained by an initial decoding using an improved , but more costly scoring function .	Various analyses , including a manual error analysis , further illustrate the good performance and potential for improvement of our approach in spite of its simplicity .	CL_D14-1133_mn_1	CL_D14-1133_mn_5	CL_D14-1133_mn_1_5
CL	D14-1133	mn	2	3	proposal	result_means	none	support	main	secondary	back	support	In this work , we introduce an approach that takes the hypotheses produced by a state-of-the-art , reranked phrase-based SMT system , and explores new parts of the search space by applying rewriting rules selected on the basis of posterior phrase-level confidence .	In the medical domain , we obtain a 1.9 BLEU improvement over a reranked baseline exploiting the same scoring function , corresponding to a 5.4 BLEU improvement over the original Moses baseline .	CL_D14-1133_mn_2	CL_D14-1133_mn_3	CL_D14-1133_mn_2_3
CL	D14-1133	mn	3	2	result_means	proposal	support	none	secondary	main	forw	support	In the medical domain , we obtain a 1.9 BLEU improvement over a reranked baseline exploiting the same scoring function , corresponding to a 5.4 BLEU improvement over the original Moses baseline .	In this work , we introduce an approach that takes the hypotheses produced by a state-of-the-art , reranked phrase-based SMT system , and explores new parts of the search space by applying rewriting rules selected on the basis of posterior phrase-level confidence .	CL_D14-1133_mn_3	CL_D14-1133_mn_2	CL_D14-1133_mn_2_3
CL	D14-1133	mn	2	4	proposal	result	none	elaboration	main	secondary	none	none	In this work , we introduce an approach that takes the hypotheses produced by a state-of-the-art , reranked phrase-based SMT system , and explores new parts of the search space by applying rewriting rules selected on the basis of posterior phrase-level confidence .	We show that if an indication of which phrases require rewriting is provided , our automatic rewriting procedure yields an additional improvement of 1.5 BLEU .	CL_D14-1133_mn_2	CL_D14-1133_mn_4	CL_D14-1133_mn_2_4
CL	D14-1133	mn	2	5	proposal	result	none	elaboration	main	secondary	none	none	In this work , we introduce an approach that takes the hypotheses produced by a state-of-the-art , reranked phrase-based SMT system , and explores new parts of the search space by applying rewriting rules selected on the basis of posterior phrase-level confidence .	Various analyses , including a manual error analysis , further illustrate the good performance and potential for improvement of our approach in spite of its simplicity .	CL_D14-1133_mn_2	CL_D14-1133_mn_5	CL_D14-1133_mn_2_5
CL	D14-1133	mn	3	4	result_means	result	support	elaboration	secondary	secondary	back	elaboration	In the medical domain , we obtain a 1.9 BLEU improvement over a reranked baseline exploiting the same scoring function , corresponding to a 5.4 BLEU improvement over the original Moses baseline .	We show that if an indication of which phrases require rewriting is provided , our automatic rewriting procedure yields an additional improvement of 1.5 BLEU .	CL_D14-1133_mn_3	CL_D14-1133_mn_4	CL_D14-1133_mn_3_4
CL	D14-1133	mn	4	3	result	result_means	elaboration	support	secondary	secondary	forw	elaboration	We show that if an indication of which phrases require rewriting is provided , our automatic rewriting procedure yields an additional improvement of 1.5 BLEU .	In the medical domain , we obtain a 1.9 BLEU improvement over a reranked baseline exploiting the same scoring function , corresponding to a 5.4 BLEU improvement over the original Moses baseline .	CL_D14-1133_mn_4	CL_D14-1133_mn_3	CL_D14-1133_mn_3_4
CL	D14-1133	mn	3	5	result_means	result	support	elaboration	secondary	secondary	none	none	In the medical domain , we obtain a 1.9 BLEU improvement over a reranked baseline exploiting the same scoring function , corresponding to a 5.4 BLEU improvement over the original Moses baseline .	Various analyses , including a manual error analysis , further illustrate the good performance and potential for improvement of our approach in spite of its simplicity .	CL_D14-1133_mn_3	CL_D14-1133_mn_5	CL_D14-1133_mn_3_5
CL	D14-1133	mn	4	5	result	result	elaboration	elaboration	secondary	secondary	back	elaboration	We show that if an indication of which phrases require rewriting is provided , our automatic rewriting procedure yields an additional improvement of 1.5 BLEU .	Various analyses , including a manual error analysis , further illustrate the good performance and potential for improvement of our approach in spite of its simplicity .	CL_D14-1133_mn_4	CL_D14-1133_mn_5	CL_D14-1133_mn_4_5
CL	D14-1133	mn	5	4	result	result	elaboration	elaboration	secondary	secondary	forw	elaboration	Various analyses , including a manual error analysis , further illustrate the good performance and potential for improvement of our approach in spite of its simplicity .	We show that if an indication of which phrases require rewriting is provided , our automatic rewriting procedure yields an additional improvement of 1.5 BLEU .	CL_D14-1133_mn_5	CL_D14-1133_mn_4	CL_D14-1133_mn_4_5
CL	D14-1134	mn	1	2	proposal	motivation_background	none	support	main	secondary	none	none	We present methods to control the lexicon size when learning a Combinatory Categorial Grammar semantic parser .	Existing methods incrementally expand the lexicon by greedily adding entries , considering a single training datapoint at a time .	CL_D14-1134_mn_1	CL_D14-1134_mn_2	CL_D14-1134_mn_1_2
CL	D14-1134	mn	1	3	proposal	proposal	none	elaboration	main	secondary	back	elaboration	We present methods to control the lexicon size when learning a Combinatory Categorial Grammar semantic parser .	We propose using corpus-level statistics for lexicon learning decisions .	CL_D14-1134_mn_1	CL_D14-1134_mn_3	CL_D14-1134_mn_1_3
CL	D14-1134	mn	3	1	proposal	proposal	elaboration	none	secondary	main	forw	elaboration	We propose using corpus-level statistics for lexicon learning decisions .	We present methods to control the lexicon size when learning a Combinatory Categorial Grammar semantic parser .	CL_D14-1134_mn_3	CL_D14-1134_mn_1	CL_D14-1134_mn_1_3
CL	D14-1134	mn	1	4	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	We present methods to control the lexicon size when learning a Combinatory Categorial Grammar semantic parser .	We introduce voting to globally consider adding entries to the lexicon , and pruning to remove entries no longer required to explain the training data .	CL_D14-1134_mn_1	CL_D14-1134_mn_4	CL_D14-1134_mn_1_4
CL	D14-1134	mn	1	5	proposal	result_means	none	support	main	secondary	back	support	We present methods to control the lexicon size when learning a Combinatory Categorial Grammar semantic parser .	Our methods result in state-of-the-art performance on the task of executing sequences of natural language instructions , achieving up to 25 % error reduction , with lexicons that are up to 70 % smaller and are qualitatively less noisy .	CL_D14-1134_mn_1	CL_D14-1134_mn_5	CL_D14-1134_mn_1_5
CL	D14-1134	mn	5	1	result_means	proposal	support	none	secondary	main	forw	support	Our methods result in state-of-the-art performance on the task of executing sequences of natural language instructions , achieving up to 25 % error reduction , with lexicons that are up to 70 % smaller and are qualitatively less noisy .	We present methods to control the lexicon size when learning a Combinatory Categorial Grammar semantic parser .	CL_D14-1134_mn_5	CL_D14-1134_mn_1	CL_D14-1134_mn_1_5
CL	D14-1134	mn	2	3	motivation_background	proposal	support	elaboration	secondary	secondary	forw	support	Existing methods incrementally expand the lexicon by greedily adding entries , considering a single training datapoint at a time .	We propose using corpus-level statistics for lexicon learning decisions .	CL_D14-1134_mn_2	CL_D14-1134_mn_3	CL_D14-1134_mn_2_3
CL	D14-1134	mn	3	2	proposal	motivation_background	elaboration	support	secondary	secondary	back	support	We propose using corpus-level statistics for lexicon learning decisions .	Existing methods incrementally expand the lexicon by greedily adding entries , considering a single training datapoint at a time .	CL_D14-1134_mn_3	CL_D14-1134_mn_2	CL_D14-1134_mn_2_3
CL	D14-1134	mn	2	4	motivation_background	proposal_implementation	support	elaboration	secondary	secondary	none	none	Existing methods incrementally expand the lexicon by greedily adding entries , considering a single training datapoint at a time .	We introduce voting to globally consider adding entries to the lexicon , and pruning to remove entries no longer required to explain the training data .	CL_D14-1134_mn_2	CL_D14-1134_mn_4	CL_D14-1134_mn_2_4
CL	D14-1134	mn	2	5	motivation_background	result_means	support	support	secondary	secondary	none	none	Existing methods incrementally expand the lexicon by greedily adding entries , considering a single training datapoint at a time .	Our methods result in state-of-the-art performance on the task of executing sequences of natural language instructions , achieving up to 25 % error reduction , with lexicons that are up to 70 % smaller and are qualitatively less noisy .	CL_D14-1134_mn_2	CL_D14-1134_mn_5	CL_D14-1134_mn_2_5
CL	D14-1134	mn	3	4	proposal	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	We propose using corpus-level statistics for lexicon learning decisions .	We introduce voting to globally consider adding entries to the lexicon , and pruning to remove entries no longer required to explain the training data .	CL_D14-1134_mn_3	CL_D14-1134_mn_4	CL_D14-1134_mn_3_4
CL	D14-1134	mn	4	3	proposal_implementation	proposal	elaboration	elaboration	secondary	secondary	forw	elaboration	We introduce voting to globally consider adding entries to the lexicon , and pruning to remove entries no longer required to explain the training data .	We propose using corpus-level statistics for lexicon learning decisions .	CL_D14-1134_mn_4	CL_D14-1134_mn_3	CL_D14-1134_mn_3_4
CL	D14-1134	mn	3	5	proposal	result_means	elaboration	support	secondary	secondary	none	none	We propose using corpus-level statistics for lexicon learning decisions .	Our methods result in state-of-the-art performance on the task of executing sequences of natural language instructions , achieving up to 25 % error reduction , with lexicons that are up to 70 % smaller and are qualitatively less noisy .	CL_D14-1134_mn_3	CL_D14-1134_mn_5	CL_D14-1134_mn_3_5
CL	D14-1134	mn	4	5	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	We introduce voting to globally consider adding entries to the lexicon , and pruning to remove entries no longer required to explain the training data .	Our methods result in state-of-the-art performance on the task of executing sequences of natural language instructions , achieving up to 25 % error reduction , with lexicons that are up to 70 % smaller and are qualitatively less noisy .	CL_D14-1134_mn_4	CL_D14-1134_mn_5	CL_D14-1134_mn_4_5
CL	D14-1135	mn	1	2	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	In this paper , we demonstrate that significant performance gains can be achieved in CCG semantic parsing by introducing a linguistically motivated grammar induction scheme .	We present a new morpho-syntactic factored lexicon that models systematic variations in morphology , syntax , and semantics across word classes .	CL_D14-1135_mn_1	CL_D14-1135_mn_2	CL_D14-1135_mn_1_2
CL	D14-1135	mn	2	1	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	We present a new morpho-syntactic factored lexicon that models systematic variations in morphology , syntax , and semantics across word classes .	In this paper , we demonstrate that significant performance gains can be achieved in CCG semantic parsing by introducing a linguistically motivated grammar induction scheme .	CL_D14-1135_mn_2	CL_D14-1135_mn_1	CL_D14-1135_mn_1_2
CL	D14-1135	mn	1	3	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	In this paper , we demonstrate that significant performance gains can be achieved in CCG semantic parsing by introducing a linguistically motivated grammar induction scheme .	The grammar uses domain-independent facts about the English language to restrict the number of incorrect parses that must be considered , thereby enabling effective learning from less data .	CL_D14-1135_mn_1	CL_D14-1135_mn_3	CL_D14-1135_mn_1_3
CL	D14-1135	mn	1	4	proposal	result_means	none	support	main	secondary	back	support	In this paper , we demonstrate that significant performance gains can be achieved in CCG semantic parsing by introducing a linguistically motivated grammar induction scheme .	Experiments in benchmark domains match previous models with one quarter of the data and provide new state-of-the-art results with all available data , including up to 45 % relative test-error reduction .	CL_D14-1135_mn_1	CL_D14-1135_mn_4	CL_D14-1135_mn_1_4
CL	D14-1135	mn	4	1	result_means	proposal	support	none	secondary	main	forw	support	Experiments in benchmark domains match previous models with one quarter of the data and provide new state-of-the-art results with all available data , including up to 45 % relative test-error reduction .	In this paper , we demonstrate that significant performance gains can be achieved in CCG semantic parsing by introducing a linguistically motivated grammar induction scheme .	CL_D14-1135_mn_4	CL_D14-1135_mn_1	CL_D14-1135_mn_1_4
CL	D14-1135	mn	2	3	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	We present a new morpho-syntactic factored lexicon that models systematic variations in morphology , syntax , and semantics across word classes .	The grammar uses domain-independent facts about the English language to restrict the number of incorrect parses that must be considered , thereby enabling effective learning from less data .	CL_D14-1135_mn_2	CL_D14-1135_mn_3	CL_D14-1135_mn_2_3
CL	D14-1135	mn	3	2	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	forw	elaboration	The grammar uses domain-independent facts about the English language to restrict the number of incorrect parses that must be considered , thereby enabling effective learning from less data .	We present a new morpho-syntactic factored lexicon that models systematic variations in morphology , syntax , and semantics across word classes .	CL_D14-1135_mn_3	CL_D14-1135_mn_2	CL_D14-1135_mn_2_3
CL	D14-1135	mn	2	4	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	We present a new morpho-syntactic factored lexicon that models systematic variations in morphology , syntax , and semantics across word classes .	Experiments in benchmark domains match previous models with one quarter of the data and provide new state-of-the-art results with all available data , including up to 45 % relative test-error reduction .	CL_D14-1135_mn_2	CL_D14-1135_mn_4	CL_D14-1135_mn_2_4
CL	D14-1135	mn	3	4	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	The grammar uses domain-independent facts about the English language to restrict the number of incorrect parses that must be considered , thereby enabling effective learning from less data .	Experiments in benchmark domains match previous models with one quarter of the data and provide new state-of-the-art results with all available data , including up to 45 % relative test-error reduction .	CL_D14-1135_mn_3	CL_D14-1135_mn_4	CL_D14-1135_mn_3_4
CL	D14-1136	mn	1	2	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	We present a model for the automatic semantic analysis of requirements elicitation documents .	Our target semantic representation employs live sequence charts , a multi-modal visual language for scenario-based programming , which can be directly translated into executable code .	CL_D14-1136_mn_1	CL_D14-1136_mn_2	CL_D14-1136_mn_1_2
CL	D14-1136	mn	2	1	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	Our target semantic representation employs live sequence charts , a multi-modal visual language for scenario-based programming , which can be directly translated into executable code .	We present a model for the automatic semantic analysis of requirements elicitation documents .	CL_D14-1136_mn_2	CL_D14-1136_mn_1	CL_D14-1136_mn_1_2
CL	D14-1136	mn	1	3	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	We present a model for the automatic semantic analysis of requirements elicitation documents .	The architecture we propose integrates sentence-level and discourse-level processing in a generative probabilistic framework for the analysis and disambiguation of individual sentences in context .	CL_D14-1136_mn_1	CL_D14-1136_mn_3	CL_D14-1136_mn_1_3
CL	D14-1136	mn	1	4	proposal	result	none	support	main	secondary	back	support	We present a model for the automatic semantic analysis of requirements elicitation documents .	We show empirically that the discourse-based model consistently outperforms the sentence-based model when constructing a system that reflects all the static ( entities , properties ) and dynamic ( behavioral scenarios ) requirements in the document .	CL_D14-1136_mn_1	CL_D14-1136_mn_4	CL_D14-1136_mn_1_4
CL	D14-1136	mn	4	1	result	proposal	support	none	secondary	main	forw	support	We show empirically that the discourse-based model consistently outperforms the sentence-based model when constructing a system that reflects all the static ( entities , properties ) and dynamic ( behavioral scenarios ) requirements in the document .	We present a model for the automatic semantic analysis of requirements elicitation documents .	CL_D14-1136_mn_4	CL_D14-1136_mn_1	CL_D14-1136_mn_1_4
CL	D14-1136	mn	2	3	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	Our target semantic representation employs live sequence charts , a multi-modal visual language for scenario-based programming , which can be directly translated into executable code .	The architecture we propose integrates sentence-level and discourse-level processing in a generative probabilistic framework for the analysis and disambiguation of individual sentences in context .	CL_D14-1136_mn_2	CL_D14-1136_mn_3	CL_D14-1136_mn_2_3
CL	D14-1136	mn	3	2	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	forw	elaboration	The architecture we propose integrates sentence-level and discourse-level processing in a generative probabilistic framework for the analysis and disambiguation of individual sentences in context .	Our target semantic representation employs live sequence charts , a multi-modal visual language for scenario-based programming , which can be directly translated into executable code .	CL_D14-1136_mn_3	CL_D14-1136_mn_2	CL_D14-1136_mn_2_3
CL	D14-1136	mn	2	4	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	Our target semantic representation employs live sequence charts , a multi-modal visual language for scenario-based programming , which can be directly translated into executable code .	We show empirically that the discourse-based model consistently outperforms the sentence-based model when constructing a system that reflects all the static ( entities , properties ) and dynamic ( behavioral scenarios ) requirements in the document .	CL_D14-1136_mn_2	CL_D14-1136_mn_4	CL_D14-1136_mn_2_4
CL	D14-1136	mn	3	4	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	The architecture we propose integrates sentence-level and discourse-level processing in a generative probabilistic framework for the analysis and disambiguation of individual sentences in context .	We show empirically that the discourse-based model consistently outperforms the sentence-based model when constructing a system that reflects all the static ( entities , properties ) and dynamic ( behavioral scenarios ) requirements in the document .	CL_D14-1136_mn_3	CL_D14-1136_mn_4	CL_D14-1136_mn_3_4
CL	D14-1137	mn	1	2	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	We propose a novel model for parsing natural language sentences into their formal semantic representations .	The model is able to perform integrated lexicon acquisition and semantic parsing , mapping each atomic element in a complete semantic representation to a contiguous word sequence in the input sentence in a recursive manner , where certain overlappings amongst such word sequences are allowed .	CL_D14-1137_mn_1	CL_D14-1137_mn_2	CL_D14-1137_mn_1_2
CL	D14-1137	mn	2	1	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	The model is able to perform integrated lexicon acquisition and semantic parsing , mapping each atomic element in a complete semantic representation to a contiguous word sequence in the input sentence in a recursive manner , where certain overlappings amongst such word sequences are allowed .	We propose a novel model for parsing natural language sentences into their formal semantic representations .	CL_D14-1137_mn_2	CL_D14-1137_mn_1	CL_D14-1137_mn_1_2
CL	D14-1137	mn	1	3	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	We propose a novel model for parsing natural language sentences into their formal semantic representations .	It defines distributions over the novel relaxed hybrid tree structures which jointly represent both sentences and semantics .	CL_D14-1137_mn_1	CL_D14-1137_mn_3	CL_D14-1137_mn_1_3
CL	D14-1137	mn	1	4	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	We propose a novel model for parsing natural language sentences into their formal semantic representations .	Such structures allow tractable dynamic programming algorithms to be developed for efficient learning and decoding .	CL_D14-1137_mn_1	CL_D14-1137_mn_4	CL_D14-1137_mn_1_4
CL	D14-1137	mn	1	5	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	We propose a novel model for parsing natural language sentences into their formal semantic representations .	Trained under a discriminative setting , our model is able to incorporate a rich set of features where certain unbounded long-distance dependencies can be captured in a principled manner .	CL_D14-1137_mn_1	CL_D14-1137_mn_5	CL_D14-1137_mn_1_5
CL	D14-1137	mn	1	6	proposal	result_means	none	support	main	secondary	back	support	We propose a novel model for parsing natural language sentences into their formal semantic representations .	We demonstrate through experiments that by exploiting a large collection of simple features , our model is shown to be competitive to previous works and achieves state-of-the-art performance on standard benchmark data across four different languages .	CL_D14-1137_mn_1	CL_D14-1137_mn_6	CL_D14-1137_mn_1_6
CL	D14-1137	mn	6	1	result_means	proposal	support	none	secondary	main	forw	support	We demonstrate through experiments that by exploiting a large collection of simple features , our model is shown to be competitive to previous works and achieves state-of-the-art performance on standard benchmark data across four different languages .	We propose a novel model for parsing natural language sentences into their formal semantic representations .	CL_D14-1137_mn_6	CL_D14-1137_mn_1	CL_D14-1137_mn_1_6
CL	D14-1137	mn	1	7	proposal	information_additional	none	info-optional	main	secondary	back	info-optional	We propose a novel model for parsing natural language sentences into their formal semantic representations .	The system and code can be downloaded from http ://statnlp.org/research/sp/ .	CL_D14-1137_mn_1	CL_D14-1137_mn_7	CL_D14-1137_mn_1_7
CL	D14-1137	mn	7	1	information_additional	proposal	info-optional	none	secondary	main	forw	info-optional	The system and code can be downloaded from http ://statnlp.org/research/sp/ .	We propose a novel model for parsing natural language sentences into their formal semantic representations .	CL_D14-1137_mn_7	CL_D14-1137_mn_1	CL_D14-1137_mn_1_7
CL	D14-1137	mn	2	3	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	The model is able to perform integrated lexicon acquisition and semantic parsing , mapping each atomic element in a complete semantic representation to a contiguous word sequence in the input sentence in a recursive manner , where certain overlappings amongst such word sequences are allowed .	It defines distributions over the novel relaxed hybrid tree structures which jointly represent both sentences and semantics .	CL_D14-1137_mn_2	CL_D14-1137_mn_3	CL_D14-1137_mn_2_3
CL	D14-1137	mn	3	2	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	forw	elaboration	It defines distributions over the novel relaxed hybrid tree structures which jointly represent both sentences and semantics .	The model is able to perform integrated lexicon acquisition and semantic parsing , mapping each atomic element in a complete semantic representation to a contiguous word sequence in the input sentence in a recursive manner , where certain overlappings amongst such word sequences are allowed .	CL_D14-1137_mn_3	CL_D14-1137_mn_2	CL_D14-1137_mn_2_3
CL	D14-1137	mn	2	4	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	none	none	The model is able to perform integrated lexicon acquisition and semantic parsing , mapping each atomic element in a complete semantic representation to a contiguous word sequence in the input sentence in a recursive manner , where certain overlappings amongst such word sequences are allowed .	Such structures allow tractable dynamic programming algorithms to be developed for efficient learning and decoding .	CL_D14-1137_mn_2	CL_D14-1137_mn_4	CL_D14-1137_mn_2_4
CL	D14-1137	mn	2	5	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	The model is able to perform integrated lexicon acquisition and semantic parsing , mapping each atomic element in a complete semantic representation to a contiguous word sequence in the input sentence in a recursive manner , where certain overlappings amongst such word sequences are allowed .	Trained under a discriminative setting , our model is able to incorporate a rich set of features where certain unbounded long-distance dependencies can be captured in a principled manner .	CL_D14-1137_mn_2	CL_D14-1137_mn_5	CL_D14-1137_mn_2_5
CL	D14-1137	mn	5	2	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	forw	elaboration	Trained under a discriminative setting , our model is able to incorporate a rich set of features where certain unbounded long-distance dependencies can be captured in a principled manner .	The model is able to perform integrated lexicon acquisition and semantic parsing , mapping each atomic element in a complete semantic representation to a contiguous word sequence in the input sentence in a recursive manner , where certain overlappings amongst such word sequences are allowed .	CL_D14-1137_mn_5	CL_D14-1137_mn_2	CL_D14-1137_mn_2_5
CL	D14-1137	mn	2	6	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	The model is able to perform integrated lexicon acquisition and semantic parsing , mapping each atomic element in a complete semantic representation to a contiguous word sequence in the input sentence in a recursive manner , where certain overlappings amongst such word sequences are allowed .	We demonstrate through experiments that by exploiting a large collection of simple features , our model is shown to be competitive to previous works and achieves state-of-the-art performance on standard benchmark data across four different languages .	CL_D14-1137_mn_2	CL_D14-1137_mn_6	CL_D14-1137_mn_2_6
CL	D14-1137	mn	2	7	proposal_implementation	information_additional	elaboration	info-optional	secondary	secondary	none	none	The model is able to perform integrated lexicon acquisition and semantic parsing , mapping each atomic element in a complete semantic representation to a contiguous word sequence in the input sentence in a recursive manner , where certain overlappings amongst such word sequences are allowed .	The system and code can be downloaded from http ://statnlp.org/research/sp/ .	CL_D14-1137_mn_2	CL_D14-1137_mn_7	CL_D14-1137_mn_2_7
CL	D14-1137	mn	3	4	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	It defines distributions over the novel relaxed hybrid tree structures which jointly represent both sentences and semantics .	Such structures allow tractable dynamic programming algorithms to be developed for efficient learning and decoding .	CL_D14-1137_mn_3	CL_D14-1137_mn_4	CL_D14-1137_mn_3_4
CL	D14-1137	mn	4	3	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	forw	elaboration	Such structures allow tractable dynamic programming algorithms to be developed for efficient learning and decoding .	It defines distributions over the novel relaxed hybrid tree structures which jointly represent both sentences and semantics .	CL_D14-1137_mn_4	CL_D14-1137_mn_3	CL_D14-1137_mn_3_4
CL	D14-1137	mn	3	5	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	none	none	It defines distributions over the novel relaxed hybrid tree structures which jointly represent both sentences and semantics .	Trained under a discriminative setting , our model is able to incorporate a rich set of features where certain unbounded long-distance dependencies can be captured in a principled manner .	CL_D14-1137_mn_3	CL_D14-1137_mn_5	CL_D14-1137_mn_3_5
CL	D14-1137	mn	3	6	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	It defines distributions over the novel relaxed hybrid tree structures which jointly represent both sentences and semantics .	We demonstrate through experiments that by exploiting a large collection of simple features , our model is shown to be competitive to previous works and achieves state-of-the-art performance on standard benchmark data across four different languages .	CL_D14-1137_mn_3	CL_D14-1137_mn_6	CL_D14-1137_mn_3_6
CL	D14-1137	mn	3	7	proposal_implementation	information_additional	elaboration	info-optional	secondary	secondary	none	none	It defines distributions over the novel relaxed hybrid tree structures which jointly represent both sentences and semantics .	The system and code can be downloaded from http ://statnlp.org/research/sp/ .	CL_D14-1137_mn_3	CL_D14-1137_mn_7	CL_D14-1137_mn_3_7
CL	D14-1137	mn	4	5	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	none	none	Such structures allow tractable dynamic programming algorithms to be developed for efficient learning and decoding .	Trained under a discriminative setting , our model is able to incorporate a rich set of features where certain unbounded long-distance dependencies can be captured in a principled manner .	CL_D14-1137_mn_4	CL_D14-1137_mn_5	CL_D14-1137_mn_4_5
CL	D14-1137	mn	4	6	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	Such structures allow tractable dynamic programming algorithms to be developed for efficient learning and decoding .	We demonstrate through experiments that by exploiting a large collection of simple features , our model is shown to be competitive to previous works and achieves state-of-the-art performance on standard benchmark data across four different languages .	CL_D14-1137_mn_4	CL_D14-1137_mn_6	CL_D14-1137_mn_4_6
CL	D14-1137	mn	4	7	proposal_implementation	information_additional	elaboration	info-optional	secondary	secondary	none	none	Such structures allow tractable dynamic programming algorithms to be developed for efficient learning and decoding .	The system and code can be downloaded from http ://statnlp.org/research/sp/ .	CL_D14-1137_mn_4	CL_D14-1137_mn_7	CL_D14-1137_mn_4_7
CL	D14-1137	mn	5	6	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	Trained under a discriminative setting , our model is able to incorporate a rich set of features where certain unbounded long-distance dependencies can be captured in a principled manner .	We demonstrate through experiments that by exploiting a large collection of simple features , our model is shown to be competitive to previous works and achieves state-of-the-art performance on standard benchmark data across four different languages .	CL_D14-1137_mn_5	CL_D14-1137_mn_6	CL_D14-1137_mn_5_6
CL	D14-1137	mn	5	7	proposal_implementation	information_additional	elaboration	info-optional	secondary	secondary	none	none	Trained under a discriminative setting , our model is able to incorporate a rich set of features where certain unbounded long-distance dependencies can be captured in a principled manner .	The system and code can be downloaded from http ://statnlp.org/research/sp/ .	CL_D14-1137_mn_5	CL_D14-1137_mn_7	CL_D14-1137_mn_5_7
CL	D14-1137	mn	6	7	result_means	information_additional	support	info-optional	secondary	secondary	none	none	We demonstrate through experiments that by exploiting a large collection of simple features , our model is shown to be competitive to previous works and achieves state-of-the-art performance on standard benchmark data across four different languages .	The system and code can be downloaded from http ://statnlp.org/research/sp/ .	CL_D14-1137_mn_6	CL_D14-1137_mn_7	CL_D14-1137_mn_6_7
CL	D14-1138	mn	1	2	motivation_background	motivation_problem	info-required	support	secondary	secondary	forw	info-required	The anchor words algorithm performs provably efficient topic model inference by finding an approximate convex hull in a high-dimensional word co-occurrence space .	However , the existing greedy algorithm often selects poor anchor words , reducing topic quality and interpretability .	CL_D14-1138_mn_1	CL_D14-1138_mn_2	CL_D14-1138_mn_1_2
CL	D14-1138	mn	2	1	motivation_problem	motivation_background	support	info-required	secondary	secondary	back	info-required	However , the existing greedy algorithm often selects poor anchor words , reducing topic quality and interpretability .	The anchor words algorithm performs provably efficient topic model inference by finding an approximate convex hull in a high-dimensional word co-occurrence space .	CL_D14-1138_mn_2	CL_D14-1138_mn_1	CL_D14-1138_mn_1_2
CL	D14-1138	mn	1	3	motivation_background	proposal	info-required	none	secondary	main	none	none	The anchor words algorithm performs provably efficient topic model inference by finding an approximate convex hull in a high-dimensional word co-occurrence space .	Rather than finding an approximate convex hull in a high-dimensional space , we propose to find an exact convex hull in a visualizable 2- or 3-dimensional space .	CL_D14-1138_mn_1	CL_D14-1138_mn_3	CL_D14-1138_mn_1_3
CL	D14-1138	mn	1	4	motivation_background	result	info-required	support	secondary	secondary	none	none	The anchor words algorithm performs provably efficient topic model inference by finding an approximate convex hull in a high-dimensional word co-occurrence space .	Such low-dimensional embeddings both improve topics and clearly show users why the algorithm selects certain words .	CL_D14-1138_mn_1	CL_D14-1138_mn_4	CL_D14-1138_mn_1_4
CL	D14-1138	mn	2	3	motivation_problem	proposal	support	none	secondary	main	forw	support	However , the existing greedy algorithm often selects poor anchor words , reducing topic quality and interpretability .	Rather than finding an approximate convex hull in a high-dimensional space , we propose to find an exact convex hull in a visualizable 2- or 3-dimensional space .	CL_D14-1138_mn_2	CL_D14-1138_mn_3	CL_D14-1138_mn_2_3
CL	D14-1138	mn	3	2	proposal	motivation_problem	none	support	main	secondary	back	support	Rather than finding an approximate convex hull in a high-dimensional space , we propose to find an exact convex hull in a visualizable 2- or 3-dimensional space .	However , the existing greedy algorithm often selects poor anchor words , reducing topic quality and interpretability .	CL_D14-1138_mn_3	CL_D14-1138_mn_2	CL_D14-1138_mn_2_3
CL	D14-1138	mn	2	4	motivation_problem	result	support	support	secondary	secondary	none	none	However , the existing greedy algorithm often selects poor anchor words , reducing topic quality and interpretability .	Such low-dimensional embeddings both improve topics and clearly show users why the algorithm selects certain words .	CL_D14-1138_mn_2	CL_D14-1138_mn_4	CL_D14-1138_mn_2_4
CL	D14-1138	mn	3	4	proposal	result	none	support	main	secondary	back	support	Rather than finding an approximate convex hull in a high-dimensional space , we propose to find an exact convex hull in a visualizable 2- or 3-dimensional space .	Such low-dimensional embeddings both improve topics and clearly show users why the algorithm selects certain words .	CL_D14-1138_mn_3	CL_D14-1138_mn_4	CL_D14-1138_mn_3_4
CL	D14-1138	mn	4	3	result	proposal	support	none	secondary	main	forw	support	Such low-dimensional embeddings both improve topics and clearly show users why the algorithm selects certain words .	Rather than finding an approximate convex hull in a high-dimensional space , we propose to find an exact convex hull in a visualizable 2- or 3-dimensional space .	CL_D14-1138_mn_4	CL_D14-1138_mn_3	CL_D14-1138_mn_3_4
CL	D14-1139	mn	1	2	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	We generalize contrastive estimation in two ways that permit adding more knowledge to unsupervised learning .	The first allows the modeler to specify not only the set of corrupted inputs for each observation , but also how bad each one is .	CL_D14-1139_mn_1	CL_D14-1139_mn_2	CL_D14-1139_mn_1_2
CL	D14-1139	mn	2	1	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	The first allows the modeler to specify not only the set of corrupted inputs for each observation , but also how bad each one is .	We generalize contrastive estimation in two ways that permit adding more knowledge to unsupervised learning .	CL_D14-1139_mn_2	CL_D14-1139_mn_1	CL_D14-1139_mn_1_2
CL	D14-1139	mn	1	3	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	We generalize contrastive estimation in two ways that permit adding more knowledge to unsupervised learning .	The second allows specifying structural preferences on the latent variable used to explain the observations .	CL_D14-1139_mn_1	CL_D14-1139_mn_3	CL_D14-1139_mn_1_3
CL	D14-1139	mn	1	4	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	We generalize contrastive estimation in two ways that permit adding more knowledge to unsupervised learning .	They require setting additional hyperparameters , which can be problematic in unsupervised learning , so we investigate new methods for unsupervised model selection and system combination .	CL_D14-1139_mn_1	CL_D14-1139_mn_4	CL_D14-1139_mn_1_4
CL	D14-1139	mn	1	5	proposal	result_means	none	support	main	secondary	back	support	We generalize contrastive estimation in two ways that permit adding more knowledge to unsupervised learning .	We instantiate these ideas for part-of-speech induction without tag dictionaries , improving over contrastive estimation as well as strong benchmarks from the PASCAL 2012 shared task .	CL_D14-1139_mn_1	CL_D14-1139_mn_5	CL_D14-1139_mn_1_5
CL	D14-1139	mn	5	1	result_means	proposal	support	none	secondary	main	forw	support	We instantiate these ideas for part-of-speech induction without tag dictionaries , improving over contrastive estimation as well as strong benchmarks from the PASCAL 2012 shared task .	We generalize contrastive estimation in two ways that permit adding more knowledge to unsupervised learning .	CL_D14-1139_mn_5	CL_D14-1139_mn_1	CL_D14-1139_mn_1_5
CL	D14-1139	mn	2	3	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	The first allows the modeler to specify not only the set of corrupted inputs for each observation , but also how bad each one is .	The second allows specifying structural preferences on the latent variable used to explain the observations .	CL_D14-1139_mn_2	CL_D14-1139_mn_3	CL_D14-1139_mn_2_3
CL	D14-1139	mn	3	2	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	forw	elaboration	The second allows specifying structural preferences on the latent variable used to explain the observations .	The first allows the modeler to specify not only the set of corrupted inputs for each observation , but also how bad each one is .	CL_D14-1139_mn_3	CL_D14-1139_mn_2	CL_D14-1139_mn_2_3
CL	D14-1139	mn	2	4	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	none	none	The first allows the modeler to specify not only the set of corrupted inputs for each observation , but also how bad each one is .	They require setting additional hyperparameters , which can be problematic in unsupervised learning , so we investigate new methods for unsupervised model selection and system combination .	CL_D14-1139_mn_2	CL_D14-1139_mn_4	CL_D14-1139_mn_2_4
CL	D14-1139	mn	2	5	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	The first allows the modeler to specify not only the set of corrupted inputs for each observation , but also how bad each one is .	We instantiate these ideas for part-of-speech induction without tag dictionaries , improving over contrastive estimation as well as strong benchmarks from the PASCAL 2012 shared task .	CL_D14-1139_mn_2	CL_D14-1139_mn_5	CL_D14-1139_mn_2_5
CL	D14-1139	mn	3	4	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	The second allows specifying structural preferences on the latent variable used to explain the observations .	They require setting additional hyperparameters , which can be problematic in unsupervised learning , so we investigate new methods for unsupervised model selection and system combination .	CL_D14-1139_mn_3	CL_D14-1139_mn_4	CL_D14-1139_mn_3_4
CL	D14-1139	mn	4	3	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	forw	elaboration	They require setting additional hyperparameters , which can be problematic in unsupervised learning , so we investigate new methods for unsupervised model selection and system combination .	The second allows specifying structural preferences on the latent variable used to explain the observations .	CL_D14-1139_mn_4	CL_D14-1139_mn_3	CL_D14-1139_mn_3_4
CL	D14-1139	mn	3	5	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	The second allows specifying structural preferences on the latent variable used to explain the observations .	We instantiate these ideas for part-of-speech induction without tag dictionaries , improving over contrastive estimation as well as strong benchmarks from the PASCAL 2012 shared task .	CL_D14-1139_mn_3	CL_D14-1139_mn_5	CL_D14-1139_mn_3_5
CL	D14-1139	mn	4	5	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	They require setting additional hyperparameters , which can be problematic in unsupervised learning , so we investigate new methods for unsupervised model selection and system combination .	We instantiate these ideas for part-of-speech induction without tag dictionaries , improving over contrastive estimation as well as strong benchmarks from the PASCAL 2012 shared task .	CL_D14-1139_mn_4	CL_D14-1139_mn_5	CL_D14-1139_mn_4_5
CL	D14-1140	mn	1	2	proposal	motivation_problem	none	support	main	secondary	none	none	We introduce a reinforcement learning-based approach to simultaneous machine translationproducing a translation while receiving input words between languages with drastically different word orders : from verb-final languages ( e.g. , German ) to verb-medial languages ( English ) .	In traditional machine translation , a translator must "wait" for source material to appear before translation begins .	CL_D14-1140_mn_1	CL_D14-1140_mn_2	CL_D14-1140_mn_1_2
CL	D14-1140	mn	1	3	proposal	proposal	none	elaboration	main	secondary	back	elaboration	We introduce a reinforcement learning-based approach to simultaneous machine translationproducing a translation while receiving input words between languages with drastically different word orders : from verb-final languages ( e.g. , German ) to verb-medial languages ( English ) .	We remove this bottleneck by predicting the final verb in advance .	CL_D14-1140_mn_1	CL_D14-1140_mn_3	CL_D14-1140_mn_1_3
CL	D14-1140	mn	3	1	proposal	proposal	elaboration	none	secondary	main	forw	elaboration	We remove this bottleneck by predicting the final verb in advance .	We introduce a reinforcement learning-based approach to simultaneous machine translationproducing a translation while receiving input words between languages with drastically different word orders : from verb-final languages ( e.g. , German ) to verb-medial languages ( English ) .	CL_D14-1140_mn_3	CL_D14-1140_mn_1	CL_D14-1140_mn_1_3
CL	D14-1140	mn	1	4	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	We introduce a reinforcement learning-based approach to simultaneous machine translationproducing a translation while receiving input words between languages with drastically different word orders : from verb-final languages ( e.g. , German ) to verb-medial languages ( English ) .	We use reinforcement learning to learn when to trust predictions about unseen , future portions of the sentence .	CL_D14-1140_mn_1	CL_D14-1140_mn_4	CL_D14-1140_mn_1_4
CL	D14-1140	mn	4	1	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	We use reinforcement learning to learn when to trust predictions about unseen , future portions of the sentence .	We introduce a reinforcement learning-based approach to simultaneous machine translationproducing a translation while receiving input words between languages with drastically different word orders : from verb-final languages ( e.g. , German ) to verb-medial languages ( English ) .	CL_D14-1140_mn_4	CL_D14-1140_mn_1	CL_D14-1140_mn_1_4
CL	D14-1140	mn	1	5	proposal	means	none	by-means	main	secondary	none	none	We introduce a reinforcement learning-based approach to simultaneous machine translationproducing a translation while receiving input words between languages with drastically different word orders : from verb-final languages ( e.g. , German ) to verb-medial languages ( English ) .	We also introduce an evaluation metric to measure expeditiousness and quality .	CL_D14-1140_mn_1	CL_D14-1140_mn_5	CL_D14-1140_mn_1_5
CL	D14-1140	mn	1	6	proposal	result	none	support	main	secondary	back	support	We introduce a reinforcement learning-based approach to simultaneous machine translationproducing a translation while receiving input words between languages with drastically different word orders : from verb-final languages ( e.g. , German ) to verb-medial languages ( English ) .	We show that our new translation model outperforms batch and monotone translation strategies .	CL_D14-1140_mn_1	CL_D14-1140_mn_6	CL_D14-1140_mn_1_6
CL	D14-1140	mn	6	1	result	proposal	support	none	secondary	main	forw	support	We show that our new translation model outperforms batch and monotone translation strategies .	We introduce a reinforcement learning-based approach to simultaneous machine translationproducing a translation while receiving input words between languages with drastically different word orders : from verb-final languages ( e.g. , German ) to verb-medial languages ( English ) .	CL_D14-1140_mn_6	CL_D14-1140_mn_1	CL_D14-1140_mn_1_6
CL	D14-1140	mn	2	3	motivation_problem	proposal	support	elaboration	secondary	secondary	forw	support	In traditional machine translation , a translator must "wait" for source material to appear before translation begins .	We remove this bottleneck by predicting the final verb in advance .	CL_D14-1140_mn_2	CL_D14-1140_mn_3	CL_D14-1140_mn_2_3
CL	D14-1140	mn	3	2	proposal	motivation_problem	elaboration	support	secondary	secondary	back	support	We remove this bottleneck by predicting the final verb in advance .	In traditional machine translation , a translator must "wait" for source material to appear before translation begins .	CL_D14-1140_mn_3	CL_D14-1140_mn_2	CL_D14-1140_mn_2_3
CL	D14-1140	mn	2	4	motivation_problem	proposal_implementation	support	elaboration	secondary	secondary	none	none	In traditional machine translation , a translator must "wait" for source material to appear before translation begins .	We use reinforcement learning to learn when to trust predictions about unseen , future portions of the sentence .	CL_D14-1140_mn_2	CL_D14-1140_mn_4	CL_D14-1140_mn_2_4
CL	D14-1140	mn	2	5	motivation_problem	means	support	by-means	secondary	secondary	none	none	In traditional machine translation , a translator must "wait" for source material to appear before translation begins .	We also introduce an evaluation metric to measure expeditiousness and quality .	CL_D14-1140_mn_2	CL_D14-1140_mn_5	CL_D14-1140_mn_2_5
CL	D14-1140	mn	2	6	motivation_problem	result	support	support	secondary	secondary	none	none	In traditional machine translation , a translator must "wait" for source material to appear before translation begins .	We show that our new translation model outperforms batch and monotone translation strategies .	CL_D14-1140_mn_2	CL_D14-1140_mn_6	CL_D14-1140_mn_2_6
CL	D14-1140	mn	3	4	proposal	proposal_implementation	elaboration	elaboration	secondary	secondary	none	none	We remove this bottleneck by predicting the final verb in advance .	We use reinforcement learning to learn when to trust predictions about unseen , future portions of the sentence .	CL_D14-1140_mn_3	CL_D14-1140_mn_4	CL_D14-1140_mn_3_4
CL	D14-1140	mn	3	5	proposal	means	elaboration	by-means	secondary	secondary	none	none	We remove this bottleneck by predicting the final verb in advance .	We also introduce an evaluation metric to measure expeditiousness and quality .	CL_D14-1140_mn_3	CL_D14-1140_mn_5	CL_D14-1140_mn_3_5
CL	D14-1140	mn	3	6	proposal	result	elaboration	support	secondary	secondary	none	none	We remove this bottleneck by predicting the final verb in advance .	We show that our new translation model outperforms batch and monotone translation strategies .	CL_D14-1140_mn_3	CL_D14-1140_mn_6	CL_D14-1140_mn_3_6
CL	D14-1140	mn	4	5	proposal_implementation	means	elaboration	by-means	secondary	secondary	none	none	We use reinforcement learning to learn when to trust predictions about unseen , future portions of the sentence .	We also introduce an evaluation metric to measure expeditiousness and quality .	CL_D14-1140_mn_4	CL_D14-1140_mn_5	CL_D14-1140_mn_4_5
CL	D14-1140	mn	4	6	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	We use reinforcement learning to learn when to trust predictions about unseen , future portions of the sentence .	We show that our new translation model outperforms batch and monotone translation strategies .	CL_D14-1140_mn_4	CL_D14-1140_mn_6	CL_D14-1140_mn_4_6
CL	D14-1140	mn	5	6	means	result	by-means	support	secondary	secondary	forw	by-means	We also introduce an evaluation metric to measure expeditiousness and quality .	We show that our new translation model outperforms batch and monotone translation strategies .	CL_D14-1140_mn_5	CL_D14-1140_mn_6	CL_D14-1140_mn_5_6
CL	D14-1140	mn	6	5	result	means	support	by-means	secondary	secondary	back	by-means	We show that our new translation model outperforms batch and monotone translation strategies .	We also introduce an evaluation metric to measure expeditiousness and quality .	CL_D14-1140_mn_6	CL_D14-1140_mn_5	CL_D14-1140_mn_5_6
CL	D14-1141	mn	1	2	motivation_background	motivation_background	info-required	support	secondary	secondary	forw	info-required	The task of unsupervised induction of probabilistic context-free grammars ( PCFGs ) has attracted a lot of attention in the field of computational linguistics .	Although it is a difficult task , work in this area is still very much in demand since it can contribute to the advancement of language parsing and modelling .	CL_D14-1141_mn_1	CL_D14-1141_mn_2	CL_D14-1141_mn_1_2
CL	D14-1141	mn	2	1	motivation_background	motivation_background	support	info-required	secondary	secondary	back	info-required	Although it is a difficult task , work in this area is still very much in demand since it can contribute to the advancement of language parsing and modelling .	The task of unsupervised induction of probabilistic context-free grammars ( PCFGs ) has attracted a lot of attention in the field of computational linguistics .	CL_D14-1141_mn_2	CL_D14-1141_mn_1	CL_D14-1141_mn_1_2
CL	D14-1141	mn	1	3	motivation_background	proposal	info-required	none	secondary	main	none	none	The task of unsupervised induction of probabilistic context-free grammars ( PCFGs ) has attracted a lot of attention in the field of computational linguistics .	In this work , we describe a new algorithm for PCFG induction based on a principled approach and capable of inducing accurate yet compact artificial natural language grammars and typical context-free grammars .	CL_D14-1141_mn_1	CL_D14-1141_mn_3	CL_D14-1141_mn_1_3
CL	D14-1141	mn	1	4	motivation_background	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	The task of unsupervised induction of probabilistic context-free grammars ( PCFGs ) has attracted a lot of attention in the field of computational linguistics .	Moreover , this algorithm can work on large grammars and datasets and infers correctly even from small samples .	CL_D14-1141_mn_1	CL_D14-1141_mn_4	CL_D14-1141_mn_1_4
CL	D14-1141	mn	1	5	motivation_background	result	info-required	support	secondary	secondary	none	none	The task of unsupervised induction of probabilistic context-free grammars ( PCFGs ) has attracted a lot of attention in the field of computational linguistics .	Our analysis shows that the type of grammars induced by our algorithm are , in theory , capable of modelling natural language .	CL_D14-1141_mn_1	CL_D14-1141_mn_5	CL_D14-1141_mn_1_5
CL	D14-1141	mn	1	6	motivation_background	result_means	info-required	elaboration	secondary	secondary	none	none	The task of unsupervised induction of probabilistic context-free grammars ( PCFGs ) has attracted a lot of attention in the field of computational linguistics .	One of our experiments shows that our algorithm can potentially outperform the state-of-the-art in unsupervised parsing on the WSJ10 corpus .	CL_D14-1141_mn_1	CL_D14-1141_mn_6	CL_D14-1141_mn_1_6
CL	D14-1141	mn	2	3	motivation_background	proposal	support	none	secondary	main	forw	support	Although it is a difficult task , work in this area is still very much in demand since it can contribute to the advancement of language parsing and modelling .	In this work , we describe a new algorithm for PCFG induction based on a principled approach and capable of inducing accurate yet compact artificial natural language grammars and typical context-free grammars .	CL_D14-1141_mn_2	CL_D14-1141_mn_3	CL_D14-1141_mn_2_3
CL	D14-1141	mn	3	2	proposal	motivation_background	none	support	main	secondary	back	support	In this work , we describe a new algorithm for PCFG induction based on a principled approach and capable of inducing accurate yet compact artificial natural language grammars and typical context-free grammars .	Although it is a difficult task , work in this area is still very much in demand since it can contribute to the advancement of language parsing and modelling .	CL_D14-1141_mn_3	CL_D14-1141_mn_2	CL_D14-1141_mn_2_3
CL	D14-1141	mn	2	4	motivation_background	proposal_implementation	support	elaboration	secondary	secondary	none	none	Although it is a difficult task , work in this area is still very much in demand since it can contribute to the advancement of language parsing and modelling .	Moreover , this algorithm can work on large grammars and datasets and infers correctly even from small samples .	CL_D14-1141_mn_2	CL_D14-1141_mn_4	CL_D14-1141_mn_2_4
CL	D14-1141	mn	2	5	motivation_background	result	support	support	secondary	secondary	none	none	Although it is a difficult task , work in this area is still very much in demand since it can contribute to the advancement of language parsing and modelling .	Our analysis shows that the type of grammars induced by our algorithm are , in theory , capable of modelling natural language .	CL_D14-1141_mn_2	CL_D14-1141_mn_5	CL_D14-1141_mn_2_5
CL	D14-1141	mn	2	6	motivation_background	result_means	support	elaboration	secondary	secondary	none	none	Although it is a difficult task , work in this area is still very much in demand since it can contribute to the advancement of language parsing and modelling .	One of our experiments shows that our algorithm can potentially outperform the state-of-the-art in unsupervised parsing on the WSJ10 corpus .	CL_D14-1141_mn_2	CL_D14-1141_mn_6	CL_D14-1141_mn_2_6
CL	D14-1141	mn	3	4	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	In this work , we describe a new algorithm for PCFG induction based on a principled approach and capable of inducing accurate yet compact artificial natural language grammars and typical context-free grammars .	Moreover , this algorithm can work on large grammars and datasets and infers correctly even from small samples .	CL_D14-1141_mn_3	CL_D14-1141_mn_4	CL_D14-1141_mn_3_4
CL	D14-1141	mn	4	3	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	Moreover , this algorithm can work on large grammars and datasets and infers correctly even from small samples .	In this work , we describe a new algorithm for PCFG induction based on a principled approach and capable of inducing accurate yet compact artificial natural language grammars and typical context-free grammars .	CL_D14-1141_mn_4	CL_D14-1141_mn_3	CL_D14-1141_mn_3_4
CL	D14-1141	mn	3	5	proposal	result	none	support	main	secondary	back	support	In this work , we describe a new algorithm for PCFG induction based on a principled approach and capable of inducing accurate yet compact artificial natural language grammars and typical context-free grammars .	Our analysis shows that the type of grammars induced by our algorithm are , in theory , capable of modelling natural language .	CL_D14-1141_mn_3	CL_D14-1141_mn_5	CL_D14-1141_mn_3_5
CL	D14-1141	mn	5	3	result	proposal	support	none	secondary	main	forw	support	Our analysis shows that the type of grammars induced by our algorithm are , in theory , capable of modelling natural language .	In this work , we describe a new algorithm for PCFG induction based on a principled approach and capable of inducing accurate yet compact artificial natural language grammars and typical context-free grammars .	CL_D14-1141_mn_5	CL_D14-1141_mn_3	CL_D14-1141_mn_3_5
CL	D14-1141	mn	3	6	proposal	result_means	none	elaboration	main	secondary	none	none	In this work , we describe a new algorithm for PCFG induction based on a principled approach and capable of inducing accurate yet compact artificial natural language grammars and typical context-free grammars .	One of our experiments shows that our algorithm can potentially outperform the state-of-the-art in unsupervised parsing on the WSJ10 corpus .	CL_D14-1141_mn_3	CL_D14-1141_mn_6	CL_D14-1141_mn_3_6
CL	D14-1141	mn	4	5	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	Moreover , this algorithm can work on large grammars and datasets and infers correctly even from small samples .	Our analysis shows that the type of grammars induced by our algorithm are , in theory , capable of modelling natural language .	CL_D14-1141_mn_4	CL_D14-1141_mn_5	CL_D14-1141_mn_4_5
CL	D14-1141	mn	4	6	proposal_implementation	result_means	elaboration	elaboration	secondary	secondary	none	none	Moreover , this algorithm can work on large grammars and datasets and infers correctly even from small samples .	One of our experiments shows that our algorithm can potentially outperform the state-of-the-art in unsupervised parsing on the WSJ10 corpus .	CL_D14-1141_mn_4	CL_D14-1141_mn_6	CL_D14-1141_mn_4_6
CL	D14-1141	mn	5	6	result	result_means	support	elaboration	secondary	secondary	back	elaboration	Our analysis shows that the type of grammars induced by our algorithm are , in theory , capable of modelling natural language .	One of our experiments shows that our algorithm can potentially outperform the state-of-the-art in unsupervised parsing on the WSJ10 corpus .	CL_D14-1141_mn_5	CL_D14-1141_mn_6	CL_D14-1141_mn_5_6
CL	D14-1141	mn	6	5	result_means	result	elaboration	support	secondary	secondary	forw	elaboration	One of our experiments shows that our algorithm can potentially outperform the state-of-the-art in unsupervised parsing on the WSJ10 corpus .	Our analysis shows that the type of grammars induced by our algorithm are , in theory , capable of modelling natural language .	CL_D14-1141_mn_6	CL_D14-1141_mn_5	CL_D14-1141_mn_5_6
CL	D14-1142	mn	1	2	motivation_background	proposal	support	none	secondary	main	forw	support	A common approach in text mining tasks such as text categorization , authorship identification or plagiarism detection is to rely on features like words , part-of-speech tags , stems , or some other high-level linguistic features .	In this work , an approach that uses character n-grams as features is proposed for the task of native language identification .	CL_D14-1142_mn_1	CL_D14-1142_mn_2	CL_D14-1142_mn_1_2
CL	D14-1142	mn	2	1	proposal	motivation_background	none	support	main	secondary	back	support	In this work , an approach that uses character n-grams as features is proposed for the task of native language identification .	A common approach in text mining tasks such as text categorization , authorship identification or plagiarism detection is to rely on features like words , part-of-speech tags , stems , or some other high-level linguistic features .	CL_D14-1142_mn_2	CL_D14-1142_mn_1	CL_D14-1142_mn_1_2
CL	D14-1142	mn	1	3	motivation_background	proposal_implementation	support	elaboration	secondary	secondary	none	none	A common approach in text mining tasks such as text categorization , authorship identification or plagiarism detection is to rely on features like words , part-of-speech tags , stems , or some other high-level linguistic features .	Instead of doing standard feature selection , the proposed approach combines several string kernels using multiple kernel learning .	CL_D14-1142_mn_1	CL_D14-1142_mn_3	CL_D14-1142_mn_1_3
CL	D14-1142	mn	1	4	motivation_background	proposal_implementation	support	elaboration	secondary	secondary	none	none	A common approach in text mining tasks such as text categorization , authorship identification or plagiarism detection is to rely on features like words , part-of-speech tags , stems , or some other high-level linguistic features .	Kernel Ridge Regression and Kernel Discriminant Analysis are independently used in the learning stage .	CL_D14-1142_mn_1	CL_D14-1142_mn_4	CL_D14-1142_mn_1_4
CL	D14-1142	mn	1	5	motivation_background	result_means	support	support	secondary	secondary	none	none	A common approach in text mining tasks such as text categorization , authorship identification or plagiarism detection is to rely on features like words , part-of-speech tags , stems , or some other high-level linguistic features .	The empirical results obtained in all the experiments conducted in this work indicate that the proposed approach achieves state of the art performance in native language identification , reaching an accuracy that is 1.7 % above the top scoring system of the 2013 NLI Shared Task .	CL_D14-1142_mn_1	CL_D14-1142_mn_5	CL_D14-1142_mn_1_5
CL	D14-1142	mn	1	6	motivation_background	conclusion	support	support	secondary	secondary	none	none	A common approach in text mining tasks such as text categorization , authorship identification or plagiarism detection is to rely on features like words , part-of-speech tags , stems , or some other high-level linguistic features .	Furthermore , the proposed approach has an important advantage in that it is language independent and linguistic theory neutral.	CL_D14-1142_mn_1	CL_D14-1142_mn_6	CL_D14-1142_mn_1_6
CL	D14-1142	mn	1	7	motivation_background	result_means	support	support	secondary	secondary	none	none	A common approach in text mining tasks such as text categorization , authorship identification or plagiarism detection is to rely on features like words , part-of-speech tags , stems , or some other high-level linguistic features .	In the cross-corpus experiment , the proposed approach shows that it can also be topic independent , improving the state of the art system by 32.3 % .	CL_D14-1142_mn_1	CL_D14-1142_mn_7	CL_D14-1142_mn_1_7
CL	D14-1142	mn	2	3	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	In this work , an approach that uses character n-grams as features is proposed for the task of native language identification .	Instead of doing standard feature selection , the proposed approach combines several string kernels using multiple kernel learning .	CL_D14-1142_mn_2	CL_D14-1142_mn_3	CL_D14-1142_mn_2_3
CL	D14-1142	mn	3	2	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	Instead of doing standard feature selection , the proposed approach combines several string kernels using multiple kernel learning .	In this work , an approach that uses character n-grams as features is proposed for the task of native language identification .	CL_D14-1142_mn_3	CL_D14-1142_mn_2	CL_D14-1142_mn_2_3
CL	D14-1142	mn	2	4	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	In this work , an approach that uses character n-grams as features is proposed for the task of native language identification .	Kernel Ridge Regression and Kernel Discriminant Analysis are independently used in the learning stage .	CL_D14-1142_mn_2	CL_D14-1142_mn_4	CL_D14-1142_mn_2_4
CL	D14-1142	mn	2	5	proposal	result_means	none	support	main	secondary	none	none	In this work , an approach that uses character n-grams as features is proposed for the task of native language identification .	The empirical results obtained in all the experiments conducted in this work indicate that the proposed approach achieves state of the art performance in native language identification , reaching an accuracy that is 1.7 % above the top scoring system of the 2013 NLI Shared Task .	CL_D14-1142_mn_2	CL_D14-1142_mn_5	CL_D14-1142_mn_2_5
CL	D14-1142	mn	2	6	proposal	conclusion	none	support	main	secondary	back	support	In this work , an approach that uses character n-grams as features is proposed for the task of native language identification .	Furthermore , the proposed approach has an important advantage in that it is language independent and linguistic theory neutral.	CL_D14-1142_mn_2	CL_D14-1142_mn_6	CL_D14-1142_mn_2_6
CL	D14-1142	mn	6	2	conclusion	proposal	support	none	secondary	main	forw	support	Furthermore , the proposed approach has an important advantage in that it is language independent and linguistic theory neutral.	In this work , an approach that uses character n-grams as features is proposed for the task of native language identification .	CL_D14-1142_mn_6	CL_D14-1142_mn_2	CL_D14-1142_mn_2_6
CL	D14-1142	mn	2	7	proposal	result_means	none	support	main	secondary	none	none	In this work , an approach that uses character n-grams as features is proposed for the task of native language identification .	In the cross-corpus experiment , the proposed approach shows that it can also be topic independent , improving the state of the art system by 32.3 % .	CL_D14-1142_mn_2	CL_D14-1142_mn_7	CL_D14-1142_mn_2_7
CL	D14-1142	mn	3	4	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	Instead of doing standard feature selection , the proposed approach combines several string kernels using multiple kernel learning .	Kernel Ridge Regression and Kernel Discriminant Analysis are independently used in the learning stage .	CL_D14-1142_mn_3	CL_D14-1142_mn_4	CL_D14-1142_mn_3_4
CL	D14-1142	mn	4	3	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	forw	elaboration	Kernel Ridge Regression and Kernel Discriminant Analysis are independently used in the learning stage .	Instead of doing standard feature selection , the proposed approach combines several string kernels using multiple kernel learning .	CL_D14-1142_mn_4	CL_D14-1142_mn_3	CL_D14-1142_mn_3_4
CL	D14-1142	mn	3	5	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	Instead of doing standard feature selection , the proposed approach combines several string kernels using multiple kernel learning .	The empirical results obtained in all the experiments conducted in this work indicate that the proposed approach achieves state of the art performance in native language identification , reaching an accuracy that is 1.7 % above the top scoring system of the 2013 NLI Shared Task .	CL_D14-1142_mn_3	CL_D14-1142_mn_5	CL_D14-1142_mn_3_5
CL	D14-1142	mn	3	6	proposal_implementation	conclusion	elaboration	support	secondary	secondary	none	none	Instead of doing standard feature selection , the proposed approach combines several string kernels using multiple kernel learning .	Furthermore , the proposed approach has an important advantage in that it is language independent and linguistic theory neutral.	CL_D14-1142_mn_3	CL_D14-1142_mn_6	CL_D14-1142_mn_3_6
CL	D14-1142	mn	3	7	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	Instead of doing standard feature selection , the proposed approach combines several string kernels using multiple kernel learning .	In the cross-corpus experiment , the proposed approach shows that it can also be topic independent , improving the state of the art system by 32.3 % .	CL_D14-1142_mn_3	CL_D14-1142_mn_7	CL_D14-1142_mn_3_7
CL	D14-1142	mn	4	5	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	Kernel Ridge Regression and Kernel Discriminant Analysis are independently used in the learning stage .	The empirical results obtained in all the experiments conducted in this work indicate that the proposed approach achieves state of the art performance in native language identification , reaching an accuracy that is 1.7 % above the top scoring system of the 2013 NLI Shared Task .	CL_D14-1142_mn_4	CL_D14-1142_mn_5	CL_D14-1142_mn_4_5
CL	D14-1142	mn	4	6	proposal_implementation	conclusion	elaboration	support	secondary	secondary	none	none	Kernel Ridge Regression and Kernel Discriminant Analysis are independently used in the learning stage .	Furthermore , the proposed approach has an important advantage in that it is language independent and linguistic theory neutral.	CL_D14-1142_mn_4	CL_D14-1142_mn_6	CL_D14-1142_mn_4_6
CL	D14-1142	mn	4	7	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	Kernel Ridge Regression and Kernel Discriminant Analysis are independently used in the learning stage .	In the cross-corpus experiment , the proposed approach shows that it can also be topic independent , improving the state of the art system by 32.3 % .	CL_D14-1142_mn_4	CL_D14-1142_mn_7	CL_D14-1142_mn_4_7
CL	D14-1142	mn	5	6	result_means	conclusion	support	support	secondary	secondary	forw	support	The empirical results obtained in all the experiments conducted in this work indicate that the proposed approach achieves state of the art performance in native language identification , reaching an accuracy that is 1.7 % above the top scoring system of the 2013 NLI Shared Task .	Furthermore , the proposed approach has an important advantage in that it is language independent and linguistic theory neutral.	CL_D14-1142_mn_5	CL_D14-1142_mn_6	CL_D14-1142_mn_5_6
CL	D14-1142	mn	6	5	conclusion	result_means	support	support	secondary	secondary	back	support	Furthermore , the proposed approach has an important advantage in that it is language independent and linguistic theory neutral.	The empirical results obtained in all the experiments conducted in this work indicate that the proposed approach achieves state of the art performance in native language identification , reaching an accuracy that is 1.7 % above the top scoring system of the 2013 NLI Shared Task .	CL_D14-1142_mn_6	CL_D14-1142_mn_5	CL_D14-1142_mn_5_6
CL	D14-1142	mn	5	7	result_means	result_means	support	support	secondary	secondary	none	none	The empirical results obtained in all the experiments conducted in this work indicate that the proposed approach achieves state of the art performance in native language identification , reaching an accuracy that is 1.7 % above the top scoring system of the 2013 NLI Shared Task .	In the cross-corpus experiment , the proposed approach shows that it can also be topic independent , improving the state of the art system by 32.3 % .	CL_D14-1142_mn_5	CL_D14-1142_mn_7	CL_D14-1142_mn_5_7
CL	D14-1142	mn	6	7	conclusion	result_means	support	support	secondary	secondary	back	support	Furthermore , the proposed approach has an important advantage in that it is language independent and linguistic theory neutral.	In the cross-corpus experiment , the proposed approach shows that it can also be topic independent , improving the state of the art system by 32.3 % .	CL_D14-1142_mn_6	CL_D14-1142_mn_7	CL_D14-1142_mn_6_7
CL	D14-1142	mn	7	6	result_means	conclusion	support	support	secondary	secondary	forw	support	In the cross-corpus experiment , the proposed approach shows that it can also be topic independent , improving the state of the art system by 32.3 % .	Furthermore , the proposed approach has an important advantage in that it is language independent and linguistic theory neutral.	CL_D14-1142_mn_7	CL_D14-1142_mn_6	CL_D14-1142_mn_6_7
CL	D14-1143	mn	1	2	motivation_background	motivation_hypothesis	info-required	support	secondary	secondary	forw	info-required	Predicting vocabulary of second language learners is essential to support their language learning ; however , because of the large size of language vocabularies , we cannot collect information on the entire vocabulary .	For practical measurements , we need to sample a small portion of words from the entire vocabulary and predict the rest of the words .	CL_D14-1143_mn_1	CL_D14-1143_mn_2	CL_D14-1143_mn_1_2
CL	D14-1143	mn	2	1	motivation_hypothesis	motivation_background	support	info-required	secondary	secondary	back	info-required	For practical measurements , we need to sample a small portion of words from the entire vocabulary and predict the rest of the words .	Predicting vocabulary of second language learners is essential to support their language learning ; however , because of the large size of language vocabularies , we cannot collect information on the entire vocabulary .	CL_D14-1143_mn_2	CL_D14-1143_mn_1	CL_D14-1143_mn_1_2
CL	D14-1143	mn	1	3	motivation_background	proposal	info-required	elaboration	secondary	secondary	none	none	Predicting vocabulary of second language learners is essential to support their language learning ; however , because of the large size of language vocabularies , we cannot collect information on the entire vocabulary .	In this study , we propose a novel framework for this sampling method .	CL_D14-1143_mn_1	CL_D14-1143_mn_3	CL_D14-1143_mn_1_3
CL	D14-1143	mn	1	4	motivation_background	motivation_problem	info-required	support	secondary	secondary	none	none	Predicting vocabulary of second language learners is essential to support their language learning ; however , because of the large size of language vocabularies , we cannot collect information on the entire vocabulary .	Current methods rely on simple heuristic techniques involving inflexible manual tuning by educational experts .	CL_D14-1143_mn_1	CL_D14-1143_mn_4	CL_D14-1143_mn_1_4
CL	D14-1143	mn	1	5	motivation_background	proposal	info-required	none	secondary	main	none	none	Predicting vocabulary of second language learners is essential to support their language learning ; however , because of the large size of language vocabularies , we cannot collect information on the entire vocabulary .	We formalize these heuristic techniques as a graph-based non-interactive active learning method as applied to a special graph .	CL_D14-1143_mn_1	CL_D14-1143_mn_5	CL_D14-1143_mn_1_5
CL	D14-1143	mn	1	6	motivation_background	result	info-required	support	secondary	secondary	none	none	Predicting vocabulary of second language learners is essential to support their language learning ; however , because of the large size of language vocabularies , we cannot collect information on the entire vocabulary .	We show that by extending the graph , we can support additional functionality such as incorporating domain specificity and sampling from multiple corpora .	CL_D14-1143_mn_1	CL_D14-1143_mn_6	CL_D14-1143_mn_1_6
CL	D14-1143	mn	1	7	motivation_background	result	info-required	elaboration	secondary	secondary	none	none	Predicting vocabulary of second language learners is essential to support their language learning ; however , because of the large size of language vocabularies , we cannot collect information on the entire vocabulary .	In our experiments , we show that our extended methods outperform other methods in terms of vocabulary prediction accuracy when the number of samples is small .	CL_D14-1143_mn_1	CL_D14-1143_mn_7	CL_D14-1143_mn_1_7
CL	D14-1143	mn	2	3	motivation_hypothesis	proposal	support	elaboration	secondary	secondary	forw	support	For practical measurements , we need to sample a small portion of words from the entire vocabulary and predict the rest of the words .	In this study , we propose a novel framework for this sampling method .	CL_D14-1143_mn_2	CL_D14-1143_mn_3	CL_D14-1143_mn_2_3
CL	D14-1143	mn	3	2	proposal	motivation_hypothesis	elaboration	support	secondary	secondary	back	support	In this study , we propose a novel framework for this sampling method .	For practical measurements , we need to sample a small portion of words from the entire vocabulary and predict the rest of the words .	CL_D14-1143_mn_3	CL_D14-1143_mn_2	CL_D14-1143_mn_2_3
CL	D14-1143	mn	2	4	motivation_hypothesis	motivation_problem	support	support	secondary	secondary	none	none	For practical measurements , we need to sample a small portion of words from the entire vocabulary and predict the rest of the words .	Current methods rely on simple heuristic techniques involving inflexible manual tuning by educational experts .	CL_D14-1143_mn_2	CL_D14-1143_mn_4	CL_D14-1143_mn_2_4
CL	D14-1143	mn	2	5	motivation_hypothesis	proposal	support	none	secondary	main	none	none	For practical measurements , we need to sample a small portion of words from the entire vocabulary and predict the rest of the words .	We formalize these heuristic techniques as a graph-based non-interactive active learning method as applied to a special graph .	CL_D14-1143_mn_2	CL_D14-1143_mn_5	CL_D14-1143_mn_2_5
CL	D14-1143	mn	2	6	motivation_hypothesis	result	support	support	secondary	secondary	none	none	For practical measurements , we need to sample a small portion of words from the entire vocabulary and predict the rest of the words .	We show that by extending the graph , we can support additional functionality such as incorporating domain specificity and sampling from multiple corpora .	CL_D14-1143_mn_2	CL_D14-1143_mn_6	CL_D14-1143_mn_2_6
CL	D14-1143	mn	2	7	motivation_hypothesis	result	support	elaboration	secondary	secondary	none	none	For practical measurements , we need to sample a small portion of words from the entire vocabulary and predict the rest of the words .	In our experiments , we show that our extended methods outperform other methods in terms of vocabulary prediction accuracy when the number of samples is small .	CL_D14-1143_mn_2	CL_D14-1143_mn_7	CL_D14-1143_mn_2_7
CL	D14-1143	mn	3	4	proposal	motivation_problem	elaboration	support	secondary	secondary	none	none	In this study , we propose a novel framework for this sampling method .	Current methods rely on simple heuristic techniques involving inflexible manual tuning by educational experts .	CL_D14-1143_mn_3	CL_D14-1143_mn_4	CL_D14-1143_mn_3_4
CL	D14-1143	mn	3	5	proposal	proposal	elaboration	none	secondary	main	forw	elaboration	In this study , we propose a novel framework for this sampling method .	We formalize these heuristic techniques as a graph-based non-interactive active learning method as applied to a special graph .	CL_D14-1143_mn_3	CL_D14-1143_mn_5	CL_D14-1143_mn_3_5
CL	D14-1143	mn	5	3	proposal	proposal	none	elaboration	main	secondary	back	elaboration	We formalize these heuristic techniques as a graph-based non-interactive active learning method as applied to a special graph .	In this study , we propose a novel framework for this sampling method .	CL_D14-1143_mn_5	CL_D14-1143_mn_3	CL_D14-1143_mn_3_5
CL	D14-1143	mn	3	6	proposal	result	elaboration	support	secondary	secondary	none	none	In this study , we propose a novel framework for this sampling method .	We show that by extending the graph , we can support additional functionality such as incorporating domain specificity and sampling from multiple corpora .	CL_D14-1143_mn_3	CL_D14-1143_mn_6	CL_D14-1143_mn_3_6
CL	D14-1143	mn	3	7	proposal	result	elaboration	elaboration	secondary	secondary	none	none	In this study , we propose a novel framework for this sampling method .	In our experiments , we show that our extended methods outperform other methods in terms of vocabulary prediction accuracy when the number of samples is small .	CL_D14-1143_mn_3	CL_D14-1143_mn_7	CL_D14-1143_mn_3_7
CL	D14-1143	mn	4	5	motivation_problem	proposal	support	none	secondary	main	forw	support	Current methods rely on simple heuristic techniques involving inflexible manual tuning by educational experts .	We formalize these heuristic techniques as a graph-based non-interactive active learning method as applied to a special graph .	CL_D14-1143_mn_4	CL_D14-1143_mn_5	CL_D14-1143_mn_4_5
CL	D14-1143	mn	5	4	proposal	motivation_problem	none	support	main	secondary	back	support	We formalize these heuristic techniques as a graph-based non-interactive active learning method as applied to a special graph .	Current methods rely on simple heuristic techniques involving inflexible manual tuning by educational experts .	CL_D14-1143_mn_5	CL_D14-1143_mn_4	CL_D14-1143_mn_4_5
CL	D14-1143	mn	4	6	motivation_problem	result	support	support	secondary	secondary	none	none	Current methods rely on simple heuristic techniques involving inflexible manual tuning by educational experts .	We show that by extending the graph , we can support additional functionality such as incorporating domain specificity and sampling from multiple corpora .	CL_D14-1143_mn_4	CL_D14-1143_mn_6	CL_D14-1143_mn_4_6
CL	D14-1143	mn	4	7	motivation_problem	result	support	elaboration	secondary	secondary	none	none	Current methods rely on simple heuristic techniques involving inflexible manual tuning by educational experts .	In our experiments , we show that our extended methods outperform other methods in terms of vocabulary prediction accuracy when the number of samples is small .	CL_D14-1143_mn_4	CL_D14-1143_mn_7	CL_D14-1143_mn_4_7
CL	D14-1143	mn	5	6	proposal	result	none	support	main	secondary	back	support	We formalize these heuristic techniques as a graph-based non-interactive active learning method as applied to a special graph .	We show that by extending the graph , we can support additional functionality such as incorporating domain specificity and sampling from multiple corpora .	CL_D14-1143_mn_5	CL_D14-1143_mn_6	CL_D14-1143_mn_5_6
CL	D14-1143	mn	6	5	result	proposal	support	none	secondary	main	forw	support	We show that by extending the graph , we can support additional functionality such as incorporating domain specificity and sampling from multiple corpora .	We formalize these heuristic techniques as a graph-based non-interactive active learning method as applied to a special graph .	CL_D14-1143_mn_6	CL_D14-1143_mn_5	CL_D14-1143_mn_5_6
CL	D14-1143	mn	5	7	proposal	result	none	elaboration	main	secondary	none	none	We formalize these heuristic techniques as a graph-based non-interactive active learning method as applied to a special graph .	In our experiments , we show that our extended methods outperform other methods in terms of vocabulary prediction accuracy when the number of samples is small .	CL_D14-1143_mn_5	CL_D14-1143_mn_7	CL_D14-1143_mn_5_7
CL	D14-1143	mn	6	7	result	result	support	elaboration	secondary	secondary	back	elaboration	We show that by extending the graph , we can support additional functionality such as incorporating domain specificity and sampling from multiple corpora .	In our experiments , we show that our extended methods outperform other methods in terms of vocabulary prediction accuracy when the number of samples is small .	CL_D14-1143_mn_6	CL_D14-1143_mn_7	CL_D14-1143_mn_6_7
CL	D14-1143	mn	7	6	result	result	elaboration	support	secondary	secondary	forw	elaboration	In our experiments , we show that our extended methods outperform other methods in terms of vocabulary prediction accuracy when the number of samples is small .	We show that by extending the graph , we can support additional functionality such as incorporating domain specificity and sampling from multiple corpora .	CL_D14-1143_mn_7	CL_D14-1143_mn_6	CL_D14-1143_mn_6_7
CL	D14-1144	mn	1	2	motivation_background	proposal	support	none	secondary	main	forw	support	Language transfer , the characteristic second language usage patterns caused by native language interference , is investigated by Second Language Acquisition ( SLA ) researchers seeking to find overused and underused linguistic features .	In this paper we develop and present a methodology for deriving ranked lists of such features .	CL_D14-1144_mn_1	CL_D14-1144_mn_2	CL_D14-1144_mn_1_2
CL	D14-1144	mn	2	1	proposal	motivation_background	none	support	main	secondary	back	support	In this paper we develop and present a methodology for deriving ranked lists of such features .	Language transfer , the characteristic second language usage patterns caused by native language interference , is investigated by Second Language Acquisition ( SLA ) researchers seeking to find overused and underused linguistic features .	CL_D14-1144_mn_2	CL_D14-1144_mn_1	CL_D14-1144_mn_1_2
CL	D14-1144	mn	1	3	motivation_background	proposal_implementation	support	elaboration	secondary	secondary	none	none	Language transfer , the characteristic second language usage patterns caused by native language interference , is investigated by Second Language Acquisition ( SLA ) researchers seeking to find overused and underused linguistic features .	Using very large learner data , we show our method's ability to find relevant candidates using sophisticated linguistic features .	CL_D14-1144_mn_1	CL_D14-1144_mn_3	CL_D14-1144_mn_1_3
CL	D14-1144	mn	1	4	motivation_background	proposal_implementation	support	elaboration	secondary	secondary	none	none	Language transfer , the characteristic second language usage patterns caused by native language interference , is investigated by Second Language Acquisition ( SLA ) researchers seeking to find overused and underused linguistic features .	To illustrate its applicability to SLA research , we formulate plausible language transfer hypotheses supported by current evidence .	CL_D14-1144_mn_1	CL_D14-1144_mn_4	CL_D14-1144_mn_1_4
CL	D14-1144	mn	1	5	motivation_background	conclusion	support	support	secondary	secondary	none	none	Language transfer , the characteristic second language usage patterns caused by native language interference , is investigated by Second Language Acquisition ( SLA ) researchers seeking to find overused and underused linguistic features .	This is the first work to extend Native Language Identification to a broader linguistic interpretation of learner data and address the automatic extraction of underused features on a pernative language basis .	CL_D14-1144_mn_1	CL_D14-1144_mn_5	CL_D14-1144_mn_1_5
CL	D14-1144	mn	2	3	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	In this paper we develop and present a methodology for deriving ranked lists of such features .	Using very large learner data , we show our method's ability to find relevant candidates using sophisticated linguistic features .	CL_D14-1144_mn_2	CL_D14-1144_mn_3	CL_D14-1144_mn_2_3
CL	D14-1144	mn	3	2	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	Using very large learner data , we show our method's ability to find relevant candidates using sophisticated linguistic features .	In this paper we develop and present a methodology for deriving ranked lists of such features .	CL_D14-1144_mn_3	CL_D14-1144_mn_2	CL_D14-1144_mn_2_3
CL	D14-1144	mn	2	4	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	In this paper we develop and present a methodology for deriving ranked lists of such features .	To illustrate its applicability to SLA research , we formulate plausible language transfer hypotheses supported by current evidence .	CL_D14-1144_mn_2	CL_D14-1144_mn_4	CL_D14-1144_mn_2_4
CL	D14-1144	mn	2	5	proposal	conclusion	none	support	main	secondary	back	support	In this paper we develop and present a methodology for deriving ranked lists of such features .	This is the first work to extend Native Language Identification to a broader linguistic interpretation of learner data and address the automatic extraction of underused features on a pernative language basis .	CL_D14-1144_mn_2	CL_D14-1144_mn_5	CL_D14-1144_mn_2_5
CL	D14-1144	mn	5	2	conclusion	proposal	support	none	secondary	main	forw	support	This is the first work to extend Native Language Identification to a broader linguistic interpretation of learner data and address the automatic extraction of underused features on a pernative language basis .	In this paper we develop and present a methodology for deriving ranked lists of such features .	CL_D14-1144_mn_5	CL_D14-1144_mn_2	CL_D14-1144_mn_2_5
CL	D14-1144	mn	3	4	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	Using very large learner data , we show our method's ability to find relevant candidates using sophisticated linguistic features .	To illustrate its applicability to SLA research , we formulate plausible language transfer hypotheses supported by current evidence .	CL_D14-1144_mn_3	CL_D14-1144_mn_4	CL_D14-1144_mn_3_4
CL	D14-1144	mn	4	3	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	forw	elaboration	To illustrate its applicability to SLA research , we formulate plausible language transfer hypotheses supported by current evidence .	Using very large learner data , we show our method's ability to find relevant candidates using sophisticated linguistic features .	CL_D14-1144_mn_4	CL_D14-1144_mn_3	CL_D14-1144_mn_3_4
CL	D14-1144	mn	3	5	proposal_implementation	conclusion	elaboration	support	secondary	secondary	none	none	Using very large learner data , we show our method's ability to find relevant candidates using sophisticated linguistic features .	This is the first work to extend Native Language Identification to a broader linguistic interpretation of learner data and address the automatic extraction of underused features on a pernative language basis .	CL_D14-1144_mn_3	CL_D14-1144_mn_5	CL_D14-1144_mn_3_5
CL	D14-1144	mn	4	5	proposal_implementation	conclusion	elaboration	support	secondary	secondary	none	none	To illustrate its applicability to SLA research , we formulate plausible language transfer hypotheses supported by current evidence .	This is the first work to extend Native Language Identification to a broader linguistic interpretation of learner data and address the automatic extraction of underused features on a pernative language basis .	CL_D14-1144_mn_4	CL_D14-1144_mn_5	CL_D14-1144_mn_4_5
CL	D14-1145	mn	1	2	motivation_background	motivation_problem	info-required	support	secondary	secondary	forw	info-required	Languages spoken by immigrants change due to contact with the local languages .	Capturing these changes is problematic for current language technologies , which are typically developed for speakers of the standard dialect only .	CL_D14-1145_mn_1	CL_D14-1145_mn_2	CL_D14-1145_mn_1_2
CL	D14-1145	mn	2	1	motivation_problem	motivation_background	support	info-required	secondary	secondary	back	info-required	Capturing these changes is problematic for current language technologies , which are typically developed for speakers of the standard dialect only .	Languages spoken by immigrants change due to contact with the local languages .	CL_D14-1145_mn_2	CL_D14-1145_mn_1	CL_D14-1145_mn_1_2
CL	D14-1145	mn	1	3	motivation_background	motivation_problem	info-required	support	secondary	secondary	none	none	Languages spoken by immigrants change due to contact with the local languages .	Even when dialectal variants are available for such technologies , we still need to predict which dialect is being used .	CL_D14-1145_mn_1	CL_D14-1145_mn_3	CL_D14-1145_mn_1_3
CL	D14-1145	mn	1	4	motivation_background	proposal	info-required	none	secondary	main	none	none	Languages spoken by immigrants change due to contact with the local languages .	In this study , we distinguish between the immigrant and the standard dialect of Turkish by focusing on Light Verb Constructions .	CL_D14-1145_mn_1	CL_D14-1145_mn_4	CL_D14-1145_mn_1_4
CL	D14-1145	mn	1	5	motivation_background	observation	info-required	support	secondary	secondary	none	none	Languages spoken by immigrants change due to contact with the local languages .	We experiment with a number of grammatical and contextual features , achieving over 84 % accuracy ( 56 % baseline ) .	CL_D14-1145_mn_1	CL_D14-1145_mn_5	CL_D14-1145_mn_1_5
CL	D14-1145	mn	2	3	motivation_problem	motivation_problem	support	support	secondary	secondary	forw	support	Capturing these changes is problematic for current language technologies , which are typically developed for speakers of the standard dialect only .	Even when dialectal variants are available for such technologies , we still need to predict which dialect is being used .	CL_D14-1145_mn_2	CL_D14-1145_mn_3	CL_D14-1145_mn_2_3
CL	D14-1145	mn	3	2	motivation_problem	motivation_problem	support	support	secondary	secondary	back	support	Even when dialectal variants are available for such technologies , we still need to predict which dialect is being used .	Capturing these changes is problematic for current language technologies , which are typically developed for speakers of the standard dialect only .	CL_D14-1145_mn_3	CL_D14-1145_mn_2	CL_D14-1145_mn_2_3
CL	D14-1145	mn	2	4	motivation_problem	proposal	support	none	secondary	main	none	none	Capturing these changes is problematic for current language technologies , which are typically developed for speakers of the standard dialect only .	In this study , we distinguish between the immigrant and the standard dialect of Turkish by focusing on Light Verb Constructions .	CL_D14-1145_mn_2	CL_D14-1145_mn_4	CL_D14-1145_mn_2_4
CL	D14-1145	mn	2	5	motivation_problem	observation	support	support	secondary	secondary	none	none	Capturing these changes is problematic for current language technologies , which are typically developed for speakers of the standard dialect only .	We experiment with a number of grammatical and contextual features , achieving over 84 % accuracy ( 56 % baseline ) .	CL_D14-1145_mn_2	CL_D14-1145_mn_5	CL_D14-1145_mn_2_5
CL	D14-1145	mn	3	4	motivation_problem	proposal	support	none	secondary	main	forw	support	Even when dialectal variants are available for such technologies , we still need to predict which dialect is being used .	In this study , we distinguish between the immigrant and the standard dialect of Turkish by focusing on Light Verb Constructions .	CL_D14-1145_mn_3	CL_D14-1145_mn_4	CL_D14-1145_mn_3_4
CL	D14-1145	mn	4	3	proposal	motivation_problem	none	support	main	secondary	back	support	In this study , we distinguish between the immigrant and the standard dialect of Turkish by focusing on Light Verb Constructions .	Even when dialectal variants are available for such technologies , we still need to predict which dialect is being used .	CL_D14-1145_mn_4	CL_D14-1145_mn_3	CL_D14-1145_mn_3_4
CL	D14-1145	mn	3	5	motivation_problem	observation	support	support	secondary	secondary	none	none	Even when dialectal variants are available for such technologies , we still need to predict which dialect is being used .	We experiment with a number of grammatical and contextual features , achieving over 84 % accuracy ( 56 % baseline ) .	CL_D14-1145_mn_3	CL_D14-1145_mn_5	CL_D14-1145_mn_3_5
CL	D14-1145	mn	4	5	proposal	observation	none	support	main	secondary	back	support	In this study , we distinguish between the immigrant and the standard dialect of Turkish by focusing on Light Verb Constructions .	We experiment with a number of grammatical and contextual features , achieving over 84 % accuracy ( 56 % baseline ) .	CL_D14-1145_mn_4	CL_D14-1145_mn_5	CL_D14-1145_mn_4_5
CL	D14-1145	mn	5	4	observation	proposal	support	none	secondary	main	forw	support	We experiment with a number of grammatical and contextual features , achieving over 84 % accuracy ( 56 % baseline ) .	In this study , we distinguish between the immigrant and the standard dialect of Turkish by focusing on Light Verb Constructions .	CL_D14-1145_mn_5	CL_D14-1145_mn_4	CL_D14-1145_mn_4_5
CL	D14-1146	mn	1	2	motivation_background	motivation_background	info-required	support	secondary	secondary	forw	info-required	Readability is used to provide users with high-quality service in text recommendation or text visualization .	With the increasing use of hand-held devices , reading device is regarded as an important factor for readability .	CL_D14-1146_mn_1	CL_D14-1146_mn_2	CL_D14-1146_mn_1_2
CL	D14-1146	mn	2	1	motivation_background	motivation_background	support	info-required	secondary	secondary	back	info-required	With the increasing use of hand-held devices , reading device is regarded as an important factor for readability .	Readability is used to provide users with high-quality service in text recommendation or text visualization .	CL_D14-1146_mn_2	CL_D14-1146_mn_1	CL_D14-1146_mn_1_2
CL	D14-1146	mn	1	3	motivation_background	proposal	info-required	none	secondary	main	none	none	Readability is used to provide users with high-quality service in text recommendation or text visualization .	Therefore , this paper investigates the relationship between readability and reading devices such as a smart phone , a tablet , and paper .	CL_D14-1146_mn_1	CL_D14-1146_mn_3	CL_D14-1146_mn_1_3
CL	D14-1146	mn	1	4	motivation_background	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	Readability is used to provide users with high-quality service in text recommendation or text visualization .	We suggest readability factors that are strongly related with the readability of a specific device by showing the correlations between various factors in each device and human-rated readability .	CL_D14-1146_mn_1	CL_D14-1146_mn_4	CL_D14-1146_mn_1_4
CL	D14-1146	mn	1	5	motivation_background	result	info-required	support	secondary	secondary	none	none	Readability is used to provide users with high-quality service in text recommendation or text visualization .	Our experimental results show that each device has its own readability characteristics , and thus different weights should be imposed on readability factors according to the device type .	CL_D14-1146_mn_1	CL_D14-1146_mn_5	CL_D14-1146_mn_1_5
CL	D14-1146	mn	1	6	motivation_background	means	info-required	elaboration	secondary	secondary	none	none	Readability is used to provide users with high-quality service in text recommendation or text visualization .	In order to prove the usefulness of the results , we apply the device-dependent readability to news article recommendation .	CL_D14-1146_mn_1	CL_D14-1146_mn_6	CL_D14-1146_mn_1_6
CL	D14-1146	mn	2	3	motivation_background	proposal	support	none	secondary	main	forw	support	With the increasing use of hand-held devices , reading device is regarded as an important factor for readability .	Therefore , this paper investigates the relationship between readability and reading devices such as a smart phone , a tablet , and paper .	CL_D14-1146_mn_2	CL_D14-1146_mn_3	CL_D14-1146_mn_2_3
CL	D14-1146	mn	3	2	proposal	motivation_background	none	support	main	secondary	back	support	Therefore , this paper investigates the relationship between readability and reading devices such as a smart phone , a tablet , and paper .	With the increasing use of hand-held devices , reading device is regarded as an important factor for readability .	CL_D14-1146_mn_3	CL_D14-1146_mn_2	CL_D14-1146_mn_2_3
CL	D14-1146	mn	2	4	motivation_background	proposal_implementation	support	elaboration	secondary	secondary	none	none	With the increasing use of hand-held devices , reading device is regarded as an important factor for readability .	We suggest readability factors that are strongly related with the readability of a specific device by showing the correlations between various factors in each device and human-rated readability .	CL_D14-1146_mn_2	CL_D14-1146_mn_4	CL_D14-1146_mn_2_4
CL	D14-1146	mn	2	5	motivation_background	result	support	support	secondary	secondary	none	none	With the increasing use of hand-held devices , reading device is regarded as an important factor for readability .	Our experimental results show that each device has its own readability characteristics , and thus different weights should be imposed on readability factors according to the device type .	CL_D14-1146_mn_2	CL_D14-1146_mn_5	CL_D14-1146_mn_2_5
CL	D14-1146	mn	2	6	motivation_background	means	support	elaboration	secondary	secondary	none	none	With the increasing use of hand-held devices , reading device is regarded as an important factor for readability .	In order to prove the usefulness of the results , we apply the device-dependent readability to news article recommendation .	CL_D14-1146_mn_2	CL_D14-1146_mn_6	CL_D14-1146_mn_2_6
CL	D14-1146	mn	3	4	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	Therefore , this paper investigates the relationship between readability and reading devices such as a smart phone , a tablet , and paper .	We suggest readability factors that are strongly related with the readability of a specific device by showing the correlations between various factors in each device and human-rated readability .	CL_D14-1146_mn_3	CL_D14-1146_mn_4	CL_D14-1146_mn_3_4
CL	D14-1146	mn	4	3	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	We suggest readability factors that are strongly related with the readability of a specific device by showing the correlations between various factors in each device and human-rated readability .	Therefore , this paper investigates the relationship between readability and reading devices such as a smart phone , a tablet , and paper .	CL_D14-1146_mn_4	CL_D14-1146_mn_3	CL_D14-1146_mn_3_4
CL	D14-1146	mn	3	5	proposal	result	none	support	main	secondary	back	support	Therefore , this paper investigates the relationship between readability and reading devices such as a smart phone , a tablet , and paper .	Our experimental results show that each device has its own readability characteristics , and thus different weights should be imposed on readability factors according to the device type .	CL_D14-1146_mn_3	CL_D14-1146_mn_5	CL_D14-1146_mn_3_5
CL	D14-1146	mn	5	3	result	proposal	support	none	secondary	main	forw	support	Our experimental results show that each device has its own readability characteristics , and thus different weights should be imposed on readability factors according to the device type .	Therefore , this paper investigates the relationship between readability and reading devices such as a smart phone , a tablet , and paper .	CL_D14-1146_mn_5	CL_D14-1146_mn_3	CL_D14-1146_mn_3_5
CL	D14-1146	mn	3	6	proposal	means	none	elaboration	main	secondary	none	none	Therefore , this paper investigates the relationship between readability and reading devices such as a smart phone , a tablet , and paper .	In order to prove the usefulness of the results , we apply the device-dependent readability to news article recommendation .	CL_D14-1146_mn_3	CL_D14-1146_mn_6	CL_D14-1146_mn_3_6
CL	D14-1146	mn	4	5	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	We suggest readability factors that are strongly related with the readability of a specific device by showing the correlations between various factors in each device and human-rated readability .	Our experimental results show that each device has its own readability characteristics , and thus different weights should be imposed on readability factors according to the device type .	CL_D14-1146_mn_4	CL_D14-1146_mn_5	CL_D14-1146_mn_4_5
CL	D14-1146	mn	4	6	proposal_implementation	means	elaboration	elaboration	secondary	secondary	none	none	We suggest readability factors that are strongly related with the readability of a specific device by showing the correlations between various factors in each device and human-rated readability .	In order to prove the usefulness of the results , we apply the device-dependent readability to news article recommendation .	CL_D14-1146_mn_4	CL_D14-1146_mn_6	CL_D14-1146_mn_4_6
CL	D14-1146	mn	5	6	result	means	support	elaboration	secondary	secondary	back	elaboration	Our experimental results show that each device has its own readability characteristics , and thus different weights should be imposed on readability factors according to the device type .	In order to prove the usefulness of the results , we apply the device-dependent readability to news article recommendation .	CL_D14-1146_mn_5	CL_D14-1146_mn_6	CL_D14-1146_mn_5_6
CL	D14-1146	mn	6	5	means	result	elaboration	support	secondary	secondary	forw	elaboration	In order to prove the usefulness of the results , we apply the device-dependent readability to news article recommendation .	Our experimental results show that each device has its own readability characteristics , and thus different weights should be imposed on readability factors according to the device type .	CL_D14-1146_mn_6	CL_D14-1146_mn_5	CL_D14-1146_mn_5_6
CL	D14-1147	mn	1	2	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	We propose a new Chinese abbreviation prediction method which can incorporate rich local information while generating the abbreviation globally .	Different to previous character tagging methods , we introduce the minimum semantic unit , which is more fine-grained than character but more coarse-grained than word , to capture word level information in the sequence labeling framework .	CL_D14-1147_mn_1	CL_D14-1147_mn_2	CL_D14-1147_mn_1_2
CL	D14-1147	mn	2	1	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	Different to previous character tagging methods , we introduce the minimum semantic unit , which is more fine-grained than character but more coarse-grained than word , to capture word level information in the sequence labeling framework .	We propose a new Chinese abbreviation prediction method which can incorporate rich local information while generating the abbreviation globally .	CL_D14-1147_mn_2	CL_D14-1147_mn_1	CL_D14-1147_mn_1_2
CL	D14-1147	mn	1	3	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	We propose a new Chinese abbreviation prediction method which can incorporate rich local information while generating the abbreviation globally .	To solve the "character duplication" problem in Chinese abbreviation prediction , we also use a substring tagging strategy to generate local substring tagging candidates .	CL_D14-1147_mn_1	CL_D14-1147_mn_3	CL_D14-1147_mn_1_3
CL	D14-1147	mn	1	4	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	We propose a new Chinese abbreviation prediction method which can incorporate rich local information while generating the abbreviation globally .	We use an integer linear programming ( ILP ) formulation with various constraints to globally decode the final abbreviation from the generated candidates .	CL_D14-1147_mn_1	CL_D14-1147_mn_4	CL_D14-1147_mn_1_4
CL	D14-1147	mn	1	5	proposal	conclusion	none	support	main	secondary	back	support	We propose a new Chinese abbreviation prediction method which can incorporate rich local information while generating the abbreviation globally .	Experiments show that our method outperforms the state-of-the-art systems , without using any extra resource .	CL_D14-1147_mn_1	CL_D14-1147_mn_5	CL_D14-1147_mn_1_5
CL	D14-1147	mn	5	1	conclusion	proposal	support	none	secondary	main	forw	support	Experiments show that our method outperforms the state-of-the-art systems , without using any extra resource .	We propose a new Chinese abbreviation prediction method which can incorporate rich local information while generating the abbreviation globally .	CL_D14-1147_mn_5	CL_D14-1147_mn_1	CL_D14-1147_mn_1_5
CL	D14-1147	mn	2	3	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	Different to previous character tagging methods , we introduce the minimum semantic unit , which is more fine-grained than character but more coarse-grained than word , to capture word level information in the sequence labeling framework .	To solve the "character duplication" problem in Chinese abbreviation prediction , we also use a substring tagging strategy to generate local substring tagging candidates .	CL_D14-1147_mn_2	CL_D14-1147_mn_3	CL_D14-1147_mn_2_3
CL	D14-1147	mn	3	2	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	forw	elaboration	To solve the "character duplication" problem in Chinese abbreviation prediction , we also use a substring tagging strategy to generate local substring tagging candidates .	Different to previous character tagging methods , we introduce the minimum semantic unit , which is more fine-grained than character but more coarse-grained than word , to capture word level information in the sequence labeling framework .	CL_D14-1147_mn_3	CL_D14-1147_mn_2	CL_D14-1147_mn_2_3
CL	D14-1147	mn	2	4	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	none	none	Different to previous character tagging methods , we introduce the minimum semantic unit , which is more fine-grained than character but more coarse-grained than word , to capture word level information in the sequence labeling framework .	We use an integer linear programming ( ILP ) formulation with various constraints to globally decode the final abbreviation from the generated candidates .	CL_D14-1147_mn_2	CL_D14-1147_mn_4	CL_D14-1147_mn_2_4
CL	D14-1147	mn	2	5	proposal_implementation	conclusion	elaboration	support	secondary	secondary	none	none	Different to previous character tagging methods , we introduce the minimum semantic unit , which is more fine-grained than character but more coarse-grained than word , to capture word level information in the sequence labeling framework .	Experiments show that our method outperforms the state-of-the-art systems , without using any extra resource .	CL_D14-1147_mn_2	CL_D14-1147_mn_5	CL_D14-1147_mn_2_5
CL	D14-1147	mn	3	4	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	To solve the "character duplication" problem in Chinese abbreviation prediction , we also use a substring tagging strategy to generate local substring tagging candidates .	We use an integer linear programming ( ILP ) formulation with various constraints to globally decode the final abbreviation from the generated candidates .	CL_D14-1147_mn_3	CL_D14-1147_mn_4	CL_D14-1147_mn_3_4
CL	D14-1147	mn	4	3	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	forw	elaboration	We use an integer linear programming ( ILP ) formulation with various constraints to globally decode the final abbreviation from the generated candidates .	To solve the "character duplication" problem in Chinese abbreviation prediction , we also use a substring tagging strategy to generate local substring tagging candidates .	CL_D14-1147_mn_4	CL_D14-1147_mn_3	CL_D14-1147_mn_3_4
CL	D14-1147	mn	3	5	proposal_implementation	conclusion	elaboration	support	secondary	secondary	none	none	To solve the "character duplication" problem in Chinese abbreviation prediction , we also use a substring tagging strategy to generate local substring tagging candidates .	Experiments show that our method outperforms the state-of-the-art systems , without using any extra resource .	CL_D14-1147_mn_3	CL_D14-1147_mn_5	CL_D14-1147_mn_3_5
CL	D14-1147	mn	4	5	proposal_implementation	conclusion	elaboration	support	secondary	secondary	none	none	We use an integer linear programming ( ILP ) formulation with various constraints to globally decode the final abbreviation from the generated candidates .	Experiments show that our method outperforms the state-of-the-art systems , without using any extra resource .	CL_D14-1147_mn_4	CL_D14-1147_mn_5	CL_D14-1147_mn_4_5
CL	D14-1148	mn	1	2	motivation_background	motivation_problem	info-required	support	secondary	secondary	forw	info-required	It has been shown that news events influence the trends of stock price movements .	However , previous work on news-driven stock market prediction rely on shallow features ( such as bags-of-words , named entities and noun phrases ) , which do not capture structured entity-relation information , and hence cannot represent complete and exact events .	CL_D14-1148_mn_1	CL_D14-1148_mn_2	CL_D14-1148_mn_1_2
CL	D14-1148	mn	2	1	motivation_problem	motivation_background	support	info-required	secondary	secondary	back	info-required	However , previous work on news-driven stock market prediction rely on shallow features ( such as bags-of-words , named entities and noun phrases ) , which do not capture structured entity-relation information , and hence cannot represent complete and exact events .	It has been shown that news events influence the trends of stock price movements .	CL_D14-1148_mn_2	CL_D14-1148_mn_1	CL_D14-1148_mn_1_2
CL	D14-1148	mn	1	3	motivation_background	motivation_background	info-required	support	secondary	secondary	none	none	It has been shown that news events influence the trends of stock price movements .	Recent advances in Open Information Extraction ( Open IE ) techniques enable the extraction of structured events from web-scale data .	CL_D14-1148_mn_1	CL_D14-1148_mn_3	CL_D14-1148_mn_1_3
CL	D14-1148	mn	1	4	motivation_background	proposal	info-required	none	secondary	main	none	none	It has been shown that news events influence the trends of stock price movements .	We propose to adapt Open IE technology for event-based stock price movement prediction , extracting structured events from large-scale public news without manual efforts .	CL_D14-1148_mn_1	CL_D14-1148_mn_4	CL_D14-1148_mn_1_4
CL	D14-1148	mn	1	5	motivation_background	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	It has been shown that news events influence the trends of stock price movements .	Both linear and nonlinear models are employed to empirically investigate the hidden and complex relationships between events and the stock market .	CL_D14-1148_mn_1	CL_D14-1148_mn_5	CL_D14-1148_mn_1_5
CL	D14-1148	mn	1	6	motivation_background	observation	info-required	support	secondary	secondary	none	none	It has been shown that news events influence the trends of stock price movements .	Largescale experiments show that the accuracy of S&P 500 index prediction is 60 % , and that of individual stock prediction can be over 70 % .	CL_D14-1148_mn_1	CL_D14-1148_mn_6	CL_D14-1148_mn_1_6
CL	D14-1148	mn	1	7	motivation_background	result_means	info-required	support	secondary	secondary	none	none	It has been shown that news events influence the trends of stock price movements .	Our event-based system outperforms bags-of-words-based baselines , and previously reported systems trained on S&P 500 stock historical data .	CL_D14-1148_mn_1	CL_D14-1148_mn_7	CL_D14-1148_mn_1_7
CL	D14-1148	mn	2	3	motivation_problem	motivation_background	support	support	secondary	secondary	forw	support	However , previous work on news-driven stock market prediction rely on shallow features ( such as bags-of-words , named entities and noun phrases ) , which do not capture structured entity-relation information , and hence cannot represent complete and exact events .	Recent advances in Open Information Extraction ( Open IE ) techniques enable the extraction of structured events from web-scale data .	CL_D14-1148_mn_2	CL_D14-1148_mn_3	CL_D14-1148_mn_2_3
CL	D14-1148	mn	3	2	motivation_background	motivation_problem	support	support	secondary	secondary	back	support	Recent advances in Open Information Extraction ( Open IE ) techniques enable the extraction of structured events from web-scale data .	However , previous work on news-driven stock market prediction rely on shallow features ( such as bags-of-words , named entities and noun phrases ) , which do not capture structured entity-relation information , and hence cannot represent complete and exact events .	CL_D14-1148_mn_3	CL_D14-1148_mn_2	CL_D14-1148_mn_2_3
CL	D14-1148	mn	2	4	motivation_problem	proposal	support	none	secondary	main	none	none	However , previous work on news-driven stock market prediction rely on shallow features ( such as bags-of-words , named entities and noun phrases ) , which do not capture structured entity-relation information , and hence cannot represent complete and exact events .	We propose to adapt Open IE technology for event-based stock price movement prediction , extracting structured events from large-scale public news without manual efforts .	CL_D14-1148_mn_2	CL_D14-1148_mn_4	CL_D14-1148_mn_2_4
CL	D14-1148	mn	2	5	motivation_problem	proposal_implementation	support	elaboration	secondary	secondary	none	none	However , previous work on news-driven stock market prediction rely on shallow features ( such as bags-of-words , named entities and noun phrases ) , which do not capture structured entity-relation information , and hence cannot represent complete and exact events .	Both linear and nonlinear models are employed to empirically investigate the hidden and complex relationships between events and the stock market .	CL_D14-1148_mn_2	CL_D14-1148_mn_5	CL_D14-1148_mn_2_5
CL	D14-1148	mn	2	6	motivation_problem	observation	support	support	secondary	secondary	none	none	However , previous work on news-driven stock market prediction rely on shallow features ( such as bags-of-words , named entities and noun phrases ) , which do not capture structured entity-relation information , and hence cannot represent complete and exact events .	Largescale experiments show that the accuracy of S&P 500 index prediction is 60 % , and that of individual stock prediction can be over 70 % .	CL_D14-1148_mn_2	CL_D14-1148_mn_6	CL_D14-1148_mn_2_6
CL	D14-1148	mn	2	7	motivation_problem	result_means	support	support	secondary	secondary	none	none	However , previous work on news-driven stock market prediction rely on shallow features ( such as bags-of-words , named entities and noun phrases ) , which do not capture structured entity-relation information , and hence cannot represent complete and exact events .	Our event-based system outperforms bags-of-words-based baselines , and previously reported systems trained on S&P 500 stock historical data .	CL_D14-1148_mn_2	CL_D14-1148_mn_7	CL_D14-1148_mn_2_7
CL	D14-1148	mn	3	4	motivation_background	proposal	support	none	secondary	main	forw	support	Recent advances in Open Information Extraction ( Open IE ) techniques enable the extraction of structured events from web-scale data .	We propose to adapt Open IE technology for event-based stock price movement prediction , extracting structured events from large-scale public news without manual efforts .	CL_D14-1148_mn_3	CL_D14-1148_mn_4	CL_D14-1148_mn_3_4
CL	D14-1148	mn	4	3	proposal	motivation_background	none	support	main	secondary	back	support	We propose to adapt Open IE technology for event-based stock price movement prediction , extracting structured events from large-scale public news without manual efforts .	Recent advances in Open Information Extraction ( Open IE ) techniques enable the extraction of structured events from web-scale data .	CL_D14-1148_mn_4	CL_D14-1148_mn_3	CL_D14-1148_mn_3_4
CL	D14-1148	mn	3	5	motivation_background	proposal_implementation	support	elaboration	secondary	secondary	none	none	Recent advances in Open Information Extraction ( Open IE ) techniques enable the extraction of structured events from web-scale data .	Both linear and nonlinear models are employed to empirically investigate the hidden and complex relationships between events and the stock market .	CL_D14-1148_mn_3	CL_D14-1148_mn_5	CL_D14-1148_mn_3_5
CL	D14-1148	mn	3	6	motivation_background	observation	support	support	secondary	secondary	none	none	Recent advances in Open Information Extraction ( Open IE ) techniques enable the extraction of structured events from web-scale data .	Largescale experiments show that the accuracy of S&P 500 index prediction is 60 % , and that of individual stock prediction can be over 70 % .	CL_D14-1148_mn_3	CL_D14-1148_mn_6	CL_D14-1148_mn_3_6
CL	D14-1148	mn	3	7	motivation_background	result_means	support	support	secondary	secondary	none	none	Recent advances in Open Information Extraction ( Open IE ) techniques enable the extraction of structured events from web-scale data .	Our event-based system outperforms bags-of-words-based baselines , and previously reported systems trained on S&P 500 stock historical data .	CL_D14-1148_mn_3	CL_D14-1148_mn_7	CL_D14-1148_mn_3_7
CL	D14-1148	mn	4	5	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	We propose to adapt Open IE technology for event-based stock price movement prediction , extracting structured events from large-scale public news without manual efforts .	Both linear and nonlinear models are employed to empirically investigate the hidden and complex relationships between events and the stock market .	CL_D14-1148_mn_4	CL_D14-1148_mn_5	CL_D14-1148_mn_4_5
CL	D14-1148	mn	5	4	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	Both linear and nonlinear models are employed to empirically investigate the hidden and complex relationships between events and the stock market .	We propose to adapt Open IE technology for event-based stock price movement prediction , extracting structured events from large-scale public news without manual efforts .	CL_D14-1148_mn_5	CL_D14-1148_mn_4	CL_D14-1148_mn_4_5
CL	D14-1148	mn	4	6	proposal	observation	none	support	main	secondary	none	none	We propose to adapt Open IE technology for event-based stock price movement prediction , extracting structured events from large-scale public news without manual efforts .	Largescale experiments show that the accuracy of S&P 500 index prediction is 60 % , and that of individual stock prediction can be over 70 % .	CL_D14-1148_mn_4	CL_D14-1148_mn_6	CL_D14-1148_mn_4_6
CL	D14-1148	mn	4	7	proposal	result_means	none	support	main	secondary	back	support	We propose to adapt Open IE technology for event-based stock price movement prediction , extracting structured events from large-scale public news without manual efforts .	Our event-based system outperforms bags-of-words-based baselines , and previously reported systems trained on S&P 500 stock historical data .	CL_D14-1148_mn_4	CL_D14-1148_mn_7	CL_D14-1148_mn_4_7
CL	D14-1148	mn	7	4	result_means	proposal	support	none	secondary	main	forw	support	Our event-based system outperforms bags-of-words-based baselines , and previously reported systems trained on S&P 500 stock historical data .	We propose to adapt Open IE technology for event-based stock price movement prediction , extracting structured events from large-scale public news without manual efforts .	CL_D14-1148_mn_7	CL_D14-1148_mn_4	CL_D14-1148_mn_4_7
CL	D14-1148	mn	5	6	proposal_implementation	observation	elaboration	support	secondary	secondary	none	none	Both linear and nonlinear models are employed to empirically investigate the hidden and complex relationships between events and the stock market .	Largescale experiments show that the accuracy of S&P 500 index prediction is 60 % , and that of individual stock prediction can be over 70 % .	CL_D14-1148_mn_5	CL_D14-1148_mn_6	CL_D14-1148_mn_5_6
CL	D14-1148	mn	5	7	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	Both linear and nonlinear models are employed to empirically investigate the hidden and complex relationships between events and the stock market .	Our event-based system outperforms bags-of-words-based baselines , and previously reported systems trained on S&P 500 stock historical data .	CL_D14-1148_mn_5	CL_D14-1148_mn_7	CL_D14-1148_mn_5_7
CL	D14-1148	mn	6	7	observation	result_means	support	support	secondary	secondary	forw	support	Largescale experiments show that the accuracy of S&P 500 index prediction is 60 % , and that of individual stock prediction can be over 70 % .	Our event-based system outperforms bags-of-words-based baselines , and previously reported systems trained on S&P 500 stock historical data .	CL_D14-1148_mn_6	CL_D14-1148_mn_7	CL_D14-1148_mn_6_7
CL	D14-1148	mn	7	6	result_means	observation	support	support	secondary	secondary	back	support	Our event-based system outperforms bags-of-words-based baselines , and previously reported systems trained on S&P 500 stock historical data .	Largescale experiments show that the accuracy of S&P 500 index prediction is 60 % , and that of individual stock prediction can be over 70 % .	CL_D14-1148_mn_7	CL_D14-1148_mn_6	CL_D14-1148_mn_6_7
CL	D14-1149	mn	1	2	motivation_problem	proposal	support	none	secondary	main	forw	support	Automatically identifying related specialist terms is a difficult and important task required to understand the lexical structure of language .	This paper develops a corpus-based method of extracting coherent clusters of satellite terminology terms on the edge of the lexicon  using co-occurrence networks of unstructured text .	CL_D14-1149_mn_1	CL_D14-1149_mn_2	CL_D14-1149_mn_1_2
CL	D14-1149	mn	2	1	proposal	motivation_problem	none	support	main	secondary	back	support	This paper develops a corpus-based method of extracting coherent clusters of satellite terminology terms on the edge of the lexicon  using co-occurrence networks of unstructured text .	Automatically identifying related specialist terms is a difficult and important task required to understand the lexical structure of language .	CL_D14-1149_mn_2	CL_D14-1149_mn_1	CL_D14-1149_mn_1_2
CL	D14-1149	mn	1	3	motivation_problem	proposal_implementation	support	elaboration	secondary	secondary	none	none	Automatically identifying related specialist terms is a difficult and important task required to understand the lexical structure of language .	Term clusters are identified by extracting communities in the co-occurrence graph , after which the largest is discarded and the remaining words are ranked by centrality within a community .	CL_D14-1149_mn_1	CL_D14-1149_mn_3	CL_D14-1149_mn_1_3
CL	D14-1149	mn	1	4	motivation_problem	information_additional	support	info-optional	secondary	secondary	none	none	Automatically identifying related specialist terms is a difficult and important task required to understand the lexical structure of language .	The method is tractable on large corpora , requires no document structure and minimal normalization .	CL_D14-1149_mn_1	CL_D14-1149_mn_4	CL_D14-1149_mn_1_4
CL	D14-1149	mn	1	5	motivation_problem	result	support	support	secondary	secondary	none	none	Automatically identifying related specialist terms is a difficult and important task required to understand the lexical structure of language .	The results suggest that the model is able to extract coherent groups of satellite terms in corpora with varying size , content and structure .	CL_D14-1149_mn_1	CL_D14-1149_mn_5	CL_D14-1149_mn_1_5
CL	D14-1149	mn	1	6	motivation_problem	result	support	elaboration	secondary	secondary	none	none	Automatically identifying related specialist terms is a difficult and important task required to understand the lexical structure of language .	The findings also confirm that language consists of a densely connected core ( observed in dictionaries ) and systematic , semantically coherent groups of terms at the edges of the lexicon .	CL_D14-1149_mn_1	CL_D14-1149_mn_6	CL_D14-1149_mn_1_6
CL	D14-1149	mn	2	3	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	This paper develops a corpus-based method of extracting coherent clusters of satellite terminology terms on the edge of the lexicon  using co-occurrence networks of unstructured text .	Term clusters are identified by extracting communities in the co-occurrence graph , after which the largest is discarded and the remaining words are ranked by centrality within a community .	CL_D14-1149_mn_2	CL_D14-1149_mn_3	CL_D14-1149_mn_2_3
CL	D14-1149	mn	3	2	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	Term clusters are identified by extracting communities in the co-occurrence graph , after which the largest is discarded and the remaining words are ranked by centrality within a community .	This paper develops a corpus-based method of extracting coherent clusters of satellite terminology terms on the edge of the lexicon  using co-occurrence networks of unstructured text .	CL_D14-1149_mn_3	CL_D14-1149_mn_2	CL_D14-1149_mn_2_3
CL	D14-1149	mn	2	4	proposal	information_additional	none	info-optional	main	secondary	none	none	This paper develops a corpus-based method of extracting coherent clusters of satellite terminology terms on the edge of the lexicon  using co-occurrence networks of unstructured text .	The method is tractable on large corpora , requires no document structure and minimal normalization .	CL_D14-1149_mn_2	CL_D14-1149_mn_4	CL_D14-1149_mn_2_4
CL	D14-1149	mn	2	5	proposal	result	none	support	main	secondary	back	support	This paper develops a corpus-based method of extracting coherent clusters of satellite terminology terms on the edge of the lexicon  using co-occurrence networks of unstructured text .	The results suggest that the model is able to extract coherent groups of satellite terms in corpora with varying size , content and structure .	CL_D14-1149_mn_2	CL_D14-1149_mn_5	CL_D14-1149_mn_2_5
CL	D14-1149	mn	5	2	result	proposal	support	none	secondary	main	forw	support	The results suggest that the model is able to extract coherent groups of satellite terms in corpora with varying size , content and structure .	This paper develops a corpus-based method of extracting coherent clusters of satellite terminology terms on the edge of the lexicon  using co-occurrence networks of unstructured text .	CL_D14-1149_mn_5	CL_D14-1149_mn_2	CL_D14-1149_mn_2_5
CL	D14-1149	mn	2	6	proposal	result	none	elaboration	main	secondary	none	none	This paper develops a corpus-based method of extracting coherent clusters of satellite terminology terms on the edge of the lexicon  using co-occurrence networks of unstructured text .	The findings also confirm that language consists of a densely connected core ( observed in dictionaries ) and systematic , semantically coherent groups of terms at the edges of the lexicon .	CL_D14-1149_mn_2	CL_D14-1149_mn_6	CL_D14-1149_mn_2_6
CL	D14-1149	mn	3	4	proposal_implementation	information_additional	elaboration	info-optional	secondary	secondary	back	info-optional	Term clusters are identified by extracting communities in the co-occurrence graph , after which the largest is discarded and the remaining words are ranked by centrality within a community .	The method is tractable on large corpora , requires no document structure and minimal normalization .	CL_D14-1149_mn_3	CL_D14-1149_mn_4	CL_D14-1149_mn_3_4
CL	D14-1149	mn	4	3	information_additional	proposal_implementation	info-optional	elaboration	secondary	secondary	forw	info-optional	The method is tractable on large corpora , requires no document structure and minimal normalization .	Term clusters are identified by extracting communities in the co-occurrence graph , after which the largest is discarded and the remaining words are ranked by centrality within a community .	CL_D14-1149_mn_4	CL_D14-1149_mn_3	CL_D14-1149_mn_3_4
CL	D14-1149	mn	3	5	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	Term clusters are identified by extracting communities in the co-occurrence graph , after which the largest is discarded and the remaining words are ranked by centrality within a community .	The results suggest that the model is able to extract coherent groups of satellite terms in corpora with varying size , content and structure .	CL_D14-1149_mn_3	CL_D14-1149_mn_5	CL_D14-1149_mn_3_5
CL	D14-1149	mn	3	6	proposal_implementation	result	elaboration	elaboration	secondary	secondary	none	none	Term clusters are identified by extracting communities in the co-occurrence graph , after which the largest is discarded and the remaining words are ranked by centrality within a community .	The findings also confirm that language consists of a densely connected core ( observed in dictionaries ) and systematic , semantically coherent groups of terms at the edges of the lexicon .	CL_D14-1149_mn_3	CL_D14-1149_mn_6	CL_D14-1149_mn_3_6
CL	D14-1149	mn	4	5	information_additional	result	info-optional	support	secondary	secondary	none	none	The method is tractable on large corpora , requires no document structure and minimal normalization .	The results suggest that the model is able to extract coherent groups of satellite terms in corpora with varying size , content and structure .	CL_D14-1149_mn_4	CL_D14-1149_mn_5	CL_D14-1149_mn_4_5
CL	D14-1149	mn	4	6	information_additional	result	info-optional	elaboration	secondary	secondary	none	none	The method is tractable on large corpora , requires no document structure and minimal normalization .	The findings also confirm that language consists of a densely connected core ( observed in dictionaries ) and systematic , semantically coherent groups of terms at the edges of the lexicon .	CL_D14-1149_mn_4	CL_D14-1149_mn_6	CL_D14-1149_mn_4_6
CL	D14-1149	mn	5	6	result	result	support	elaboration	secondary	secondary	back	elaboration	The results suggest that the model is able to extract coherent groups of satellite terms in corpora with varying size , content and structure .	The findings also confirm that language consists of a densely connected core ( observed in dictionaries ) and systematic , semantically coherent groups of terms at the edges of the lexicon .	CL_D14-1149_mn_5	CL_D14-1149_mn_6	CL_D14-1149_mn_5_6
CL	D14-1149	mn	6	5	result	result	elaboration	support	secondary	secondary	forw	elaboration	The findings also confirm that language consists of a densely connected core ( observed in dictionaries ) and systematic , semantically coherent groups of terms at the edges of the lexicon .	The results suggest that the model is able to extract coherent groups of satellite terms in corpora with varying size , content and structure .	CL_D14-1149_mn_6	CL_D14-1149_mn_5	CL_D14-1149_mn_5_6
CL	D14-1150	ab	1	2	motivation_background	proposal	support	none	secondary	main	forw	support	Given the large amounts of online textual documents available these days , e.g. , news articles , weblogs , and scientific papers , effective methods for extracting keyphrases , which provide a high-level topic description of a document , are greatly needed .	In this paper , we propose a supervised model for keyphrase extraction from research papers , which are embedded in citation networks .	CL_D14-1150_ab_1	CL_D14-1150_ab_2	CL_D14-1150_ab_1_2
CL	D14-1150	ab	2	1	proposal	motivation_background	none	support	main	secondary	back	support	In this paper , we propose a supervised model for keyphrase extraction from research papers , which are embedded in citation networks .	Given the large amounts of online textual documents available these days , e.g. , news articles , weblogs , and scientific papers , effective methods for extracting keyphrases , which provide a high-level topic description of a document , are greatly needed .	CL_D14-1150_ab_2	CL_D14-1150_ab_1	CL_D14-1150_ab_1_2
CL	D14-1150	ab	1	3	motivation_background	proposal_implementation	support	elaboration	secondary	secondary	none	none	Given the large amounts of online textual documents available these days , e.g. , news articles , weblogs , and scientific papers , effective methods for extracting keyphrases , which provide a high-level topic description of a document , are greatly needed .	To this end , we design novel features based on citation network information and use them in conjunction with traditional features for keyphrase extraction to obtain remarkable improvements in performance over strong baselines .	CL_D14-1150_ab_1	CL_D14-1150_ab_3	CL_D14-1150_ab_1_3
CL	D14-1150	ab	2	3	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	In this paper , we propose a supervised model for keyphrase extraction from research papers , which are embedded in citation networks .	To this end , we design novel features based on citation network information and use them in conjunction with traditional features for keyphrase extraction to obtain remarkable improvements in performance over strong baselines .	CL_D14-1150_ab_2	CL_D14-1150_ab_3	CL_D14-1150_ab_2_3
CL	D14-1150	ab	3	2	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	To this end , we design novel features based on citation network information and use them in conjunction with traditional features for keyphrase extraction to obtain remarkable improvements in performance over strong baselines .	In this paper , we propose a supervised model for keyphrase extraction from research papers , which are embedded in citation networks .	CL_D14-1150_ab_3	CL_D14-1150_ab_2	CL_D14-1150_ab_2_3
CL	D14-1151	ab	1	2	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	We propose to use coreference chains extracted from a large corpus as a resource for semantic tasks .	We extract three million coreference chains and train word embeddings on them .	CL_D14-1151_ab_1	CL_D14-1151_ab_2	CL_D14-1151_ab_1_2
CL	D14-1151	ab	2	1	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	We extract three million coreference chains and train word embeddings on them .	We propose to use coreference chains extracted from a large corpus as a resource for semantic tasks .	CL_D14-1151_ab_2	CL_D14-1151_ab_1	CL_D14-1151_ab_1_2
CL	D14-1151	ab	1	3	proposal	proposal_implementation	none	sequence	main	secondary	none	none	We propose to use coreference chains extracted from a large corpus as a resource for semantic tasks .	Then , we compare these embeddings to word vectors derived from raw text data and show that coreference-based word embeddings improve F1 on the task of antonym classification by up to .09 .	CL_D14-1151_ab_1	CL_D14-1151_ab_3	CL_D14-1151_ab_1_3
CL	D14-1151	ab	2	3	proposal_implementation	proposal_implementation	elaboration	sequence	secondary	secondary	back	sequence	We extract three million coreference chains and train word embeddings on them .	Then , we compare these embeddings to word vectors derived from raw text data and show that coreference-based word embeddings improve F1 on the task of antonym classification by up to .09 .	CL_D14-1151_ab_2	CL_D14-1151_ab_3	CL_D14-1151_ab_2_3
CL	D14-1151	ab	3	2	proposal_implementation	proposal_implementation	sequence	elaboration	secondary	secondary	forw	sequence	Then , we compare these embeddings to word vectors derived from raw text data and show that coreference-based word embeddings improve F1 on the task of antonym classification by up to .09 .	We extract three million coreference chains and train word embeddings on them .	CL_D14-1151_ab_3	CL_D14-1151_ab_2	CL_D14-1151_ab_2_3
CL	D14-1152	ab	1	2	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	This paper proposes to apply the continuous vector representations of words for discovering keywords from a financial sentiment lexicon .	In order to capture more keywords , we also incorporate syntactic information into the Continuous Bag-of-Words ( CBOW ) model .	CL_D14-1152_ab_1	CL_D14-1152_ab_2	CL_D14-1152_ab_1_2
CL	D14-1152	ab	2	1	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	In order to capture more keywords , we also incorporate syntactic information into the Continuous Bag-of-Words ( CBOW ) model .	This paper proposes to apply the continuous vector representations of words for discovering keywords from a financial sentiment lexicon .	CL_D14-1152_ab_2	CL_D14-1152_ab_1	CL_D14-1152_ab_1_2
CL	D14-1152	ab	1	3	proposal	conclusion	none	support	main	secondary	back	support	This paper proposes to apply the continuous vector representations of words for discovering keywords from a financial sentiment lexicon .	Experimental results on a task of financial risk prediction using the discovered keywords demonstrate that the proposed approach is good at predicting financial risk .	CL_D14-1152_ab_1	CL_D14-1152_ab_3	CL_D14-1152_ab_1_3
CL	D14-1152	ab	3	1	conclusion	proposal	support	none	secondary	main	forw	support	Experimental results on a task of financial risk prediction using the discovered keywords demonstrate that the proposed approach is good at predicting financial risk .	This paper proposes to apply the continuous vector representations of words for discovering keywords from a financial sentiment lexicon .	CL_D14-1152_ab_3	CL_D14-1152_ab_1	CL_D14-1152_ab_1_3
CL	D14-1152	ab	2	3	proposal_implementation	conclusion	elaboration	support	secondary	secondary	none	none	In order to capture more keywords , we also incorporate syntactic information into the Continuous Bag-of-Words ( CBOW ) model .	Experimental results on a task of financial risk prediction using the discovered keywords demonstrate that the proposed approach is good at predicting financial risk .	CL_D14-1152_ab_2	CL_D14-1152_ab_3	CL_D14-1152_ab_2_3
CL	D14-1153	ab	1	2	motivation_background	proposal	support	none	secondary	main	forw	support	When it is not possible to compare the suspicious document to the source document ( s ) plagiarism has been committed from , the evidence of plagiarism has to be looked for intrinsically in the document itself .	In this paper , we introduce a novel languageindependent intrinsic plagiarism detection method which is based on a new text representation that we called n-gram classes .	CL_D14-1153_ab_1	CL_D14-1153_ab_2	CL_D14-1153_ab_1_2
CL	D14-1153	ab	2	1	proposal	motivation_background	none	support	main	secondary	back	support	In this paper , we introduce a novel languageindependent intrinsic plagiarism detection method which is based on a new text representation that we called n-gram classes .	When it is not possible to compare the suspicious document to the source document ( s ) plagiarism has been committed from , the evidence of plagiarism has to be looked for intrinsically in the document itself .	CL_D14-1153_ab_2	CL_D14-1153_ab_1	CL_D14-1153_ab_1_2
CL	D14-1153	ab	1	3	motivation_background	means	support	by-means	secondary	secondary	none	none	When it is not possible to compare the suspicious document to the source document ( s ) plagiarism has been committed from , the evidence of plagiarism has to be looked for intrinsically in the document itself .	The proposed method was evaluated on three publicly available standard corpora .	CL_D14-1153_ab_1	CL_D14-1153_ab_3	CL_D14-1153_ab_1_3
CL	D14-1153	ab	1	4	motivation_background	result	support	support	secondary	secondary	none	none	When it is not possible to compare the suspicious document to the source document ( s ) plagiarism has been committed from , the evidence of plagiarism has to be looked for intrinsically in the document itself .	The obtained results are comparable to the ones obtained by the best state-of-the-art methods .	CL_D14-1153_ab_1	CL_D14-1153_ab_4	CL_D14-1153_ab_1_4
CL	D14-1153	ab	2	3	proposal	means	none	by-means	main	secondary	none	none	In this paper , we introduce a novel languageindependent intrinsic plagiarism detection method which is based on a new text representation that we called n-gram classes .	The proposed method was evaluated on three publicly available standard corpora .	CL_D14-1153_ab_2	CL_D14-1153_ab_3	CL_D14-1153_ab_2_3
CL	D14-1153	ab	2	4	proposal	result	none	support	main	secondary	back	support	In this paper , we introduce a novel languageindependent intrinsic plagiarism detection method which is based on a new text representation that we called n-gram classes .	The obtained results are comparable to the ones obtained by the best state-of-the-art methods .	CL_D14-1153_ab_2	CL_D14-1153_ab_4	CL_D14-1153_ab_2_4
CL	D14-1153	ab	4	2	result	proposal	support	none	secondary	main	forw	support	The obtained results are comparable to the ones obtained by the best state-of-the-art methods .	In this paper , we introduce a novel languageindependent intrinsic plagiarism detection method which is based on a new text representation that we called n-gram classes .	CL_D14-1153_ab_4	CL_D14-1153_ab_2	CL_D14-1153_ab_2_4
CL	D14-1153	ab	3	4	means	result	by-means	support	secondary	secondary	forw	by-means	The proposed method was evaluated on three publicly available standard corpora .	The obtained results are comparable to the ones obtained by the best state-of-the-art methods .	CL_D14-1153_ab_3	CL_D14-1153_ab_4	CL_D14-1153_ab_3_4
CL	D14-1153	ab	4	3	result	means	support	by-means	secondary	secondary	back	by-means	The obtained results are comparable to the ones obtained by the best state-of-the-art methods .	The proposed method was evaluated on three publicly available standard corpora .	CL_D14-1153_ab_4	CL_D14-1153_ab_3	CL_D14-1153_ab_3_4
CL	D14-1154	ab	1	2	motivation_background	motivation_problem	info-required	support	secondary	secondary	forw	info-required	Several recent papers on Arabic dialect identification have hinted that using a word unigram model is sufficient and effective for the task .	However , most previous work was done on a standard fairly homogeneous dataset of dialectal user comments .	CL_D14-1154_ab_1	CL_D14-1154_ab_2	CL_D14-1154_ab_1_2
CL	D14-1154	ab	2	1	motivation_problem	motivation_background	support	info-required	secondary	secondary	back	info-required	However , most previous work was done on a standard fairly homogeneous dataset of dialectal user comments .	Several recent papers on Arabic dialect identification have hinted that using a word unigram model is sufficient and effective for the task .	CL_D14-1154_ab_2	CL_D14-1154_ab_1	CL_D14-1154_ab_1_2
CL	D14-1154	ab	1	3	motivation_background	proposal	info-required	none	secondary	main	none	none	Several recent papers on Arabic dialect identification have hinted that using a word unigram model is sufficient and effective for the task .	In this paper , we show that training on the standard dataset does not generalize , because a unigram model may be tuned to topics in the comments and does not capture the distinguishing features of dialects .	CL_D14-1154_ab_1	CL_D14-1154_ab_3	CL_D14-1154_ab_1_3
CL	D14-1154	ab	1	4	motivation_background	proposal	info-required	elaboration	secondary	secondary	none	none	Several recent papers on Arabic dialect identification have hinted that using a word unigram model is sufficient and effective for the task .	We show that effective dialect identification requires that we account for the distinguishing lexical , morphological , and phonological phenomena of dialects .	CL_D14-1154_ab_1	CL_D14-1154_ab_4	CL_D14-1154_ab_1_4
CL	D14-1154	ab	1	5	motivation_background	observation	info-required	support	secondary	secondary	none	none	Several recent papers on Arabic dialect identification have hinted that using a word unigram model is sufficient and effective for the task .	We show that accounting for such can improve dialect detection accuracy by nearly 10 % absolute .	CL_D14-1154_ab_1	CL_D14-1154_ab_5	CL_D14-1154_ab_1_5
CL	D14-1154	ab	2	3	motivation_problem	proposal	support	none	secondary	main	forw	support	However , most previous work was done on a standard fairly homogeneous dataset of dialectal user comments .	In this paper , we show that training on the standard dataset does not generalize , because a unigram model may be tuned to topics in the comments and does not capture the distinguishing features of dialects .	CL_D14-1154_ab_2	CL_D14-1154_ab_3	CL_D14-1154_ab_2_3
CL	D14-1154	ab	3	2	proposal	motivation_problem	none	support	main	secondary	back	support	In this paper , we show that training on the standard dataset does not generalize , because a unigram model may be tuned to topics in the comments and does not capture the distinguishing features of dialects .	However , most previous work was done on a standard fairly homogeneous dataset of dialectal user comments .	CL_D14-1154_ab_3	CL_D14-1154_ab_2	CL_D14-1154_ab_2_3
CL	D14-1154	ab	2	4	motivation_problem	proposal	support	elaboration	secondary	secondary	none	none	However , most previous work was done on a standard fairly homogeneous dataset of dialectal user comments .	We show that effective dialect identification requires that we account for the distinguishing lexical , morphological , and phonological phenomena of dialects .	CL_D14-1154_ab_2	CL_D14-1154_ab_4	CL_D14-1154_ab_2_4
CL	D14-1154	ab	2	5	motivation_problem	observation	support	support	secondary	secondary	none	none	However , most previous work was done on a standard fairly homogeneous dataset of dialectal user comments .	We show that accounting for such can improve dialect detection accuracy by nearly 10 % absolute .	CL_D14-1154_ab_2	CL_D14-1154_ab_5	CL_D14-1154_ab_2_5
CL	D14-1154	ab	3	4	proposal	proposal	none	elaboration	main	secondary	back	elaboration	In this paper , we show that training on the standard dataset does not generalize , because a unigram model may be tuned to topics in the comments and does not capture the distinguishing features of dialects .	We show that effective dialect identification requires that we account for the distinguishing lexical , morphological , and phonological phenomena of dialects .	CL_D14-1154_ab_3	CL_D14-1154_ab_4	CL_D14-1154_ab_3_4
CL	D14-1154	ab	4	3	proposal	proposal	elaboration	none	secondary	main	forw	elaboration	We show that effective dialect identification requires that we account for the distinguishing lexical , morphological , and phonological phenomena of dialects .	In this paper , we show that training on the standard dataset does not generalize , because a unigram model may be tuned to topics in the comments and does not capture the distinguishing features of dialects .	CL_D14-1154_ab_4	CL_D14-1154_ab_3	CL_D14-1154_ab_3_4
CL	D14-1154	ab	3	5	proposal	observation	none	support	main	secondary	none	none	In this paper , we show that training on the standard dataset does not generalize , because a unigram model may be tuned to topics in the comments and does not capture the distinguishing features of dialects .	We show that accounting for such can improve dialect detection accuracy by nearly 10 % absolute .	CL_D14-1154_ab_3	CL_D14-1154_ab_5	CL_D14-1154_ab_3_5
CL	D14-1154	ab	4	5	proposal	observation	elaboration	support	secondary	secondary	back	support	We show that effective dialect identification requires that we account for the distinguishing lexical , morphological , and phonological phenomena of dialects .	We show that accounting for such can improve dialect detection accuracy by nearly 10 % absolute .	CL_D14-1154_ab_4	CL_D14-1154_ab_5	CL_D14-1154_ab_4_5
CL	D14-1154	ab	5	4	observation	proposal	support	elaboration	secondary	secondary	forw	support	We show that accounting for such can improve dialect detection accuracy by nearly 10 % absolute .	We show that effective dialect identification requires that we account for the distinguishing lexical , morphological , and phonological phenomena of dialects .	CL_D14-1154_ab_5	CL_D14-1154_ab_4	CL_D14-1154_ab_4_5
CL	D14-1155	ab	1	2	proposal	proposal	none	elaboration	main	secondary	back	elaboration	In this paper , we explore the use of keyboard strokes as a means to access the real-time writing process of online authors , analogously to prosody in speech analysis , in the context of deception detection .	We show that differences in keystroke patterns like editing maneuvers and duration of pauses can help distinguish between truthful and deceptive writing .	CL_D14-1155_ab_1	CL_D14-1155_ab_2	CL_D14-1155_ab_1_2
CL	D14-1155	ab	2	1	proposal	proposal	elaboration	none	secondary	main	forw	elaboration	We show that differences in keystroke patterns like editing maneuvers and duration of pauses can help distinguish between truthful and deceptive writing .	In this paper , we explore the use of keyboard strokes as a means to access the real-time writing process of online authors , analogously to prosody in speech analysis , in the context of deception detection .	CL_D14-1155_ab_2	CL_D14-1155_ab_1	CL_D14-1155_ab_1_2
CL	D14-1155	ab	1	3	proposal	result	none	support	main	secondary	back	support	In this paper , we explore the use of keyboard strokes as a means to access the real-time writing process of online authors , analogously to prosody in speech analysis , in the context of deception detection .	Empirical results show that incorporating keystroke-based features lead to improved performance in deception detection in two different domains : online reviews and essays .	CL_D14-1155_ab_1	CL_D14-1155_ab_3	CL_D14-1155_ab_1_3
CL	D14-1155	ab	3	1	result	proposal	support	none	secondary	main	forw	support	Empirical results show that incorporating keystroke-based features lead to improved performance in deception detection in two different domains : online reviews and essays .	In this paper , we explore the use of keyboard strokes as a means to access the real-time writing process of online authors , analogously to prosody in speech analysis , in the context of deception detection .	CL_D14-1155_ab_3	CL_D14-1155_ab_1	CL_D14-1155_ab_1_3
CL	D14-1155	ab	2	3	proposal	result	elaboration	support	secondary	secondary	none	none	We show that differences in keystroke patterns like editing maneuvers and duration of pauses can help distinguish between truthful and deceptive writing .	Empirical results show that incorporating keystroke-based features lead to improved performance in deception detection in two different domains : online reviews and essays .	CL_D14-1155_ab_2	CL_D14-1155_ab_3	CL_D14-1155_ab_2_3
CL	D14-1156	ab	1	2	motivation_background	motivation_background	info-required	support	secondary	secondary	forw	info-required	Statistical language modeling ( LM ) that purports to quantify the acceptability of a given piece of text has long been an interesting yet challenging research area .	In particular , language modeling for information retrieval ( IR ) has enjoyed remarkable empirical success ; one emerging stream of the LM approach for IR is to employ the pseudo-relevance feedback process to enhance the representation of an input query so as to improve retrieval effectiveness .	CL_D14-1156_ab_1	CL_D14-1156_ab_2	CL_D14-1156_ab_1_2
CL	D14-1156	ab	2	1	motivation_background	motivation_background	support	info-required	secondary	secondary	back	info-required	In particular , language modeling for information retrieval ( IR ) has enjoyed remarkable empirical success ; one emerging stream of the LM approach for IR is to employ the pseudo-relevance feedback process to enhance the representation of an input query so as to improve retrieval effectiveness .	Statistical language modeling ( LM ) that purports to quantify the acceptability of a given piece of text has long been an interesting yet challenging research area .	CL_D14-1156_ab_2	CL_D14-1156_ab_1	CL_D14-1156_ab_1_2
CL	D14-1156	ab	1	3	motivation_background	proposal	info-required	none	secondary	main	none	none	Statistical language modeling ( LM ) that purports to quantify the acceptability of a given piece of text has long been an interesting yet challenging research area .	This paper presents a continuation of such a general line of research and the main contribution is threefold .	CL_D14-1156_ab_1	CL_D14-1156_ab_3	CL_D14-1156_ab_1_3
CL	D14-1156	ab	1	4	motivation_background	proposal	info-required	elaboration	secondary	secondary	none	none	Statistical language modeling ( LM ) that purports to quantify the acceptability of a given piece of text has long been an interesting yet challenging research area .	First , we propose a principled framework which can unify the relationships among several widely-used query modeling formulations .	CL_D14-1156_ab_1	CL_D14-1156_ab_4	CL_D14-1156_ab_1_4
CL	D14-1156	ab	1	5	motivation_background	proposal	info-required	sequence	secondary	secondary	none	none	Statistical language modeling ( LM ) that purports to quantify the acceptability of a given piece of text has long been an interesting yet challenging research area .	Second , on top of the successfully developed framework , we propose an extended query modeling formulation by incorporating critical query-specific information cues to guide the model estimation .	CL_D14-1156_ab_1	CL_D14-1156_ab_5	CL_D14-1156_ab_1_5
CL	D14-1156	ab	1	6	motivation_background	proposal	info-required	sequence	secondary	secondary	none	none	Statistical language modeling ( LM ) that purports to quantify the acceptability of a given piece of text has long been an interesting yet challenging research area .	Third , we further adopt and formalize such a framework to the speech recognition and summarization tasks .	CL_D14-1156_ab_1	CL_D14-1156_ab_6	CL_D14-1156_ab_1_6
CL	D14-1156	ab	1	7	motivation_background	result	info-required	support	secondary	secondary	none	none	Statistical language modeling ( LM ) that purports to quantify the acceptability of a given piece of text has long been an interesting yet challenging research area .	A series of empirical experiments reveal the feasibility of such an LM framework and the performance merits of the deduced models on these two tasks .	CL_D14-1156_ab_1	CL_D14-1156_ab_7	CL_D14-1156_ab_1_7
CL	D14-1156	ab	2	3	motivation_background	proposal	support	none	secondary	main	forw	support	In particular , language modeling for information retrieval ( IR ) has enjoyed remarkable empirical success ; one emerging stream of the LM approach for IR is to employ the pseudo-relevance feedback process to enhance the representation of an input query so as to improve retrieval effectiveness .	This paper presents a continuation of such a general line of research and the main contribution is threefold .	CL_D14-1156_ab_2	CL_D14-1156_ab_3	CL_D14-1156_ab_2_3
CL	D14-1156	ab	3	2	proposal	motivation_background	none	support	main	secondary	back	support	This paper presents a continuation of such a general line of research and the main contribution is threefold .	In particular , language modeling for information retrieval ( IR ) has enjoyed remarkable empirical success ; one emerging stream of the LM approach for IR is to employ the pseudo-relevance feedback process to enhance the representation of an input query so as to improve retrieval effectiveness .	CL_D14-1156_ab_3	CL_D14-1156_ab_2	CL_D14-1156_ab_2_3
CL	D14-1156	ab	2	4	motivation_background	proposal	support	elaboration	secondary	secondary	none	none	In particular , language modeling for information retrieval ( IR ) has enjoyed remarkable empirical success ; one emerging stream of the LM approach for IR is to employ the pseudo-relevance feedback process to enhance the representation of an input query so as to improve retrieval effectiveness .	First , we propose a principled framework which can unify the relationships among several widely-used query modeling formulations .	CL_D14-1156_ab_2	CL_D14-1156_ab_4	CL_D14-1156_ab_2_4
CL	D14-1156	ab	2	5	motivation_background	proposal	support	sequence	secondary	secondary	none	none	In particular , language modeling for information retrieval ( IR ) has enjoyed remarkable empirical success ; one emerging stream of the LM approach for IR is to employ the pseudo-relevance feedback process to enhance the representation of an input query so as to improve retrieval effectiveness .	Second , on top of the successfully developed framework , we propose an extended query modeling formulation by incorporating critical query-specific information cues to guide the model estimation .	CL_D14-1156_ab_2	CL_D14-1156_ab_5	CL_D14-1156_ab_2_5
CL	D14-1156	ab	2	6	motivation_background	proposal	support	sequence	secondary	secondary	none	none	In particular , language modeling for information retrieval ( IR ) has enjoyed remarkable empirical success ; one emerging stream of the LM approach for IR is to employ the pseudo-relevance feedback process to enhance the representation of an input query so as to improve retrieval effectiveness .	Third , we further adopt and formalize such a framework to the speech recognition and summarization tasks .	CL_D14-1156_ab_2	CL_D14-1156_ab_6	CL_D14-1156_ab_2_6
CL	D14-1156	ab	2	7	motivation_background	result	support	support	secondary	secondary	none	none	In particular , language modeling for information retrieval ( IR ) has enjoyed remarkable empirical success ; one emerging stream of the LM approach for IR is to employ the pseudo-relevance feedback process to enhance the representation of an input query so as to improve retrieval effectiveness .	A series of empirical experiments reveal the feasibility of such an LM framework and the performance merits of the deduced models on these two tasks .	CL_D14-1156_ab_2	CL_D14-1156_ab_7	CL_D14-1156_ab_2_7
CL	D14-1156	ab	3	4	proposal	proposal	none	elaboration	main	secondary	back	elaboration	This paper presents a continuation of such a general line of research and the main contribution is threefold .	First , we propose a principled framework which can unify the relationships among several widely-used query modeling formulations .	CL_D14-1156_ab_3	CL_D14-1156_ab_4	CL_D14-1156_ab_3_4
CL	D14-1156	ab	4	3	proposal	proposal	elaboration	none	secondary	main	forw	elaboration	First , we propose a principled framework which can unify the relationships among several widely-used query modeling formulations .	This paper presents a continuation of such a general line of research and the main contribution is threefold .	CL_D14-1156_ab_4	CL_D14-1156_ab_3	CL_D14-1156_ab_3_4
CL	D14-1156	ab	3	5	proposal	proposal	none	sequence	main	secondary	none	none	This paper presents a continuation of such a general line of research and the main contribution is threefold .	Second , on top of the successfully developed framework , we propose an extended query modeling formulation by incorporating critical query-specific information cues to guide the model estimation .	CL_D14-1156_ab_3	CL_D14-1156_ab_5	CL_D14-1156_ab_3_5
CL	D14-1156	ab	3	6	proposal	proposal	none	sequence	main	secondary	none	none	This paper presents a continuation of such a general line of research and the main contribution is threefold .	Third , we further adopt and formalize such a framework to the speech recognition and summarization tasks .	CL_D14-1156_ab_3	CL_D14-1156_ab_6	CL_D14-1156_ab_3_6
CL	D14-1156	ab	3	7	proposal	result	none	support	main	secondary	back	support	This paper presents a continuation of such a general line of research and the main contribution is threefold .	A series of empirical experiments reveal the feasibility of such an LM framework and the performance merits of the deduced models on these two tasks .	CL_D14-1156_ab_3	CL_D14-1156_ab_7	CL_D14-1156_ab_3_7
CL	D14-1156	ab	7	3	result	proposal	support	none	secondary	main	forw	support	A series of empirical experiments reveal the feasibility of such an LM framework and the performance merits of the deduced models on these two tasks .	This paper presents a continuation of such a general line of research and the main contribution is threefold .	CL_D14-1156_ab_7	CL_D14-1156_ab_3	CL_D14-1156_ab_3_7
CL	D14-1156	ab	4	5	proposal	proposal	elaboration	sequence	secondary	secondary	back	sequence	First , we propose a principled framework which can unify the relationships among several widely-used query modeling formulations .	Second , on top of the successfully developed framework , we propose an extended query modeling formulation by incorporating critical query-specific information cues to guide the model estimation .	CL_D14-1156_ab_4	CL_D14-1156_ab_5	CL_D14-1156_ab_4_5
CL	D14-1156	ab	5	4	proposal	proposal	sequence	elaboration	secondary	secondary	forw	sequence	Second , on top of the successfully developed framework , we propose an extended query modeling formulation by incorporating critical query-specific information cues to guide the model estimation .	First , we propose a principled framework which can unify the relationships among several widely-used query modeling formulations .	CL_D14-1156_ab_5	CL_D14-1156_ab_4	CL_D14-1156_ab_4_5
CL	D14-1156	ab	4	6	proposal	proposal	elaboration	sequence	secondary	secondary	none	none	First , we propose a principled framework which can unify the relationships among several widely-used query modeling formulations .	Third , we further adopt and formalize such a framework to the speech recognition and summarization tasks .	CL_D14-1156_ab_4	CL_D14-1156_ab_6	CL_D14-1156_ab_4_6
CL	D14-1156	ab	4	7	proposal	result	elaboration	support	secondary	secondary	none	none	First , we propose a principled framework which can unify the relationships among several widely-used query modeling formulations .	A series of empirical experiments reveal the feasibility of such an LM framework and the performance merits of the deduced models on these two tasks .	CL_D14-1156_ab_4	CL_D14-1156_ab_7	CL_D14-1156_ab_4_7
CL	D14-1156	ab	5	6	proposal	proposal	sequence	sequence	secondary	secondary	back	sequence	Second , on top of the successfully developed framework , we propose an extended query modeling formulation by incorporating critical query-specific information cues to guide the model estimation .	Third , we further adopt and formalize such a framework to the speech recognition and summarization tasks .	CL_D14-1156_ab_5	CL_D14-1156_ab_6	CL_D14-1156_ab_5_6
CL	D14-1156	ab	6	5	proposal	proposal	sequence	sequence	secondary	secondary	forw	sequence	Third , we further adopt and formalize such a framework to the speech recognition and summarization tasks .	Second , on top of the successfully developed framework , we propose an extended query modeling formulation by incorporating critical query-specific information cues to guide the model estimation .	CL_D14-1156_ab_6	CL_D14-1156_ab_5	CL_D14-1156_ab_5_6
CL	D14-1156	ab	5	7	proposal	result	sequence	support	secondary	secondary	none	none	Second , on top of the successfully developed framework , we propose an extended query modeling formulation by incorporating critical query-specific information cues to guide the model estimation .	A series of empirical experiments reveal the feasibility of such an LM framework and the performance merits of the deduced models on these two tasks .	CL_D14-1156_ab_5	CL_D14-1156_ab_7	CL_D14-1156_ab_5_7
CL	D14-1156	ab	6	7	proposal	result	sequence	support	secondary	secondary	none	none	Third , we further adopt and formalize such a framework to the speech recognition and summarization tasks .	A series of empirical experiments reveal the feasibility of such an LM framework and the performance merits of the deduced models on these two tasks .	CL_D14-1156_ab_6	CL_D14-1156_ab_7	CL_D14-1156_ab_6_7
CL	D14-1157	ab	1	2	proposal_implementation	result	none	support	main	secondary	back	support	We study the topic dynamics of interactions in political debates using the 2012 Republican presidential primary debates as data .	We show that the tendency of candidates to shift topics changes over the course of the election campaign , and that it is correlated with their relative power .	CL_D14-1157_ab_1	CL_D14-1157_ab_2	CL_D14-1157_ab_1_2
CL	D14-1157	ab	2	1	result	proposal_implementation	support	none	secondary	main	forw	support	We show that the tendency of candidates to shift topics changes over the course of the election campaign , and that it is correlated with their relative power .	We study the topic dynamics of interactions in political debates using the 2012 Republican presidential primary debates as data .	CL_D14-1157_ab_2	CL_D14-1157_ab_1	CL_D14-1157_ab_1_2
CL	D14-1157	ab	1	3	proposal_implementation	result	none	elaboration	main	secondary	none	none	We study the topic dynamics of interactions in political debates using the 2012 Republican presidential primary debates as data .	We also show that our topic shift features help predict candidates' relative rankings .	CL_D14-1157_ab_1	CL_D14-1157_ab_3	CL_D14-1157_ab_1_3
CL	D14-1157	ab	2	3	result	result	support	elaboration	secondary	secondary	back	elaboration	We show that the tendency of candidates to shift topics changes over the course of the election campaign , and that it is correlated with their relative power .	We also show that our topic shift features help predict candidates' relative rankings .	CL_D14-1157_ab_2	CL_D14-1157_ab_3	CL_D14-1157_ab_2_3
CL	D14-1157	ab	3	2	result	result	elaboration	support	secondary	secondary	forw	elaboration	We also show that our topic shift features help predict candidates' relative rankings .	We show that the tendency of candidates to shift topics changes over the course of the election campaign , and that it is correlated with their relative power .	CL_D14-1157_ab_3	CL_D14-1157_ab_2	CL_D14-1157_ab_2_3
CL	D14-1158	ab	1	2	proposal_implementation	proposal_implementation	none	elaboration	main	secondary	back	elaboration	We present power low rank ensembles ( PLRE ) , a flexible framework for n-gram language modeling where ensembles of low rank matrices and tensors are used to obtain smoothed probability estimates of words in context .	Our method can be understood as a generalization of n-gram modeling to non-integer n , and includes standard techniques such as absolute discounting and Kneser-Ney smoothing as special cases .	CL_D14-1158_ab_1	CL_D14-1158_ab_2	CL_D14-1158_ab_1_2
CL	D14-1158	ab	2	1	proposal_implementation	proposal_implementation	elaboration	none	secondary	main	forw	elaboration	Our method can be understood as a generalization of n-gram modeling to non-integer n , and includes standard techniques such as absolute discounting and Kneser-Ney smoothing as special cases .	We present power low rank ensembles ( PLRE ) , a flexible framework for n-gram language modeling where ensembles of low rank matrices and tensors are used to obtain smoothed probability estimates of words in context .	CL_D14-1158_ab_2	CL_D14-1158_ab_1	CL_D14-1158_ab_1_2
CL	D14-1158	ab	1	3	proposal_implementation	result	none	support	main	secondary	back	support	We present power low rank ensembles ( PLRE ) , a flexible framework for n-gram language modeling where ensembles of low rank matrices and tensors are used to obtain smoothed probability estimates of words in context .	PLRE training is efficient and our approach outperforms state-of-the-art modified Kneser Ney baselines in terms of perplexity on large corpora as well as on BLEU score in a downstream machine translation task .	CL_D14-1158_ab_1	CL_D14-1158_ab_3	CL_D14-1158_ab_1_3
CL	D14-1158	ab	3	1	result	proposal_implementation	support	none	secondary	main	forw	support	PLRE training is efficient and our approach outperforms state-of-the-art modified Kneser Ney baselines in terms of perplexity on large corpora as well as on BLEU score in a downstream machine translation task .	We present power low rank ensembles ( PLRE ) , a flexible framework for n-gram language modeling where ensembles of low rank matrices and tensors are used to obtain smoothed probability estimates of words in context .	CL_D14-1158_ab_3	CL_D14-1158_ab_1	CL_D14-1158_ab_1_3
CL	D14-1158	ab	2	3	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	Our method can be understood as a generalization of n-gram modeling to non-integer n , and includes standard techniques such as absolute discounting and Kneser-Ney smoothing as special cases .	PLRE training is efficient and our approach outperforms state-of-the-art modified Kneser Ney baselines in terms of perplexity on large corpora as well as on BLEU score in a downstream machine translation task .	CL_D14-1158_ab_2	CL_D14-1158_ab_3	CL_D14-1158_ab_2_3
CL	D14-1159	ab	1	2	motivation_background	proposal	support	none	secondary	main	forw	support	Machine reading calls for programs that read and understand text , but most current work only attempts to extract facts from redundant web-scale corpora .	In this paper , we focus on a new reading comprehension task that requires complex reasoning over a single document .	CL_D14-1159_ab_1	CL_D14-1159_ab_2	CL_D14-1159_ab_1_2
CL	D14-1159	ab	2	1	proposal	motivation_background	none	support	main	secondary	back	support	In this paper , we focus on a new reading comprehension task that requires complex reasoning over a single document .	Machine reading calls for programs that read and understand text , but most current work only attempts to extract facts from redundant web-scale corpora .	CL_D14-1159_ab_2	CL_D14-1159_ab_1	CL_D14-1159_ab_1_2
CL	D14-1159	ab	1	3	motivation_background	proposal_implementation	support	elaboration	secondary	secondary	none	none	Machine reading calls for programs that read and understand text , but most current work only attempts to extract facts from redundant web-scale corpora .	The input is a paragraph describing a biological process , and the goal is to answer questions that require an understanding of the relations between entities and events in the process .	CL_D14-1159_ab_1	CL_D14-1159_ab_3	CL_D14-1159_ab_1_3
CL	D14-1159	ab	1	4	motivation_background	proposal_implementation	support	elaboration	secondary	secondary	none	none	Machine reading calls for programs that read and understand text , but most current work only attempts to extract facts from redundant web-scale corpora .	To answer the questions , we first predict a rich structure representing the process in the paragraph .	CL_D14-1159_ab_1	CL_D14-1159_ab_4	CL_D14-1159_ab_1_4
CL	D14-1159	ab	1	5	motivation_background	proposal_implementation	support	sequence	secondary	secondary	none	none	Machine reading calls for programs that read and understand text , but most current work only attempts to extract facts from redundant web-scale corpora .	Then , we map the question to a formal query , which is executed against the predicted structure .	CL_D14-1159_ab_1	CL_D14-1159_ab_5	CL_D14-1159_ab_1_5
CL	D14-1159	ab	1	6	motivation_background	result	support	support	secondary	secondary	none	none	Machine reading calls for programs that read and understand text , but most current work only attempts to extract facts from redundant web-scale corpora .	We demonstrate that answering questions via predicted structures substantially improves accuracy over baselines that use shallower representations .	CL_D14-1159_ab_1	CL_D14-1159_ab_6	CL_D14-1159_ab_1_6
CL	D14-1159	ab	2	3	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	In this paper , we focus on a new reading comprehension task that requires complex reasoning over a single document .	The input is a paragraph describing a biological process , and the goal is to answer questions that require an understanding of the relations between entities and events in the process .	CL_D14-1159_ab_2	CL_D14-1159_ab_3	CL_D14-1159_ab_2_3
CL	D14-1159	ab	3	2	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	The input is a paragraph describing a biological process , and the goal is to answer questions that require an understanding of the relations between entities and events in the process .	In this paper , we focus on a new reading comprehension task that requires complex reasoning over a single document .	CL_D14-1159_ab_3	CL_D14-1159_ab_2	CL_D14-1159_ab_2_3
CL	D14-1159	ab	2	4	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	In this paper , we focus on a new reading comprehension task that requires complex reasoning over a single document .	To answer the questions , we first predict a rich structure representing the process in the paragraph .	CL_D14-1159_ab_2	CL_D14-1159_ab_4	CL_D14-1159_ab_2_4
CL	D14-1159	ab	2	5	proposal	proposal_implementation	none	sequence	main	secondary	none	none	In this paper , we focus on a new reading comprehension task that requires complex reasoning over a single document .	Then , we map the question to a formal query , which is executed against the predicted structure .	CL_D14-1159_ab_2	CL_D14-1159_ab_5	CL_D14-1159_ab_2_5
CL	D14-1159	ab	2	6	proposal	result	none	support	main	secondary	back	support	In this paper , we focus on a new reading comprehension task that requires complex reasoning over a single document .	We demonstrate that answering questions via predicted structures substantially improves accuracy over baselines that use shallower representations .	CL_D14-1159_ab_2	CL_D14-1159_ab_6	CL_D14-1159_ab_2_6
CL	D14-1159	ab	6	2	result	proposal	support	none	secondary	main	forw	support	We demonstrate that answering questions via predicted structures substantially improves accuracy over baselines that use shallower representations .	In this paper , we focus on a new reading comprehension task that requires complex reasoning over a single document .	CL_D14-1159_ab_6	CL_D14-1159_ab_2	CL_D14-1159_ab_2_6
CL	D14-1159	ab	3	4	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	The input is a paragraph describing a biological process , and the goal is to answer questions that require an understanding of the relations between entities and events in the process .	To answer the questions , we first predict a rich structure representing the process in the paragraph .	CL_D14-1159_ab_3	CL_D14-1159_ab_4	CL_D14-1159_ab_3_4
CL	D14-1159	ab	4	3	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	forw	elaboration	To answer the questions , we first predict a rich structure representing the process in the paragraph .	The input is a paragraph describing a biological process , and the goal is to answer questions that require an understanding of the relations between entities and events in the process .	CL_D14-1159_ab_4	CL_D14-1159_ab_3	CL_D14-1159_ab_3_4
CL	D14-1159	ab	3	5	proposal_implementation	proposal_implementation	elaboration	sequence	secondary	secondary	none	none	The input is a paragraph describing a biological process , and the goal is to answer questions that require an understanding of the relations between entities and events in the process .	Then , we map the question to a formal query , which is executed against the predicted structure .	CL_D14-1159_ab_3	CL_D14-1159_ab_5	CL_D14-1159_ab_3_5
CL	D14-1159	ab	3	6	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	The input is a paragraph describing a biological process , and the goal is to answer questions that require an understanding of the relations between entities and events in the process .	We demonstrate that answering questions via predicted structures substantially improves accuracy over baselines that use shallower representations .	CL_D14-1159_ab_3	CL_D14-1159_ab_6	CL_D14-1159_ab_3_6
CL	D14-1159	ab	4	5	proposal_implementation	proposal_implementation	elaboration	sequence	secondary	secondary	back	sequence	To answer the questions , we first predict a rich structure representing the process in the paragraph .	Then , we map the question to a formal query , which is executed against the predicted structure .	CL_D14-1159_ab_4	CL_D14-1159_ab_5	CL_D14-1159_ab_4_5
CL	D14-1159	ab	5	4	proposal_implementation	proposal_implementation	sequence	elaboration	secondary	secondary	forw	sequence	Then , we map the question to a formal query , which is executed against the predicted structure .	To answer the questions , we first predict a rich structure representing the process in the paragraph .	CL_D14-1159_ab_5	CL_D14-1159_ab_4	CL_D14-1159_ab_4_5
CL	D14-1159	ab	4	6	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	To answer the questions , we first predict a rich structure representing the process in the paragraph .	We demonstrate that answering questions via predicted structures substantially improves accuracy over baselines that use shallower representations .	CL_D14-1159_ab_4	CL_D14-1159_ab_6	CL_D14-1159_ab_4_6
CL	D14-1159	ab	5	6	proposal_implementation	result	sequence	support	secondary	secondary	none	none	Then , we map the question to a formal query , which is executed against the predicted structure .	We demonstrate that answering questions via predicted structures substantially improves accuracy over baselines that use shallower representations .	CL_D14-1159_ab_5	CL_D14-1159_ab_6	CL_D14-1159_ab_5_6
CL	D14-1160	ab	1	2	motivation_background	motivation_hypothesis	info-required	info-required	secondary	secondary	forw	info-required	Connecting words with senses , namely , sight , hearing , taste , smell and touch , to comprehend the sensorial information in language is a straightforward task for humans by using commonsense knowledge .	With this in mind , a lexicon associating words with senses would be crucial for the computational tasks aiming at interpretation of language .	CL_D14-1160_ab_1	CL_D14-1160_ab_2	CL_D14-1160_ab_1_2
CL	D14-1160	ab	2	1	motivation_hypothesis	motivation_background	info-required	info-required	secondary	secondary	back	info-required	With this in mind , a lexicon associating words with senses would be crucial for the computational tasks aiming at interpretation of language .	Connecting words with senses , namely , sight , hearing , taste , smell and touch , to comprehend the sensorial information in language is a straightforward task for humans by using commonsense knowledge .	CL_D14-1160_ab_2	CL_D14-1160_ab_1	CL_D14-1160_ab_1_2
CL	D14-1160	ab	1	3	motivation_background	motivation_problem	info-required	support	secondary	secondary	none	none	Connecting words with senses , namely , sight , hearing , taste , smell and touch , to comprehend the sensorial information in language is a straightforward task for humans by using commonsense knowledge .	However , to the best of our knowledge , there is no systematic attempt in the literature to build such a resource .	CL_D14-1160_ab_1	CL_D14-1160_ab_3	CL_D14-1160_ab_1_3
CL	D14-1160	ab	1	4	motivation_background	proposal	info-required	none	secondary	main	none	none	Connecting words with senses , namely , sight , hearing , taste , smell and touch , to comprehend the sensorial information in language is a straightforward task for humans by using commonsense knowledge .	In this paper , we present a sensorial lexicon that associates English words with senses .	CL_D14-1160_ab_1	CL_D14-1160_ab_4	CL_D14-1160_ab_1_4
CL	D14-1160	ab	1	5	motivation_background	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	Connecting words with senses , namely , sight , hearing , taste , smell and touch , to comprehend the sensorial information in language is a straightforward task for humans by using commonsense knowledge .	To obtain this resource , we apply a computational method based on bootstrapping and corpus statistics .	CL_D14-1160_ab_1	CL_D14-1160_ab_5	CL_D14-1160_ab_1_5
CL	D14-1160	ab	1	6	motivation_background	means	info-required	by-means	secondary	secondary	none	none	Connecting words with senses , namely , sight , hearing , taste , smell and touch , to comprehend the sensorial information in language is a straightforward task for humans by using commonsense knowledge .	The quality of the resulting lexicon is evaluated with a gold standard created via crowdsourcing .	CL_D14-1160_ab_1	CL_D14-1160_ab_6	CL_D14-1160_ab_1_6
CL	D14-1160	ab	1	7	motivation_background	result	info-required	support	secondary	secondary	none	none	Connecting words with senses , namely , sight , hearing , taste , smell and touch , to comprehend the sensorial information in language is a straightforward task for humans by using commonsense knowledge .	The results show that a simple classifier relying on the lexicon outperforms two baselines on a sensory classification task , both at word and sentence level , and confirm the soundness of the proposed approach for the construction of the lexicon and the usefulness of the resource for computational applications .	CL_D14-1160_ab_1	CL_D14-1160_ab_7	CL_D14-1160_ab_1_7
CL	D14-1160	ab	2	3	motivation_hypothesis	motivation_problem	info-required	support	secondary	secondary	forw	info-required	With this in mind , a lexicon associating words with senses would be crucial for the computational tasks aiming at interpretation of language .	However , to the best of our knowledge , there is no systematic attempt in the literature to build such a resource .	CL_D14-1160_ab_2	CL_D14-1160_ab_3	CL_D14-1160_ab_2_3
CL	D14-1160	ab	3	2	motivation_problem	motivation_hypothesis	support	info-required	secondary	secondary	back	info-required	However , to the best of our knowledge , there is no systematic attempt in the literature to build such a resource .	With this in mind , a lexicon associating words with senses would be crucial for the computational tasks aiming at interpretation of language .	CL_D14-1160_ab_3	CL_D14-1160_ab_2	CL_D14-1160_ab_2_3
CL	D14-1160	ab	2	4	motivation_hypothesis	proposal	info-required	none	secondary	main	none	none	With this in mind , a lexicon associating words with senses would be crucial for the computational tasks aiming at interpretation of language .	In this paper , we present a sensorial lexicon that associates English words with senses .	CL_D14-1160_ab_2	CL_D14-1160_ab_4	CL_D14-1160_ab_2_4
CL	D14-1160	ab	2	5	motivation_hypothesis	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	With this in mind , a lexicon associating words with senses would be crucial for the computational tasks aiming at interpretation of language .	To obtain this resource , we apply a computational method based on bootstrapping and corpus statistics .	CL_D14-1160_ab_2	CL_D14-1160_ab_5	CL_D14-1160_ab_2_5
CL	D14-1160	ab	2	6	motivation_hypothesis	means	info-required	by-means	secondary	secondary	none	none	With this in mind , a lexicon associating words with senses would be crucial for the computational tasks aiming at interpretation of language .	The quality of the resulting lexicon is evaluated with a gold standard created via crowdsourcing .	CL_D14-1160_ab_2	CL_D14-1160_ab_6	CL_D14-1160_ab_2_6
CL	D14-1160	ab	2	7	motivation_hypothesis	result	info-required	support	secondary	secondary	none	none	With this in mind , a lexicon associating words with senses would be crucial for the computational tasks aiming at interpretation of language .	The results show that a simple classifier relying on the lexicon outperforms two baselines on a sensory classification task , both at word and sentence level , and confirm the soundness of the proposed approach for the construction of the lexicon and the usefulness of the resource for computational applications .	CL_D14-1160_ab_2	CL_D14-1160_ab_7	CL_D14-1160_ab_2_7
CL	D14-1160	ab	3	4	motivation_problem	proposal	support	none	secondary	main	forw	support	However , to the best of our knowledge , there is no systematic attempt in the literature to build such a resource .	In this paper , we present a sensorial lexicon that associates English words with senses .	CL_D14-1160_ab_3	CL_D14-1160_ab_4	CL_D14-1160_ab_3_4
CL	D14-1160	ab	4	3	proposal	motivation_problem	none	support	main	secondary	back	support	In this paper , we present a sensorial lexicon that associates English words with senses .	However , to the best of our knowledge , there is no systematic attempt in the literature to build such a resource .	CL_D14-1160_ab_4	CL_D14-1160_ab_3	CL_D14-1160_ab_3_4
CL	D14-1160	ab	3	5	motivation_problem	proposal_implementation	support	elaboration	secondary	secondary	none	none	However , to the best of our knowledge , there is no systematic attempt in the literature to build such a resource .	To obtain this resource , we apply a computational method based on bootstrapping and corpus statistics .	CL_D14-1160_ab_3	CL_D14-1160_ab_5	CL_D14-1160_ab_3_5
CL	D14-1160	ab	3	6	motivation_problem	means	support	by-means	secondary	secondary	none	none	However , to the best of our knowledge , there is no systematic attempt in the literature to build such a resource .	The quality of the resulting lexicon is evaluated with a gold standard created via crowdsourcing .	CL_D14-1160_ab_3	CL_D14-1160_ab_6	CL_D14-1160_ab_3_6
CL	D14-1160	ab	3	7	motivation_problem	result	support	support	secondary	secondary	none	none	However , to the best of our knowledge , there is no systematic attempt in the literature to build such a resource .	The results show that a simple classifier relying on the lexicon outperforms two baselines on a sensory classification task , both at word and sentence level , and confirm the soundness of the proposed approach for the construction of the lexicon and the usefulness of the resource for computational applications .	CL_D14-1160_ab_3	CL_D14-1160_ab_7	CL_D14-1160_ab_3_7
CL	D14-1160	ab	4	5	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	In this paper , we present a sensorial lexicon that associates English words with senses .	To obtain this resource , we apply a computational method based on bootstrapping and corpus statistics .	CL_D14-1160_ab_4	CL_D14-1160_ab_5	CL_D14-1160_ab_4_5
CL	D14-1160	ab	5	4	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	To obtain this resource , we apply a computational method based on bootstrapping and corpus statistics .	In this paper , we present a sensorial lexicon that associates English words with senses .	CL_D14-1160_ab_5	CL_D14-1160_ab_4	CL_D14-1160_ab_4_5
CL	D14-1160	ab	4	6	proposal	means	none	by-means	main	secondary	none	none	In this paper , we present a sensorial lexicon that associates English words with senses .	The quality of the resulting lexicon is evaluated with a gold standard created via crowdsourcing .	CL_D14-1160_ab_4	CL_D14-1160_ab_6	CL_D14-1160_ab_4_6
CL	D14-1160	ab	4	7	proposal	result	none	support	main	secondary	back	support	In this paper , we present a sensorial lexicon that associates English words with senses .	The results show that a simple classifier relying on the lexicon outperforms two baselines on a sensory classification task , both at word and sentence level , and confirm the soundness of the proposed approach for the construction of the lexicon and the usefulness of the resource for computational applications .	CL_D14-1160_ab_4	CL_D14-1160_ab_7	CL_D14-1160_ab_4_7
CL	D14-1160	ab	7	4	result	proposal	support	none	secondary	main	forw	support	The results show that a simple classifier relying on the lexicon outperforms two baselines on a sensory classification task , both at word and sentence level , and confirm the soundness of the proposed approach for the construction of the lexicon and the usefulness of the resource for computational applications .	In this paper , we present a sensorial lexicon that associates English words with senses .	CL_D14-1160_ab_7	CL_D14-1160_ab_4	CL_D14-1160_ab_4_7
CL	D14-1160	ab	5	6	proposal_implementation	means	elaboration	by-means	secondary	secondary	none	none	To obtain this resource , we apply a computational method based on bootstrapping and corpus statistics .	The quality of the resulting lexicon is evaluated with a gold standard created via crowdsourcing .	CL_D14-1160_ab_5	CL_D14-1160_ab_6	CL_D14-1160_ab_5_6
CL	D14-1160	ab	5	7	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	To obtain this resource , we apply a computational method based on bootstrapping and corpus statistics .	The results show that a simple classifier relying on the lexicon outperforms two baselines on a sensory classification task , both at word and sentence level , and confirm the soundness of the proposed approach for the construction of the lexicon and the usefulness of the resource for computational applications .	CL_D14-1160_ab_5	CL_D14-1160_ab_7	CL_D14-1160_ab_5_7
CL	D14-1160	ab	6	7	means	result	by-means	support	secondary	secondary	forw	by-means	The quality of the resulting lexicon is evaluated with a gold standard created via crowdsourcing .	The results show that a simple classifier relying on the lexicon outperforms two baselines on a sensory classification task , both at word and sentence level , and confirm the soundness of the proposed approach for the construction of the lexicon and the usefulness of the resource for computational applications .	CL_D14-1160_ab_6	CL_D14-1160_ab_7	CL_D14-1160_ab_6_7
CL	D14-1160	ab	7	6	result	means	support	by-means	secondary	secondary	back	by-means	The results show that a simple classifier relying on the lexicon outperforms two baselines on a sensory classification task , both at word and sentence level , and confirm the soundness of the proposed approach for the construction of the lexicon and the usefulness of the resource for computational applications .	The quality of the resulting lexicon is evaluated with a gold standard created via crowdsourcing .	CL_D14-1160_ab_7	CL_D14-1160_ab_6	CL_D14-1160_ab_6_7
CL	D14-1161	ab	1	2	motivation_background	proposal	support	none	secondary	main	forw	support	Many forms of word relatedness have been developed , providing different perspectives on word similarity .	We introduce a Bayesian probabilistic tensor factorization model for synthesizing a single word vector representation and per-perspective linear transformations from any number of word similarity matrices .	CL_D14-1161_ab_1	CL_D14-1161_ab_2	CL_D14-1161_ab_1_2
CL	D14-1161	ab	2	1	proposal	motivation_background	none	support	main	secondary	back	support	We introduce a Bayesian probabilistic tensor factorization model for synthesizing a single word vector representation and per-perspective linear transformations from any number of word similarity matrices .	Many forms of word relatedness have been developed , providing different perspectives on word similarity .	CL_D14-1161_ab_2	CL_D14-1161_ab_1	CL_D14-1161_ab_1_2
CL	D14-1161	ab	1	3	motivation_background	proposal	support	elaboration	secondary	secondary	none	none	Many forms of word relatedness have been developed , providing different perspectives on word similarity .	The resulting word vectors , when combined with the per-perspective linear transformation , approximately recreate while also regularizing and generalizing , each word similarity perspective .	CL_D14-1161_ab_1	CL_D14-1161_ab_3	CL_D14-1161_ab_1_3
CL	D14-1161	ab	1	4	motivation_background	conclusion	support	support	secondary	secondary	none	none	Many forms of word relatedness have been developed , providing different perspectives on word similarity .	Our method can combine manually created semantic resources with neural word embeddings to separate synonyms and antonyms , and is capable of generalizing to words outside the vocabulary of any particular perspective .	CL_D14-1161_ab_1	CL_D14-1161_ab_4	CL_D14-1161_ab_1_4
CL	D14-1161	ab	1	5	motivation_background	result_means	support	support	secondary	secondary	none	none	Many forms of word relatedness have been developed , providing different perspectives on word similarity .	We evaluated the word embeddings with GRE antonym questions , the result achieves the state-of-the-art performance .	CL_D14-1161_ab_1	CL_D14-1161_ab_5	CL_D14-1161_ab_1_5
CL	D14-1161	ab	2	3	proposal	proposal	none	elaboration	main	secondary	back	elaboration	We introduce a Bayesian probabilistic tensor factorization model for synthesizing a single word vector representation and per-perspective linear transformations from any number of word similarity matrices .	The resulting word vectors , when combined with the per-perspective linear transformation , approximately recreate while also regularizing and generalizing , each word similarity perspective .	CL_D14-1161_ab_2	CL_D14-1161_ab_3	CL_D14-1161_ab_2_3
CL	D14-1161	ab	3	2	proposal	proposal	elaboration	none	secondary	main	forw	elaboration	The resulting word vectors , when combined with the per-perspective linear transformation , approximately recreate while also regularizing and generalizing , each word similarity perspective .	We introduce a Bayesian probabilistic tensor factorization model for synthesizing a single word vector representation and per-perspective linear transformations from any number of word similarity matrices .	CL_D14-1161_ab_3	CL_D14-1161_ab_2	CL_D14-1161_ab_2_3
CL	D14-1161	ab	2	4	proposal	conclusion	none	support	main	secondary	back	support	We introduce a Bayesian probabilistic tensor factorization model for synthesizing a single word vector representation and per-perspective linear transformations from any number of word similarity matrices .	Our method can combine manually created semantic resources with neural word embeddings to separate synonyms and antonyms , and is capable of generalizing to words outside the vocabulary of any particular perspective .	CL_D14-1161_ab_2	CL_D14-1161_ab_4	CL_D14-1161_ab_2_4
CL	D14-1161	ab	4	2	conclusion	proposal	support	none	secondary	main	forw	support	Our method can combine manually created semantic resources with neural word embeddings to separate synonyms and antonyms , and is capable of generalizing to words outside the vocabulary of any particular perspective .	We introduce a Bayesian probabilistic tensor factorization model for synthesizing a single word vector representation and per-perspective linear transformations from any number of word similarity matrices .	CL_D14-1161_ab_4	CL_D14-1161_ab_2	CL_D14-1161_ab_2_4
CL	D14-1161	ab	2	5	proposal	result_means	none	support	main	secondary	none	none	We introduce a Bayesian probabilistic tensor factorization model for synthesizing a single word vector representation and per-perspective linear transformations from any number of word similarity matrices .	We evaluated the word embeddings with GRE antonym questions , the result achieves the state-of-the-art performance .	CL_D14-1161_ab_2	CL_D14-1161_ab_5	CL_D14-1161_ab_2_5
CL	D14-1161	ab	3	4	proposal	conclusion	elaboration	support	secondary	secondary	none	none	The resulting word vectors , when combined with the per-perspective linear transformation , approximately recreate while also regularizing and generalizing , each word similarity perspective .	Our method can combine manually created semantic resources with neural word embeddings to separate synonyms and antonyms , and is capable of generalizing to words outside the vocabulary of any particular perspective .	CL_D14-1161_ab_3	CL_D14-1161_ab_4	CL_D14-1161_ab_3_4
CL	D14-1161	ab	3	5	proposal	result_means	elaboration	support	secondary	secondary	none	none	The resulting word vectors , when combined with the per-perspective linear transformation , approximately recreate while also regularizing and generalizing , each word similarity perspective .	We evaluated the word embeddings with GRE antonym questions , the result achieves the state-of-the-art performance .	CL_D14-1161_ab_3	CL_D14-1161_ab_5	CL_D14-1161_ab_3_5
CL	D14-1161	ab	4	5	conclusion	result_means	support	support	secondary	secondary	back	support	Our method can combine manually created semantic resources with neural word embeddings to separate synonyms and antonyms , and is capable of generalizing to words outside the vocabulary of any particular perspective .	We evaluated the word embeddings with GRE antonym questions , the result achieves the state-of-the-art performance .	CL_D14-1161_ab_4	CL_D14-1161_ab_5	CL_D14-1161_ab_4_5
CL	D14-1161	ab	5	4	result_means	conclusion	support	support	secondary	secondary	forw	support	We evaluated the word embeddings with GRE antonym questions , the result achieves the state-of-the-art performance .	Our method can combine manually created semantic resources with neural word embeddings to separate synonyms and antonyms , and is capable of generalizing to words outside the vocabulary of any particular perspective .	CL_D14-1161_ab_5	CL_D14-1161_ab_4	CL_D14-1161_ab_4_5
CL	D14-1162	ab	1	2	motivation_background	proposal	support	none	secondary	main	forw	support	Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic , but the origin of these regularities has remained opaque .	We analyze and make explicit the model properties needed for such regularities to emerge in word vectors .	CL_D14-1162_ab_1	CL_D14-1162_ab_2	CL_D14-1162_ab_1_2
CL	D14-1162	ab	2	1	proposal	motivation_background	none	support	main	secondary	back	support	We analyze and make explicit the model properties needed for such regularities to emerge in word vectors .	Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic , but the origin of these regularities has remained opaque .	CL_D14-1162_ab_2	CL_D14-1162_ab_1	CL_D14-1162_ab_1_2
CL	D14-1162	ab	1	3	motivation_background	proposal_implementation	support	elaboration	secondary	secondary	none	none	Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic , but the origin of these regularities has remained opaque .	The result is a new global log-bilinear regression model that combines the advantages of the two major model families in the literature : global matrix factorization and local context window methods .	CL_D14-1162_ab_1	CL_D14-1162_ab_3	CL_D14-1162_ab_1_3
CL	D14-1162	ab	1	4	motivation_background	proposal_implementation	support	elaboration	secondary	secondary	none	none	Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic , but the origin of these regularities has remained opaque .	Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word co-occurrence matrix , rather than on the entire sparse matrix or on individual context windows in a large corpus .	CL_D14-1162_ab_1	CL_D14-1162_ab_4	CL_D14-1162_ab_1_4
CL	D14-1162	ab	1	5	motivation_background	observation	support	support	secondary	secondary	none	none	Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic , but the origin of these regularities has remained opaque .	The model produces a vector space with meaningful sub-structure , as evidenced by its performance of 75 % on a recent word analogy task .	CL_D14-1162_ab_1	CL_D14-1162_ab_5	CL_D14-1162_ab_1_5
CL	D14-1162	ab	1	6	motivation_background	result	support	elaboration	secondary	secondary	none	none	Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic , but the origin of these regularities has remained opaque .	It also outperforms related models on similarity tasks and named entity recognition .	CL_D14-1162_ab_1	CL_D14-1162_ab_6	CL_D14-1162_ab_1_6
CL	D14-1162	ab	2	3	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	We analyze and make explicit the model properties needed for such regularities to emerge in word vectors .	The result is a new global log-bilinear regression model that combines the advantages of the two major model families in the literature : global matrix factorization and local context window methods .	CL_D14-1162_ab_2	CL_D14-1162_ab_3	CL_D14-1162_ab_2_3
CL	D14-1162	ab	3	2	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	The result is a new global log-bilinear regression model that combines the advantages of the two major model families in the literature : global matrix factorization and local context window methods .	We analyze and make explicit the model properties needed for such regularities to emerge in word vectors .	CL_D14-1162_ab_3	CL_D14-1162_ab_2	CL_D14-1162_ab_2_3
CL	D14-1162	ab	2	4	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	We analyze and make explicit the model properties needed for such regularities to emerge in word vectors .	Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word co-occurrence matrix , rather than on the entire sparse matrix or on individual context windows in a large corpus .	CL_D14-1162_ab_2	CL_D14-1162_ab_4	CL_D14-1162_ab_2_4
CL	D14-1162	ab	2	5	proposal	observation	none	support	main	secondary	none	none	We analyze and make explicit the model properties needed for such regularities to emerge in word vectors .	The model produces a vector space with meaningful sub-structure , as evidenced by its performance of 75 % on a recent word analogy task .	CL_D14-1162_ab_2	CL_D14-1162_ab_5	CL_D14-1162_ab_2_5
CL	D14-1162	ab	2	6	proposal	result	none	elaboration	main	secondary	none	none	We analyze and make explicit the model properties needed for such regularities to emerge in word vectors .	It also outperforms related models on similarity tasks and named entity recognition .	CL_D14-1162_ab_2	CL_D14-1162_ab_6	CL_D14-1162_ab_2_6
CL	D14-1162	ab	3	4	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	The result is a new global log-bilinear regression model that combines the advantages of the two major model families in the literature : global matrix factorization and local context window methods .	Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word co-occurrence matrix , rather than on the entire sparse matrix or on individual context windows in a large corpus .	CL_D14-1162_ab_3	CL_D14-1162_ab_4	CL_D14-1162_ab_3_4
CL	D14-1162	ab	4	3	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	forw	elaboration	Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word co-occurrence matrix , rather than on the entire sparse matrix or on individual context windows in a large corpus .	The result is a new global log-bilinear regression model that combines the advantages of the two major model families in the literature : global matrix factorization and local context window methods .	CL_D14-1162_ab_4	CL_D14-1162_ab_3	CL_D14-1162_ab_3_4
CL	D14-1162	ab	3	5	proposal_implementation	observation	elaboration	support	secondary	secondary	back	support	The result is a new global log-bilinear regression model that combines the advantages of the two major model families in the literature : global matrix factorization and local context window methods .	The model produces a vector space with meaningful sub-structure , as evidenced by its performance of 75 % on a recent word analogy task .	CL_D14-1162_ab_3	CL_D14-1162_ab_5	CL_D14-1162_ab_3_5
CL	D14-1162	ab	5	3	observation	proposal_implementation	support	elaboration	secondary	secondary	forw	support	The model produces a vector space with meaningful sub-structure , as evidenced by its performance of 75 % on a recent word analogy task .	The result is a new global log-bilinear regression model that combines the advantages of the two major model families in the literature : global matrix factorization and local context window methods .	CL_D14-1162_ab_5	CL_D14-1162_ab_3	CL_D14-1162_ab_3_5
CL	D14-1162	ab	3	6	proposal_implementation	result	elaboration	elaboration	secondary	secondary	none	none	The result is a new global log-bilinear regression model that combines the advantages of the two major model families in the literature : global matrix factorization and local context window methods .	It also outperforms related models on similarity tasks and named entity recognition .	CL_D14-1162_ab_3	CL_D14-1162_ab_6	CL_D14-1162_ab_3_6
CL	D14-1162	ab	4	5	proposal_implementation	observation	elaboration	support	secondary	secondary	none	none	Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word co-occurrence matrix , rather than on the entire sparse matrix or on individual context windows in a large corpus .	The model produces a vector space with meaningful sub-structure , as evidenced by its performance of 75 % on a recent word analogy task .	CL_D14-1162_ab_4	CL_D14-1162_ab_5	CL_D14-1162_ab_4_5
CL	D14-1162	ab	4	6	proposal_implementation	result	elaboration	elaboration	secondary	secondary	none	none	Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word co-occurrence matrix , rather than on the entire sparse matrix or on individual context windows in a large corpus .	It also outperforms related models on similarity tasks and named entity recognition .	CL_D14-1162_ab_4	CL_D14-1162_ab_6	CL_D14-1162_ab_4_6
CL	D14-1162	ab	5	6	observation	result	support	elaboration	secondary	secondary	back	elaboration	The model produces a vector space with meaningful sub-structure , as evidenced by its performance of 75 % on a recent word analogy task .	It also outperforms related models on similarity tasks and named entity recognition .	CL_D14-1162_ab_5	CL_D14-1162_ab_6	CL_D14-1162_ab_5_6
CL	D14-1162	ab	6	5	result	observation	elaboration	support	secondary	secondary	forw	elaboration	It also outperforms related models on similarity tasks and named entity recognition .	The model produces a vector space with meaningful sub-structure , as evidenced by its performance of 75 % on a recent word analogy task .	CL_D14-1162_ab_6	CL_D14-1162_ab_5	CL_D14-1162_ab_5_6
CL	D14-1163	ab	1	2	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	We introduce a novel compositional language model that works on Predicate-Argument Structures ( PASs ) .	Our model jointly learns word representations and their composition functions using bag-of-words and dependency-based contexts .	CL_D14-1163_ab_1	CL_D14-1163_ab_2	CL_D14-1163_ab_1_2
CL	D14-1163	ab	1	3	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	We introduce a novel compositional language model that works on Predicate-Argument Structures ( PASs ) .	Unlike previous word-sequence-based models , our PAS-based model composes arguments into predicates by using the category information from the PAS .	CL_D14-1163_ab_1	CL_D14-1163_ab_3	CL_D14-1163_ab_1_3
CL	D14-1163	ab	1	4	proposal	proposal	none	elaboration	main	secondary	back	elaboration	We introduce a novel compositional language model that works on Predicate-Argument Structures ( PASs ) .	This enables our model to capture longrange dependencies between words and to better handle constructs such as verb-object and subject-verb-object relations .	CL_D14-1163_ab_1	CL_D14-1163_ab_4	CL_D14-1163_ab_1_4
CL	D14-1163	ab	4	1	proposal	proposal	elaboration	none	secondary	main	forw	elaboration	This enables our model to capture longrange dependencies between words and to better handle constructs such as verb-object and subject-verb-object relations .	We introduce a novel compositional language model that works on Predicate-Argument Structures ( PASs ) .	CL_D14-1163_ab_4	CL_D14-1163_ab_1	CL_D14-1163_ab_1_4
CL	D14-1163	ab	1	5	proposal	result_means	none	support	main	secondary	back	support	We introduce a novel compositional language model that works on Predicate-Argument Structures ( PASs ) .	We verify this experimentally using two phrase similarity datasets and achieve results comparable to or higher than the previous best results .	CL_D14-1163_ab_1	CL_D14-1163_ab_5	CL_D14-1163_ab_1_5
CL	D14-1163	ab	5	1	result_means	proposal	support	none	secondary	main	forw	support	We verify this experimentally using two phrase similarity datasets and achieve results comparable to or higher than the previous best results .	We introduce a novel compositional language model that works on Predicate-Argument Structures ( PASs ) .	CL_D14-1163_ab_5	CL_D14-1163_ab_1	CL_D14-1163_ab_1_5
CL	D14-1163	ab	1	6	proposal	result_means	none	elaboration	main	secondary	none	none	We introduce a novel compositional language model that works on Predicate-Argument Structures ( PASs ) .	Our system achieves these results without the need for pre-trained word vectors and using a much smaller training corpus ; despite this , for the subject-verb-object dataset our model improves upon the state of the art by as much as 10 % in relative performance .	CL_D14-1163_ab_1	CL_D14-1163_ab_6	CL_D14-1163_ab_1_6
CL	D14-1163	ab	2	3	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	Our model jointly learns word representations and their composition functions using bag-of-words and dependency-based contexts .	Unlike previous word-sequence-based models , our PAS-based model composes arguments into predicates by using the category information from the PAS .	CL_D14-1163_ab_2	CL_D14-1163_ab_3	CL_D14-1163_ab_2_3
CL	D14-1163	ab	3	2	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	forw	elaboration	Unlike previous word-sequence-based models , our PAS-based model composes arguments into predicates by using the category information from the PAS .	Our model jointly learns word representations and their composition functions using bag-of-words and dependency-based contexts .	CL_D14-1163_ab_3	CL_D14-1163_ab_2	CL_D14-1163_ab_2_3
CL	D14-1163	ab	2	4	proposal_implementation	proposal	elaboration	elaboration	secondary	secondary	forw	elaboration	Our model jointly learns word representations and their composition functions using bag-of-words and dependency-based contexts .	This enables our model to capture longrange dependencies between words and to better handle constructs such as verb-object and subject-verb-object relations .	CL_D14-1163_ab_2	CL_D14-1163_ab_4	CL_D14-1163_ab_2_4
CL	D14-1163	ab	4	2	proposal	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	This enables our model to capture longrange dependencies between words and to better handle constructs such as verb-object and subject-verb-object relations .	Our model jointly learns word representations and their composition functions using bag-of-words and dependency-based contexts .	CL_D14-1163_ab_4	CL_D14-1163_ab_2	CL_D14-1163_ab_2_4
CL	D14-1163	ab	2	5	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	Our model jointly learns word representations and their composition functions using bag-of-words and dependency-based contexts .	We verify this experimentally using two phrase similarity datasets and achieve results comparable to or higher than the previous best results .	CL_D14-1163_ab_2	CL_D14-1163_ab_5	CL_D14-1163_ab_2_5
CL	D14-1163	ab	2	6	proposal_implementation	result_means	elaboration	elaboration	secondary	secondary	none	none	Our model jointly learns word representations and their composition functions using bag-of-words and dependency-based contexts .	Our system achieves these results without the need for pre-trained word vectors and using a much smaller training corpus ; despite this , for the subject-verb-object dataset our model improves upon the state of the art by as much as 10 % in relative performance .	CL_D14-1163_ab_2	CL_D14-1163_ab_6	CL_D14-1163_ab_2_6
CL	D14-1163	ab	3	4	proposal_implementation	proposal	elaboration	elaboration	secondary	secondary	none	none	Unlike previous word-sequence-based models , our PAS-based model composes arguments into predicates by using the category information from the PAS .	This enables our model to capture longrange dependencies between words and to better handle constructs such as verb-object and subject-verb-object relations .	CL_D14-1163_ab_3	CL_D14-1163_ab_4	CL_D14-1163_ab_3_4
CL	D14-1163	ab	3	5	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	Unlike previous word-sequence-based models , our PAS-based model composes arguments into predicates by using the category information from the PAS .	We verify this experimentally using two phrase similarity datasets and achieve results comparable to or higher than the previous best results .	CL_D14-1163_ab_3	CL_D14-1163_ab_5	CL_D14-1163_ab_3_5
CL	D14-1163	ab	3	6	proposal_implementation	result_means	elaboration	elaboration	secondary	secondary	none	none	Unlike previous word-sequence-based models , our PAS-based model composes arguments into predicates by using the category information from the PAS .	Our system achieves these results without the need for pre-trained word vectors and using a much smaller training corpus ; despite this , for the subject-verb-object dataset our model improves upon the state of the art by as much as 10 % in relative performance .	CL_D14-1163_ab_3	CL_D14-1163_ab_6	CL_D14-1163_ab_3_6
CL	D14-1163	ab	4	5	proposal	result_means	elaboration	support	secondary	secondary	none	none	This enables our model to capture longrange dependencies between words and to better handle constructs such as verb-object and subject-verb-object relations .	We verify this experimentally using two phrase similarity datasets and achieve results comparable to or higher than the previous best results .	CL_D14-1163_ab_4	CL_D14-1163_ab_5	CL_D14-1163_ab_4_5
CL	D14-1163	ab	4	6	proposal	result_means	elaboration	elaboration	secondary	secondary	none	none	This enables our model to capture longrange dependencies between words and to better handle constructs such as verb-object and subject-verb-object relations .	Our system achieves these results without the need for pre-trained word vectors and using a much smaller training corpus ; despite this , for the subject-verb-object dataset our model improves upon the state of the art by as much as 10 % in relative performance .	CL_D14-1163_ab_4	CL_D14-1163_ab_6	CL_D14-1163_ab_4_6
CL	D14-1163	ab	5	6	result_means	result_means	support	elaboration	secondary	secondary	back	elaboration	We verify this experimentally using two phrase similarity datasets and achieve results comparable to or higher than the previous best results .	Our system achieves these results without the need for pre-trained word vectors and using a much smaller training corpus ; despite this , for the subject-verb-object dataset our model improves upon the state of the art by as much as 10 % in relative performance .	CL_D14-1163_ab_5	CL_D14-1163_ab_6	CL_D14-1163_ab_5_6
CL	D14-1163	ab	6	5	result_means	result_means	elaboration	support	secondary	secondary	forw	elaboration	Our system achieves these results without the need for pre-trained word vectors and using a much smaller training corpus ; despite this , for the subject-verb-object dataset our model improves upon the state of the art by as much as 10 % in relative performance .	We verify this experimentally using two phrase similarity datasets and achieve results comparable to or higher than the previous best results .	CL_D14-1163_ab_6	CL_D14-1163_ab_5	CL_D14-1163_ab_5_6
CL	D14-1164	ab	1	2	motivation_problem	proposal_implementation	support	none	secondary	main	forw	support	Broad-coverage relation extraction either requires expensive supervised training data , or suffers from drawbacks inherent to distant supervision .	We present an approach for providing partial supervision to a distantly supervised relation extractor using a small number of carefully selected examples .	CL_D14-1164_ab_1	CL_D14-1164_ab_2	CL_D14-1164_ab_1_2
CL	D14-1164	ab	2	1	proposal_implementation	motivation_problem	none	support	main	secondary	back	support	We present an approach for providing partial supervision to a distantly supervised relation extractor using a small number of carefully selected examples .	Broad-coverage relation extraction either requires expensive supervised training data , or suffers from drawbacks inherent to distant supervision .	CL_D14-1164_ab_2	CL_D14-1164_ab_1	CL_D14-1164_ab_1_2
CL	D14-1164	ab	1	3	motivation_problem	proposal	support	elaboration	secondary	secondary	none	none	Broad-coverage relation extraction either requires expensive supervised training data , or suffers from drawbacks inherent to distant supervision .	We compare against established active learning criteria and propose a novel criterion to sample examples which are both uncertain and representative .	CL_D14-1164_ab_1	CL_D14-1164_ab_3	CL_D14-1164_ab_1_3
CL	D14-1164	ab	1	4	motivation_problem	proposal	support	elaboration	secondary	secondary	none	none	Broad-coverage relation extraction either requires expensive supervised training data , or suffers from drawbacks inherent to distant supervision .	In this way , we combine the benefits of fine-grained supervision for difficult examples with the coverage of a large distantly supervised corpus .	CL_D14-1164_ab_1	CL_D14-1164_ab_4	CL_D14-1164_ab_1_4
CL	D14-1164	ab	1	5	motivation_problem	observation	support	support	secondary	secondary	none	none	Broad-coverage relation extraction either requires expensive supervised training data , or suffers from drawbacks inherent to distant supervision .	Our approach gives a substantial increase of 3.9 % end-to-end F1 on the 2013 KBP Slot Filling evaluation , yielding a net F1 of 37.7 % .	CL_D14-1164_ab_1	CL_D14-1164_ab_5	CL_D14-1164_ab_1_5
CL	D14-1164	ab	2	3	proposal_implementation	proposal	none	elaboration	main	secondary	back	elaboration	We present an approach for providing partial supervision to a distantly supervised relation extractor using a small number of carefully selected examples .	We compare against established active learning criteria and propose a novel criterion to sample examples which are both uncertain and representative .	CL_D14-1164_ab_2	CL_D14-1164_ab_3	CL_D14-1164_ab_2_3
CL	D14-1164	ab	3	2	proposal	proposal_implementation	elaboration	none	secondary	main	forw	elaboration	We compare against established active learning criteria and propose a novel criterion to sample examples which are both uncertain and representative .	We present an approach for providing partial supervision to a distantly supervised relation extractor using a small number of carefully selected examples .	CL_D14-1164_ab_3	CL_D14-1164_ab_2	CL_D14-1164_ab_2_3
CL	D14-1164	ab	2	4	proposal_implementation	proposal	none	elaboration	main	secondary	none	none	We present an approach for providing partial supervision to a distantly supervised relation extractor using a small number of carefully selected examples .	In this way , we combine the benefits of fine-grained supervision for difficult examples with the coverage of a large distantly supervised corpus .	CL_D14-1164_ab_2	CL_D14-1164_ab_4	CL_D14-1164_ab_2_4
CL	D14-1164	ab	2	5	proposal_implementation	observation	none	support	main	secondary	back	support	We present an approach for providing partial supervision to a distantly supervised relation extractor using a small number of carefully selected examples .	Our approach gives a substantial increase of 3.9 % end-to-end F1 on the 2013 KBP Slot Filling evaluation , yielding a net F1 of 37.7 % .	CL_D14-1164_ab_2	CL_D14-1164_ab_5	CL_D14-1164_ab_2_5
CL	D14-1164	ab	5	2	observation	proposal_implementation	support	none	secondary	main	forw	support	Our approach gives a substantial increase of 3.9 % end-to-end F1 on the 2013 KBP Slot Filling evaluation , yielding a net F1 of 37.7 % .	We present an approach for providing partial supervision to a distantly supervised relation extractor using a small number of carefully selected examples .	CL_D14-1164_ab_5	CL_D14-1164_ab_2	CL_D14-1164_ab_2_5
CL	D14-1164	ab	3	4	proposal	proposal	elaboration	elaboration	secondary	secondary	back	elaboration	We compare against established active learning criteria and propose a novel criterion to sample examples which are both uncertain and representative .	In this way , we combine the benefits of fine-grained supervision for difficult examples with the coverage of a large distantly supervised corpus .	CL_D14-1164_ab_3	CL_D14-1164_ab_4	CL_D14-1164_ab_3_4
CL	D14-1164	ab	4	3	proposal	proposal	elaboration	elaboration	secondary	secondary	forw	elaboration	In this way , we combine the benefits of fine-grained supervision for difficult examples with the coverage of a large distantly supervised corpus .	We compare against established active learning criteria and propose a novel criterion to sample examples which are both uncertain and representative .	CL_D14-1164_ab_4	CL_D14-1164_ab_3	CL_D14-1164_ab_3_4
CL	D14-1164	ab	3	5	proposal	observation	elaboration	support	secondary	secondary	none	none	We compare against established active learning criteria and propose a novel criterion to sample examples which are both uncertain and representative .	Our approach gives a substantial increase of 3.9 % end-to-end F1 on the 2013 KBP Slot Filling evaluation , yielding a net F1 of 37.7 % .	CL_D14-1164_ab_3	CL_D14-1164_ab_5	CL_D14-1164_ab_3_5
CL	D14-1164	ab	4	5	proposal	observation	elaboration	support	secondary	secondary	none	none	In this way , we combine the benefits of fine-grained supervision for difficult examples with the coverage of a large distantly supervised corpus .	Our approach gives a substantial increase of 3.9 % end-to-end F1 on the 2013 KBP Slot Filling evaluation , yielding a net F1 of 37.7 % .	CL_D14-1164_ab_4	CL_D14-1164_ab_5	CL_D14-1164_ab_4_5
CL	D14-1165	ab	1	2	motivation_background	proposal	support	none	secondary	main	forw	support	While relation extraction has traditionally been viewed as a task relying solely on textual data , recent work has shown that by taking as input existing facts in the form of entity-relation triples from both knowledge bases and textual data , the performance of relation extraction can be improved significantly .	Following this new paradigm , we propose a tensor decomposition approach for knowledge base embedding that is highly scalable , and is especially suitable for relation extraction .	CL_D14-1165_ab_1	CL_D14-1165_ab_2	CL_D14-1165_ab_1_2
CL	D14-1165	ab	2	1	proposal	motivation_background	none	support	main	secondary	back	support	Following this new paradigm , we propose a tensor decomposition approach for knowledge base embedding that is highly scalable , and is especially suitable for relation extraction .	While relation extraction has traditionally been viewed as a task relying solely on textual data , recent work has shown that by taking as input existing facts in the form of entity-relation triples from both knowledge bases and textual data , the performance of relation extraction can be improved significantly .	CL_D14-1165_ab_2	CL_D14-1165_ab_1	CL_D14-1165_ab_1_2
CL	D14-1165	ab	1	3	motivation_background	result_means	support	support	secondary	secondary	none	none	While relation extraction has traditionally been viewed as a task relying solely on textual data , recent work has shown that by taking as input existing facts in the form of entity-relation triples from both knowledge bases and textual data , the performance of relation extraction can be improved significantly .	By leveraging relational domain knowledge about entity type information , our learning algorithm is significantly faster than previous approaches and is better able to discover new relations missing from the database .	CL_D14-1165_ab_1	CL_D14-1165_ab_3	CL_D14-1165_ab_1_3
CL	D14-1165	ab	1	4	motivation_background	result	support	elaboration	secondary	secondary	none	none	While relation extraction has traditionally been viewed as a task relying solely on textual data , recent work has shown that by taking as input existing facts in the form of entity-relation triples from both knowledge bases and textual data , the performance of relation extraction can be improved significantly .	In addition , when applied to a relation extraction task , our approach alone is comparable to several existing systems , and improves the weighted mean average precision of a state-of-the-art method by 10 points when used as a subcomponent .	CL_D14-1165_ab_1	CL_D14-1165_ab_4	CL_D14-1165_ab_1_4
CL	D14-1165	ab	2	3	proposal	result_means	none	support	main	secondary	back	support	Following this new paradigm , we propose a tensor decomposition approach for knowledge base embedding that is highly scalable , and is especially suitable for relation extraction .	By leveraging relational domain knowledge about entity type information , our learning algorithm is significantly faster than previous approaches and is better able to discover new relations missing from the database .	CL_D14-1165_ab_2	CL_D14-1165_ab_3	CL_D14-1165_ab_2_3
CL	D14-1165	ab	3	2	result_means	proposal	support	none	secondary	main	forw	support	By leveraging relational domain knowledge about entity type information , our learning algorithm is significantly faster than previous approaches and is better able to discover new relations missing from the database .	Following this new paradigm , we propose a tensor decomposition approach for knowledge base embedding that is highly scalable , and is especially suitable for relation extraction .	CL_D14-1165_ab_3	CL_D14-1165_ab_2	CL_D14-1165_ab_2_3
CL	D14-1165	ab	2	4	proposal	result	none	elaboration	main	secondary	none	none	Following this new paradigm , we propose a tensor decomposition approach for knowledge base embedding that is highly scalable , and is especially suitable for relation extraction .	In addition , when applied to a relation extraction task , our approach alone is comparable to several existing systems , and improves the weighted mean average precision of a state-of-the-art method by 10 points when used as a subcomponent .	CL_D14-1165_ab_2	CL_D14-1165_ab_4	CL_D14-1165_ab_2_4
CL	D14-1165	ab	3	4	result_means	result	support	elaboration	secondary	secondary	back	elaboration	By leveraging relational domain knowledge about entity type information , our learning algorithm is significantly faster than previous approaches and is better able to discover new relations missing from the database .	In addition , when applied to a relation extraction task , our approach alone is comparable to several existing systems , and improves the weighted mean average precision of a state-of-the-art method by 10 points when used as a subcomponent .	CL_D14-1165_ab_3	CL_D14-1165_ab_4	CL_D14-1165_ab_3_4
CL	D14-1165	ab	4	3	result	result_means	elaboration	support	secondary	secondary	forw	elaboration	In addition , when applied to a relation extraction task , our approach alone is comparable to several existing systems , and improves the weighted mean average precision of a state-of-the-art method by 10 points when used as a subcomponent .	By leveraging relational domain knowledge about entity type information , our learning algorithm is significantly faster than previous approaches and is better able to discover new relations missing from the database .	CL_D14-1165_ab_4	CL_D14-1165_ab_3	CL_D14-1165_ab_3_4
CL	D14-1166	ab	1	2	motivation_background	motivation_background	info-required	info-optional	secondary	secondary	back	info-optional	A promising approach to relation extraction , called weak or distant supervision , exploits an existing database of facts as training data , by aligning it to an unlabeled collection of text documents .	Using this approach , the task of relation extraction can easily be scaled to hundreds of different relationships .	CL_D14-1166_ab_1	CL_D14-1166_ab_2	CL_D14-1166_ab_1_2
CL	D14-1166	ab	2	1	motivation_background	motivation_background	info-optional	info-required	secondary	secondary	forw	info-optional	Using this approach , the task of relation extraction can easily be scaled to hundreds of different relationships .	A promising approach to relation extraction , called weak or distant supervision , exploits an existing database of facts as training data , by aligning it to an unlabeled collection of text documents .	CL_D14-1166_ab_2	CL_D14-1166_ab_1	CL_D14-1166_ab_1_2
CL	D14-1166	ab	1	3	motivation_background	motivation_problem	info-required	info-required	secondary	secondary	forw	info-required	A promising approach to relation extraction , called weak or distant supervision , exploits an existing database of facts as training data , by aligning it to an unlabeled collection of text documents .	However , distant supervision leads to a challenging multiple instance , multiple label learning problem .	CL_D14-1166_ab_1	CL_D14-1166_ab_3	CL_D14-1166_ab_1_3
CL	D14-1166	ab	3	1	motivation_problem	motivation_background	info-required	info-required	secondary	secondary	back	info-required	However , distant supervision leads to a challenging multiple instance , multiple label learning problem .	A promising approach to relation extraction , called weak or distant supervision , exploits an existing database of facts as training data , by aligning it to an unlabeled collection of text documents .	CL_D14-1166_ab_3	CL_D14-1166_ab_1	CL_D14-1166_ab_1_3
CL	D14-1166	ab	1	4	motivation_background	motivation_problem	info-required	support	secondary	secondary	none	none	A promising approach to relation extraction , called weak or distant supervision , exploits an existing database of facts as training data , by aligning it to an unlabeled collection of text documents .	Most of the proposed solutions to this problem are based on non-convex formulations , and are thus prone to local min-ima .	CL_D14-1166_ab_1	CL_D14-1166_ab_4	CL_D14-1166_ab_1_4
CL	D14-1166	ab	1	5	motivation_background	proposal	info-required	none	secondary	main	none	none	A promising approach to relation extraction , called weak or distant supervision , exploits an existing database of facts as training data , by aligning it to an unlabeled collection of text documents .	In this article , we propose a new approach to the problem of weakly supervised relation extraction , based on discriminative clustering and leading to a convex formulation .	CL_D14-1166_ab_1	CL_D14-1166_ab_5	CL_D14-1166_ab_1_5
CL	D14-1166	ab	1	6	motivation_background	result	info-required	support	secondary	secondary	none	none	A promising approach to relation extraction , called weak or distant supervision , exploits an existing database of facts as training data , by aligning it to an unlabeled collection of text documents .	We demonstrate that our approach outperforms state-of-the-art methods on the challenging dataset introduced by Riedel et al. ( 2010 ) .	CL_D14-1166_ab_1	CL_D14-1166_ab_6	CL_D14-1166_ab_1_6
CL	D14-1166	ab	2	3	motivation_background	motivation_problem	info-optional	info-required	secondary	secondary	none	none	Using this approach , the task of relation extraction can easily be scaled to hundreds of different relationships .	However , distant supervision leads to a challenging multiple instance , multiple label learning problem .	CL_D14-1166_ab_2	CL_D14-1166_ab_3	CL_D14-1166_ab_2_3
CL	D14-1166	ab	2	4	motivation_background	motivation_problem	info-optional	support	secondary	secondary	none	none	Using this approach , the task of relation extraction can easily be scaled to hundreds of different relationships .	Most of the proposed solutions to this problem are based on non-convex formulations , and are thus prone to local min-ima .	CL_D14-1166_ab_2	CL_D14-1166_ab_4	CL_D14-1166_ab_2_4
CL	D14-1166	ab	2	5	motivation_background	proposal	info-optional	none	secondary	main	none	none	Using this approach , the task of relation extraction can easily be scaled to hundreds of different relationships .	In this article , we propose a new approach to the problem of weakly supervised relation extraction , based on discriminative clustering and leading to a convex formulation .	CL_D14-1166_ab_2	CL_D14-1166_ab_5	CL_D14-1166_ab_2_5
CL	D14-1166	ab	2	6	motivation_background	result	info-optional	support	secondary	secondary	none	none	Using this approach , the task of relation extraction can easily be scaled to hundreds of different relationships .	We demonstrate that our approach outperforms state-of-the-art methods on the challenging dataset introduced by Riedel et al. ( 2010 ) .	CL_D14-1166_ab_2	CL_D14-1166_ab_6	CL_D14-1166_ab_2_6
CL	D14-1166	ab	3	4	motivation_problem	motivation_problem	info-required	support	secondary	secondary	forw	info-required	However , distant supervision leads to a challenging multiple instance , multiple label learning problem .	Most of the proposed solutions to this problem are based on non-convex formulations , and are thus prone to local min-ima .	CL_D14-1166_ab_3	CL_D14-1166_ab_4	CL_D14-1166_ab_3_4
CL	D14-1166	ab	4	3	motivation_problem	motivation_problem	support	info-required	secondary	secondary	back	info-required	Most of the proposed solutions to this problem are based on non-convex formulations , and are thus prone to local min-ima .	However , distant supervision leads to a challenging multiple instance , multiple label learning problem .	CL_D14-1166_ab_4	CL_D14-1166_ab_3	CL_D14-1166_ab_3_4
CL	D14-1166	ab	3	5	motivation_problem	proposal	info-required	none	secondary	main	none	none	However , distant supervision leads to a challenging multiple instance , multiple label learning problem .	In this article , we propose a new approach to the problem of weakly supervised relation extraction , based on discriminative clustering and leading to a convex formulation .	CL_D14-1166_ab_3	CL_D14-1166_ab_5	CL_D14-1166_ab_3_5
CL	D14-1166	ab	3	6	motivation_problem	result	info-required	support	secondary	secondary	none	none	However , distant supervision leads to a challenging multiple instance , multiple label learning problem .	We demonstrate that our approach outperforms state-of-the-art methods on the challenging dataset introduced by Riedel et al. ( 2010 ) .	CL_D14-1166_ab_3	CL_D14-1166_ab_6	CL_D14-1166_ab_3_6
CL	D14-1166	ab	4	5	motivation_problem	proposal	support	none	secondary	main	forw	support	Most of the proposed solutions to this problem are based on non-convex formulations , and are thus prone to local min-ima .	In this article , we propose a new approach to the problem of weakly supervised relation extraction , based on discriminative clustering and leading to a convex formulation .	CL_D14-1166_ab_4	CL_D14-1166_ab_5	CL_D14-1166_ab_4_5
CL	D14-1166	ab	5	4	proposal	motivation_problem	none	support	main	secondary	back	support	In this article , we propose a new approach to the problem of weakly supervised relation extraction , based on discriminative clustering and leading to a convex formulation .	Most of the proposed solutions to this problem are based on non-convex formulations , and are thus prone to local min-ima .	CL_D14-1166_ab_5	CL_D14-1166_ab_4	CL_D14-1166_ab_4_5
CL	D14-1166	ab	4	6	motivation_problem	result	support	support	secondary	secondary	none	none	Most of the proposed solutions to this problem are based on non-convex formulations , and are thus prone to local min-ima .	We demonstrate that our approach outperforms state-of-the-art methods on the challenging dataset introduced by Riedel et al. ( 2010 ) .	CL_D14-1166_ab_4	CL_D14-1166_ab_6	CL_D14-1166_ab_4_6
CL	D14-1166	ab	5	6	proposal	result	none	support	main	secondary	back	support	In this article , we propose a new approach to the problem of weakly supervised relation extraction , based on discriminative clustering and leading to a convex formulation .	We demonstrate that our approach outperforms state-of-the-art methods on the challenging dataset introduced by Riedel et al. ( 2010 ) .	CL_D14-1166_ab_5	CL_D14-1166_ab_6	CL_D14-1166_ab_5_6
CL	D14-1166	ab	6	5	result	proposal	support	none	secondary	main	forw	support	We demonstrate that our approach outperforms state-of-the-art methods on the challenging dataset introduced by Riedel et al. ( 2010 ) .	In this article , we propose a new approach to the problem of weakly supervised relation extraction , based on discriminative clustering and leading to a convex formulation .	CL_D14-1166_ab_6	CL_D14-1166_ab_5	CL_D14-1166_ab_5_6
CL	D14-1167	ab	1	2	proposal	proposal	none	elaboration	secondary	main	back	elaboration	We examine the embedding approach to reason new relational facts from a large-scale knowledge graph and a text corpus .	We propose a novel method of jointly embedding entities and words into the same continuous vector space .	CL_D14-1167_ab_1	CL_D14-1167_ab_2	CL_D14-1167_ab_1_2
CL	D14-1167	ab	2	1	proposal	proposal	elaboration	none	main	secondary	forw	elaboration	We propose a novel method of jointly embedding entities and words into the same continuous vector space .	We examine the embedding approach to reason new relational facts from a large-scale knowledge graph and a text corpus .	CL_D14-1167_ab_2	CL_D14-1167_ab_1	CL_D14-1167_ab_1_2
CL	D14-1167	ab	1	3	proposal	proposal	none	elaboration	secondary	secondary	none	none	We examine the embedding approach to reason new relational facts from a large-scale knowledge graph and a text corpus .	The embedding process attempts to preserve the relations between entities in the knowledge graph and the concurrences of words in the text corpus .	CL_D14-1167_ab_1	CL_D14-1167_ab_3	CL_D14-1167_ab_1_3
CL	D14-1167	ab	1	4	proposal	means	none	by-means	secondary	secondary	none	none	We examine the embedding approach to reason new relational facts from a large-scale knowledge graph and a text corpus .	Entity names and Wikipedia anchors are utilized to align the embeddings of entities and words in the same space .	CL_D14-1167_ab_1	CL_D14-1167_ab_4	CL_D14-1167_ab_1_4
CL	D14-1167	ab	1	5	proposal	result_means	none	support	secondary	secondary	none	none	We examine the embedding approach to reason new relational facts from a large-scale knowledge graph and a text corpus .	Large scale experiments on Freebase and a Wikipedia/NY Times corpus show that jointly embedding brings promising improvement in the accuracy of predicting facts , compared to separately embedding knowledge graphs and text .	CL_D14-1167_ab_1	CL_D14-1167_ab_5	CL_D14-1167_ab_1_5
CL	D14-1167	ab	1	6	proposal	result	none	elaboration	secondary	secondary	none	none	We examine the embedding approach to reason new relational facts from a large-scale knowledge graph and a text corpus .	Particularly , jointly embedding enables the prediction of facts containing entities out of the knowledge graph , which cannot be handled by previous embedding methods .	CL_D14-1167_ab_1	CL_D14-1167_ab_6	CL_D14-1167_ab_1_6
CL	D14-1167	ab	1	7	proposal	result	none	support	secondary	secondary	none	none	We examine the embedding approach to reason new relational facts from a large-scale knowledge graph and a text corpus .	At the same time , concerning the quality of the word embeddings , experiments on the analogical reasoning task show that jointly embedding is comparable to or slightly better than word2vec ( Skip-Gram ) .	CL_D14-1167_ab_1	CL_D14-1167_ab_7	CL_D14-1167_ab_1_7
CL	D14-1167	ab	2	3	proposal	proposal	elaboration	elaboration	main	secondary	back	elaboration	We propose a novel method of jointly embedding entities and words into the same continuous vector space .	The embedding process attempts to preserve the relations between entities in the knowledge graph and the concurrences of words in the text corpus .	CL_D14-1167_ab_2	CL_D14-1167_ab_3	CL_D14-1167_ab_2_3
CL	D14-1167	ab	3	2	proposal	proposal	elaboration	elaboration	secondary	main	forw	elaboration	The embedding process attempts to preserve the relations between entities in the knowledge graph and the concurrences of words in the text corpus .	We propose a novel method of jointly embedding entities and words into the same continuous vector space .	CL_D14-1167_ab_3	CL_D14-1167_ab_2	CL_D14-1167_ab_2_3
CL	D14-1167	ab	2	4	proposal	means	elaboration	by-means	main	secondary	none	none	We propose a novel method of jointly embedding entities and words into the same continuous vector space .	Entity names and Wikipedia anchors are utilized to align the embeddings of entities and words in the same space .	CL_D14-1167_ab_2	CL_D14-1167_ab_4	CL_D14-1167_ab_2_4
CL	D14-1167	ab	2	5	proposal	result_means	elaboration	support	main	secondary	back	support	We propose a novel method of jointly embedding entities and words into the same continuous vector space .	Large scale experiments on Freebase and a Wikipedia/NY Times corpus show that jointly embedding brings promising improvement in the accuracy of predicting facts , compared to separately embedding knowledge graphs and text .	CL_D14-1167_ab_2	CL_D14-1167_ab_5	CL_D14-1167_ab_2_5
CL	D14-1167	ab	5	2	result_means	proposal	support	elaboration	secondary	main	forw	support	Large scale experiments on Freebase and a Wikipedia/NY Times corpus show that jointly embedding brings promising improvement in the accuracy of predicting facts , compared to separately embedding knowledge graphs and text .	We propose a novel method of jointly embedding entities and words into the same continuous vector space .	CL_D14-1167_ab_5	CL_D14-1167_ab_2	CL_D14-1167_ab_2_5
CL	D14-1167	ab	2	6	proposal	result	elaboration	elaboration	main	secondary	none	none	We propose a novel method of jointly embedding entities and words into the same continuous vector space .	Particularly , jointly embedding enables the prediction of facts containing entities out of the knowledge graph , which cannot be handled by previous embedding methods .	CL_D14-1167_ab_2	CL_D14-1167_ab_6	CL_D14-1167_ab_2_6
CL	D14-1167	ab	2	7	proposal	result	elaboration	support	main	secondary	back	support	We propose a novel method of jointly embedding entities and words into the same continuous vector space .	At the same time , concerning the quality of the word embeddings , experiments on the analogical reasoning task show that jointly embedding is comparable to or slightly better than word2vec ( Skip-Gram ) .	CL_D14-1167_ab_2	CL_D14-1167_ab_7	CL_D14-1167_ab_2_7
CL	D14-1167	ab	7	2	result	proposal	support	elaboration	secondary	main	forw	support	At the same time , concerning the quality of the word embeddings , experiments on the analogical reasoning task show that jointly embedding is comparable to or slightly better than word2vec ( Skip-Gram ) .	We propose a novel method of jointly embedding entities and words into the same continuous vector space .	CL_D14-1167_ab_7	CL_D14-1167_ab_2	CL_D14-1167_ab_2_7
CL	D14-1167	ab	3	4	proposal	means	elaboration	by-means	secondary	secondary	back	by-means	The embedding process attempts to preserve the relations between entities in the knowledge graph and the concurrences of words in the text corpus .	Entity names and Wikipedia anchors are utilized to align the embeddings of entities and words in the same space .	CL_D14-1167_ab_3	CL_D14-1167_ab_4	CL_D14-1167_ab_3_4
CL	D14-1167	ab	4	3	means	proposal	by-means	elaboration	secondary	secondary	forw	by-means	Entity names and Wikipedia anchors are utilized to align the embeddings of entities and words in the same space .	The embedding process attempts to preserve the relations between entities in the knowledge graph and the concurrences of words in the text corpus .	CL_D14-1167_ab_4	CL_D14-1167_ab_3	CL_D14-1167_ab_3_4
CL	D14-1167	ab	3	5	proposal	result_means	elaboration	support	secondary	secondary	none	none	The embedding process attempts to preserve the relations between entities in the knowledge graph and the concurrences of words in the text corpus .	Large scale experiments on Freebase and a Wikipedia/NY Times corpus show that jointly embedding brings promising improvement in the accuracy of predicting facts , compared to separately embedding knowledge graphs and text .	CL_D14-1167_ab_3	CL_D14-1167_ab_5	CL_D14-1167_ab_3_5
CL	D14-1167	ab	3	6	proposal	result	elaboration	elaboration	secondary	secondary	none	none	The embedding process attempts to preserve the relations between entities in the knowledge graph and the concurrences of words in the text corpus .	Particularly , jointly embedding enables the prediction of facts containing entities out of the knowledge graph , which cannot be handled by previous embedding methods .	CL_D14-1167_ab_3	CL_D14-1167_ab_6	CL_D14-1167_ab_3_6
CL	D14-1167	ab	3	7	proposal	result	elaboration	support	secondary	secondary	none	none	The embedding process attempts to preserve the relations between entities in the knowledge graph and the concurrences of words in the text corpus .	At the same time , concerning the quality of the word embeddings , experiments on the analogical reasoning task show that jointly embedding is comparable to or slightly better than word2vec ( Skip-Gram ) .	CL_D14-1167_ab_3	CL_D14-1167_ab_7	CL_D14-1167_ab_3_7
CL	D14-1167	ab	4	5	means	result_means	by-means	support	secondary	secondary	none	none	Entity names and Wikipedia anchors are utilized to align the embeddings of entities and words in the same space .	Large scale experiments on Freebase and a Wikipedia/NY Times corpus show that jointly embedding brings promising improvement in the accuracy of predicting facts , compared to separately embedding knowledge graphs and text .	CL_D14-1167_ab_4	CL_D14-1167_ab_5	CL_D14-1167_ab_4_5
CL	D14-1167	ab	4	6	means	result	by-means	elaboration	secondary	secondary	none	none	Entity names and Wikipedia anchors are utilized to align the embeddings of entities and words in the same space .	Particularly , jointly embedding enables the prediction of facts containing entities out of the knowledge graph , which cannot be handled by previous embedding methods .	CL_D14-1167_ab_4	CL_D14-1167_ab_6	CL_D14-1167_ab_4_6
CL	D14-1167	ab	4	7	means	result	by-means	support	secondary	secondary	none	none	Entity names and Wikipedia anchors are utilized to align the embeddings of entities and words in the same space .	At the same time , concerning the quality of the word embeddings , experiments on the analogical reasoning task show that jointly embedding is comparable to or slightly better than word2vec ( Skip-Gram ) .	CL_D14-1167_ab_4	CL_D14-1167_ab_7	CL_D14-1167_ab_4_7
CL	D14-1167	ab	5	6	result_means	result	support	elaboration	secondary	secondary	back	elaboration	Large scale experiments on Freebase and a Wikipedia/NY Times corpus show that jointly embedding brings promising improvement in the accuracy of predicting facts , compared to separately embedding knowledge graphs and text .	Particularly , jointly embedding enables the prediction of facts containing entities out of the knowledge graph , which cannot be handled by previous embedding methods .	CL_D14-1167_ab_5	CL_D14-1167_ab_6	CL_D14-1167_ab_5_6
CL	D14-1167	ab	6	5	result	result_means	elaboration	support	secondary	secondary	forw	elaboration	Particularly , jointly embedding enables the prediction of facts containing entities out of the knowledge graph , which cannot be handled by previous embedding methods .	Large scale experiments on Freebase and a Wikipedia/NY Times corpus show that jointly embedding brings promising improvement in the accuracy of predicting facts , compared to separately embedding knowledge graphs and text .	CL_D14-1167_ab_6	CL_D14-1167_ab_5	CL_D14-1167_ab_5_6
CL	D14-1167	ab	5	7	result_means	result	support	support	secondary	secondary	none	none	Large scale experiments on Freebase and a Wikipedia/NY Times corpus show that jointly embedding brings promising improvement in the accuracy of predicting facts , compared to separately embedding knowledge graphs and text .	At the same time , concerning the quality of the word embeddings , experiments on the analogical reasoning task show that jointly embedding is comparable to or slightly better than word2vec ( Skip-Gram ) .	CL_D14-1167_ab_5	CL_D14-1167_ab_7	CL_D14-1167_ab_5_7
CL	D14-1167	ab	6	7	result	result	elaboration	support	secondary	secondary	none	none	Particularly , jointly embedding enables the prediction of facts containing entities out of the knowledge graph , which cannot be handled by previous embedding methods .	At the same time , concerning the quality of the word embeddings , experiments on the analogical reasoning task show that jointly embedding is comparable to or slightly better than word2vec ( Skip-Gram ) .	CL_D14-1167_ab_6	CL_D14-1167_ab_7	CL_D14-1167_ab_6_7
CL	D14-1168	ab	1	2	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	We propose a novel abstractive summarization system for product reviews by taking advantage of their discourse structure .	First , we apply a discourse parser to each review and obtain a discourse tree representation for every review .	CL_D14-1168_ab_1	CL_D14-1168_ab_2	CL_D14-1168_ab_1_2
CL	D14-1168	ab	2	1	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	First , we apply a discourse parser to each review and obtain a discourse tree representation for every review .	We propose a novel abstractive summarization system for product reviews by taking advantage of their discourse structure .	CL_D14-1168_ab_2	CL_D14-1168_ab_1	CL_D14-1168_ab_1_2
CL	D14-1168	ab	1	3	proposal	proposal_implementation	none	sequence	main	secondary	none	none	We propose a novel abstractive summarization system for product reviews by taking advantage of their discourse structure .	We then modify the discourse trees such that every leaf node only contains the aspect words .	CL_D14-1168_ab_1	CL_D14-1168_ab_3	CL_D14-1168_ab_1_3
CL	D14-1168	ab	1	4	proposal	proposal_implementation	none	sequence	main	secondary	none	none	We propose a novel abstractive summarization system for product reviews by taking advantage of their discourse structure .	Second , we aggregate the aspect discourse trees and generate a graph .	CL_D14-1168_ab_1	CL_D14-1168_ab_4	CL_D14-1168_ab_1_4
CL	D14-1168	ab	1	5	proposal	proposal_implementation	none	sequence	main	secondary	none	none	We propose a novel abstractive summarization system for product reviews by taking advantage of their discourse structure .	We then select a subgraph representing the most important aspects and the rhetorical relations between them using a PageRank algorithm , and transform the selected subgraph into an aspect tree .	CL_D14-1168_ab_1	CL_D14-1168_ab_5	CL_D14-1168_ab_1_5
CL	D14-1168	ab	1	6	proposal	proposal_implementation	none	sequence	main	secondary	none	none	We propose a novel abstractive summarization system for product reviews by taking advantage of their discourse structure .	Finally , we generate a natural language summary by applying a template-based NLG framework .	CL_D14-1168_ab_1	CL_D14-1168_ab_6	CL_D14-1168_ab_1_6
CL	D14-1168	ab	1	7	proposal	result	none	support	main	secondary	back	support	We propose a novel abstractive summarization system for product reviews by taking advantage of their discourse structure .	Quantitative and qualitative analysis of the results , based on two user studies , show that our approach significantly outperforms extractive and abstractive baselines .	CL_D14-1168_ab_1	CL_D14-1168_ab_7	CL_D14-1168_ab_1_7
CL	D14-1168	ab	7	1	result	proposal	support	none	secondary	main	forw	support	Quantitative and qualitative analysis of the results , based on two user studies , show that our approach significantly outperforms extractive and abstractive baselines .	We propose a novel abstractive summarization system for product reviews by taking advantage of their discourse structure .	CL_D14-1168_ab_7	CL_D14-1168_ab_1	CL_D14-1168_ab_1_7
CL	D14-1168	ab	2	3	proposal_implementation	proposal_implementation	elaboration	sequence	secondary	secondary	back	sequence	First , we apply a discourse parser to each review and obtain a discourse tree representation for every review .	We then modify the discourse trees such that every leaf node only contains the aspect words .	CL_D14-1168_ab_2	CL_D14-1168_ab_3	CL_D14-1168_ab_2_3
CL	D14-1168	ab	3	2	proposal_implementation	proposal_implementation	sequence	elaboration	secondary	secondary	forw	sequence	We then modify the discourse trees such that every leaf node only contains the aspect words .	First , we apply a discourse parser to each review and obtain a discourse tree representation for every review .	CL_D14-1168_ab_3	CL_D14-1168_ab_2	CL_D14-1168_ab_2_3
CL	D14-1168	ab	2	4	proposal_implementation	proposal_implementation	elaboration	sequence	secondary	secondary	none	none	First , we apply a discourse parser to each review and obtain a discourse tree representation for every review .	Second , we aggregate the aspect discourse trees and generate a graph .	CL_D14-1168_ab_2	CL_D14-1168_ab_4	CL_D14-1168_ab_2_4
CL	D14-1168	ab	2	5	proposal_implementation	proposal_implementation	elaboration	sequence	secondary	secondary	none	none	First , we apply a discourse parser to each review and obtain a discourse tree representation for every review .	We then select a subgraph representing the most important aspects and the rhetorical relations between them using a PageRank algorithm , and transform the selected subgraph into an aspect tree .	CL_D14-1168_ab_2	CL_D14-1168_ab_5	CL_D14-1168_ab_2_5
CL	D14-1168	ab	2	6	proposal_implementation	proposal_implementation	elaboration	sequence	secondary	secondary	none	none	First , we apply a discourse parser to each review and obtain a discourse tree representation for every review .	Finally , we generate a natural language summary by applying a template-based NLG framework .	CL_D14-1168_ab_2	CL_D14-1168_ab_6	CL_D14-1168_ab_2_6
CL	D14-1168	ab	2	7	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	First , we apply a discourse parser to each review and obtain a discourse tree representation for every review .	Quantitative and qualitative analysis of the results , based on two user studies , show that our approach significantly outperforms extractive and abstractive baselines .	CL_D14-1168_ab_2	CL_D14-1168_ab_7	CL_D14-1168_ab_2_7
CL	D14-1168	ab	3	4	proposal_implementation	proposal_implementation	sequence	sequence	secondary	secondary	back	sequence	We then modify the discourse trees such that every leaf node only contains the aspect words .	Second , we aggregate the aspect discourse trees and generate a graph .	CL_D14-1168_ab_3	CL_D14-1168_ab_4	CL_D14-1168_ab_3_4
CL	D14-1168	ab	4	3	proposal_implementation	proposal_implementation	sequence	sequence	secondary	secondary	forw	sequence	Second , we aggregate the aspect discourse trees and generate a graph .	We then modify the discourse trees such that every leaf node only contains the aspect words .	CL_D14-1168_ab_4	CL_D14-1168_ab_3	CL_D14-1168_ab_3_4
CL	D14-1168	ab	3	5	proposal_implementation	proposal_implementation	sequence	sequence	secondary	secondary	none	none	We then modify the discourse trees such that every leaf node only contains the aspect words .	We then select a subgraph representing the most important aspects and the rhetorical relations between them using a PageRank algorithm , and transform the selected subgraph into an aspect tree .	CL_D14-1168_ab_3	CL_D14-1168_ab_5	CL_D14-1168_ab_3_5
CL	D14-1168	ab	3	6	proposal_implementation	proposal_implementation	sequence	sequence	secondary	secondary	none	none	We then modify the discourse trees such that every leaf node only contains the aspect words .	Finally , we generate a natural language summary by applying a template-based NLG framework .	CL_D14-1168_ab_3	CL_D14-1168_ab_6	CL_D14-1168_ab_3_6
CL	D14-1168	ab	3	7	proposal_implementation	result	sequence	support	secondary	secondary	none	none	We then modify the discourse trees such that every leaf node only contains the aspect words .	Quantitative and qualitative analysis of the results , based on two user studies , show that our approach significantly outperforms extractive and abstractive baselines .	CL_D14-1168_ab_3	CL_D14-1168_ab_7	CL_D14-1168_ab_3_7
CL	D14-1168	ab	4	5	proposal_implementation	proposal_implementation	sequence	sequence	secondary	secondary	back	sequence	Second , we aggregate the aspect discourse trees and generate a graph .	We then select a subgraph representing the most important aspects and the rhetorical relations between them using a PageRank algorithm , and transform the selected subgraph into an aspect tree .	CL_D14-1168_ab_4	CL_D14-1168_ab_5	CL_D14-1168_ab_4_5
CL	D14-1168	ab	5	4	proposal_implementation	proposal_implementation	sequence	sequence	secondary	secondary	forw	sequence	We then select a subgraph representing the most important aspects and the rhetorical relations between them using a PageRank algorithm , and transform the selected subgraph into an aspect tree .	Second , we aggregate the aspect discourse trees and generate a graph .	CL_D14-1168_ab_5	CL_D14-1168_ab_4	CL_D14-1168_ab_4_5
CL	D14-1168	ab	4	6	proposal_implementation	proposal_implementation	sequence	sequence	secondary	secondary	none	none	Second , we aggregate the aspect discourse trees and generate a graph .	Finally , we generate a natural language summary by applying a template-based NLG framework .	CL_D14-1168_ab_4	CL_D14-1168_ab_6	CL_D14-1168_ab_4_6
CL	D14-1168	ab	4	7	proposal_implementation	result	sequence	support	secondary	secondary	none	none	Second , we aggregate the aspect discourse trees and generate a graph .	Quantitative and qualitative analysis of the results , based on two user studies , show that our approach significantly outperforms extractive and abstractive baselines .	CL_D14-1168_ab_4	CL_D14-1168_ab_7	CL_D14-1168_ab_4_7
CL	D14-1168	ab	5	6	proposal_implementation	proposal_implementation	sequence	sequence	secondary	secondary	back	sequence	We then select a subgraph representing the most important aspects and the rhetorical relations between them using a PageRank algorithm , and transform the selected subgraph into an aspect tree .	Finally , we generate a natural language summary by applying a template-based NLG framework .	CL_D14-1168_ab_5	CL_D14-1168_ab_6	CL_D14-1168_ab_5_6
CL	D14-1168	ab	6	5	proposal_implementation	proposal_implementation	sequence	sequence	secondary	secondary	forw	sequence	Finally , we generate a natural language summary by applying a template-based NLG framework .	We then select a subgraph representing the most important aspects and the rhetorical relations between them using a PageRank algorithm , and transform the selected subgraph into an aspect tree .	CL_D14-1168_ab_6	CL_D14-1168_ab_5	CL_D14-1168_ab_5_6
CL	D14-1168	ab	5	7	proposal_implementation	result	sequence	support	secondary	secondary	none	none	We then select a subgraph representing the most important aspects and the rhetorical relations between them using a PageRank algorithm , and transform the selected subgraph into an aspect tree .	Quantitative and qualitative analysis of the results , based on two user studies , show that our approach significantly outperforms extractive and abstractive baselines .	CL_D14-1168_ab_5	CL_D14-1168_ab_7	CL_D14-1168_ab_5_7
CL	D14-1168	ab	6	7	proposal_implementation	result	sequence	support	secondary	secondary	none	none	Finally , we generate a natural language summary by applying a template-based NLG framework .	Quantitative and qualitative analysis of the results , based on two user studies , show that our approach significantly outperforms extractive and abstractive baselines .	CL_D14-1168_ab_6	CL_D14-1168_ab_7	CL_D14-1168_ab_6_7
CL	D14-1169	ab	1	2	motivation_background	motivation_background	info-required	support	secondary	secondary	forw	info-required	Clustering aspect-related phrases in terms of product's property is a precursor process to aspect-level sentiment analysis which is a central task in sentiment analysis .	Most of existing methods for addressing this problem are context-based models which assume that domain synonymous phrases share similar co-occurrence contexts .	CL_D14-1169_ab_1	CL_D14-1169_ab_2	CL_D14-1169_ab_1_2
CL	D14-1169	ab	2	1	motivation_background	motivation_background	support	info-required	secondary	secondary	back	info-required	Most of existing methods for addressing this problem are context-based models which assume that domain synonymous phrases share similar co-occurrence contexts .	Clustering aspect-related phrases in terms of product's property is a precursor process to aspect-level sentiment analysis which is a central task in sentiment analysis .	CL_D14-1169_ab_2	CL_D14-1169_ab_1	CL_D14-1169_ab_1_2
CL	D14-1169	ab	1	3	motivation_background	proposal	info-required	none	secondary	main	none	none	Clustering aspect-related phrases in terms of product's property is a precursor process to aspect-level sentiment analysis which is a central task in sentiment analysis .	In this paper , we explore a novel idea , sentiment distribution consistency , which states that different phrases ( e.g. "price" , "money" , "worth" , and "cost" ) of the same aspect tend to have consistent sentiment distribution .	CL_D14-1169_ab_1	CL_D14-1169_ab_3	CL_D14-1169_ab_1_3
CL	D14-1169	ab	1	4	motivation_background	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	Clustering aspect-related phrases in terms of product's property is a precursor process to aspect-level sentiment analysis which is a central task in sentiment analysis .	Through formalizing sentiment distribution consistency as soft constraint , we propose a novel unsupervised model in the framework of Posterior Regularization ( PR ) to cluster aspect-related phrases .	CL_D14-1169_ab_1	CL_D14-1169_ab_4	CL_D14-1169_ab_1_4
CL	D14-1169	ab	1	5	motivation_background	result	info-required	support	secondary	secondary	none	none	Clustering aspect-related phrases in terms of product's property is a precursor process to aspect-level sentiment analysis which is a central task in sentiment analysis .	Experiments demonstrate that our approach outperforms baselines remarkably .	CL_D14-1169_ab_1	CL_D14-1169_ab_5	CL_D14-1169_ab_1_5
CL	D14-1169	ab	2	3	motivation_background	proposal	support	none	secondary	main	forw	support	Most of existing methods for addressing this problem are context-based models which assume that domain synonymous phrases share similar co-occurrence contexts .	In this paper , we explore a novel idea , sentiment distribution consistency , which states that different phrases ( e.g. "price" , "money" , "worth" , and "cost" ) of the same aspect tend to have consistent sentiment distribution .	CL_D14-1169_ab_2	CL_D14-1169_ab_3	CL_D14-1169_ab_2_3
CL	D14-1169	ab	3	2	proposal	motivation_background	none	support	main	secondary	back	support	In this paper , we explore a novel idea , sentiment distribution consistency , which states that different phrases ( e.g. "price" , "money" , "worth" , and "cost" ) of the same aspect tend to have consistent sentiment distribution .	Most of existing methods for addressing this problem are context-based models which assume that domain synonymous phrases share similar co-occurrence contexts .	CL_D14-1169_ab_3	CL_D14-1169_ab_2	CL_D14-1169_ab_2_3
CL	D14-1169	ab	2	4	motivation_background	proposal_implementation	support	elaboration	secondary	secondary	none	none	Most of existing methods for addressing this problem are context-based models which assume that domain synonymous phrases share similar co-occurrence contexts .	Through formalizing sentiment distribution consistency as soft constraint , we propose a novel unsupervised model in the framework of Posterior Regularization ( PR ) to cluster aspect-related phrases .	CL_D14-1169_ab_2	CL_D14-1169_ab_4	CL_D14-1169_ab_2_4
CL	D14-1169	ab	2	5	motivation_background	result	support	support	secondary	secondary	none	none	Most of existing methods for addressing this problem are context-based models which assume that domain synonymous phrases share similar co-occurrence contexts .	Experiments demonstrate that our approach outperforms baselines remarkably .	CL_D14-1169_ab_2	CL_D14-1169_ab_5	CL_D14-1169_ab_2_5
CL	D14-1169	ab	3	4	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	In this paper , we explore a novel idea , sentiment distribution consistency , which states that different phrases ( e.g. "price" , "money" , "worth" , and "cost" ) of the same aspect tend to have consistent sentiment distribution .	Through formalizing sentiment distribution consistency as soft constraint , we propose a novel unsupervised model in the framework of Posterior Regularization ( PR ) to cluster aspect-related phrases .	CL_D14-1169_ab_3	CL_D14-1169_ab_4	CL_D14-1169_ab_3_4
CL	D14-1169	ab	4	3	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	Through formalizing sentiment distribution consistency as soft constraint , we propose a novel unsupervised model in the framework of Posterior Regularization ( PR ) to cluster aspect-related phrases .	In this paper , we explore a novel idea , sentiment distribution consistency , which states that different phrases ( e.g. "price" , "money" , "worth" , and "cost" ) of the same aspect tend to have consistent sentiment distribution .	CL_D14-1169_ab_4	CL_D14-1169_ab_3	CL_D14-1169_ab_3_4
CL	D14-1169	ab	3	5	proposal	result	none	support	main	secondary	back	support	In this paper , we explore a novel idea , sentiment distribution consistency , which states that different phrases ( e.g. "price" , "money" , "worth" , and "cost" ) of the same aspect tend to have consistent sentiment distribution .	Experiments demonstrate that our approach outperforms baselines remarkably .	CL_D14-1169_ab_3	CL_D14-1169_ab_5	CL_D14-1169_ab_3_5
CL	D14-1169	ab	5	3	result	proposal	support	none	secondary	main	forw	support	Experiments demonstrate that our approach outperforms baselines remarkably .	In this paper , we explore a novel idea , sentiment distribution consistency , which states that different phrases ( e.g. "price" , "money" , "worth" , and "cost" ) of the same aspect tend to have consistent sentiment distribution .	CL_D14-1169_ab_5	CL_D14-1169_ab_3	CL_D14-1169_ab_3_5
CL	D14-1169	ab	4	5	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	Through formalizing sentiment distribution consistency as soft constraint , we propose a novel unsupervised model in the framework of Posterior Regularization ( PR ) to cluster aspect-related phrases .	Experiments demonstrate that our approach outperforms baselines remarkably .	CL_D14-1169_ab_4	CL_D14-1169_ab_5	CL_D14-1169_ab_4_5
CL	D14-1170	ab	1	2	proposal	proposal	elaboration	elaboration	secondary	secondary	back	elaboration	In this paper , we investigate a challenging task of automatic related work generation .	Given multiple reference papers as input , the task aims to generate a related work section for a target paper .	CL_D14-1170_ab_1	CL_D14-1170_ab_2	CL_D14-1170_ab_1_2
CL	D14-1170	ab	2	1	proposal	proposal	elaboration	elaboration	secondary	secondary	forw	elaboration	Given multiple reference papers as input , the task aims to generate a related work section for a target paper .	In this paper , we investigate a challenging task of automatic related work generation .	CL_D14-1170_ab_2	CL_D14-1170_ab_1	CL_D14-1170_ab_1_2
CL	D14-1170	ab	1	3	proposal	information_additional	elaboration	info-optional	secondary	secondary	none	none	In this paper , we investigate a challenging task of automatic related work generation .	The generated related work section can be used as a draft for the author to complete his or her final related work section .	CL_D14-1170_ab_1	CL_D14-1170_ab_3	CL_D14-1170_ab_1_3
CL	D14-1170	ab	1	4	proposal	proposal	elaboration	none	secondary	main	forw	elaboration	In this paper , we investigate a challenging task of automatic related work generation .	We propose our Automatic Related Work Generation system called ARWG to address this task .	CL_D14-1170_ab_1	CL_D14-1170_ab_4	CL_D14-1170_ab_1_4
CL	D14-1170	ab	4	1	proposal	proposal	none	elaboration	main	secondary	back	elaboration	We propose our Automatic Related Work Generation system called ARWG to address this task .	In this paper , we investigate a challenging task of automatic related work generation .	CL_D14-1170_ab_4	CL_D14-1170_ab_1	CL_D14-1170_ab_1_4
CL	D14-1170	ab	1	5	proposal	proposal_implementation	elaboration	elaboration	secondary	secondary	none	none	In this paper , we investigate a challenging task of automatic related work generation .	It first exploits a PLSA model to split the sentence set of the given papers into different topic-biased parts , and then applies regression models to learn the importance of the sentences .	CL_D14-1170_ab_1	CL_D14-1170_ab_5	CL_D14-1170_ab_1_5
CL	D14-1170	ab	1	6	proposal	proposal_implementation	elaboration	sequence	secondary	secondary	none	none	In this paper , we investigate a challenging task of automatic related work generation .	At last it employs an optimization framework to generate the related work section .	CL_D14-1170_ab_1	CL_D14-1170_ab_6	CL_D14-1170_ab_1_6
CL	D14-1170	ab	1	7	proposal	result_means	elaboration	support	secondary	secondary	none	none	In this paper , we investigate a challenging task of automatic related work generation .	Our evaluation results on a test set of 150 target papers along with their reference papers show that our proposed ARWG system can generate related work sections with better quality .	CL_D14-1170_ab_1	CL_D14-1170_ab_7	CL_D14-1170_ab_1_7
CL	D14-1170	ab	1	8	proposal	result	elaboration	elaboration	secondary	secondary	none	none	In this paper , we investigate a challenging task of automatic related work generation .	A user study is also performed to show ARWG can achieve an improvement over generic multi-document summarization baselines .	CL_D14-1170_ab_1	CL_D14-1170_ab_8	CL_D14-1170_ab_1_8
CL	D14-1170	ab	2	3	proposal	information_additional	elaboration	info-optional	secondary	secondary	none	none	Given multiple reference papers as input , the task aims to generate a related work section for a target paper .	The generated related work section can be used as a draft for the author to complete his or her final related work section .	CL_D14-1170_ab_2	CL_D14-1170_ab_3	CL_D14-1170_ab_2_3
CL	D14-1170	ab	2	4	proposal	proposal	elaboration	none	secondary	main	none	none	Given multiple reference papers as input , the task aims to generate a related work section for a target paper .	We propose our Automatic Related Work Generation system called ARWG to address this task .	CL_D14-1170_ab_2	CL_D14-1170_ab_4	CL_D14-1170_ab_2_4
CL	D14-1170	ab	2	5	proposal	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	Given multiple reference papers as input , the task aims to generate a related work section for a target paper .	It first exploits a PLSA model to split the sentence set of the given papers into different topic-biased parts , and then applies regression models to learn the importance of the sentences .	CL_D14-1170_ab_2	CL_D14-1170_ab_5	CL_D14-1170_ab_2_5
CL	D14-1170	ab	5	2	proposal_implementation	proposal	elaboration	elaboration	secondary	secondary	forw	elaboration	It first exploits a PLSA model to split the sentence set of the given papers into different topic-biased parts , and then applies regression models to learn the importance of the sentences .	Given multiple reference papers as input , the task aims to generate a related work section for a target paper .	CL_D14-1170_ab_5	CL_D14-1170_ab_2	CL_D14-1170_ab_2_5
CL	D14-1170	ab	2	6	proposal	proposal_implementation	elaboration	sequence	secondary	secondary	none	none	Given multiple reference papers as input , the task aims to generate a related work section for a target paper .	At last it employs an optimization framework to generate the related work section .	CL_D14-1170_ab_2	CL_D14-1170_ab_6	CL_D14-1170_ab_2_6
CL	D14-1170	ab	2	7	proposal	result_means	elaboration	support	secondary	secondary	none	none	Given multiple reference papers as input , the task aims to generate a related work section for a target paper .	Our evaluation results on a test set of 150 target papers along with their reference papers show that our proposed ARWG system can generate related work sections with better quality .	CL_D14-1170_ab_2	CL_D14-1170_ab_7	CL_D14-1170_ab_2_7
CL	D14-1170	ab	2	8	proposal	result	elaboration	elaboration	secondary	secondary	none	none	Given multiple reference papers as input , the task aims to generate a related work section for a target paper .	A user study is also performed to show ARWG can achieve an improvement over generic multi-document summarization baselines .	CL_D14-1170_ab_2	CL_D14-1170_ab_8	CL_D14-1170_ab_2_8
CL	D14-1170	ab	3	4	information_additional	proposal	info-optional	none	secondary	main	none	none	The generated related work section can be used as a draft for the author to complete his or her final related work section .	We propose our Automatic Related Work Generation system called ARWG to address this task .	CL_D14-1170_ab_3	CL_D14-1170_ab_4	CL_D14-1170_ab_3_4
CL	D14-1170	ab	3	5	information_additional	proposal_implementation	info-optional	elaboration	secondary	secondary	none	none	The generated related work section can be used as a draft for the author to complete his or her final related work section .	It first exploits a PLSA model to split the sentence set of the given papers into different topic-biased parts , and then applies regression models to learn the importance of the sentences .	CL_D14-1170_ab_3	CL_D14-1170_ab_5	CL_D14-1170_ab_3_5
CL	D14-1170	ab	3	6	information_additional	proposal_implementation	info-optional	sequence	secondary	secondary	forw	info-optional	The generated related work section can be used as a draft for the author to complete his or her final related work section .	At last it employs an optimization framework to generate the related work section .	CL_D14-1170_ab_3	CL_D14-1170_ab_6	CL_D14-1170_ab_3_6
CL	D14-1170	ab	6	3	proposal_implementation	information_additional	sequence	info-optional	secondary	secondary	back	info-optional	At last it employs an optimization framework to generate the related work section .	The generated related work section can be used as a draft for the author to complete his or her final related work section .	CL_D14-1170_ab_6	CL_D14-1170_ab_3	CL_D14-1170_ab_3_6
CL	D14-1170	ab	3	7	information_additional	result_means	info-optional	support	secondary	secondary	none	none	The generated related work section can be used as a draft for the author to complete his or her final related work section .	Our evaluation results on a test set of 150 target papers along with their reference papers show that our proposed ARWG system can generate related work sections with better quality .	CL_D14-1170_ab_3	CL_D14-1170_ab_7	CL_D14-1170_ab_3_7
CL	D14-1170	ab	3	8	information_additional	result	info-optional	elaboration	secondary	secondary	none	none	The generated related work section can be used as a draft for the author to complete his or her final related work section .	A user study is also performed to show ARWG can achieve an improvement over generic multi-document summarization baselines .	CL_D14-1170_ab_3	CL_D14-1170_ab_8	CL_D14-1170_ab_3_8
CL	D14-1170	ab	4	5	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	We propose our Automatic Related Work Generation system called ARWG to address this task .	It first exploits a PLSA model to split the sentence set of the given papers into different topic-biased parts , and then applies regression models to learn the importance of the sentences .	CL_D14-1170_ab_4	CL_D14-1170_ab_5	CL_D14-1170_ab_4_5
CL	D14-1170	ab	4	6	proposal	proposal_implementation	none	sequence	main	secondary	none	none	We propose our Automatic Related Work Generation system called ARWG to address this task .	At last it employs an optimization framework to generate the related work section .	CL_D14-1170_ab_4	CL_D14-1170_ab_6	CL_D14-1170_ab_4_6
CL	D14-1170	ab	4	7	proposal	result_means	none	support	main	secondary	back	support	We propose our Automatic Related Work Generation system called ARWG to address this task .	Our evaluation results on a test set of 150 target papers along with their reference papers show that our proposed ARWG system can generate related work sections with better quality .	CL_D14-1170_ab_4	CL_D14-1170_ab_7	CL_D14-1170_ab_4_7
CL	D14-1170	ab	7	4	result_means	proposal	support	none	secondary	main	forw	support	Our evaluation results on a test set of 150 target papers along with their reference papers show that our proposed ARWG system can generate related work sections with better quality .	We propose our Automatic Related Work Generation system called ARWG to address this task .	CL_D14-1170_ab_7	CL_D14-1170_ab_4	CL_D14-1170_ab_4_7
CL	D14-1170	ab	4	8	proposal	result	none	elaboration	main	secondary	none	none	We propose our Automatic Related Work Generation system called ARWG to address this task .	A user study is also performed to show ARWG can achieve an improvement over generic multi-document summarization baselines .	CL_D14-1170_ab_4	CL_D14-1170_ab_8	CL_D14-1170_ab_4_8
CL	D14-1170	ab	5	6	proposal_implementation	proposal_implementation	elaboration	sequence	secondary	secondary	back	sequence	It first exploits a PLSA model to split the sentence set of the given papers into different topic-biased parts , and then applies regression models to learn the importance of the sentences .	At last it employs an optimization framework to generate the related work section .	CL_D14-1170_ab_5	CL_D14-1170_ab_6	CL_D14-1170_ab_5_6
CL	D14-1170	ab	6	5	proposal_implementation	proposal_implementation	sequence	elaboration	secondary	secondary	forw	sequence	At last it employs an optimization framework to generate the related work section .	It first exploits a PLSA model to split the sentence set of the given papers into different topic-biased parts , and then applies regression models to learn the importance of the sentences .	CL_D14-1170_ab_6	CL_D14-1170_ab_5	CL_D14-1170_ab_5_6
CL	D14-1170	ab	5	7	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	It first exploits a PLSA model to split the sentence set of the given papers into different topic-biased parts , and then applies regression models to learn the importance of the sentences .	Our evaluation results on a test set of 150 target papers along with their reference papers show that our proposed ARWG system can generate related work sections with better quality .	CL_D14-1170_ab_5	CL_D14-1170_ab_7	CL_D14-1170_ab_5_7
CL	D14-1170	ab	5	8	proposal_implementation	result	elaboration	elaboration	secondary	secondary	none	none	It first exploits a PLSA model to split the sentence set of the given papers into different topic-biased parts , and then applies regression models to learn the importance of the sentences .	A user study is also performed to show ARWG can achieve an improvement over generic multi-document summarization baselines .	CL_D14-1170_ab_5	CL_D14-1170_ab_8	CL_D14-1170_ab_5_8
CL	D14-1170	ab	6	7	proposal_implementation	result_means	sequence	support	secondary	secondary	none	none	At last it employs an optimization framework to generate the related work section .	Our evaluation results on a test set of 150 target papers along with their reference papers show that our proposed ARWG system can generate related work sections with better quality .	CL_D14-1170_ab_6	CL_D14-1170_ab_7	CL_D14-1170_ab_6_7
CL	D14-1170	ab	6	8	proposal_implementation	result	sequence	elaboration	secondary	secondary	none	none	At last it employs an optimization framework to generate the related work section .	A user study is also performed to show ARWG can achieve an improvement over generic multi-document summarization baselines .	CL_D14-1170_ab_6	CL_D14-1170_ab_8	CL_D14-1170_ab_6_8
CL	D14-1170	ab	7	8	result_means	result	support	elaboration	secondary	secondary	back	elaboration	Our evaluation results on a test set of 150 target papers along with their reference papers show that our proposed ARWG system can generate related work sections with better quality .	A user study is also performed to show ARWG can achieve an improvement over generic multi-document summarization baselines .	CL_D14-1170_ab_7	CL_D14-1170_ab_8	CL_D14-1170_ab_7_8
CL	D14-1170	ab	8	7	result	result_means	elaboration	support	secondary	secondary	forw	elaboration	A user study is also performed to show ARWG can achieve an improvement over generic multi-document summarization baselines .	Our evaluation results on a test set of 150 target papers along with their reference papers show that our proposed ARWG system can generate related work sections with better quality .	CL_D14-1170_ab_8	CL_D14-1170_ab_7	CL_D14-1170_ab_7_8
CL	D14-1171	ab	1	2	motivation_background	motivation_problem	info-required	support	secondary	secondary	forw	info-required	There are several NLP systems whose accuracy depends crucially on finding misspellings fast .	However , the classical approach is based on a quadratic time algorithm with 80 % coverage .	CL_D14-1171_ab_1	CL_D14-1171_ab_2	CL_D14-1171_ab_1_2
CL	D14-1171	ab	2	1	motivation_problem	motivation_background	support	info-required	secondary	secondary	back	info-required	However , the classical approach is based on a quadratic time algorithm with 80 % coverage .	There are several NLP systems whose accuracy depends crucially on finding misspellings fast .	CL_D14-1171_ab_2	CL_D14-1171_ab_1	CL_D14-1171_ab_1_2
CL	D14-1171	ab	1	3	motivation_background	proposal	info-required	none	secondary	main	none	none	There are several NLP systems whose accuracy depends crucially on finding misspellings fast .	We present a novel algorithm for misspelling detection , which runs in constant time and improves the coverage to more than 96 % .	CL_D14-1171_ab_1	CL_D14-1171_ab_3	CL_D14-1171_ab_1_3
CL	D14-1171	ab	1	4	motivation_background	proposal_implementation	info-required	sequence	secondary	secondary	none	none	There are several NLP systems whose accuracy depends crucially on finding misspellings fast .	We use this algorithm together with a cross document coreference system in order to find proper name misspellings .	CL_D14-1171_ab_1	CL_D14-1171_ab_4	CL_D14-1171_ab_1_4
CL	D14-1171	ab	1	5	motivation_background	result	info-required	support	secondary	secondary	none	none	There are several NLP systems whose accuracy depends crucially on finding misspellings fast .	The experiments confirmed significant improvement over the state of the art .	CL_D14-1171_ab_1	CL_D14-1171_ab_5	CL_D14-1171_ab_1_5
CL	D14-1171	ab	2	3	motivation_problem	proposal	support	none	secondary	main	forw	support	However , the classical approach is based on a quadratic time algorithm with 80 % coverage .	We present a novel algorithm for misspelling detection , which runs in constant time and improves the coverage to more than 96 % .	CL_D14-1171_ab_2	CL_D14-1171_ab_3	CL_D14-1171_ab_2_3
CL	D14-1171	ab	3	2	proposal	motivation_problem	none	support	main	secondary	back	support	We present a novel algorithm for misspelling detection , which runs in constant time and improves the coverage to more than 96 % .	However , the classical approach is based on a quadratic time algorithm with 80 % coverage .	CL_D14-1171_ab_3	CL_D14-1171_ab_2	CL_D14-1171_ab_2_3
CL	D14-1171	ab	2	4	motivation_problem	proposal_implementation	support	sequence	secondary	secondary	none	none	However , the classical approach is based on a quadratic time algorithm with 80 % coverage .	We use this algorithm together with a cross document coreference system in order to find proper name misspellings .	CL_D14-1171_ab_2	CL_D14-1171_ab_4	CL_D14-1171_ab_2_4
CL	D14-1171	ab	2	5	motivation_problem	result	support	support	secondary	secondary	none	none	However , the classical approach is based on a quadratic time algorithm with 80 % coverage .	The experiments confirmed significant improvement over the state of the art .	CL_D14-1171_ab_2	CL_D14-1171_ab_5	CL_D14-1171_ab_2_5
CL	D14-1171	ab	3	4	proposal	proposal_implementation	none	sequence	main	secondary	back	sequence	We present a novel algorithm for misspelling detection , which runs in constant time and improves the coverage to more than 96 % .	We use this algorithm together with a cross document coreference system in order to find proper name misspellings .	CL_D14-1171_ab_3	CL_D14-1171_ab_4	CL_D14-1171_ab_3_4
CL	D14-1171	ab	4	3	proposal_implementation	proposal	sequence	none	secondary	main	forw	sequence	We use this algorithm together with a cross document coreference system in order to find proper name misspellings .	We present a novel algorithm for misspelling detection , which runs in constant time and improves the coverage to more than 96 % .	CL_D14-1171_ab_4	CL_D14-1171_ab_3	CL_D14-1171_ab_3_4
CL	D14-1171	ab	3	5	proposal	result	none	support	main	secondary	back	support	We present a novel algorithm for misspelling detection , which runs in constant time and improves the coverage to more than 96 % .	The experiments confirmed significant improvement over the state of the art .	CL_D14-1171_ab_3	CL_D14-1171_ab_5	CL_D14-1171_ab_3_5
CL	D14-1171	ab	5	3	result	proposal	support	none	secondary	main	forw	support	The experiments confirmed significant improvement over the state of the art .	We present a novel algorithm for misspelling detection , which runs in constant time and improves the coverage to more than 96 % .	CL_D14-1171_ab_5	CL_D14-1171_ab_3	CL_D14-1171_ab_3_5
CL	D14-1171	ab	4	5	proposal_implementation	result	sequence	support	secondary	secondary	none	none	We use this algorithm together with a cross document coreference system in order to find proper name misspellings .	The experiments confirmed significant improvement over the state of the art .	CL_D14-1171_ab_4	CL_D14-1171_ab_5	CL_D14-1171_ab_4_5
CL	D14-1172	ab	1	2	motivation_background	proposal	support	none	secondary	main	forw	support	Learning from errors is a crucial aspect of improving expertise .	Based on this notion , we discuss a robust statistical framework for analysing the impact of different error types on machine translation ( MT ) output quality .	CL_D14-1172_ab_1	CL_D14-1172_ab_2	CL_D14-1172_ab_1_2
CL	D14-1172	ab	2	1	proposal	motivation_background	none	support	main	secondary	back	support	Based on this notion , we discuss a robust statistical framework for analysing the impact of different error types on machine translation ( MT ) output quality .	Learning from errors is a crucial aspect of improving expertise .	CL_D14-1172_ab_2	CL_D14-1172_ab_1	CL_D14-1172_ab_1_2
CL	D14-1172	ab	1	3	motivation_background	proposal_implementation	support	elaboration	secondary	secondary	none	none	Learning from errors is a crucial aspect of improving expertise .	Our approach is based on linear mixed-effects models , which allow the analysis of error-annotated MT output taking into account the variability inherent to the specific experimental setting from which the empirical observations are drawn .	CL_D14-1172_ab_1	CL_D14-1172_ab_3	CL_D14-1172_ab_1_3
CL	D14-1172	ab	1	4	motivation_background	means	support	by-means	secondary	secondary	none	none	Learning from errors is a crucial aspect of improving expertise .	Our experiments are carried out on different language pairs involving Chinese , Arabic and Russian as target languages .	CL_D14-1172_ab_1	CL_D14-1172_ab_4	CL_D14-1172_ab_1_4
CL	D14-1172	ab	1	5	motivation_background	result	support	support	secondary	secondary	none	none	Learning from errors is a crucial aspect of improving expertise .	Interesting findings are reported , concerning the impact of different error types both at the level of human perception of quality and with respect to performance results measured with automatic metrics .	CL_D14-1172_ab_1	CL_D14-1172_ab_5	CL_D14-1172_ab_1_5
CL	D14-1172	ab	2	3	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	Based on this notion , we discuss a robust statistical framework for analysing the impact of different error types on machine translation ( MT ) output quality .	Our approach is based on linear mixed-effects models , which allow the analysis of error-annotated MT output taking into account the variability inherent to the specific experimental setting from which the empirical observations are drawn .	CL_D14-1172_ab_2	CL_D14-1172_ab_3	CL_D14-1172_ab_2_3
CL	D14-1172	ab	3	2	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	Our approach is based on linear mixed-effects models , which allow the analysis of error-annotated MT output taking into account the variability inherent to the specific experimental setting from which the empirical observations are drawn .	Based on this notion , we discuss a robust statistical framework for analysing the impact of different error types on machine translation ( MT ) output quality .	CL_D14-1172_ab_3	CL_D14-1172_ab_2	CL_D14-1172_ab_2_3
CL	D14-1172	ab	2	4	proposal	means	none	by-means	main	secondary	none	none	Based on this notion , we discuss a robust statistical framework for analysing the impact of different error types on machine translation ( MT ) output quality .	Our experiments are carried out on different language pairs involving Chinese , Arabic and Russian as target languages .	CL_D14-1172_ab_2	CL_D14-1172_ab_4	CL_D14-1172_ab_2_4
CL	D14-1172	ab	2	5	proposal	result	none	support	main	secondary	back	support	Based on this notion , we discuss a robust statistical framework for analysing the impact of different error types on machine translation ( MT ) output quality .	Interesting findings are reported , concerning the impact of different error types both at the level of human perception of quality and with respect to performance results measured with automatic metrics .	CL_D14-1172_ab_2	CL_D14-1172_ab_5	CL_D14-1172_ab_2_5
CL	D14-1172	ab	5	2	result	proposal	support	none	secondary	main	forw	support	Interesting findings are reported , concerning the impact of different error types both at the level of human perception of quality and with respect to performance results measured with automatic metrics .	Based on this notion , we discuss a robust statistical framework for analysing the impact of different error types on machine translation ( MT ) output quality .	CL_D14-1172_ab_5	CL_D14-1172_ab_2	CL_D14-1172_ab_2_5
CL	D14-1172	ab	3	4	proposal_implementation	means	elaboration	by-means	secondary	secondary	none	none	Our approach is based on linear mixed-effects models , which allow the analysis of error-annotated MT output taking into account the variability inherent to the specific experimental setting from which the empirical observations are drawn .	Our experiments are carried out on different language pairs involving Chinese , Arabic and Russian as target languages .	CL_D14-1172_ab_3	CL_D14-1172_ab_4	CL_D14-1172_ab_3_4
CL	D14-1172	ab	3	5	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	Our approach is based on linear mixed-effects models , which allow the analysis of error-annotated MT output taking into account the variability inherent to the specific experimental setting from which the empirical observations are drawn .	Interesting findings are reported , concerning the impact of different error types both at the level of human perception of quality and with respect to performance results measured with automatic metrics .	CL_D14-1172_ab_3	CL_D14-1172_ab_5	CL_D14-1172_ab_3_5
CL	D14-1172	ab	4	5	means	result	by-means	support	secondary	secondary	forw	by-means	Our experiments are carried out on different language pairs involving Chinese , Arabic and Russian as target languages .	Interesting findings are reported , concerning the impact of different error types both at the level of human perception of quality and with respect to performance results measured with automatic metrics .	CL_D14-1172_ab_4	CL_D14-1172_ab_5	CL_D14-1172_ab_4_5
CL	D14-1172	ab	5	4	result	means	support	by-means	secondary	secondary	back	by-means	Interesting findings are reported , concerning the impact of different error types both at the level of human perception of quality and with respect to performance results measured with automatic metrics .	Our experiments are carried out on different language pairs involving Chinese , Arabic and Russian as target languages .	CL_D14-1172_ab_5	CL_D14-1172_ab_4	CL_D14-1172_ab_4_5
CL	D14-1173	ab	1	2	motivation_background	motivation_background	info-required	info-required	secondary	secondary	forw	info-required	Languages that have no explicit word delimiters often have to be segmented for statistical machine translation ( SMT ) .	This is commonly performed by automated segmenters trained on manually annotated corpora .	CL_D14-1173_ab_1	CL_D14-1173_ab_2	CL_D14-1173_ab_1_2
CL	D14-1173	ab	2	1	motivation_background	motivation_background	info-required	info-required	secondary	secondary	back	info-required	This is commonly performed by automated segmenters trained on manually annotated corpora .	Languages that have no explicit word delimiters often have to be segmented for statistical machine translation ( SMT ) .	CL_D14-1173_ab_2	CL_D14-1173_ab_1	CL_D14-1173_ab_1_2
CL	D14-1173	ab	1	3	motivation_background	motivation_problem	info-required	support	secondary	secondary	none	none	Languages that have no explicit word delimiters often have to be segmented for statistical machine translation ( SMT ) .	However , the word segmentation ( WS ) schemes of these annotated corpora are handcrafted for general usage , and may not be suitable for SMT .	CL_D14-1173_ab_1	CL_D14-1173_ab_3	CL_D14-1173_ab_1_3
CL	D14-1173	ab	1	4	motivation_background	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	Languages that have no explicit word delimiters often have to be segmented for statistical machine translation ( SMT ) .	An analysis was performed to test this hypothesis using a manually annotated word alignment ( WA ) corpus for Chinese-English SMT .	CL_D14-1173_ab_1	CL_D14-1173_ab_4	CL_D14-1173_ab_1_4
CL	D14-1173	ab	1	5	motivation_background	result_means	info-required	by-means	secondary	secondary	none	none	Languages that have no explicit word delimiters often have to be segmented for statistical machine translation ( SMT ) .	An analysis revealed that 74.60 % of the sentences in the WA corpus if segmented using an automated segmenter trained on the Penn Chinese Treebank ( CTB ) will contain conflicts with the gold WA annotations .	CL_D14-1173_ab_1	CL_D14-1173_ab_5	CL_D14-1173_ab_1_5
CL	D14-1173	ab	1	6	motivation_background	proposal	info-required	none	secondary	main	none	none	Languages that have no explicit word delimiters often have to be segmented for statistical machine translation ( SMT ) .	We formulated an approach based on word splitting with reference to the annotated WA to alleviate these conflicts .	CL_D14-1173_ab_1	CL_D14-1173_ab_6	CL_D14-1173_ab_1_6
CL	D14-1173	ab	1	7	motivation_background	result_means	info-required	support	secondary	secondary	none	none	Languages that have no explicit word delimiters often have to be segmented for statistical machine translation ( SMT ) .	Experimental results show that the refined WS reduced word alignment error rate by 6.82 % and achieved the highest BLEU improvement ( 0.63 on average ) on the Chinese-English open machine translation ( OpenMT ) corpora compared to related work .	CL_D14-1173_ab_1	CL_D14-1173_ab_7	CL_D14-1173_ab_1_7
CL	D14-1173	ab	2	3	motivation_background	motivation_problem	info-required	support	secondary	secondary	forw	info-required	This is commonly performed by automated segmenters trained on manually annotated corpora .	However , the word segmentation ( WS ) schemes of these annotated corpora are handcrafted for general usage , and may not be suitable for SMT .	CL_D14-1173_ab_2	CL_D14-1173_ab_3	CL_D14-1173_ab_2_3
CL	D14-1173	ab	3	2	motivation_problem	motivation_background	support	info-required	secondary	secondary	back	info-required	However , the word segmentation ( WS ) schemes of these annotated corpora are handcrafted for general usage , and may not be suitable for SMT .	This is commonly performed by automated segmenters trained on manually annotated corpora .	CL_D14-1173_ab_3	CL_D14-1173_ab_2	CL_D14-1173_ab_2_3
CL	D14-1173	ab	2	4	motivation_background	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	This is commonly performed by automated segmenters trained on manually annotated corpora .	An analysis was performed to test this hypothesis using a manually annotated word alignment ( WA ) corpus for Chinese-English SMT .	CL_D14-1173_ab_2	CL_D14-1173_ab_4	CL_D14-1173_ab_2_4
CL	D14-1173	ab	2	5	motivation_background	result_means	info-required	by-means	secondary	secondary	none	none	This is commonly performed by automated segmenters trained on manually annotated corpora .	An analysis revealed that 74.60 % of the sentences in the WA corpus if segmented using an automated segmenter trained on the Penn Chinese Treebank ( CTB ) will contain conflicts with the gold WA annotations .	CL_D14-1173_ab_2	CL_D14-1173_ab_5	CL_D14-1173_ab_2_5
CL	D14-1173	ab	2	6	motivation_background	proposal	info-required	none	secondary	main	none	none	This is commonly performed by automated segmenters trained on manually annotated corpora .	We formulated an approach based on word splitting with reference to the annotated WA to alleviate these conflicts .	CL_D14-1173_ab_2	CL_D14-1173_ab_6	CL_D14-1173_ab_2_6
CL	D14-1173	ab	2	7	motivation_background	result_means	info-required	support	secondary	secondary	none	none	This is commonly performed by automated segmenters trained on manually annotated corpora .	Experimental results show that the refined WS reduced word alignment error rate by 6.82 % and achieved the highest BLEU improvement ( 0.63 on average ) on the Chinese-English open machine translation ( OpenMT ) corpora compared to related work .	CL_D14-1173_ab_2	CL_D14-1173_ab_7	CL_D14-1173_ab_2_7
CL	D14-1173	ab	3	4	motivation_problem	proposal_implementation	support	elaboration	secondary	secondary	none	none	However , the word segmentation ( WS ) schemes of these annotated corpora are handcrafted for general usage , and may not be suitable for SMT .	An analysis was performed to test this hypothesis using a manually annotated word alignment ( WA ) corpus for Chinese-English SMT .	CL_D14-1173_ab_3	CL_D14-1173_ab_4	CL_D14-1173_ab_3_4
CL	D14-1173	ab	3	5	motivation_problem	result_means	support	by-means	secondary	secondary	none	none	However , the word segmentation ( WS ) schemes of these annotated corpora are handcrafted for general usage , and may not be suitable for SMT .	An analysis revealed that 74.60 % of the sentences in the WA corpus if segmented using an automated segmenter trained on the Penn Chinese Treebank ( CTB ) will contain conflicts with the gold WA annotations .	CL_D14-1173_ab_3	CL_D14-1173_ab_5	CL_D14-1173_ab_3_5
CL	D14-1173	ab	3	6	motivation_problem	proposal	support	none	secondary	main	forw	support	However , the word segmentation ( WS ) schemes of these annotated corpora are handcrafted for general usage , and may not be suitable for SMT .	We formulated an approach based on word splitting with reference to the annotated WA to alleviate these conflicts .	CL_D14-1173_ab_3	CL_D14-1173_ab_6	CL_D14-1173_ab_3_6
CL	D14-1173	ab	6	3	proposal	motivation_problem	none	support	main	secondary	back	support	We formulated an approach based on word splitting with reference to the annotated WA to alleviate these conflicts .	However , the word segmentation ( WS ) schemes of these annotated corpora are handcrafted for general usage , and may not be suitable for SMT .	CL_D14-1173_ab_6	CL_D14-1173_ab_3	CL_D14-1173_ab_3_6
CL	D14-1173	ab	3	7	motivation_problem	result_means	support	support	secondary	secondary	none	none	However , the word segmentation ( WS ) schemes of these annotated corpora are handcrafted for general usage , and may not be suitable for SMT .	Experimental results show that the refined WS reduced word alignment error rate by 6.82 % and achieved the highest BLEU improvement ( 0.63 on average ) on the Chinese-English open machine translation ( OpenMT ) corpora compared to related work .	CL_D14-1173_ab_3	CL_D14-1173_ab_7	CL_D14-1173_ab_3_7
CL	D14-1173	ab	4	5	proposal_implementation	result_means	elaboration	by-means	secondary	secondary	back	by-means	An analysis was performed to test this hypothesis using a manually annotated word alignment ( WA ) corpus for Chinese-English SMT .	An analysis revealed that 74.60 % of the sentences in the WA corpus if segmented using an automated segmenter trained on the Penn Chinese Treebank ( CTB ) will contain conflicts with the gold WA annotations .	CL_D14-1173_ab_4	CL_D14-1173_ab_5	CL_D14-1173_ab_4_5
CL	D14-1173	ab	5	4	result_means	proposal_implementation	by-means	elaboration	secondary	secondary	forw	by-means	An analysis revealed that 74.60 % of the sentences in the WA corpus if segmented using an automated segmenter trained on the Penn Chinese Treebank ( CTB ) will contain conflicts with the gold WA annotations .	An analysis was performed to test this hypothesis using a manually annotated word alignment ( WA ) corpus for Chinese-English SMT .	CL_D14-1173_ab_5	CL_D14-1173_ab_4	CL_D14-1173_ab_4_5
CL	D14-1173	ab	4	6	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	An analysis was performed to test this hypothesis using a manually annotated word alignment ( WA ) corpus for Chinese-English SMT .	We formulated an approach based on word splitting with reference to the annotated WA to alleviate these conflicts .	CL_D14-1173_ab_4	CL_D14-1173_ab_6	CL_D14-1173_ab_4_6
CL	D14-1173	ab	6	4	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	We formulated an approach based on word splitting with reference to the annotated WA to alleviate these conflicts .	An analysis was performed to test this hypothesis using a manually annotated word alignment ( WA ) corpus for Chinese-English SMT .	CL_D14-1173_ab_6	CL_D14-1173_ab_4	CL_D14-1173_ab_4_6
CL	D14-1173	ab	4	7	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	An analysis was performed to test this hypothesis using a manually annotated word alignment ( WA ) corpus for Chinese-English SMT .	Experimental results show that the refined WS reduced word alignment error rate by 6.82 % and achieved the highest BLEU improvement ( 0.63 on average ) on the Chinese-English open machine translation ( OpenMT ) corpora compared to related work .	CL_D14-1173_ab_4	CL_D14-1173_ab_7	CL_D14-1173_ab_4_7
CL	D14-1173	ab	5	6	result_means	proposal	by-means	none	secondary	main	none	none	An analysis revealed that 74.60 % of the sentences in the WA corpus if segmented using an automated segmenter trained on the Penn Chinese Treebank ( CTB ) will contain conflicts with the gold WA annotations .	We formulated an approach based on word splitting with reference to the annotated WA to alleviate these conflicts .	CL_D14-1173_ab_5	CL_D14-1173_ab_6	CL_D14-1173_ab_5_6
CL	D14-1173	ab	5	7	result_means	result_means	by-means	support	secondary	secondary	none	none	An analysis revealed that 74.60 % of the sentences in the WA corpus if segmented using an automated segmenter trained on the Penn Chinese Treebank ( CTB ) will contain conflicts with the gold WA annotations .	Experimental results show that the refined WS reduced word alignment error rate by 6.82 % and achieved the highest BLEU improvement ( 0.63 on average ) on the Chinese-English open machine translation ( OpenMT ) corpora compared to related work .	CL_D14-1173_ab_5	CL_D14-1173_ab_7	CL_D14-1173_ab_5_7
CL	D14-1173	ab	6	7	proposal	result_means	none	support	main	secondary	back	support	We formulated an approach based on word splitting with reference to the annotated WA to alleviate these conflicts .	Experimental results show that the refined WS reduced word alignment error rate by 6.82 % and achieved the highest BLEU improvement ( 0.63 on average ) on the Chinese-English open machine translation ( OpenMT ) corpora compared to related work .	CL_D14-1173_ab_6	CL_D14-1173_ab_7	CL_D14-1173_ab_6_7
CL	D14-1173	ab	7	6	result_means	proposal	support	none	secondary	main	forw	support	Experimental results show that the refined WS reduced word alignment error rate by 6.82 % and achieved the highest BLEU improvement ( 0.63 on average ) on the Chinese-English open machine translation ( OpenMT ) corpora compared to related work .	We formulated an approach based on word splitting with reference to the annotated WA to alleviate these conflicts .	CL_D14-1173_ab_7	CL_D14-1173_ab_6	CL_D14-1173_ab_6_7
CL	D14-1174	ab	1	2	motivation_background	motivation_hypothesis	info-required	support	secondary	secondary	forw	info-required	To overcome the scarceness of bilingual corpora for some language pairs in machine translation , pivot-based SMT uses pivot language as a `` bridge '' to generate source-target translation from source-pivot and pivot-target translation .	One of the key issues is to estimate the probabilities for the generated phrase pairs .	CL_D14-1174_ab_1	CL_D14-1174_ab_2	CL_D14-1174_ab_1_2
CL	D14-1174	ab	2	1	motivation_hypothesis	motivation_background	support	info-required	secondary	secondary	back	info-required	One of the key issues is to estimate the probabilities for the generated phrase pairs .	To overcome the scarceness of bilingual corpora for some language pairs in machine translation , pivot-based SMT uses pivot language as a `` bridge '' to generate source-target translation from source-pivot and pivot-target translation .	CL_D14-1174_ab_2	CL_D14-1174_ab_1	CL_D14-1174_ab_1_2
CL	D14-1174	ab	1	3	motivation_background	proposal_implementation	info-required	none	secondary	main	none	none	To overcome the scarceness of bilingual corpora for some language pairs in machine translation , pivot-based SMT uses pivot language as a `` bridge '' to generate source-target translation from source-pivot and pivot-target translation .	In this paper , we present a novel approach to calculate the translation probability by pivoting the co-occurrence count of source-pivot and pivot-target phrase pairs .	CL_D14-1174_ab_1	CL_D14-1174_ab_3	CL_D14-1174_ab_1_3
CL	D14-1174	ab	1	4	motivation_background	result_means	info-required	support	secondary	secondary	none	none	To overcome the scarceness of bilingual corpora for some language pairs in machine translation , pivot-based SMT uses pivot language as a `` bridge '' to generate source-target translation from source-pivot and pivot-target translation .	Experimental results on Europarl data and web data show that our method leads to significant improvements over the baseline systems .	CL_D14-1174_ab_1	CL_D14-1174_ab_4	CL_D14-1174_ab_1_4
CL	D14-1174	ab	2	3	motivation_hypothesis	proposal_implementation	support	none	secondary	main	forw	support	One of the key issues is to estimate the probabilities for the generated phrase pairs .	In this paper , we present a novel approach to calculate the translation probability by pivoting the co-occurrence count of source-pivot and pivot-target phrase pairs .	CL_D14-1174_ab_2	CL_D14-1174_ab_3	CL_D14-1174_ab_2_3
CL	D14-1174	ab	3	2	proposal_implementation	motivation_hypothesis	none	support	main	secondary	back	support	In this paper , we present a novel approach to calculate the translation probability by pivoting the co-occurrence count of source-pivot and pivot-target phrase pairs .	One of the key issues is to estimate the probabilities for the generated phrase pairs .	CL_D14-1174_ab_3	CL_D14-1174_ab_2	CL_D14-1174_ab_2_3
CL	D14-1174	ab	2	4	motivation_hypothesis	result_means	support	support	secondary	secondary	none	none	One of the key issues is to estimate the probabilities for the generated phrase pairs .	Experimental results on Europarl data and web data show that our method leads to significant improvements over the baseline systems .	CL_D14-1174_ab_2	CL_D14-1174_ab_4	CL_D14-1174_ab_2_4
CL	D14-1174	ab	3	4	proposal_implementation	result_means	none	support	main	secondary	back	support	In this paper , we present a novel approach to calculate the translation probability by pivoting the co-occurrence count of source-pivot and pivot-target phrase pairs .	Experimental results on Europarl data and web data show that our method leads to significant improvements over the baseline systems .	CL_D14-1174_ab_3	CL_D14-1174_ab_4	CL_D14-1174_ab_3_4
CL	D14-1174	ab	4	3	result_means	proposal_implementation	support	none	secondary	main	forw	support	Experimental results on Europarl data and web data show that our method leads to significant improvements over the baseline systems .	In this paper , we present a novel approach to calculate the translation probability by pivoting the co-occurrence count of source-pivot and pivot-target phrase pairs .	CL_D14-1174_ab_4	CL_D14-1174_ab_3	CL_D14-1174_ab_3_4
CL	D14-1175	ab	1	2	motivation_problem	proposal	support	none	secondary	main	forw	support	Translating into morphologically rich languages is a particularly difficult problem in machine translation due to the high degree of inflectional ambiguity in the target language , often only poorly captured by existing word translation models .	We present a general approach that exploits source-side contexts of foreign words to improve translation prediction accuracy .	CL_D14-1175_ab_1	CL_D14-1175_ab_2	CL_D14-1175_ab_1_2
CL	D14-1175	ab	2	1	proposal	motivation_problem	none	support	main	secondary	back	support	We present a general approach that exploits source-side contexts of foreign words to improve translation prediction accuracy .	Translating into morphologically rich languages is a particularly difficult problem in machine translation due to the high degree of inflectional ambiguity in the target language , often only poorly captured by existing word translation models .	CL_D14-1175_ab_2	CL_D14-1175_ab_1	CL_D14-1175_ab_1_2
CL	D14-1175	ab	1	3	motivation_problem	proposal_implementation	support	elaboration	secondary	secondary	none	none	Translating into morphologically rich languages is a particularly difficult problem in machine translation due to the high degree of inflectional ambiguity in the target language , often only poorly captured by existing word translation models .	Our approach is based on a probabilistic neural network which does not require linguistic annotation nor manual feature engineering .	CL_D14-1175_ab_1	CL_D14-1175_ab_3	CL_D14-1175_ab_1_3
CL	D14-1175	ab	1	4	motivation_problem	result	support	support	secondary	secondary	none	none	Translating into morphologically rich languages is a particularly difficult problem in machine translation due to the high degree of inflectional ambiguity in the target language , often only poorly captured by existing word translation models .	We report significant improvements in word translation prediction accuracy for three morphologically rich target languages .	CL_D14-1175_ab_1	CL_D14-1175_ab_4	CL_D14-1175_ab_1_4
CL	D14-1175	ab	1	5	motivation_problem	result	support	elaboration	secondary	secondary	none	none	Translating into morphologically rich languages is a particularly difficult problem in machine translation due to the high degree of inflectional ambiguity in the target language , often only poorly captured by existing word translation models .	In addition , preliminary results for integrating our approach into a large-scale English-Russian statistical machine translation system show small but statistically significant improvements in translation quality .	CL_D14-1175_ab_1	CL_D14-1175_ab_5	CL_D14-1175_ab_1_5
CL	D14-1175	ab	2	3	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	We present a general approach that exploits source-side contexts of foreign words to improve translation prediction accuracy .	Our approach is based on a probabilistic neural network which does not require linguistic annotation nor manual feature engineering .	CL_D14-1175_ab_2	CL_D14-1175_ab_3	CL_D14-1175_ab_2_3
CL	D14-1175	ab	3	2	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	Our approach is based on a probabilistic neural network which does not require linguistic annotation nor manual feature engineering .	We present a general approach that exploits source-side contexts of foreign words to improve translation prediction accuracy .	CL_D14-1175_ab_3	CL_D14-1175_ab_2	CL_D14-1175_ab_2_3
CL	D14-1175	ab	2	4	proposal	result	none	support	main	secondary	back	support	We present a general approach that exploits source-side contexts of foreign words to improve translation prediction accuracy .	We report significant improvements in word translation prediction accuracy for three morphologically rich target languages .	CL_D14-1175_ab_2	CL_D14-1175_ab_4	CL_D14-1175_ab_2_4
CL	D14-1175	ab	4	2	result	proposal	support	none	secondary	main	forw	support	We report significant improvements in word translation prediction accuracy for three morphologically rich target languages .	We present a general approach that exploits source-side contexts of foreign words to improve translation prediction accuracy .	CL_D14-1175_ab_4	CL_D14-1175_ab_2	CL_D14-1175_ab_2_4
CL	D14-1175	ab	2	5	proposal	result	none	elaboration	main	secondary	none	none	We present a general approach that exploits source-side contexts of foreign words to improve translation prediction accuracy .	In addition , preliminary results for integrating our approach into a large-scale English-Russian statistical machine translation system show small but statistically significant improvements in translation quality .	CL_D14-1175_ab_2	CL_D14-1175_ab_5	CL_D14-1175_ab_2_5
CL	D14-1175	ab	3	4	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	Our approach is based on a probabilistic neural network which does not require linguistic annotation nor manual feature engineering .	We report significant improvements in word translation prediction accuracy for three morphologically rich target languages .	CL_D14-1175_ab_3	CL_D14-1175_ab_4	CL_D14-1175_ab_3_4
CL	D14-1175	ab	3	5	proposal_implementation	result	elaboration	elaboration	secondary	secondary	none	none	Our approach is based on a probabilistic neural network which does not require linguistic annotation nor manual feature engineering .	In addition , preliminary results for integrating our approach into a large-scale English-Russian statistical machine translation system show small but statistically significant improvements in translation quality .	CL_D14-1175_ab_3	CL_D14-1175_ab_5	CL_D14-1175_ab_3_5
CL	D14-1175	ab	4	5	result	result	support	elaboration	secondary	secondary	back	elaboration	We report significant improvements in word translation prediction accuracy for three morphologically rich target languages .	In addition , preliminary results for integrating our approach into a large-scale English-Russian statistical machine translation system show small but statistically significant improvements in translation quality .	CL_D14-1175_ab_4	CL_D14-1175_ab_5	CL_D14-1175_ab_4_5
CL	D14-1175	ab	5	4	result	result	elaboration	support	secondary	secondary	forw	elaboration	In addition , preliminary results for integrating our approach into a large-scale English-Russian statistical machine translation system show small but statistically significant improvements in translation quality .	We report significant improvements in word translation prediction accuracy for three morphologically rich target languages .	CL_D14-1175_ab_5	CL_D14-1175_ab_4	CL_D14-1175_ab_4_5
CL	D14-1176	ab	1	2	proposal_implementation	proposal_implementation	none	elaboration	main	secondary	back	elaboration	This paper presents a novel approach to improve reordering in phrase-based machine translation by using richer , syntactic representations of units of bilingual language models ( BiLMs ) .	Our method to include syntactic information is simple in implementation and requires minimal changes in the decoding algorithm .	CL_D14-1176_ab_1	CL_D14-1176_ab_2	CL_D14-1176_ab_1_2
CL	D14-1176	ab	2	1	proposal_implementation	proposal_implementation	elaboration	none	secondary	main	forw	elaboration	Our method to include syntactic information is simple in implementation and requires minimal changes in the decoding algorithm .	This paper presents a novel approach to improve reordering in phrase-based machine translation by using richer , syntactic representations of units of bilingual language models ( BiLMs ) .	CL_D14-1176_ab_2	CL_D14-1176_ab_1	CL_D14-1176_ab_1_2
CL	D14-1176	ab	1	3	proposal_implementation	means	none	by-means	main	secondary	none	none	This paper presents a novel approach to improve reordering in phrase-based machine translation by using richer , syntactic representations of units of bilingual language models ( BiLMs ) .	The approach is evaluated in a series of Arabic-English and Chinese-English translation experiments .	CL_D14-1176_ab_1	CL_D14-1176_ab_3	CL_D14-1176_ab_1_3
CL	D14-1176	ab	1	4	proposal_implementation	result	none	support	main	secondary	back	support	This paper presents a novel approach to improve reordering in phrase-based machine translation by using richer , syntactic representations of units of bilingual language models ( BiLMs ) .	The best models demonstrate significant improvements in BLEU and TER over the phrase-based baseline , as well as over the lexicalized BiLM by Niehues et al. ( 2011 ) .	CL_D14-1176_ab_1	CL_D14-1176_ab_4	CL_D14-1176_ab_1_4
CL	D14-1176	ab	4	1	result	proposal_implementation	support	none	secondary	main	forw	support	The best models demonstrate significant improvements in BLEU and TER over the phrase-based baseline , as well as over the lexicalized BiLM by Niehues et al. ( 2011 ) .	This paper presents a novel approach to improve reordering in phrase-based machine translation by using richer , syntactic representations of units of bilingual language models ( BiLMs ) .	CL_D14-1176_ab_4	CL_D14-1176_ab_1	CL_D14-1176_ab_1_4
CL	D14-1176	ab	1	5	proposal_implementation	observation	none	elaboration	main	secondary	none	none	This paper presents a novel approach to improve reordering in phrase-based machine translation by using richer , syntactic representations of units of bilingual language models ( BiLMs ) .	Further improvements of up to 0.45 BLEU for Arabic-English and up to 0.59 BLEU for Chinese-English are obtained by combining our dependency BiLM with a lexicalized BiLM .	CL_D14-1176_ab_1	CL_D14-1176_ab_5	CL_D14-1176_ab_1_5
CL	D14-1176	ab	1	6	proposal_implementation	observation	none	elaboration	main	secondary	none	none	This paper presents a novel approach to improve reordering in phrase-based machine translation by using richer , syntactic representations of units of bilingual language models ( BiLMs ) .	An improvement of 0.98 BLEU is obtained for Chinese-English in the setting of an increased distortion limit .	CL_D14-1176_ab_1	CL_D14-1176_ab_6	CL_D14-1176_ab_1_6
CL	D14-1176	ab	2	3	proposal_implementation	means	elaboration	by-means	secondary	secondary	none	none	Our method to include syntactic information is simple in implementation and requires minimal changes in the decoding algorithm .	The approach is evaluated in a series of Arabic-English and Chinese-English translation experiments .	CL_D14-1176_ab_2	CL_D14-1176_ab_3	CL_D14-1176_ab_2_3
CL	D14-1176	ab	2	4	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	Our method to include syntactic information is simple in implementation and requires minimal changes in the decoding algorithm .	The best models demonstrate significant improvements in BLEU and TER over the phrase-based baseline , as well as over the lexicalized BiLM by Niehues et al. ( 2011 ) .	CL_D14-1176_ab_2	CL_D14-1176_ab_4	CL_D14-1176_ab_2_4
CL	D14-1176	ab	2	5	proposal_implementation	observation	elaboration	elaboration	secondary	secondary	none	none	Our method to include syntactic information is simple in implementation and requires minimal changes in the decoding algorithm .	Further improvements of up to 0.45 BLEU for Arabic-English and up to 0.59 BLEU for Chinese-English are obtained by combining our dependency BiLM with a lexicalized BiLM .	CL_D14-1176_ab_2	CL_D14-1176_ab_5	CL_D14-1176_ab_2_5
CL	D14-1176	ab	2	6	proposal_implementation	observation	elaboration	elaboration	secondary	secondary	none	none	Our method to include syntactic information is simple in implementation and requires minimal changes in the decoding algorithm .	An improvement of 0.98 BLEU is obtained for Chinese-English in the setting of an increased distortion limit .	CL_D14-1176_ab_2	CL_D14-1176_ab_6	CL_D14-1176_ab_2_6
CL	D14-1176	ab	3	4	means	result	by-means	support	secondary	secondary	forw	by-means	The approach is evaluated in a series of Arabic-English and Chinese-English translation experiments .	The best models demonstrate significant improvements in BLEU and TER over the phrase-based baseline , as well as over the lexicalized BiLM by Niehues et al. ( 2011 ) .	CL_D14-1176_ab_3	CL_D14-1176_ab_4	CL_D14-1176_ab_3_4
CL	D14-1176	ab	4	3	result	means	support	by-means	secondary	secondary	back	by-means	The best models demonstrate significant improvements in BLEU and TER over the phrase-based baseline , as well as over the lexicalized BiLM by Niehues et al. ( 2011 ) .	The approach is evaluated in a series of Arabic-English and Chinese-English translation experiments .	CL_D14-1176_ab_4	CL_D14-1176_ab_3	CL_D14-1176_ab_3_4
CL	D14-1176	ab	3	5	means	observation	by-means	elaboration	secondary	secondary	none	none	The approach is evaluated in a series of Arabic-English and Chinese-English translation experiments .	Further improvements of up to 0.45 BLEU for Arabic-English and up to 0.59 BLEU for Chinese-English are obtained by combining our dependency BiLM with a lexicalized BiLM .	CL_D14-1176_ab_3	CL_D14-1176_ab_5	CL_D14-1176_ab_3_5
CL	D14-1176	ab	3	6	means	observation	by-means	elaboration	secondary	secondary	none	none	The approach is evaluated in a series of Arabic-English and Chinese-English translation experiments .	An improvement of 0.98 BLEU is obtained for Chinese-English in the setting of an increased distortion limit .	CL_D14-1176_ab_3	CL_D14-1176_ab_6	CL_D14-1176_ab_3_6
CL	D14-1176	ab	4	5	result	observation	support	elaboration	secondary	secondary	back	elaboration	The best models demonstrate significant improvements in BLEU and TER over the phrase-based baseline , as well as over the lexicalized BiLM by Niehues et al. ( 2011 ) .	Further improvements of up to 0.45 BLEU for Arabic-English and up to 0.59 BLEU for Chinese-English are obtained by combining our dependency BiLM with a lexicalized BiLM .	CL_D14-1176_ab_4	CL_D14-1176_ab_5	CL_D14-1176_ab_4_5
CL	D14-1176	ab	5	4	observation	result	elaboration	support	secondary	secondary	forw	elaboration	Further improvements of up to 0.45 BLEU for Arabic-English and up to 0.59 BLEU for Chinese-English are obtained by combining our dependency BiLM with a lexicalized BiLM .	The best models demonstrate significant improvements in BLEU and TER over the phrase-based baseline , as well as over the lexicalized BiLM by Niehues et al. ( 2011 ) .	CL_D14-1176_ab_5	CL_D14-1176_ab_4	CL_D14-1176_ab_4_5
CL	D14-1176	ab	4	6	result	observation	support	elaboration	secondary	secondary	none	none	The best models demonstrate significant improvements in BLEU and TER over the phrase-based baseline , as well as over the lexicalized BiLM by Niehues et al. ( 2011 ) .	An improvement of 0.98 BLEU is obtained for Chinese-English in the setting of an increased distortion limit .	CL_D14-1176_ab_4	CL_D14-1176_ab_6	CL_D14-1176_ab_4_6
CL	D14-1176	ab	5	6	observation	observation	elaboration	elaboration	secondary	secondary	back	elaboration	Further improvements of up to 0.45 BLEU for Arabic-English and up to 0.59 BLEU for Chinese-English are obtained by combining our dependency BiLM with a lexicalized BiLM .	An improvement of 0.98 BLEU is obtained for Chinese-English in the setting of an increased distortion limit .	CL_D14-1176_ab_5	CL_D14-1176_ab_6	CL_D14-1176_ab_5_6
CL	D14-1176	ab	6	5	observation	observation	elaboration	elaboration	secondary	secondary	forw	elaboration	An improvement of 0.98 BLEU is obtained for Chinese-English in the setting of an increased distortion limit .	Further improvements of up to 0.45 BLEU for Arabic-English and up to 0.59 BLEU for Chinese-English are obtained by combining our dependency BiLM with a lexicalized BiLM .	CL_D14-1176_ab_6	CL_D14-1176_ab_5	CL_D14-1176_ab_5_6
CL	D14-1177	ab	1	2	motivation_problem	proposal	support	none	secondary	main	forw	support	Automatically compiling bilingual dictionaries of technical terms from comparable corpora is a challenging problem , yet with many potential applications .	In this paper , we exploit two independent observations about term translations : ( a ) terms are often formed by corresponding sub-lexical units across languages and ( b ) a term and its translation tend to appear in similar lexical context .	CL_D14-1177_ab_1	CL_D14-1177_ab_2	CL_D14-1177_ab_1_2
CL	D14-1177	ab	2	1	proposal	motivation_problem	none	support	main	secondary	back	support	In this paper , we exploit two independent observations about term translations : ( a ) terms are often formed by corresponding sub-lexical units across languages and ( b ) a term and its translation tend to appear in similar lexical context .	Automatically compiling bilingual dictionaries of technical terms from comparable corpora is a challenging problem , yet with many potential applications .	CL_D14-1177_ab_2	CL_D14-1177_ab_1	CL_D14-1177_ab_1_2
CL	D14-1177	ab	1	3	motivation_problem	proposal_implementation	support	elaboration	secondary	secondary	none	none	Automatically compiling bilingual dictionaries of technical terms from comparable corpora is a challenging problem , yet with many potential applications .	Based on the first observation , we develop a new character n-gram compositional method , a logistic regression classifier , for learning a string similarity measure of term translations .	CL_D14-1177_ab_1	CL_D14-1177_ab_3	CL_D14-1177_ab_1_3
CL	D14-1177	ab	1	4	motivation_problem	proposal_implementation	support	sequence	secondary	secondary	none	none	Automatically compiling bilingual dictionaries of technical terms from comparable corpora is a challenging problem , yet with many potential applications .	According to the second observation , we use an existing context-based approach .	CL_D14-1177_ab_1	CL_D14-1177_ab_4	CL_D14-1177_ab_1_4
CL	D14-1177	ab	1	5	motivation_problem	means	support	by-means	secondary	secondary	none	none	Automatically compiling bilingual dictionaries of technical terms from comparable corpora is a challenging problem , yet with many potential applications .	For evaluation , we investigate the performance of compositional and context-based methods on : ( a ) similar and unrelated languages , ( b ) corpora of different degree of comparability and ( c ) the translation of frequent and rare terms .	CL_D14-1177_ab_1	CL_D14-1177_ab_5	CL_D14-1177_ab_1_5
CL	D14-1177	ab	1	6	motivation_problem	result	support	support	secondary	secondary	none	none	Automatically compiling bilingual dictionaries of technical terms from comparable corpora is a challenging problem , yet with many potential applications .	Finally , we combine the two translation clues , namely string and contextual similarity , in a linear model and we show substantial improvements over the two translation signals .	CL_D14-1177_ab_1	CL_D14-1177_ab_6	CL_D14-1177_ab_1_6
CL	D14-1177	ab	2	3	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	In this paper , we exploit two independent observations about term translations : ( a ) terms are often formed by corresponding sub-lexical units across languages and ( b ) a term and its translation tend to appear in similar lexical context .	Based on the first observation , we develop a new character n-gram compositional method , a logistic regression classifier , for learning a string similarity measure of term translations .	CL_D14-1177_ab_2	CL_D14-1177_ab_3	CL_D14-1177_ab_2_3
CL	D14-1177	ab	3	2	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	Based on the first observation , we develop a new character n-gram compositional method , a logistic regression classifier , for learning a string similarity measure of term translations .	In this paper , we exploit two independent observations about term translations : ( a ) terms are often formed by corresponding sub-lexical units across languages and ( b ) a term and its translation tend to appear in similar lexical context .	CL_D14-1177_ab_3	CL_D14-1177_ab_2	CL_D14-1177_ab_2_3
CL	D14-1177	ab	2	4	proposal	proposal_implementation	none	sequence	main	secondary	none	none	In this paper , we exploit two independent observations about term translations : ( a ) terms are often formed by corresponding sub-lexical units across languages and ( b ) a term and its translation tend to appear in similar lexical context .	According to the second observation , we use an existing context-based approach .	CL_D14-1177_ab_2	CL_D14-1177_ab_4	CL_D14-1177_ab_2_4
CL	D14-1177	ab	2	5	proposal	means	none	by-means	main	secondary	none	none	In this paper , we exploit two independent observations about term translations : ( a ) terms are often formed by corresponding sub-lexical units across languages and ( b ) a term and its translation tend to appear in similar lexical context .	For evaluation , we investigate the performance of compositional and context-based methods on : ( a ) similar and unrelated languages , ( b ) corpora of different degree of comparability and ( c ) the translation of frequent and rare terms .	CL_D14-1177_ab_2	CL_D14-1177_ab_5	CL_D14-1177_ab_2_5
CL	D14-1177	ab	2	6	proposal	result	none	support	main	secondary	back	support	In this paper , we exploit two independent observations about term translations : ( a ) terms are often formed by corresponding sub-lexical units across languages and ( b ) a term and its translation tend to appear in similar lexical context .	Finally , we combine the two translation clues , namely string and contextual similarity , in a linear model and we show substantial improvements over the two translation signals .	CL_D14-1177_ab_2	CL_D14-1177_ab_6	CL_D14-1177_ab_2_6
CL	D14-1177	ab	6	2	result	proposal	support	none	secondary	main	forw	support	Finally , we combine the two translation clues , namely string and contextual similarity , in a linear model and we show substantial improvements over the two translation signals .	In this paper , we exploit two independent observations about term translations : ( a ) terms are often formed by corresponding sub-lexical units across languages and ( b ) a term and its translation tend to appear in similar lexical context .	CL_D14-1177_ab_6	CL_D14-1177_ab_2	CL_D14-1177_ab_2_6
CL	D14-1177	ab	3	4	proposal_implementation	proposal_implementation	elaboration	sequence	secondary	secondary	back	sequence	Based on the first observation , we develop a new character n-gram compositional method , a logistic regression classifier , for learning a string similarity measure of term translations .	According to the second observation , we use an existing context-based approach .	CL_D14-1177_ab_3	CL_D14-1177_ab_4	CL_D14-1177_ab_3_4
CL	D14-1177	ab	4	3	proposal_implementation	proposal_implementation	sequence	elaboration	secondary	secondary	forw	sequence	According to the second observation , we use an existing context-based approach .	Based on the first observation , we develop a new character n-gram compositional method , a logistic regression classifier , for learning a string similarity measure of term translations .	CL_D14-1177_ab_4	CL_D14-1177_ab_3	CL_D14-1177_ab_3_4
CL	D14-1177	ab	3	5	proposal_implementation	means	elaboration	by-means	secondary	secondary	none	none	Based on the first observation , we develop a new character n-gram compositional method , a logistic regression classifier , for learning a string similarity measure of term translations .	For evaluation , we investigate the performance of compositional and context-based methods on : ( a ) similar and unrelated languages , ( b ) corpora of different degree of comparability and ( c ) the translation of frequent and rare terms .	CL_D14-1177_ab_3	CL_D14-1177_ab_5	CL_D14-1177_ab_3_5
CL	D14-1177	ab	3	6	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	Based on the first observation , we develop a new character n-gram compositional method , a logistic regression classifier , for learning a string similarity measure of term translations .	Finally , we combine the two translation clues , namely string and contextual similarity , in a linear model and we show substantial improvements over the two translation signals .	CL_D14-1177_ab_3	CL_D14-1177_ab_6	CL_D14-1177_ab_3_6
CL	D14-1177	ab	4	5	proposal_implementation	means	sequence	by-means	secondary	secondary	none	none	According to the second observation , we use an existing context-based approach .	For evaluation , we investigate the performance of compositional and context-based methods on : ( a ) similar and unrelated languages , ( b ) corpora of different degree of comparability and ( c ) the translation of frequent and rare terms .	CL_D14-1177_ab_4	CL_D14-1177_ab_5	CL_D14-1177_ab_4_5
CL	D14-1177	ab	4	6	proposal_implementation	result	sequence	support	secondary	secondary	none	none	According to the second observation , we use an existing context-based approach .	Finally , we combine the two translation clues , namely string and contextual similarity , in a linear model and we show substantial improvements over the two translation signals .	CL_D14-1177_ab_4	CL_D14-1177_ab_6	CL_D14-1177_ab_4_6
CL	D14-1177	ab	5	6	means	result	by-means	support	secondary	secondary	forw	by-means	For evaluation , we investigate the performance of compositional and context-based methods on : ( a ) similar and unrelated languages , ( b ) corpora of different degree of comparability and ( c ) the translation of frequent and rare terms .	Finally , we combine the two translation clues , namely string and contextual similarity , in a linear model and we show substantial improvements over the two translation signals .	CL_D14-1177_ab_5	CL_D14-1177_ab_6	CL_D14-1177_ab_5_6
CL	D14-1177	ab	6	5	result	means	support	by-means	secondary	secondary	back	by-means	Finally , we combine the two translation clues , namely string and contextual similarity , in a linear model and we show substantial improvements over the two translation signals .	For evaluation , we investigate the performance of compositional and context-based methods on : ( a ) similar and unrelated languages , ( b ) corpora of different degree of comparability and ( c ) the translation of frequent and rare terms .	CL_D14-1177_ab_6	CL_D14-1177_ab_5	CL_D14-1177_ab_5_6
CL	D14-1178	ab	1	2	motivation_background	motivation_background	info-required	info-required	secondary	secondary	forw	info-required	Vector space models ( VSMs ) are mathematically well-defined frameworks that have been widely used in the distributional approaches to semantics .	In VSMs , high-dimensional vectors represent linguistic entities .	CL_D14-1178_ab_1	CL_D14-1178_ab_2	CL_D14-1178_ab_1_2
CL	D14-1178	ab	2	1	motivation_background	motivation_background	info-required	info-required	secondary	secondary	back	info-required	In VSMs , high-dimensional vectors represent linguistic entities .	Vector space models ( VSMs ) are mathematically well-defined frameworks that have been widely used in the distributional approaches to semantics .	CL_D14-1178_ab_2	CL_D14-1178_ab_1	CL_D14-1178_ab_1_2
CL	D14-1178	ab	1	3	motivation_background	motivation_background	info-required	info-required	secondary	secondary	none	none	Vector space models ( VSMs ) are mathematically well-defined frameworks that have been widely used in the distributional approaches to semantics .	In an application , the similarity of vectorsand thus the entities that they represent is computed by a distance formula .	CL_D14-1178_ab_1	CL_D14-1178_ab_3	CL_D14-1178_ab_1_3
CL	D14-1178	ab	1	4	motivation_background	motivation_problem	info-required	info-required	secondary	secondary	none	none	Vector space models ( VSMs ) are mathematically well-defined frameworks that have been widely used in the distributional approaches to semantics .	The high dimensionality of vectors , however , is a barrier to the performance of methods that employ VSMs .	CL_D14-1178_ab_1	CL_D14-1178_ab_4	CL_D14-1178_ab_1_4
CL	D14-1178	ab	1	5	motivation_background	motivation_background	info-required	support	secondary	secondary	none	none	Vector space models ( VSMs ) are mathematically well-defined frameworks that have been widely used in the distributional approaches to semantics .	Consequently , a dimensionality reduction technique is employed to alleviate this problem .	CL_D14-1178_ab_1	CL_D14-1178_ab_5	CL_D14-1178_ab_1_5
CL	D14-1178	ab	1	6	motivation_background	proposal	info-required	none	secondary	main	none	none	Vector space models ( VSMs ) are mathematically well-defined frameworks that have been widely used in the distributional approaches to semantics .	This paper introduces a novel technique called Random Manhattan Indexing ( RMI ) for the construction of l1 normed VSMs at reduced dimensionality .	CL_D14-1178_ab_1	CL_D14-1178_ab_6	CL_D14-1178_ab_1_6
CL	D14-1178	ab	1	7	motivation_background	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	Vector space models ( VSMs ) are mathematically well-defined frameworks that have been widely used in the distributional approaches to semantics .	RMI combines the construction of a VSM and dimension reduction into an incremental and thus scalable two-step procedure .	CL_D14-1178_ab_1	CL_D14-1178_ab_7	CL_D14-1178_ab_1_7
CL	D14-1178	ab	1	8	motivation_background	proposal_implementation	info-required	by-means	secondary	secondary	none	none	Vector space models ( VSMs ) are mathematically well-defined frameworks that have been widely used in the distributional approaches to semantics .	In order to attain its goal , RMI employs the sparse Cauchy random projections .	CL_D14-1178_ab_1	CL_D14-1178_ab_8	CL_D14-1178_ab_1_8
CL	D14-1178	ab	1	9	motivation_background	proposal	info-required	elaboration	secondary	secondary	none	none	Vector space models ( VSMs ) are mathematically well-defined frameworks that have been widely used in the distributional approaches to semantics .	We further introduce Random Manhattan Integer Indexing ( RMII ) : a computationally enhanced version of RMI .	CL_D14-1178_ab_1	CL_D14-1178_ab_9	CL_D14-1178_ab_1_9
CL	D14-1178	ab	1	10	motivation_background	conclusion	info-required	support	secondary	secondary	none	none	Vector space models ( VSMs ) are mathematically well-defined frameworks that have been widely used in the distributional approaches to semantics .	As shown in the reported experiments , RMI and RMII can be used reliably to estimate the l1 distances between vectors in a vector space of low dimensionality .	CL_D14-1178_ab_1	CL_D14-1178_ab_10	CL_D14-1178_ab_1_10
CL	D14-1178	ab	2	3	motivation_background	motivation_background	info-required	info-required	secondary	secondary	forw	info-required	In VSMs , high-dimensional vectors represent linguistic entities .	In an application , the similarity of vectorsand thus the entities that they represent is computed by a distance formula .	CL_D14-1178_ab_2	CL_D14-1178_ab_3	CL_D14-1178_ab_2_3
CL	D14-1178	ab	3	2	motivation_background	motivation_background	info-required	info-required	secondary	secondary	back	info-required	In an application , the similarity of vectorsand thus the entities that they represent is computed by a distance formula .	In VSMs , high-dimensional vectors represent linguistic entities .	CL_D14-1178_ab_3	CL_D14-1178_ab_2	CL_D14-1178_ab_2_3
CL	D14-1178	ab	2	4	motivation_background	motivation_problem	info-required	info-required	secondary	secondary	none	none	In VSMs , high-dimensional vectors represent linguistic entities .	The high dimensionality of vectors , however , is a barrier to the performance of methods that employ VSMs .	CL_D14-1178_ab_2	CL_D14-1178_ab_4	CL_D14-1178_ab_2_4
CL	D14-1178	ab	2	5	motivation_background	motivation_background	info-required	support	secondary	secondary	none	none	In VSMs , high-dimensional vectors represent linguistic entities .	Consequently , a dimensionality reduction technique is employed to alleviate this problem .	CL_D14-1178_ab_2	CL_D14-1178_ab_5	CL_D14-1178_ab_2_5
CL	D14-1178	ab	2	6	motivation_background	proposal	info-required	none	secondary	main	none	none	In VSMs , high-dimensional vectors represent linguistic entities .	This paper introduces a novel technique called Random Manhattan Indexing ( RMI ) for the construction of l1 normed VSMs at reduced dimensionality .	CL_D14-1178_ab_2	CL_D14-1178_ab_6	CL_D14-1178_ab_2_6
CL	D14-1178	ab	2	7	motivation_background	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	In VSMs , high-dimensional vectors represent linguistic entities .	RMI combines the construction of a VSM and dimension reduction into an incremental and thus scalable two-step procedure .	CL_D14-1178_ab_2	CL_D14-1178_ab_7	CL_D14-1178_ab_2_7
CL	D14-1178	ab	2	8	motivation_background	proposal_implementation	info-required	by-means	secondary	secondary	none	none	In VSMs , high-dimensional vectors represent linguistic entities .	In order to attain its goal , RMI employs the sparse Cauchy random projections .	CL_D14-1178_ab_2	CL_D14-1178_ab_8	CL_D14-1178_ab_2_8
CL	D14-1178	ab	2	9	motivation_background	proposal	info-required	elaboration	secondary	secondary	none	none	In VSMs , high-dimensional vectors represent linguistic entities .	We further introduce Random Manhattan Integer Indexing ( RMII ) : a computationally enhanced version of RMI .	CL_D14-1178_ab_2	CL_D14-1178_ab_9	CL_D14-1178_ab_2_9
CL	D14-1178	ab	2	10	motivation_background	conclusion	info-required	support	secondary	secondary	none	none	In VSMs , high-dimensional vectors represent linguistic entities .	As shown in the reported experiments , RMI and RMII can be used reliably to estimate the l1 distances between vectors in a vector space of low dimensionality .	CL_D14-1178_ab_2	CL_D14-1178_ab_10	CL_D14-1178_ab_2_10
CL	D14-1178	ab	3	4	motivation_background	motivation_problem	info-required	info-required	secondary	secondary	forw	info-required	In an application , the similarity of vectorsand thus the entities that they represent is computed by a distance formula .	The high dimensionality of vectors , however , is a barrier to the performance of methods that employ VSMs .	CL_D14-1178_ab_3	CL_D14-1178_ab_4	CL_D14-1178_ab_3_4
CL	D14-1178	ab	4	3	motivation_problem	motivation_background	info-required	info-required	secondary	secondary	back	info-required	The high dimensionality of vectors , however , is a barrier to the performance of methods that employ VSMs .	In an application , the similarity of vectorsand thus the entities that they represent is computed by a distance formula .	CL_D14-1178_ab_4	CL_D14-1178_ab_3	CL_D14-1178_ab_3_4
CL	D14-1178	ab	3	5	motivation_background	motivation_background	info-required	support	secondary	secondary	none	none	In an application , the similarity of vectorsand thus the entities that they represent is computed by a distance formula .	Consequently , a dimensionality reduction technique is employed to alleviate this problem .	CL_D14-1178_ab_3	CL_D14-1178_ab_5	CL_D14-1178_ab_3_5
CL	D14-1178	ab	3	6	motivation_background	proposal	info-required	none	secondary	main	none	none	In an application , the similarity of vectorsand thus the entities that they represent is computed by a distance formula .	This paper introduces a novel technique called Random Manhattan Indexing ( RMI ) for the construction of l1 normed VSMs at reduced dimensionality .	CL_D14-1178_ab_3	CL_D14-1178_ab_6	CL_D14-1178_ab_3_6
CL	D14-1178	ab	3	7	motivation_background	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	In an application , the similarity of vectorsand thus the entities that they represent is computed by a distance formula .	RMI combines the construction of a VSM and dimension reduction into an incremental and thus scalable two-step procedure .	CL_D14-1178_ab_3	CL_D14-1178_ab_7	CL_D14-1178_ab_3_7
CL	D14-1178	ab	3	8	motivation_background	proposal_implementation	info-required	by-means	secondary	secondary	none	none	In an application , the similarity of vectorsand thus the entities that they represent is computed by a distance formula .	In order to attain its goal , RMI employs the sparse Cauchy random projections .	CL_D14-1178_ab_3	CL_D14-1178_ab_8	CL_D14-1178_ab_3_8
CL	D14-1178	ab	3	9	motivation_background	proposal	info-required	elaboration	secondary	secondary	none	none	In an application , the similarity of vectorsand thus the entities that they represent is computed by a distance formula .	We further introduce Random Manhattan Integer Indexing ( RMII ) : a computationally enhanced version of RMI .	CL_D14-1178_ab_3	CL_D14-1178_ab_9	CL_D14-1178_ab_3_9
CL	D14-1178	ab	3	10	motivation_background	conclusion	info-required	support	secondary	secondary	none	none	In an application , the similarity of vectorsand thus the entities that they represent is computed by a distance formula .	As shown in the reported experiments , RMI and RMII can be used reliably to estimate the l1 distances between vectors in a vector space of low dimensionality .	CL_D14-1178_ab_3	CL_D14-1178_ab_10	CL_D14-1178_ab_3_10
CL	D14-1178	ab	4	5	motivation_problem	motivation_background	info-required	support	secondary	secondary	forw	info-required	The high dimensionality of vectors , however , is a barrier to the performance of methods that employ VSMs .	Consequently , a dimensionality reduction technique is employed to alleviate this problem .	CL_D14-1178_ab_4	CL_D14-1178_ab_5	CL_D14-1178_ab_4_5
CL	D14-1178	ab	5	4	motivation_background	motivation_problem	support	info-required	secondary	secondary	back	info-required	Consequently , a dimensionality reduction technique is employed to alleviate this problem .	The high dimensionality of vectors , however , is a barrier to the performance of methods that employ VSMs .	CL_D14-1178_ab_5	CL_D14-1178_ab_4	CL_D14-1178_ab_4_5
CL	D14-1178	ab	4	6	motivation_problem	proposal	info-required	none	secondary	main	none	none	The high dimensionality of vectors , however , is a barrier to the performance of methods that employ VSMs .	This paper introduces a novel technique called Random Manhattan Indexing ( RMI ) for the construction of l1 normed VSMs at reduced dimensionality .	CL_D14-1178_ab_4	CL_D14-1178_ab_6	CL_D14-1178_ab_4_6
CL	D14-1178	ab	4	7	motivation_problem	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	The high dimensionality of vectors , however , is a barrier to the performance of methods that employ VSMs .	RMI combines the construction of a VSM and dimension reduction into an incremental and thus scalable two-step procedure .	CL_D14-1178_ab_4	CL_D14-1178_ab_7	CL_D14-1178_ab_4_7
CL	D14-1178	ab	4	8	motivation_problem	proposal_implementation	info-required	by-means	secondary	secondary	none	none	The high dimensionality of vectors , however , is a barrier to the performance of methods that employ VSMs .	In order to attain its goal , RMI employs the sparse Cauchy random projections .	CL_D14-1178_ab_4	CL_D14-1178_ab_8	CL_D14-1178_ab_4_8
CL	D14-1178	ab	4	9	motivation_problem	proposal	info-required	elaboration	secondary	secondary	none	none	The high dimensionality of vectors , however , is a barrier to the performance of methods that employ VSMs .	We further introduce Random Manhattan Integer Indexing ( RMII ) : a computationally enhanced version of RMI .	CL_D14-1178_ab_4	CL_D14-1178_ab_9	CL_D14-1178_ab_4_9
CL	D14-1178	ab	4	10	motivation_problem	conclusion	info-required	support	secondary	secondary	none	none	The high dimensionality of vectors , however , is a barrier to the performance of methods that employ VSMs .	As shown in the reported experiments , RMI and RMII can be used reliably to estimate the l1 distances between vectors in a vector space of low dimensionality .	CL_D14-1178_ab_4	CL_D14-1178_ab_10	CL_D14-1178_ab_4_10
CL	D14-1178	ab	5	6	motivation_background	proposal	support	none	secondary	main	forw	support	Consequently , a dimensionality reduction technique is employed to alleviate this problem .	This paper introduces a novel technique called Random Manhattan Indexing ( RMI ) for the construction of l1 normed VSMs at reduced dimensionality .	CL_D14-1178_ab_5	CL_D14-1178_ab_6	CL_D14-1178_ab_5_6
CL	D14-1178	ab	6	5	proposal	motivation_background	none	support	main	secondary	back	support	This paper introduces a novel technique called Random Manhattan Indexing ( RMI ) for the construction of l1 normed VSMs at reduced dimensionality .	Consequently , a dimensionality reduction technique is employed to alleviate this problem .	CL_D14-1178_ab_6	CL_D14-1178_ab_5	CL_D14-1178_ab_5_6
CL	D14-1178	ab	5	7	motivation_background	proposal_implementation	support	elaboration	secondary	secondary	none	none	Consequently , a dimensionality reduction technique is employed to alleviate this problem .	RMI combines the construction of a VSM and dimension reduction into an incremental and thus scalable two-step procedure .	CL_D14-1178_ab_5	CL_D14-1178_ab_7	CL_D14-1178_ab_5_7
CL	D14-1178	ab	5	8	motivation_background	proposal_implementation	support	by-means	secondary	secondary	none	none	Consequently , a dimensionality reduction technique is employed to alleviate this problem .	In order to attain its goal , RMI employs the sparse Cauchy random projections .	CL_D14-1178_ab_5	CL_D14-1178_ab_8	CL_D14-1178_ab_5_8
CL	D14-1178	ab	5	9	motivation_background	proposal	support	elaboration	secondary	secondary	none	none	Consequently , a dimensionality reduction technique is employed to alleviate this problem .	We further introduce Random Manhattan Integer Indexing ( RMII ) : a computationally enhanced version of RMI .	CL_D14-1178_ab_5	CL_D14-1178_ab_9	CL_D14-1178_ab_5_9
CL	D14-1178	ab	5	10	motivation_background	conclusion	support	support	secondary	secondary	none	none	Consequently , a dimensionality reduction technique is employed to alleviate this problem .	As shown in the reported experiments , RMI and RMII can be used reliably to estimate the l1 distances between vectors in a vector space of low dimensionality .	CL_D14-1178_ab_5	CL_D14-1178_ab_10	CL_D14-1178_ab_5_10
CL	D14-1178	ab	6	7	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	This paper introduces a novel technique called Random Manhattan Indexing ( RMI ) for the construction of l1 normed VSMs at reduced dimensionality .	RMI combines the construction of a VSM and dimension reduction into an incremental and thus scalable two-step procedure .	CL_D14-1178_ab_6	CL_D14-1178_ab_7	CL_D14-1178_ab_6_7
CL	D14-1178	ab	6	8	proposal	proposal_implementation	none	by-means	main	secondary	none	none	This paper introduces a novel technique called Random Manhattan Indexing ( RMI ) for the construction of l1 normed VSMs at reduced dimensionality .	In order to attain its goal , RMI employs the sparse Cauchy random projections .	CL_D14-1178_ab_6	CL_D14-1178_ab_8	CL_D14-1178_ab_6_8
CL	D14-1178	ab	6	9	proposal	proposal	none	elaboration	main	secondary	back	elaboration	This paper introduces a novel technique called Random Manhattan Indexing ( RMI ) for the construction of l1 normed VSMs at reduced dimensionality .	We further introduce Random Manhattan Integer Indexing ( RMII ) : a computationally enhanced version of RMI .	CL_D14-1178_ab_6	CL_D14-1178_ab_9	CL_D14-1178_ab_6_9
CL	D14-1178	ab	9	6	proposal	proposal	elaboration	none	secondary	main	forw	elaboration	We further introduce Random Manhattan Integer Indexing ( RMII ) : a computationally enhanced version of RMI .	This paper introduces a novel technique called Random Manhattan Indexing ( RMI ) for the construction of l1 normed VSMs at reduced dimensionality .	CL_D14-1178_ab_9	CL_D14-1178_ab_6	CL_D14-1178_ab_6_9
CL	D14-1178	ab	6	10	proposal	conclusion	none	support	main	secondary	back	support	This paper introduces a novel technique called Random Manhattan Indexing ( RMI ) for the construction of l1 normed VSMs at reduced dimensionality .	As shown in the reported experiments , RMI and RMII can be used reliably to estimate the l1 distances between vectors in a vector space of low dimensionality .	CL_D14-1178_ab_6	CL_D14-1178_ab_10	CL_D14-1178_ab_6_10
CL	D14-1178	ab	10	6	conclusion	proposal	support	none	secondary	main	forw	support	As shown in the reported experiments , RMI and RMII can be used reliably to estimate the l1 distances between vectors in a vector space of low dimensionality .	This paper introduces a novel technique called Random Manhattan Indexing ( RMI ) for the construction of l1 normed VSMs at reduced dimensionality .	CL_D14-1178_ab_10	CL_D14-1178_ab_6	CL_D14-1178_ab_6_10
CL	D14-1178	ab	7	8	proposal_implementation	proposal_implementation	elaboration	by-means	secondary	secondary	back	by-means	RMI combines the construction of a VSM and dimension reduction into an incremental and thus scalable two-step procedure .	In order to attain its goal , RMI employs the sparse Cauchy random projections .	CL_D14-1178_ab_7	CL_D14-1178_ab_8	CL_D14-1178_ab_7_8
CL	D14-1178	ab	8	7	proposal_implementation	proposal_implementation	by-means	elaboration	secondary	secondary	forw	by-means	In order to attain its goal , RMI employs the sparse Cauchy random projections .	RMI combines the construction of a VSM and dimension reduction into an incremental and thus scalable two-step procedure .	CL_D14-1178_ab_8	CL_D14-1178_ab_7	CL_D14-1178_ab_7_8
CL	D14-1178	ab	7	9	proposal_implementation	proposal	elaboration	elaboration	secondary	secondary	forw	elaboration	RMI combines the construction of a VSM and dimension reduction into an incremental and thus scalable two-step procedure .	We further introduce Random Manhattan Integer Indexing ( RMII ) : a computationally enhanced version of RMI .	CL_D14-1178_ab_7	CL_D14-1178_ab_9	CL_D14-1178_ab_7_9
CL	D14-1178	ab	9	7	proposal	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	We further introduce Random Manhattan Integer Indexing ( RMII ) : a computationally enhanced version of RMI .	RMI combines the construction of a VSM and dimension reduction into an incremental and thus scalable two-step procedure .	CL_D14-1178_ab_9	CL_D14-1178_ab_7	CL_D14-1178_ab_7_9
CL	D14-1178	ab	7	10	proposal_implementation	conclusion	elaboration	support	secondary	secondary	none	none	RMI combines the construction of a VSM and dimension reduction into an incremental and thus scalable two-step procedure .	As shown in the reported experiments , RMI and RMII can be used reliably to estimate the l1 distances between vectors in a vector space of low dimensionality .	CL_D14-1178_ab_7	CL_D14-1178_ab_10	CL_D14-1178_ab_7_10
CL	D14-1178	ab	8	9	proposal_implementation	proposal	by-means	elaboration	secondary	secondary	none	none	In order to attain its goal , RMI employs the sparse Cauchy random projections .	We further introduce Random Manhattan Integer Indexing ( RMII ) : a computationally enhanced version of RMI .	CL_D14-1178_ab_8	CL_D14-1178_ab_9	CL_D14-1178_ab_8_9
CL	D14-1178	ab	8	10	proposal_implementation	conclusion	by-means	support	secondary	secondary	none	none	In order to attain its goal , RMI employs the sparse Cauchy random projections .	As shown in the reported experiments , RMI and RMII can be used reliably to estimate the l1 distances between vectors in a vector space of low dimensionality .	CL_D14-1178_ab_8	CL_D14-1178_ab_10	CL_D14-1178_ab_8_10
CL	D14-1178	ab	9	10	proposal	conclusion	elaboration	support	secondary	secondary	none	none	We further introduce Random Manhattan Integer Indexing ( RMII ) : a computationally enhanced version of RMI .	As shown in the reported experiments , RMI and RMII can be used reliably to estimate the l1 distances between vectors in a vector space of low dimensionality .	CL_D14-1178_ab_9	CL_D14-1178_ab_10	CL_D14-1178_ab_9_10
CL	D14-1179	ab	1	2	proposal	proposal	none	elaboration	main	secondary	back	elaboration	In this paper , we propose a novel neural network model called RNN Encoder- Decoder that consists of two recurrent neural networks ( RNN ) .	One RNN encodes a sequence of symbols into a fixed-length vector representation , and the other decodes the representation into another sequence of symbols .	CL_D14-1179_ab_1	CL_D14-1179_ab_2	CL_D14-1179_ab_1_2
CL	D14-1179	ab	2	1	proposal	proposal	elaboration	none	secondary	main	forw	elaboration	One RNN encodes a sequence of symbols into a fixed-length vector representation , and the other decodes the representation into another sequence of symbols .	In this paper , we propose a novel neural network model called RNN Encoder- Decoder that consists of two recurrent neural networks ( RNN ) .	CL_D14-1179_ab_2	CL_D14-1179_ab_1	CL_D14-1179_ab_1_2
CL	D14-1179	ab	1	3	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	In this paper , we propose a novel neural network model called RNN Encoder- Decoder that consists of two recurrent neural networks ( RNN ) .	The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence .	CL_D14-1179_ab_1	CL_D14-1179_ab_3	CL_D14-1179_ab_1_3
CL	D14-1179	ab	1	4	proposal	conclusion	none	support	main	secondary	back	support	In this paper , we propose a novel neural network model called RNN Encoder- Decoder that consists of two recurrent neural networks ( RNN ) .	The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model .	CL_D14-1179_ab_1	CL_D14-1179_ab_4	CL_D14-1179_ab_1_4
CL	D14-1179	ab	4	1	conclusion	proposal	support	none	secondary	main	forw	support	The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model .	In this paper , we propose a novel neural network model called RNN Encoder- Decoder that consists of two recurrent neural networks ( RNN ) .	CL_D14-1179_ab_4	CL_D14-1179_ab_1	CL_D14-1179_ab_1_4
CL	D14-1179	ab	1	5	proposal	result	none	elaboration	main	secondary	none	none	In this paper , we propose a novel neural network model called RNN Encoder- Decoder that consists of two recurrent neural networks ( RNN ) .	Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .	CL_D14-1179_ab_1	CL_D14-1179_ab_5	CL_D14-1179_ab_1_5
CL	D14-1179	ab	2	3	proposal	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	One RNN encodes a sequence of symbols into a fixed-length vector representation , and the other decodes the representation into another sequence of symbols .	The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence .	CL_D14-1179_ab_2	CL_D14-1179_ab_3	CL_D14-1179_ab_2_3
CL	D14-1179	ab	3	2	proposal_implementation	proposal	elaboration	elaboration	secondary	secondary	forw	elaboration	The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence .	One RNN encodes a sequence of symbols into a fixed-length vector representation , and the other decodes the representation into another sequence of symbols .	CL_D14-1179_ab_3	CL_D14-1179_ab_2	CL_D14-1179_ab_2_3
CL	D14-1179	ab	2	4	proposal	conclusion	elaboration	support	secondary	secondary	none	none	One RNN encodes a sequence of symbols into a fixed-length vector representation , and the other decodes the representation into another sequence of symbols .	The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model .	CL_D14-1179_ab_2	CL_D14-1179_ab_4	CL_D14-1179_ab_2_4
CL	D14-1179	ab	2	5	proposal	result	elaboration	elaboration	secondary	secondary	none	none	One RNN encodes a sequence of symbols into a fixed-length vector representation , and the other decodes the representation into another sequence of symbols .	Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .	CL_D14-1179_ab_2	CL_D14-1179_ab_5	CL_D14-1179_ab_2_5
CL	D14-1179	ab	3	4	proposal_implementation	conclusion	elaboration	support	secondary	secondary	none	none	The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence .	The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model .	CL_D14-1179_ab_3	CL_D14-1179_ab_4	CL_D14-1179_ab_3_4
CL	D14-1179	ab	3	5	proposal_implementation	result	elaboration	elaboration	secondary	secondary	none	none	The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence .	Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .	CL_D14-1179_ab_3	CL_D14-1179_ab_5	CL_D14-1179_ab_3_5
CL	D14-1179	ab	4	5	conclusion	result	support	elaboration	secondary	secondary	back	elaboration	The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model .	Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .	CL_D14-1179_ab_4	CL_D14-1179_ab_5	CL_D14-1179_ab_4_5
CL	D14-1179	ab	5	4	result	conclusion	elaboration	support	secondary	secondary	forw	elaboration	Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .	The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model .	CL_D14-1179_ab_5	CL_D14-1179_ab_4	CL_D14-1179_ab_4_5
CL	D14-1180	ab	1	2	proposal_implementation	motivation_problem	none	support	main	secondary	back	support	This paper applies type-based Markov Chain Monte Carlo ( MCMC ) algorithms to the problem of learning Synchronous Context-Free Grammar ( SCFG ) rules from a forest that represents all possible rules consistent with a fixed word alignment .	While type-based MCMC has been shown to be effective in a number of NLP applications , our setting , where the tree structure of the sentence is itself a hidden variable , presents a number of challenges to type-based inference .	CL_D14-1180_ab_1	CL_D14-1180_ab_2	CL_D14-1180_ab_1_2
CL	D14-1180	ab	2	1	motivation_problem	proposal_implementation	support	none	secondary	main	forw	support	While type-based MCMC has been shown to be effective in a number of NLP applications , our setting , where the tree structure of the sentence is itself a hidden variable , presents a number of challenges to type-based inference .	This paper applies type-based Markov Chain Monte Carlo ( MCMC ) algorithms to the problem of learning Synchronous Context-Free Grammar ( SCFG ) rules from a forest that represents all possible rules consistent with a fixed word alignment .	CL_D14-1180_ab_2	CL_D14-1180_ab_1	CL_D14-1180_ab_1_2
CL	D14-1180	ab	1	3	proposal_implementation	proposal	none	elaboration	main	secondary	back	elaboration	This paper applies type-based Markov Chain Monte Carlo ( MCMC ) algorithms to the problem of learning Synchronous Context-Free Grammar ( SCFG ) rules from a forest that represents all possible rules consistent with a fixed word alignment .	We describe methods for defining variable types and efficiently indexing variables in order to overcome these challenges .	CL_D14-1180_ab_1	CL_D14-1180_ab_3	CL_D14-1180_ab_1_3
CL	D14-1180	ab	3	1	proposal	proposal_implementation	elaboration	none	secondary	main	forw	elaboration	We describe methods for defining variable types and efficiently indexing variables in order to overcome these challenges .	This paper applies type-based Markov Chain Monte Carlo ( MCMC ) algorithms to the problem of learning Synchronous Context-Free Grammar ( SCFG ) rules from a forest that represents all possible rules consistent with a fixed word alignment .	CL_D14-1180_ab_3	CL_D14-1180_ab_1	CL_D14-1180_ab_1_3
CL	D14-1180	ab	1	4	proposal_implementation	result	none	support	main	secondary	none	none	This paper applies type-based Markov Chain Monte Carlo ( MCMC ) algorithms to the problem of learning Synchronous Context-Free Grammar ( SCFG ) rules from a forest that represents all possible rules consistent with a fixed word alignment .	These methods lead to improvements in both log likelihood and BLEU score in our experiments .	CL_D14-1180_ab_1	CL_D14-1180_ab_4	CL_D14-1180_ab_1_4
CL	D14-1180	ab	2	3	motivation_problem	proposal	support	elaboration	secondary	secondary	none	none	While type-based MCMC has been shown to be effective in a number of NLP applications , our setting , where the tree structure of the sentence is itself a hidden variable , presents a number of challenges to type-based inference .	We describe methods for defining variable types and efficiently indexing variables in order to overcome these challenges .	CL_D14-1180_ab_2	CL_D14-1180_ab_3	CL_D14-1180_ab_2_3
CL	D14-1180	ab	2	4	motivation_problem	result	support	support	secondary	secondary	none	none	While type-based MCMC has been shown to be effective in a number of NLP applications , our setting , where the tree structure of the sentence is itself a hidden variable , presents a number of challenges to type-based inference .	These methods lead to improvements in both log likelihood and BLEU score in our experiments .	CL_D14-1180_ab_2	CL_D14-1180_ab_4	CL_D14-1180_ab_2_4
CL	D14-1180	ab	3	4	proposal	result	elaboration	support	secondary	secondary	back	support	We describe methods for defining variable types and efficiently indexing variables in order to overcome these challenges .	These methods lead to improvements in both log likelihood and BLEU score in our experiments .	CL_D14-1180_ab_3	CL_D14-1180_ab_4	CL_D14-1180_ab_3_4
CL	D14-1180	ab	4	3	result	proposal	support	elaboration	secondary	secondary	forw	support	These methods lead to improvements in both log likelihood and BLEU score in our experiments .	We describe methods for defining variable types and efficiently indexing variables in order to overcome these challenges .	CL_D14-1180_ab_4	CL_D14-1180_ab_3	CL_D14-1180_ab_3_4
CL	D14-1181	ab	1	2	proposal	result	none	support	main	secondary	back	support	We report on a series of experiments with convolutional neural networks ( CNN ) trained on top of pre-trained word vectors for sentence-level classification tasks .	We show that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks .	CL_D14-1181_ab_1	CL_D14-1181_ab_2	CL_D14-1181_ab_1_2
CL	D14-1181	ab	2	1	result	proposal	support	none	secondary	main	forw	support	We show that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks .	We report on a series of experiments with convolutional neural networks ( CNN ) trained on top of pre-trained word vectors for sentence-level classification tasks .	CL_D14-1181_ab_2	CL_D14-1181_ab_1	CL_D14-1181_ab_1_2
CL	D14-1181	ab	1	3	proposal	result	none	elaboration	main	secondary	none	none	We report on a series of experiments with convolutional neural networks ( CNN ) trained on top of pre-trained word vectors for sentence-level classification tasks .	Learning task-specific vectors through fine-tuning offers further gains in performance .	CL_D14-1181_ab_1	CL_D14-1181_ab_3	CL_D14-1181_ab_1_3
CL	D14-1181	ab	1	4	proposal	proposal	none	elaboration	main	secondary	back	elaboration	We report on a series of experiments with convolutional neural networks ( CNN ) trained on top of pre-trained word vectors for sentence-level classification tasks .	We additionally propose a simple modification to the architecture to allow for the use of both task-specific and static vectors .	CL_D14-1181_ab_1	CL_D14-1181_ab_4	CL_D14-1181_ab_1_4
CL	D14-1181	ab	4	1	proposal	proposal	elaboration	none	secondary	main	forw	elaboration	We additionally propose a simple modification to the architecture to allow for the use of both task-specific and static vectors .	We report on a series of experiments with convolutional neural networks ( CNN ) trained on top of pre-trained word vectors for sentence-level classification tasks .	CL_D14-1181_ab_4	CL_D14-1181_ab_1	CL_D14-1181_ab_1_4
CL	D14-1181	ab	1	5	proposal	result	none	support	main	secondary	back	support	We report on a series of experiments with convolutional neural networks ( CNN ) trained on top of pre-trained word vectors for sentence-level classification tasks .	The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks , which include sentiment analysis and question classification .	CL_D14-1181_ab_1	CL_D14-1181_ab_5	CL_D14-1181_ab_1_5
CL	D14-1181	ab	5	1	result	proposal	support	none	secondary	main	forw	support	The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks , which include sentiment analysis and question classification .	We report on a series of experiments with convolutional neural networks ( CNN ) trained on top of pre-trained word vectors for sentence-level classification tasks .	CL_D14-1181_ab_5	CL_D14-1181_ab_1	CL_D14-1181_ab_1_5
CL	D14-1181	ab	2	3	result	result	support	elaboration	secondary	secondary	back	elaboration	We show that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks .	Learning task-specific vectors through fine-tuning offers further gains in performance .	CL_D14-1181_ab_2	CL_D14-1181_ab_3	CL_D14-1181_ab_2_3
CL	D14-1181	ab	3	2	result	result	elaboration	support	secondary	secondary	forw	elaboration	Learning task-specific vectors through fine-tuning offers further gains in performance .	We show that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks .	CL_D14-1181_ab_3	CL_D14-1181_ab_2	CL_D14-1181_ab_2_3
CL	D14-1181	ab	2	4	result	proposal	support	elaboration	secondary	secondary	none	none	We show that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks .	We additionally propose a simple modification to the architecture to allow for the use of both task-specific and static vectors .	CL_D14-1181_ab_2	CL_D14-1181_ab_4	CL_D14-1181_ab_2_4
CL	D14-1181	ab	2	5	result	result	support	support	secondary	secondary	none	none	We show that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks .	The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks , which include sentiment analysis and question classification .	CL_D14-1181_ab_2	CL_D14-1181_ab_5	CL_D14-1181_ab_2_5
CL	D14-1181	ab	3	4	result	proposal	elaboration	elaboration	secondary	secondary	none	none	Learning task-specific vectors through fine-tuning offers further gains in performance .	We additionally propose a simple modification to the architecture to allow for the use of both task-specific and static vectors .	CL_D14-1181_ab_3	CL_D14-1181_ab_4	CL_D14-1181_ab_3_4
CL	D14-1181	ab	3	5	result	result	elaboration	support	secondary	secondary	none	none	Learning task-specific vectors through fine-tuning offers further gains in performance .	The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks , which include sentiment analysis and question classification .	CL_D14-1181_ab_3	CL_D14-1181_ab_5	CL_D14-1181_ab_3_5
CL	D14-1181	ab	4	5	proposal	result	elaboration	support	secondary	secondary	none	none	We additionally propose a simple modification to the architecture to allow for the use of both task-specific and static vectors .	The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks , which include sentiment analysis and question classification .	CL_D14-1181_ab_4	CL_D14-1181_ab_5	CL_D14-1181_ab_4_5
CL	D14-1182	ab	1	2	motivation_background	motivation_hypothesis	info-required	support	secondary	secondary	forw	info-required	Markov chain Monte Carlo ( MCMC ) approximates the posterior distribution of latent variable models by generating many samples and averaging over them .	In practice , however , it is often more convenient to cut corners , using only a single sample or following a suboptimal averaging strategy .	CL_D14-1182_ab_1	CL_D14-1182_ab_2	CL_D14-1182_ab_1_2
CL	D14-1182	ab	2	1	motivation_hypothesis	motivation_background	support	info-required	secondary	secondary	back	info-required	In practice , however , it is often more convenient to cut corners , using only a single sample or following a suboptimal averaging strategy .	Markov chain Monte Carlo ( MCMC ) approximates the posterior distribution of latent variable models by generating many samples and averaging over them .	CL_D14-1182_ab_2	CL_D14-1182_ab_1	CL_D14-1182_ab_1_2
CL	D14-1182	ab	1	3	motivation_background	proposal	info-required	none	secondary	main	none	none	Markov chain Monte Carlo ( MCMC ) approximates the posterior distribution of latent variable models by generating many samples and averaging over them .	We systematically study different strategies for averaging MCMC samples and show empirically that averaging properly leads to significant improvements in prediction .	CL_D14-1182_ab_1	CL_D14-1182_ab_3	CL_D14-1182_ab_1_3
CL	D14-1182	ab	2	3	motivation_hypothesis	proposal	support	none	secondary	main	forw	support	In practice , however , it is often more convenient to cut corners , using only a single sample or following a suboptimal averaging strategy .	We systematically study different strategies for averaging MCMC samples and show empirically that averaging properly leads to significant improvements in prediction .	CL_D14-1182_ab_2	CL_D14-1182_ab_3	CL_D14-1182_ab_2_3
CL	D14-1182	ab	3	2	proposal	motivation_hypothesis	none	support	main	secondary	back	support	We systematically study different strategies for averaging MCMC samples and show empirically that averaging properly leads to significant improvements in prediction .	In practice , however , it is often more convenient to cut corners , using only a single sample or following a suboptimal averaging strategy .	CL_D14-1182_ab_3	CL_D14-1182_ab_2	CL_D14-1182_ab_2_3
CL	D14-1183	ab	1	2	motivation_background	motivation_background	info-required	info-required	secondary	secondary	forw	info-required	Phrase reordering is a challenge for statistical machine translation systems .	Posing phrase movements as a prediction problem using contextual features modeled by maximum entropy-based classifier is superior to the commonly used lexicalized reordering model .	CL_D14-1183_ab_1	CL_D14-1183_ab_2	CL_D14-1183_ab_1_2
CL	D14-1183	ab	2	1	motivation_background	motivation_background	info-required	info-required	secondary	secondary	back	info-required	Posing phrase movements as a prediction problem using contextual features modeled by maximum entropy-based classifier is superior to the commonly used lexicalized reordering model .	Phrase reordering is a challenge for statistical machine translation systems .	CL_D14-1183_ab_2	CL_D14-1183_ab_1	CL_D14-1183_ab_1_2
CL	D14-1183	ab	1	3	motivation_background	motivation_problem	info-required	support	secondary	secondary	none	none	Phrase reordering is a challenge for statistical machine translation systems .	However , Training this discriminative model using large-scale parallel corpus might be computationally expensive .	CL_D14-1183_ab_1	CL_D14-1183_ab_3	CL_D14-1183_ab_1_3
CL	D14-1183	ab	1	4	motivation_background	proposal	info-required	none	secondary	main	none	none	Phrase reordering is a challenge for statistical machine translation systems .	In this paper , we explore recent advancements in solving large-scale classification problems .	CL_D14-1183_ab_1	CL_D14-1183_ab_4	CL_D14-1183_ab_1_4
CL	D14-1183	ab	1	5	motivation_background	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	Phrase reordering is a challenge for statistical machine translation systems .	Using the dual problem to multinomial logistic regression , we managed to shrink the training data while iterating and produce significant saving in computation and memory while preserving the accuracy .	CL_D14-1183_ab_1	CL_D14-1183_ab_5	CL_D14-1183_ab_1_5
CL	D14-1183	ab	2	3	motivation_background	motivation_problem	info-required	support	secondary	secondary	forw	info-required	Posing phrase movements as a prediction problem using contextual features modeled by maximum entropy-based classifier is superior to the commonly used lexicalized reordering model .	However , Training this discriminative model using large-scale parallel corpus might be computationally expensive .	CL_D14-1183_ab_2	CL_D14-1183_ab_3	CL_D14-1183_ab_2_3
CL	D14-1183	ab	3	2	motivation_problem	motivation_background	support	info-required	secondary	secondary	back	info-required	However , Training this discriminative model using large-scale parallel corpus might be computationally expensive .	Posing phrase movements as a prediction problem using contextual features modeled by maximum entropy-based classifier is superior to the commonly used lexicalized reordering model .	CL_D14-1183_ab_3	CL_D14-1183_ab_2	CL_D14-1183_ab_2_3
CL	D14-1183	ab	2	4	motivation_background	proposal	info-required	none	secondary	main	none	none	Posing phrase movements as a prediction problem using contextual features modeled by maximum entropy-based classifier is superior to the commonly used lexicalized reordering model .	In this paper , we explore recent advancements in solving large-scale classification problems .	CL_D14-1183_ab_2	CL_D14-1183_ab_4	CL_D14-1183_ab_2_4
CL	D14-1183	ab	2	5	motivation_background	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	Posing phrase movements as a prediction problem using contextual features modeled by maximum entropy-based classifier is superior to the commonly used lexicalized reordering model .	Using the dual problem to multinomial logistic regression , we managed to shrink the training data while iterating and produce significant saving in computation and memory while preserving the accuracy .	CL_D14-1183_ab_2	CL_D14-1183_ab_5	CL_D14-1183_ab_2_5
CL	D14-1183	ab	3	4	motivation_problem	proposal	support	none	secondary	main	forw	support	However , Training this discriminative model using large-scale parallel corpus might be computationally expensive .	In this paper , we explore recent advancements in solving large-scale classification problems .	CL_D14-1183_ab_3	CL_D14-1183_ab_4	CL_D14-1183_ab_3_4
CL	D14-1183	ab	4	3	proposal	motivation_problem	none	support	main	secondary	back	support	In this paper , we explore recent advancements in solving large-scale classification problems .	However , Training this discriminative model using large-scale parallel corpus might be computationally expensive .	CL_D14-1183_ab_4	CL_D14-1183_ab_3	CL_D14-1183_ab_3_4
CL	D14-1183	ab	3	5	motivation_problem	proposal_implementation	support	elaboration	secondary	secondary	none	none	However , Training this discriminative model using large-scale parallel corpus might be computationally expensive .	Using the dual problem to multinomial logistic regression , we managed to shrink the training data while iterating and produce significant saving in computation and memory while preserving the accuracy .	CL_D14-1183_ab_3	CL_D14-1183_ab_5	CL_D14-1183_ab_3_5
CL	D14-1183	ab	4	5	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	In this paper , we explore recent advancements in solving large-scale classification problems .	Using the dual problem to multinomial logistic regression , we managed to shrink the training data while iterating and produce significant saving in computation and memory while preserving the accuracy .	CL_D14-1183_ab_4	CL_D14-1183_ab_5	CL_D14-1183_ab_4_5
CL	D14-1183	ab	5	4	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	Using the dual problem to multinomial logistic regression , we managed to shrink the training data while iterating and produce significant saving in computation and memory while preserving the accuracy .	In this paper , we explore recent advancements in solving large-scale classification problems .	CL_D14-1183_ab_5	CL_D14-1183_ab_4	CL_D14-1183_ab_4_5
CL	D14-1184	ab	1	2	proposal	conclusion	none	support	main	secondary	back	support	In this paper , we present two improvements to the beam search approach for solving homophonic substitution ciphers presented in Nuhn et al. ( 2013 ) : An improved rest cost estimation together with an optimized strategy for obtaining the order in which the symbols of the cipher are deciphered reduces the beam size needed to successfully decipher the Zodiac-408 cipher from several million down to less than one hundred : The search effort is reduced from several hours of computation time to just a few seconds on a single CPU .	These improvements allow us to successfully decipher the second part of the famous Beale cipher ( see ( Ward et al. , 1885 ) and e.g. ( King , 1993 ) ) : Having 182 different cipher symbols while having a length of just 762 symbols , the decipherment is way more challenging than the decipherment of the previously deciphered Zodiac-408 cipher ( length 408 , 54 different symbols ) .	CL_D14-1184_ab_1	CL_D14-1184_ab_2	CL_D14-1184_ab_1_2
CL	D14-1184	ab	2	1	conclusion	proposal	support	none	secondary	main	forw	support	These improvements allow us to successfully decipher the second part of the famous Beale cipher ( see ( Ward et al. , 1885 ) and e.g. ( King , 1993 ) ) : Having 182 different cipher symbols while having a length of just 762 symbols , the decipherment is way more challenging than the decipherment of the previously deciphered Zodiac-408 cipher ( length 408 , 54 different symbols ) .	In this paper , we present two improvements to the beam search approach for solving homophonic substitution ciphers presented in Nuhn et al. ( 2013 ) : An improved rest cost estimation together with an optimized strategy for obtaining the order in which the symbols of the cipher are deciphered reduces the beam size needed to successfully decipher the Zodiac-408 cipher from several million down to less than one hundred : The search effort is reduced from several hours of computation time to just a few seconds on a single CPU .	CL_D14-1184_ab_2	CL_D14-1184_ab_1	CL_D14-1184_ab_1_2
CL	D14-1184	ab	1	3	proposal	information_additional	none	info-optional	main	secondary	none	none	In this paper , we present two improvements to the beam search approach for solving homophonic substitution ciphers presented in Nuhn et al. ( 2013 ) : An improved rest cost estimation together with an optimized strategy for obtaining the order in which the symbols of the cipher are deciphered reduces the beam size needed to successfully decipher the Zodiac-408 cipher from several million down to less than one hundred : The search effort is reduced from several hours of computation time to just a few seconds on a single CPU .	To the best of our knowledge , this cipher has not been deciphered automatically before .	CL_D14-1184_ab_1	CL_D14-1184_ab_3	CL_D14-1184_ab_1_3
CL	D14-1184	ab	2	3	conclusion	information_additional	support	info-optional	secondary	secondary	back	info-optional	These improvements allow us to successfully decipher the second part of the famous Beale cipher ( see ( Ward et al. , 1885 ) and e.g. ( King , 1993 ) ) : Having 182 different cipher symbols while having a length of just 762 symbols , the decipherment is way more challenging than the decipherment of the previously deciphered Zodiac-408 cipher ( length 408 , 54 different symbols ) .	To the best of our knowledge , this cipher has not been deciphered automatically before .	CL_D14-1184_ab_2	CL_D14-1184_ab_3	CL_D14-1184_ab_2_3
CL	D14-1184	ab	3	2	information_additional	conclusion	info-optional	support	secondary	secondary	forw	info-optional	To the best of our knowledge , this cipher has not been deciphered automatically before .	These improvements allow us to successfully decipher the second part of the famous Beale cipher ( see ( Ward et al. , 1885 ) and e.g. ( King , 1993 ) ) : Having 182 different cipher symbols while having a length of just 762 symbols , the decipherment is way more challenging than the decipherment of the previously deciphered Zodiac-408 cipher ( length 408 , 54 different symbols ) .	CL_D14-1184_ab_3	CL_D14-1184_ab_2	CL_D14-1184_ab_2_3
CL	D14-1185	ab	1	2	motivation_background	motivation_problem	info-required	info-required	secondary	secondary	forw	info-required	Manual analysis and decryption of enciphered documents is a tedious and error prone work .	Often even after spending large amounts of time on a particular cipher - no decipherment can be found .	CL_D14-1185_ab_1	CL_D14-1185_ab_2	CL_D14-1185_ab_1_2
CL	D14-1185	ab	2	1	motivation_problem	motivation_background	info-required	info-required	secondary	secondary	back	info-required	Often even after spending large amounts of time on a particular cipher - no decipherment can be found .	Manual analysis and decryption of enciphered documents is a tedious and error prone work .	CL_D14-1185_ab_2	CL_D14-1185_ab_1	CL_D14-1185_ab_1_2
CL	D14-1185	ab	1	3	motivation_background	motivation_hypothesis	info-required	support	secondary	secondary	none	none	Manual analysis and decryption of enciphered documents is a tedious and error prone work .	Automating the decryption of various types of ciphers makes it possible to sift through the large number of encrypted messages found in libraries and archives , and to focus human effort only on a small but potentially interesting subset of them .	CL_D14-1185_ab_1	CL_D14-1185_ab_3	CL_D14-1185_ab_1_3
CL	D14-1185	ab	1	4	motivation_background	proposal	info-required	none	secondary	main	none	none	Manual analysis and decryption of enciphered documents is a tedious and error prone work .	In this work , we train a classifier that is able to predict which encipherment method has been used to generate a given ciphertext .	CL_D14-1185_ab_1	CL_D14-1185_ab_4	CL_D14-1185_ab_1_4
CL	D14-1185	ab	1	5	motivation_background	result_means	info-required	support	secondary	secondary	none	none	Manual analysis and decryption of enciphered documents is a tedious and error prone work .	We are able to distinguish 50 different cipher types ( specified by the American Cryptogram Association ) with an accuracy of 58.5 % .	CL_D14-1185_ab_1	CL_D14-1185_ab_5	CL_D14-1185_ab_1_5
CL	D14-1185	ab	1	6	motivation_background	observation	info-required	elaboration	secondary	secondary	none	none	Manual analysis and decryption of enciphered documents is a tedious and error prone work .	This is a 11.2 % absolute improvement over the best previously published classifier .	CL_D14-1185_ab_1	CL_D14-1185_ab_6	CL_D14-1185_ab_1_6
CL	D14-1185	ab	2	3	motivation_problem	motivation_hypothesis	info-required	support	secondary	secondary	forw	info-required	Often even after spending large amounts of time on a particular cipher - no decipherment can be found .	Automating the decryption of various types of ciphers makes it possible to sift through the large number of encrypted messages found in libraries and archives , and to focus human effort only on a small but potentially interesting subset of them .	CL_D14-1185_ab_2	CL_D14-1185_ab_3	CL_D14-1185_ab_2_3
CL	D14-1185	ab	3	2	motivation_hypothesis	motivation_problem	support	info-required	secondary	secondary	back	info-required	Automating the decryption of various types of ciphers makes it possible to sift through the large number of encrypted messages found in libraries and archives , and to focus human effort only on a small but potentially interesting subset of them .	Often even after spending large amounts of time on a particular cipher - no decipherment can be found .	CL_D14-1185_ab_3	CL_D14-1185_ab_2	CL_D14-1185_ab_2_3
CL	D14-1185	ab	2	4	motivation_problem	proposal	info-required	none	secondary	main	none	none	Often even after spending large amounts of time on a particular cipher - no decipherment can be found .	In this work , we train a classifier that is able to predict which encipherment method has been used to generate a given ciphertext .	CL_D14-1185_ab_2	CL_D14-1185_ab_4	CL_D14-1185_ab_2_4
CL	D14-1185	ab	2	5	motivation_problem	result_means	info-required	support	secondary	secondary	none	none	Often even after spending large amounts of time on a particular cipher - no decipherment can be found .	We are able to distinguish 50 different cipher types ( specified by the American Cryptogram Association ) with an accuracy of 58.5 % .	CL_D14-1185_ab_2	CL_D14-1185_ab_5	CL_D14-1185_ab_2_5
CL	D14-1185	ab	2	6	motivation_problem	observation	info-required	elaboration	secondary	secondary	none	none	Often even after spending large amounts of time on a particular cipher - no decipherment can be found .	This is a 11.2 % absolute improvement over the best previously published classifier .	CL_D14-1185_ab_2	CL_D14-1185_ab_6	CL_D14-1185_ab_2_6
CL	D14-1185	ab	3	4	motivation_hypothesis	proposal	support	none	secondary	main	forw	support	Automating the decryption of various types of ciphers makes it possible to sift through the large number of encrypted messages found in libraries and archives , and to focus human effort only on a small but potentially interesting subset of them .	In this work , we train a classifier that is able to predict which encipherment method has been used to generate a given ciphertext .	CL_D14-1185_ab_3	CL_D14-1185_ab_4	CL_D14-1185_ab_3_4
CL	D14-1185	ab	4	3	proposal	motivation_hypothesis	none	support	main	secondary	back	support	In this work , we train a classifier that is able to predict which encipherment method has been used to generate a given ciphertext .	Automating the decryption of various types of ciphers makes it possible to sift through the large number of encrypted messages found in libraries and archives , and to focus human effort only on a small but potentially interesting subset of them .	CL_D14-1185_ab_4	CL_D14-1185_ab_3	CL_D14-1185_ab_3_4
CL	D14-1185	ab	3	5	motivation_hypothesis	result_means	support	support	secondary	secondary	none	none	Automating the decryption of various types of ciphers makes it possible to sift through the large number of encrypted messages found in libraries and archives , and to focus human effort only on a small but potentially interesting subset of them .	We are able to distinguish 50 different cipher types ( specified by the American Cryptogram Association ) with an accuracy of 58.5 % .	CL_D14-1185_ab_3	CL_D14-1185_ab_5	CL_D14-1185_ab_3_5
CL	D14-1185	ab	3	6	motivation_hypothesis	observation	support	elaboration	secondary	secondary	none	none	Automating the decryption of various types of ciphers makes it possible to sift through the large number of encrypted messages found in libraries and archives , and to focus human effort only on a small but potentially interesting subset of them .	This is a 11.2 % absolute improvement over the best previously published classifier .	CL_D14-1185_ab_3	CL_D14-1185_ab_6	CL_D14-1185_ab_3_6
CL	D14-1185	ab	4	5	proposal	result_means	none	support	main	secondary	back	support	In this work , we train a classifier that is able to predict which encipherment method has been used to generate a given ciphertext .	We are able to distinguish 50 different cipher types ( specified by the American Cryptogram Association ) with an accuracy of 58.5 % .	CL_D14-1185_ab_4	CL_D14-1185_ab_5	CL_D14-1185_ab_4_5
CL	D14-1185	ab	5	4	result_means	proposal	support	none	secondary	main	forw	support	We are able to distinguish 50 different cipher types ( specified by the American Cryptogram Association ) with an accuracy of 58.5 % .	In this work , we train a classifier that is able to predict which encipherment method has been used to generate a given ciphertext .	CL_D14-1185_ab_5	CL_D14-1185_ab_4	CL_D14-1185_ab_4_5
CL	D14-1185	ab	4	6	proposal	observation	none	elaboration	main	secondary	none	none	In this work , we train a classifier that is able to predict which encipherment method has been used to generate a given ciphertext .	This is a 11.2 % absolute improvement over the best previously published classifier .	CL_D14-1185_ab_4	CL_D14-1185_ab_6	CL_D14-1185_ab_4_6
CL	D14-1185	ab	5	6	result_means	observation	support	elaboration	secondary	secondary	back	elaboration	We are able to distinguish 50 different cipher types ( specified by the American Cryptogram Association ) with an accuracy of 58.5 % .	This is a 11.2 % absolute improvement over the best previously published classifier .	CL_D14-1185_ab_5	CL_D14-1185_ab_6	CL_D14-1185_ab_5_6
CL	D14-1185	ab	6	5	observation	result_means	elaboration	support	secondary	secondary	forw	elaboration	This is a 11.2 % absolute improvement over the best previously published classifier .	We are able to distinguish 50 different cipher types ( specified by the American Cryptogram Association ) with an accuracy of 58.5 % .	CL_D14-1185_ab_6	CL_D14-1185_ab_5	CL_D14-1185_ab_5_6
CL	D14-1186	ab	1	2	motivation_background	motivation_problem	info-required	support	secondary	secondary	forw	info-required	Previous work often used a pipelined framework where Chinese word segmentation is followed by term extraction and keyword extraction .	Such framework suffers from error propagation and is unable to leverage information in later modules for prior components .	CL_D14-1186_ab_1	CL_D14-1186_ab_2	CL_D14-1186_ab_1_2
CL	D14-1186	ab	2	1	motivation_problem	motivation_background	support	info-required	secondary	secondary	back	info-required	Such framework suffers from error propagation and is unable to leverage information in later modules for prior components .	Previous work often used a pipelined framework where Chinese word segmentation is followed by term extraction and keyword extraction .	CL_D14-1186_ab_2	CL_D14-1186_ab_1	CL_D14-1186_ab_1_2
CL	D14-1186	ab	1	3	motivation_background	proposal	info-required	none	secondary	main	none	none	Previous work often used a pipelined framework where Chinese word segmentation is followed by term extraction and keyword extraction .	In this paper , we propose a four-level Dirichlet Process based model ( DP-4 ) to jointly learn the word distributions from the corpus , domain and document levels simultaneously .	CL_D14-1186_ab_1	CL_D14-1186_ab_3	CL_D14-1186_ab_1_3
CL	D14-1186	ab	1	4	motivation_background	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	Previous work often used a pipelined framework where Chinese word segmentation is followed by term extraction and keyword extraction .	Based on the DP-4 model , a sentence-wise Gibbs sampler is adopted to obtain proper segmentation results .	CL_D14-1186_ab_1	CL_D14-1186_ab_4	CL_D14-1186_ab_1_4
CL	D14-1186	ab	1	5	motivation_background	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	Previous work often used a pipelined framework where Chinese word segmentation is followed by term extraction and keyword extraction .	Meanwhile , terms and keywords are acquired in the sampling process .	CL_D14-1186_ab_1	CL_D14-1186_ab_5	CL_D14-1186_ab_1_5
CL	D14-1186	ab	1	6	motivation_background	conclusion	info-required	support	secondary	secondary	none	none	Previous work often used a pipelined framework where Chinese word segmentation is followed by term extraction and keyword extraction .	Experimental results have shown the effectiveness of our method .	CL_D14-1186_ab_1	CL_D14-1186_ab_6	CL_D14-1186_ab_1_6
CL	D14-1186	ab	2	3	motivation_problem	proposal	support	none	secondary	main	forw	support	Such framework suffers from error propagation and is unable to leverage information in later modules for prior components .	In this paper , we propose a four-level Dirichlet Process based model ( DP-4 ) to jointly learn the word distributions from the corpus , domain and document levels simultaneously .	CL_D14-1186_ab_2	CL_D14-1186_ab_3	CL_D14-1186_ab_2_3
CL	D14-1186	ab	3	2	proposal	motivation_problem	none	support	main	secondary	back	support	In this paper , we propose a four-level Dirichlet Process based model ( DP-4 ) to jointly learn the word distributions from the corpus , domain and document levels simultaneously .	Such framework suffers from error propagation and is unable to leverage information in later modules for prior components .	CL_D14-1186_ab_3	CL_D14-1186_ab_2	CL_D14-1186_ab_2_3
CL	D14-1186	ab	2	4	motivation_problem	proposal_implementation	support	elaboration	secondary	secondary	none	none	Such framework suffers from error propagation and is unable to leverage information in later modules for prior components .	Based on the DP-4 model , a sentence-wise Gibbs sampler is adopted to obtain proper segmentation results .	CL_D14-1186_ab_2	CL_D14-1186_ab_4	CL_D14-1186_ab_2_4
CL	D14-1186	ab	2	5	motivation_problem	proposal_implementation	support	elaboration	secondary	secondary	none	none	Such framework suffers from error propagation and is unable to leverage information in later modules for prior components .	Meanwhile , terms and keywords are acquired in the sampling process .	CL_D14-1186_ab_2	CL_D14-1186_ab_5	CL_D14-1186_ab_2_5
CL	D14-1186	ab	2	6	motivation_problem	conclusion	support	support	secondary	secondary	none	none	Such framework suffers from error propagation and is unable to leverage information in later modules for prior components .	Experimental results have shown the effectiveness of our method .	CL_D14-1186_ab_2	CL_D14-1186_ab_6	CL_D14-1186_ab_2_6
CL	D14-1186	ab	3	4	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	In this paper , we propose a four-level Dirichlet Process based model ( DP-4 ) to jointly learn the word distributions from the corpus , domain and document levels simultaneously .	Based on the DP-4 model , a sentence-wise Gibbs sampler is adopted to obtain proper segmentation results .	CL_D14-1186_ab_3	CL_D14-1186_ab_4	CL_D14-1186_ab_3_4
CL	D14-1186	ab	4	3	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	Based on the DP-4 model , a sentence-wise Gibbs sampler is adopted to obtain proper segmentation results .	In this paper , we propose a four-level Dirichlet Process based model ( DP-4 ) to jointly learn the word distributions from the corpus , domain and document levels simultaneously .	CL_D14-1186_ab_4	CL_D14-1186_ab_3	CL_D14-1186_ab_3_4
CL	D14-1186	ab	3	5	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	In this paper , we propose a four-level Dirichlet Process based model ( DP-4 ) to jointly learn the word distributions from the corpus , domain and document levels simultaneously .	Meanwhile , terms and keywords are acquired in the sampling process .	CL_D14-1186_ab_3	CL_D14-1186_ab_5	CL_D14-1186_ab_3_5
CL	D14-1186	ab	3	6	proposal	conclusion	none	support	main	secondary	back	support	In this paper , we propose a four-level Dirichlet Process based model ( DP-4 ) to jointly learn the word distributions from the corpus , domain and document levels simultaneously .	Experimental results have shown the effectiveness of our method .	CL_D14-1186_ab_3	CL_D14-1186_ab_6	CL_D14-1186_ab_3_6
CL	D14-1186	ab	6	3	conclusion	proposal	support	none	secondary	main	forw	support	Experimental results have shown the effectiveness of our method .	In this paper , we propose a four-level Dirichlet Process based model ( DP-4 ) to jointly learn the word distributions from the corpus , domain and document levels simultaneously .	CL_D14-1186_ab_6	CL_D14-1186_ab_3	CL_D14-1186_ab_3_6
CL	D14-1186	ab	4	5	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	Based on the DP-4 model , a sentence-wise Gibbs sampler is adopted to obtain proper segmentation results .	Meanwhile , terms and keywords are acquired in the sampling process .	CL_D14-1186_ab_4	CL_D14-1186_ab_5	CL_D14-1186_ab_4_5
CL	D14-1186	ab	5	4	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	forw	elaboration	Meanwhile , terms and keywords are acquired in the sampling process .	Based on the DP-4 model , a sentence-wise Gibbs sampler is adopted to obtain proper segmentation results .	CL_D14-1186_ab_5	CL_D14-1186_ab_4	CL_D14-1186_ab_4_5
CL	D14-1186	ab	4	6	proposal_implementation	conclusion	elaboration	support	secondary	secondary	none	none	Based on the DP-4 model , a sentence-wise Gibbs sampler is adopted to obtain proper segmentation results .	Experimental results have shown the effectiveness of our method .	CL_D14-1186_ab_4	CL_D14-1186_ab_6	CL_D14-1186_ab_4_6
CL	D14-1186	ab	5	6	proposal_implementation	conclusion	elaboration	support	secondary	secondary	none	none	Meanwhile , terms and keywords are acquired in the sampling process .	Experimental results have shown the effectiveness of our method .	CL_D14-1186_ab_5	CL_D14-1186_ab_6	CL_D14-1186_ab_5_6
CL	D14-1187	ab	1	2	motivation_problem	proposal	support	none	secondary	main	forw	support	When Part-of-Speech annotated data is scarce , e.g. for under-resourced languages , one can turn to cross-lingual transfer and crawled dictionaries to collect partially supervised data .	We cast this problem in the framework of ambiguous learning and show how to learn an accurate history-based model .	CL_D14-1187_ab_1	CL_D14-1187_ab_2	CL_D14-1187_ab_1_2
CL	D14-1187	ab	2	1	proposal	motivation_problem	none	support	main	secondary	back	support	We cast this problem in the framework of ambiguous learning and show how to learn an accurate history-based model .	When Part-of-Speech annotated data is scarce , e.g. for under-resourced languages , one can turn to cross-lingual transfer and crawled dictionaries to collect partially supervised data .	CL_D14-1187_ab_2	CL_D14-1187_ab_1	CL_D14-1187_ab_1_2
CL	D14-1187	ab	1	3	motivation_problem	result	support	support	secondary	secondary	none	none	When Part-of-Speech annotated data is scarce , e.g. for under-resourced languages , one can turn to cross-lingual transfer and crawled dictionaries to collect partially supervised data .	Experiments on ten languages show significant improvements over prior state of the art performance .	CL_D14-1187_ab_1	CL_D14-1187_ab_3	CL_D14-1187_ab_1_3
CL	D14-1187	ab	2	3	proposal	result	none	support	main	secondary	back	support	We cast this problem in the framework of ambiguous learning and show how to learn an accurate history-based model .	Experiments on ten languages show significant improvements over prior state of the art performance .	CL_D14-1187_ab_2	CL_D14-1187_ab_3	CL_D14-1187_ab_2_3
CL	D14-1187	ab	3	2	result	proposal	support	none	secondary	main	forw	support	Experiments on ten languages show significant improvements over prior state of the art performance .	We cast this problem in the framework of ambiguous learning and show how to learn an accurate history-based model .	CL_D14-1187_ab_3	CL_D14-1187_ab_2	CL_D14-1187_ab_2_3
CL	D14-1188	ab	1	2	proposal	proposal	none	elaboration	main	secondary	back	elaboration	We introduce new features for incorporating semantic predicate-argument structures in machine translation ( MT ) .	The methods focus on the completeness of the semantic structures of the translations , as well as the order of the translated semantic roles .	CL_D14-1188_ab_1	CL_D14-1188_ab_2	CL_D14-1188_ab_1_2
CL	D14-1188	ab	2	1	proposal	proposal	elaboration	none	secondary	main	forw	elaboration	The methods focus on the completeness of the semantic structures of the translations , as well as the order of the translated semantic roles .	We introduce new features for incorporating semantic predicate-argument structures in machine translation ( MT ) .	CL_D14-1188_ab_2	CL_D14-1188_ab_1	CL_D14-1188_ab_1_2
CL	D14-1188	ab	1	3	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	We introduce new features for incorporating semantic predicate-argument structures in machine translation ( MT ) .	We experiment with translation rules which contain the core arguments for the predicates in the source side of a MT system , and observe that using these rules significantly improves the translation quality .	CL_D14-1188_ab_1	CL_D14-1188_ab_3	CL_D14-1188_ab_1_3
CL	D14-1188	ab	1	4	proposal	proposal	none	elaboration	main	secondary	back	elaboration	We introduce new features for incorporating semantic predicate-argument structures in machine translation ( MT ) .	We also present a new semantic feature that resembles a language model .	CL_D14-1188_ab_1	CL_D14-1188_ab_4	CL_D14-1188_ab_1_4
CL	D14-1188	ab	4	1	proposal	proposal	elaboration	none	secondary	main	forw	elaboration	We also present a new semantic feature that resembles a language model .	We introduce new features for incorporating semantic predicate-argument structures in machine translation ( MT ) .	CL_D14-1188_ab_4	CL_D14-1188_ab_1	CL_D14-1188_ab_1_4
CL	D14-1188	ab	1	5	proposal	result	none	support	main	secondary	none	none	We introduce new features for incorporating semantic predicate-argument structures in machine translation ( MT ) .	Our results show that the language model feature can also significantly improve MT results .	CL_D14-1188_ab_1	CL_D14-1188_ab_5	CL_D14-1188_ab_1_5
CL	D14-1188	ab	2	3	proposal	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	The methods focus on the completeness of the semantic structures of the translations , as well as the order of the translated semantic roles .	We experiment with translation rules which contain the core arguments for the predicates in the source side of a MT system , and observe that using these rules significantly improves the translation quality .	CL_D14-1188_ab_2	CL_D14-1188_ab_3	CL_D14-1188_ab_2_3
CL	D14-1188	ab	3	2	proposal_implementation	proposal	elaboration	elaboration	secondary	secondary	forw	elaboration	We experiment with translation rules which contain the core arguments for the predicates in the source side of a MT system , and observe that using these rules significantly improves the translation quality .	The methods focus on the completeness of the semantic structures of the translations , as well as the order of the translated semantic roles .	CL_D14-1188_ab_3	CL_D14-1188_ab_2	CL_D14-1188_ab_2_3
CL	D14-1188	ab	2	4	proposal	proposal	elaboration	elaboration	secondary	secondary	none	none	The methods focus on the completeness of the semantic structures of the translations , as well as the order of the translated semantic roles .	We also present a new semantic feature that resembles a language model .	CL_D14-1188_ab_2	CL_D14-1188_ab_4	CL_D14-1188_ab_2_4
CL	D14-1188	ab	2	5	proposal	result	elaboration	support	secondary	secondary	none	none	The methods focus on the completeness of the semantic structures of the translations , as well as the order of the translated semantic roles .	Our results show that the language model feature can also significantly improve MT results .	CL_D14-1188_ab_2	CL_D14-1188_ab_5	CL_D14-1188_ab_2_5
CL	D14-1188	ab	3	4	proposal_implementation	proposal	elaboration	elaboration	secondary	secondary	none	none	We experiment with translation rules which contain the core arguments for the predicates in the source side of a MT system , and observe that using these rules significantly improves the translation quality .	We also present a new semantic feature that resembles a language model .	CL_D14-1188_ab_3	CL_D14-1188_ab_4	CL_D14-1188_ab_3_4
CL	D14-1188	ab	3	5	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	We experiment with translation rules which contain the core arguments for the predicates in the source side of a MT system , and observe that using these rules significantly improves the translation quality .	Our results show that the language model feature can also significantly improve MT results .	CL_D14-1188_ab_3	CL_D14-1188_ab_5	CL_D14-1188_ab_3_5
CL	D14-1188	ab	4	5	proposal	result	elaboration	support	secondary	secondary	back	support	We also present a new semantic feature that resembles a language model .	Our results show that the language model feature can also significantly improve MT results .	CL_D14-1188_ab_4	CL_D14-1188_ab_5	CL_D14-1188_ab_4_5
CL	D14-1188	ab	5	4	result	proposal	support	elaboration	secondary	secondary	forw	support	Our results show that the language model feature can also significantly improve MT results .	We also present a new semantic feature that resembles a language model .	CL_D14-1188_ab_5	CL_D14-1188_ab_4	CL_D14-1188_ab_4_5
CL	D14-1189	ab	1	2	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	We propose a simple unsupervised approach to detecting non-compositional components in multiword expressions based on Wiktionary .	The approach makes use of the definitions , synonyms and translations in Wiktionary , and is applicable to any type of MWE in any language , assuming the MWE is contained in Wiktionary .	CL_D14-1189_ab_1	CL_D14-1189_ab_2	CL_D14-1189_ab_1_2
CL	D14-1189	ab	2	1	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	The approach makes use of the definitions , synonyms and translations in Wiktionary , and is applicable to any type of MWE in any language , assuming the MWE is contained in Wiktionary .	We propose a simple unsupervised approach to detecting non-compositional components in multiword expressions based on Wiktionary .	CL_D14-1189_ab_2	CL_D14-1189_ab_1	CL_D14-1189_ab_1_2
CL	D14-1189	ab	1	3	proposal	result	none	support	main	secondary	back	support	We propose a simple unsupervised approach to detecting non-compositional components in multiword expressions based on Wiktionary .	Our experiments show that the proposed approach achieves higher F-score than state-of-the-art methods .	CL_D14-1189_ab_1	CL_D14-1189_ab_3	CL_D14-1189_ab_1_3
CL	D14-1189	ab	3	1	result	proposal	support	none	secondary	main	forw	support	Our experiments show that the proposed approach achieves higher F-score than state-of-the-art methods .	We propose a simple unsupervised approach to detecting non-compositional components in multiword expressions based on Wiktionary .	CL_D14-1189_ab_3	CL_D14-1189_ab_1	CL_D14-1189_ab_1_3
CL	D14-1189	ab	2	3	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	The approach makes use of the definitions , synonyms and translations in Wiktionary , and is applicable to any type of MWE in any language , assuming the MWE is contained in Wiktionary .	Our experiments show that the proposed approach achieves higher F-score than state-of-the-art methods .	CL_D14-1189_ab_2	CL_D14-1189_ab_3	CL_D14-1189_ab_2_3
CL	D14-1190	ab	1	2	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	We propose a model for jointly predicting multiple emotions in natural language sentences .	Our model is based on a low-rank coregionalisation approach , which combines a vector-valued Gaussian Process with a rich parameterisation scheme .	CL_D14-1190_ab_1	CL_D14-1190_ab_2	CL_D14-1190_ab_1_2
CL	D14-1190	ab	2	1	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	Our model is based on a low-rank coregionalisation approach , which combines a vector-valued Gaussian Process with a rich parameterisation scheme .	We propose a model for jointly predicting multiple emotions in natural language sentences .	CL_D14-1190_ab_2	CL_D14-1190_ab_1	CL_D14-1190_ab_1_2
CL	D14-1190	ab	1	3	proposal	result	none	support	main	secondary	back	support	We propose a model for jointly predicting multiple emotions in natural language sentences .	We show that our approach is able to learn correlations and anti-correlations between emotions on a news headlines dataset .	CL_D14-1190_ab_1	CL_D14-1190_ab_3	CL_D14-1190_ab_1_3
CL	D14-1190	ab	3	1	result	proposal	support	none	secondary	main	forw	support	We show that our approach is able to learn correlations and anti-correlations between emotions on a news headlines dataset .	We propose a model for jointly predicting multiple emotions in natural language sentences .	CL_D14-1190_ab_3	CL_D14-1190_ab_1	CL_D14-1190_ab_1_3
CL	D14-1190	ab	1	4	proposal	result	none	elaboration	main	secondary	none	none	We propose a model for jointly predicting multiple emotions in natural language sentences .	The proposed model outperforms both single-task baselines and other multi-task approaches .	CL_D14-1190_ab_1	CL_D14-1190_ab_4	CL_D14-1190_ab_1_4
CL	D14-1190	ab	2	3	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	Our model is based on a low-rank coregionalisation approach , which combines a vector-valued Gaussian Process with a rich parameterisation scheme .	We show that our approach is able to learn correlations and anti-correlations between emotions on a news headlines dataset .	CL_D14-1190_ab_2	CL_D14-1190_ab_3	CL_D14-1190_ab_2_3
CL	D14-1190	ab	2	4	proposal_implementation	result	elaboration	elaboration	secondary	secondary	none	none	Our model is based on a low-rank coregionalisation approach , which combines a vector-valued Gaussian Process with a rich parameterisation scheme .	The proposed model outperforms both single-task baselines and other multi-task approaches .	CL_D14-1190_ab_2	CL_D14-1190_ab_4	CL_D14-1190_ab_2_4
CL	D14-1190	ab	3	4	result	result	support	elaboration	secondary	secondary	back	elaboration	We show that our approach is able to learn correlations and anti-correlations between emotions on a news headlines dataset .	The proposed model outperforms both single-task baselines and other multi-task approaches .	CL_D14-1190_ab_3	CL_D14-1190_ab_4	CL_D14-1190_ab_3_4
CL	D14-1190	ab	4	3	result	result	elaboration	support	secondary	secondary	forw	elaboration	The proposed model outperforms both single-task baselines and other multi-task approaches .	We show that our approach is able to learn correlations and anti-correlations between emotions on a news headlines dataset .	CL_D14-1190_ab_4	CL_D14-1190_ab_3	CL_D14-1190_ab_3_4
CL	D14-1191	ab	1	2	motivation_background	proposal	support	none	secondary	main	forw	support	Previous work on extracting ideology from text has focused on domains where expression of political views is expected , but it's unclear if current technology can work in domains where displays of ideology are considered inappropriate .	We present a supervised ensemble n-gram model for ideology extraction with topic adjustments and apply it to one such domain : research papers written by academic economists .	CL_D14-1191_ab_1	CL_D14-1191_ab_2	CL_D14-1191_ab_1_2
CL	D14-1191	ab	2	1	proposal	motivation_background	none	support	main	secondary	back	support	We present a supervised ensemble n-gram model for ideology extraction with topic adjustments and apply it to one such domain : research papers written by academic economists .	Previous work on extracting ideology from text has focused on domains where expression of political views is expected , but it's unclear if current technology can work in domains where displays of ideology are considered inappropriate .	CL_D14-1191_ab_2	CL_D14-1191_ab_1	CL_D14-1191_ab_1_2
CL	D14-1191	ab	1	3	motivation_background	result	support	support	secondary	secondary	none	none	Previous work on extracting ideology from text has focused on domains where expression of political views is expected , but it's unclear if current technology can work in domains where displays of ideology are considered inappropriate .	We show economists' political leanings can be correctly predicted , that our predictions generalize to new domains , and that they correlate with public policy-relevant research findings .	CL_D14-1191_ab_1	CL_D14-1191_ab_3	CL_D14-1191_ab_1_3
CL	D14-1191	ab	1	4	motivation_background	result	support	support	secondary	secondary	none	none	Previous work on extracting ideology from text has focused on domains where expression of political views is expected , but it's unclear if current technology can work in domains where displays of ideology are considered inappropriate .	We also present evidence that unsupervised models can under-perform in domains where ideological expression is discouraged .	CL_D14-1191_ab_1	CL_D14-1191_ab_4	CL_D14-1191_ab_1_4
CL	D14-1191	ab	2	3	proposal	result	none	support	main	secondary	back	support	We present a supervised ensemble n-gram model for ideology extraction with topic adjustments and apply it to one such domain : research papers written by academic economists .	We show economists' political leanings can be correctly predicted , that our predictions generalize to new domains , and that they correlate with public policy-relevant research findings .	CL_D14-1191_ab_2	CL_D14-1191_ab_3	CL_D14-1191_ab_2_3
CL	D14-1191	ab	3	2	result	proposal	support	none	secondary	main	forw	support	We show economists' political leanings can be correctly predicted , that our predictions generalize to new domains , and that they correlate with public policy-relevant research findings .	We present a supervised ensemble n-gram model for ideology extraction with topic adjustments and apply it to one such domain : research papers written by academic economists .	CL_D14-1191_ab_3	CL_D14-1191_ab_2	CL_D14-1191_ab_2_3
CL	D14-1191	ab	2	4	proposal	result	none	support	main	secondary	back	support	We present a supervised ensemble n-gram model for ideology extraction with topic adjustments and apply it to one such domain : research papers written by academic economists .	We also present evidence that unsupervised models can under-perform in domains where ideological expression is discouraged .	CL_D14-1191_ab_2	CL_D14-1191_ab_4	CL_D14-1191_ab_2_4
CL	D14-1191	ab	4	2	result	proposal	support	none	secondary	main	forw	support	We also present evidence that unsupervised models can under-perform in domains where ideological expression is discouraged .	We present a supervised ensemble n-gram model for ideology extraction with topic adjustments and apply it to one such domain : research papers written by academic economists .	CL_D14-1191_ab_4	CL_D14-1191_ab_2	CL_D14-1191_ab_2_4
CL	D14-1191	ab	3	4	result	result	support	support	secondary	secondary	none	none	We show economists' political leanings can be correctly predicted , that our predictions generalize to new domains , and that they correlate with public policy-relevant research findings .	We also present evidence that unsupervised models can under-perform in domains where ideological expression is discouraged .	CL_D14-1191_ab_3	CL_D14-1191_ab_4	CL_D14-1191_ab_3_4
CL	D14-1192	ab	1	2	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	We develop a statistical model of saccadic eye movements during reading of isolated sentences .	The model is focused on representing individual differences between readers and supports the inference of the most likely reader for a novel set of eye movement patterns .	CL_D14-1192_ab_1	CL_D14-1192_ab_2	CL_D14-1192_ab_1_2
CL	D14-1192	ab	2	1	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	The model is focused on representing individual differences between readers and supports the inference of the most likely reader for a novel set of eye movement patterns .	We develop a statistical model of saccadic eye movements during reading of isolated sentences .	CL_D14-1192_ab_2	CL_D14-1192_ab_1	CL_D14-1192_ab_1_2
CL	D14-1192	ab	1	3	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	We develop a statistical model of saccadic eye movements during reading of isolated sentences .	We empirically study the model for biometric reader identification using eye-tracking data collected from 20 individuals and observe that the model distinguishes between 20 readers with an accuracy of up to 98 % .	CL_D14-1192_ab_1	CL_D14-1192_ab_3	CL_D14-1192_ab_1_3
CL	D14-1192	ab	2	3	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	The model is focused on representing individual differences between readers and supports the inference of the most likely reader for a novel set of eye movement patterns .	We empirically study the model for biometric reader identification using eye-tracking data collected from 20 individuals and observe that the model distinguishes between 20 readers with an accuracy of up to 98 % .	CL_D14-1192_ab_2	CL_D14-1192_ab_3	CL_D14-1192_ab_2_3
CL	D14-1192	ab	3	2	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	forw	elaboration	We empirically study the model for biometric reader identification using eye-tracking data collected from 20 individuals and observe that the model distinguishes between 20 readers with an accuracy of up to 98 % .	The model is focused on representing individual differences between readers and supports the inference of the most likely reader for a novel set of eye movement patterns .	CL_D14-1192_ab_3	CL_D14-1192_ab_2	CL_D14-1192_ab_2_3
CL	D14-1193	ab	1	2	motivation_background	motivation_hypothesis	info-required	support	secondary	secondary	forw	info-required	Multi-label text categorization ( MTC ) is supervised learning , where a document may be assigned with multiple categories ( labels ) simultaneously .	The labels in the MTC are correlated and the correlation results in some hidden components , which represent the "share" variance of correlated labels .	CL_D14-1193_ab_1	CL_D14-1193_ab_2	CL_D14-1193_ab_1_2
CL	D14-1193	ab	2	1	motivation_hypothesis	motivation_background	support	info-required	secondary	secondary	back	info-required	The labels in the MTC are correlated and the correlation results in some hidden components , which represent the "share" variance of correlated labels .	Multi-label text categorization ( MTC ) is supervised learning , where a document may be assigned with multiple categories ( labels ) simultaneously .	CL_D14-1193_ab_2	CL_D14-1193_ab_1	CL_D14-1193_ab_1_2
CL	D14-1193	ab	1	3	motivation_background	proposal	info-required	none	secondary	main	none	none	Multi-label text categorization ( MTC ) is supervised learning , where a document may be assigned with multiple categories ( labels ) simultaneously .	In this paper , we propose a method with hidden components for MTC .	CL_D14-1193_ab_1	CL_D14-1193_ab_3	CL_D14-1193_ab_1_3
CL	D14-1193	ab	1	4	motivation_background	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	Multi-label text categorization ( MTC ) is supervised learning , where a document may be assigned with multiple categories ( labels ) simultaneously .	The proposed method employs PCA to capture the hidden components , and incorporates them into a joint learning framework to improve the performance .	CL_D14-1193_ab_1	CL_D14-1193_ab_4	CL_D14-1193_ab_1_4
CL	D14-1193	ab	1	5	motivation_background	result	info-required	support	secondary	secondary	none	none	Multi-label text categorization ( MTC ) is supervised learning , where a document may be assigned with multiple categories ( labels ) simultaneously .	Experiments with real-world data sets and evaluation metrics validate the effectiveness of the proposed method .	CL_D14-1193_ab_1	CL_D14-1193_ab_5	CL_D14-1193_ab_1_5
CL	D14-1193	ab	2	3	motivation_hypothesis	proposal	support	none	secondary	main	forw	support	The labels in the MTC are correlated and the correlation results in some hidden components , which represent the "share" variance of correlated labels .	In this paper , we propose a method with hidden components for MTC .	CL_D14-1193_ab_2	CL_D14-1193_ab_3	CL_D14-1193_ab_2_3
CL	D14-1193	ab	3	2	proposal	motivation_hypothesis	none	support	main	secondary	back	support	In this paper , we propose a method with hidden components for MTC .	The labels in the MTC are correlated and the correlation results in some hidden components , which represent the "share" variance of correlated labels .	CL_D14-1193_ab_3	CL_D14-1193_ab_2	CL_D14-1193_ab_2_3
CL	D14-1193	ab	2	4	motivation_hypothesis	proposal_implementation	support	elaboration	secondary	secondary	none	none	The labels in the MTC are correlated and the correlation results in some hidden components , which represent the "share" variance of correlated labels .	The proposed method employs PCA to capture the hidden components , and incorporates them into a joint learning framework to improve the performance .	CL_D14-1193_ab_2	CL_D14-1193_ab_4	CL_D14-1193_ab_2_4
CL	D14-1193	ab	2	5	motivation_hypothesis	result	support	support	secondary	secondary	none	none	The labels in the MTC are correlated and the correlation results in some hidden components , which represent the "share" variance of correlated labels .	Experiments with real-world data sets and evaluation metrics validate the effectiveness of the proposed method .	CL_D14-1193_ab_2	CL_D14-1193_ab_5	CL_D14-1193_ab_2_5
CL	D14-1193	ab	3	4	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	In this paper , we propose a method with hidden components for MTC .	The proposed method employs PCA to capture the hidden components , and incorporates them into a joint learning framework to improve the performance .	CL_D14-1193_ab_3	CL_D14-1193_ab_4	CL_D14-1193_ab_3_4
CL	D14-1193	ab	4	3	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	The proposed method employs PCA to capture the hidden components , and incorporates them into a joint learning framework to improve the performance .	In this paper , we propose a method with hidden components for MTC .	CL_D14-1193_ab_4	CL_D14-1193_ab_3	CL_D14-1193_ab_3_4
CL	D14-1193	ab	3	5	proposal	result	none	support	main	secondary	back	support	In this paper , we propose a method with hidden components for MTC .	Experiments with real-world data sets and evaluation metrics validate the effectiveness of the proposed method .	CL_D14-1193_ab_3	CL_D14-1193_ab_5	CL_D14-1193_ab_3_5
CL	D14-1193	ab	5	3	result	proposal	support	none	secondary	main	forw	support	Experiments with real-world data sets and evaluation metrics validate the effectiveness of the proposed method .	In this paper , we propose a method with hidden components for MTC .	CL_D14-1193_ab_5	CL_D14-1193_ab_3	CL_D14-1193_ab_3_5
CL	D14-1193	ab	4	5	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	The proposed method employs PCA to capture the hidden components , and incorporates them into a joint learning framework to improve the performance .	Experiments with real-world data sets and evaluation metrics validate the effectiveness of the proposed method .	CL_D14-1193_ab_4	CL_D14-1193_ab_5	CL_D14-1193_ab_4_5
CL	D14-1194	ab	1	2	proposal_implementation	proposal_implementation	none	elaboration	main	secondary	back	elaboration	We describe a convolutional neural network that learns feature representations for short textual posts using hashtags as a supervised signal.	The proposed approach is trained on up to 5.5 billion words predicting 100,000 possible hashtags .	CL_D14-1194_ab_1	CL_D14-1194_ab_2	CL_D14-1194_ab_1_2
CL	D14-1194	ab	2	1	proposal_implementation	proposal_implementation	elaboration	none	secondary	main	forw	elaboration	The proposed approach is trained on up to 5.5 billion words predicting 100,000 possible hashtags .	We describe a convolutional neural network that learns feature representations for short textual posts using hashtags as a supervised signal.	CL_D14-1194_ab_2	CL_D14-1194_ab_1	CL_D14-1194_ab_1_2
CL	D14-1194	ab	1	3	proposal_implementation	result	none	support	main	secondary	back	support	We describe a convolutional neural network that learns feature representations for short textual posts using hashtags as a supervised signal.	As well as strong performance on the hashtag prediction task itself , we show that its learned representation of text ( ignoring the hashtag labels ) is useful for other tasks as well .	CL_D14-1194_ab_1	CL_D14-1194_ab_3	CL_D14-1194_ab_1_3
CL	D14-1194	ab	3	1	result	proposal_implementation	support	none	secondary	main	forw	support	As well as strong performance on the hashtag prediction task itself , we show that its learned representation of text ( ignoring the hashtag labels ) is useful for other tasks as well .	We describe a convolutional neural network that learns feature representations for short textual posts using hashtags as a supervised signal.	CL_D14-1194_ab_3	CL_D14-1194_ab_1	CL_D14-1194_ab_1_3
CL	D14-1194	ab	1	4	proposal_implementation	result	none	elaboration	main	secondary	none	none	We describe a convolutional neural network that learns feature representations for short textual posts using hashtags as a supervised signal.	To that end , we present results on a document recommendation task , where it also outperforms a number of baselines .	CL_D14-1194_ab_1	CL_D14-1194_ab_4	CL_D14-1194_ab_1_4
CL	D14-1194	ab	2	3	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	The proposed approach is trained on up to 5.5 billion words predicting 100,000 possible hashtags .	As well as strong performance on the hashtag prediction task itself , we show that its learned representation of text ( ignoring the hashtag labels ) is useful for other tasks as well .	CL_D14-1194_ab_2	CL_D14-1194_ab_3	CL_D14-1194_ab_2_3
CL	D14-1194	ab	2	4	proposal_implementation	result	elaboration	elaboration	secondary	secondary	none	none	The proposed approach is trained on up to 5.5 billion words predicting 100,000 possible hashtags .	To that end , we present results on a document recommendation task , where it also outperforms a number of baselines .	CL_D14-1194_ab_2	CL_D14-1194_ab_4	CL_D14-1194_ab_2_4
CL	D14-1194	ab	3	4	result	result	support	elaboration	secondary	secondary	back	elaboration	As well as strong performance on the hashtag prediction task itself , we show that its learned representation of text ( ignoring the hashtag labels ) is useful for other tasks as well .	To that end , we present results on a document recommendation task , where it also outperforms a number of baselines .	CL_D14-1194_ab_3	CL_D14-1194_ab_4	CL_D14-1194_ab_3_4
CL	D14-1194	ab	4	3	result	result	elaboration	support	secondary	secondary	forw	elaboration	To that end , we present results on a document recommendation task , where it also outperforms a number of baselines .	As well as strong performance on the hashtag prediction task itself , we show that its learned representation of text ( ignoring the hashtag labels ) is useful for other tasks as well .	CL_D14-1194_ab_4	CL_D14-1194_ab_3	CL_D14-1194_ab_3_4
CL	D14-1195	ab	1	2	proposal_implementation	proposal	none	elaboration	main	secondary	back	elaboration	In this paper , we provide a new method for decoding tree transduction based sentence compression models augmented with language model scores , by jointly decoding two components .	In our proposed solution , rich local discriminative features can be easily integrated without increasing computational complexity .	CL_D14-1195_ab_1	CL_D14-1195_ab_2	CL_D14-1195_ab_1_2
CL	D14-1195	ab	2	1	proposal	proposal_implementation	elaboration	none	secondary	main	forw	elaboration	In our proposed solution , rich local discriminative features can be easily integrated without increasing computational complexity .	In this paper , we provide a new method for decoding tree transduction based sentence compression models augmented with language model scores , by jointly decoding two components .	CL_D14-1195_ab_2	CL_D14-1195_ab_1	CL_D14-1195_ab_1_2
CL	D14-1195	ab	1	3	proposal_implementation	proposal_implementation	none	elaboration	main	secondary	none	none	In this paper , we provide a new method for decoding tree transduction based sentence compression models augmented with language model scores , by jointly decoding two components .	Utilizing an unobvious fact that the resulted two components can be independently decoded , we conduct efficient joint decoding based on dual decomposition .	CL_D14-1195_ab_1	CL_D14-1195_ab_3	CL_D14-1195_ab_1_3
CL	D14-1195	ab	1	4	proposal_implementation	result	none	support	main	secondary	back	support	In this paper , we provide a new method for decoding tree transduction based sentence compression models augmented with language model scores , by jointly decoding two components .	Experimental results show that our method outperforms traditional beam search decoding and achieves the state-of-the-art performance .	CL_D14-1195_ab_1	CL_D14-1195_ab_4	CL_D14-1195_ab_1_4
CL	D14-1195	ab	4	1	result	proposal_implementation	support	none	secondary	main	forw	support	Experimental results show that our method outperforms traditional beam search decoding and achieves the state-of-the-art performance .	In this paper , we provide a new method for decoding tree transduction based sentence compression models augmented with language model scores , by jointly decoding two components .	CL_D14-1195_ab_4	CL_D14-1195_ab_1	CL_D14-1195_ab_1_4
CL	D14-1195	ab	2	3	proposal	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	In our proposed solution , rich local discriminative features can be easily integrated without increasing computational complexity .	Utilizing an unobvious fact that the resulted two components can be independently decoded , we conduct efficient joint decoding based on dual decomposition .	CL_D14-1195_ab_2	CL_D14-1195_ab_3	CL_D14-1195_ab_2_3
CL	D14-1195	ab	3	2	proposal_implementation	proposal	elaboration	elaboration	secondary	secondary	forw	elaboration	Utilizing an unobvious fact that the resulted two components can be independently decoded , we conduct efficient joint decoding based on dual decomposition .	In our proposed solution , rich local discriminative features can be easily integrated without increasing computational complexity .	CL_D14-1195_ab_3	CL_D14-1195_ab_2	CL_D14-1195_ab_2_3
CL	D14-1195	ab	2	4	proposal	result	elaboration	support	secondary	secondary	none	none	In our proposed solution , rich local discriminative features can be easily integrated without increasing computational complexity .	Experimental results show that our method outperforms traditional beam search decoding and achieves the state-of-the-art performance .	CL_D14-1195_ab_2	CL_D14-1195_ab_4	CL_D14-1195_ab_2_4
CL	D14-1195	ab	3	4	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	Utilizing an unobvious fact that the resulted two components can be independently decoded , we conduct efficient joint decoding based on dual decomposition .	Experimental results show that our method outperforms traditional beam search decoding and achieves the state-of-the-art performance .	CL_D14-1195_ab_3	CL_D14-1195_ab_4	CL_D14-1195_ab_3_4
CL	D14-1196	ab	1	2	proposal_implementation	motivation_background	none	info-required	main	secondary	none	none	The current state-of-the-art single-document summarization method generates a summary by solving a Tree Knapsack Problem ( TKP ) , which is the problem of finding the optimal rooted subtree of the dependency-based discourse tree ( DEP-DT ) of a document .	We can obtain a gold DEP-DT by transforming a gold Rhetorical Structure Theory-based discourse tree ( RST-DT ) .	CL_D14-1196_ab_1	CL_D14-1196_ab_2	CL_D14-1196_ab_1_2
CL	D14-1196	ab	1	3	proposal_implementation	motivation_problem	none	support	main	secondary	none	none	The current state-of-the-art single-document summarization method generates a summary by solving a Tree Knapsack Problem ( TKP ) , which is the problem of finding the optimal rooted subtree of the dependency-based discourse tree ( DEP-DT ) of a document .	However , there is still a large difference between the ROUGE scores of a system with a gold DEP-DT and a system with a DEP-DT obtained from an automatically parsed RST-DT .	CL_D14-1196_ab_1	CL_D14-1196_ab_3	CL_D14-1196_ab_1_3
CL	D14-1196	ab	1	4	proposal_implementation	proposal	none	elaboration	main	secondary	back	elaboration	The current state-of-the-art single-document summarization method generates a summary by solving a Tree Knapsack Problem ( TKP ) , which is the problem of finding the optimal rooted subtree of the dependency-based discourse tree ( DEP-DT ) of a document .	To improve the ROUGE score , we propose a novel discourse parser that directly generates the DEP-DT .	CL_D14-1196_ab_1	CL_D14-1196_ab_4	CL_D14-1196_ab_1_4
CL	D14-1196	ab	4	1	proposal	proposal_implementation	elaboration	none	secondary	main	forw	elaboration	To improve the ROUGE score , we propose a novel discourse parser that directly generates the DEP-DT .	The current state-of-the-art single-document summarization method generates a summary by solving a Tree Knapsack Problem ( TKP ) , which is the problem of finding the optimal rooted subtree of the dependency-based discourse tree ( DEP-DT ) of a document .	CL_D14-1196_ab_4	CL_D14-1196_ab_1	CL_D14-1196_ab_1_4
CL	D14-1196	ab	1	5	proposal_implementation	result	none	support	main	secondary	back	support	The current state-of-the-art single-document summarization method generates a summary by solving a Tree Knapsack Problem ( TKP ) , which is the problem of finding the optimal rooted subtree of the dependency-based discourse tree ( DEP-DT ) of a document .	The evaluation results showed that the TKP with our parser outperformed that with the state-of-the-art RST-DT parser , and achieved almost equivalent ROUGE scores to the TKP with the gold DEP-DT .	CL_D14-1196_ab_1	CL_D14-1196_ab_5	CL_D14-1196_ab_1_5
CL	D14-1196	ab	5	1	result	proposal_implementation	support	none	secondary	main	forw	support	The evaluation results showed that the TKP with our parser outperformed that with the state-of-the-art RST-DT parser , and achieved almost equivalent ROUGE scores to the TKP with the gold DEP-DT .	The current state-of-the-art single-document summarization method generates a summary by solving a Tree Knapsack Problem ( TKP ) , which is the problem of finding the optimal rooted subtree of the dependency-based discourse tree ( DEP-DT ) of a document .	CL_D14-1196_ab_5	CL_D14-1196_ab_1	CL_D14-1196_ab_1_5
CL	D14-1196	ab	2	3	motivation_background	motivation_problem	info-required	support	secondary	secondary	forw	info-required	We can obtain a gold DEP-DT by transforming a gold Rhetorical Structure Theory-based discourse tree ( RST-DT ) .	However , there is still a large difference between the ROUGE scores of a system with a gold DEP-DT and a system with a DEP-DT obtained from an automatically parsed RST-DT .	CL_D14-1196_ab_2	CL_D14-1196_ab_3	CL_D14-1196_ab_2_3
CL	D14-1196	ab	3	2	motivation_problem	motivation_background	support	info-required	secondary	secondary	back	info-required	However , there is still a large difference between the ROUGE scores of a system with a gold DEP-DT and a system with a DEP-DT obtained from an automatically parsed RST-DT .	We can obtain a gold DEP-DT by transforming a gold Rhetorical Structure Theory-based discourse tree ( RST-DT ) .	CL_D14-1196_ab_3	CL_D14-1196_ab_2	CL_D14-1196_ab_2_3
CL	D14-1196	ab	2	4	motivation_background	proposal	info-required	elaboration	secondary	secondary	none	none	We can obtain a gold DEP-DT by transforming a gold Rhetorical Structure Theory-based discourse tree ( RST-DT ) .	To improve the ROUGE score , we propose a novel discourse parser that directly generates the DEP-DT .	CL_D14-1196_ab_2	CL_D14-1196_ab_4	CL_D14-1196_ab_2_4
CL	D14-1196	ab	2	5	motivation_background	result	info-required	support	secondary	secondary	none	none	We can obtain a gold DEP-DT by transforming a gold Rhetorical Structure Theory-based discourse tree ( RST-DT ) .	The evaluation results showed that the TKP with our parser outperformed that with the state-of-the-art RST-DT parser , and achieved almost equivalent ROUGE scores to the TKP with the gold DEP-DT .	CL_D14-1196_ab_2	CL_D14-1196_ab_5	CL_D14-1196_ab_2_5
CL	D14-1196	ab	3	4	motivation_problem	proposal	support	elaboration	secondary	secondary	forw	support	However , there is still a large difference between the ROUGE scores of a system with a gold DEP-DT and a system with a DEP-DT obtained from an automatically parsed RST-DT .	To improve the ROUGE score , we propose a novel discourse parser that directly generates the DEP-DT .	CL_D14-1196_ab_3	CL_D14-1196_ab_4	CL_D14-1196_ab_3_4
CL	D14-1196	ab	4	3	proposal	motivation_problem	elaboration	support	secondary	secondary	back	support	To improve the ROUGE score , we propose a novel discourse parser that directly generates the DEP-DT .	However , there is still a large difference between the ROUGE scores of a system with a gold DEP-DT and a system with a DEP-DT obtained from an automatically parsed RST-DT .	CL_D14-1196_ab_4	CL_D14-1196_ab_3	CL_D14-1196_ab_3_4
CL	D14-1196	ab	3	5	motivation_problem	result	support	support	secondary	secondary	none	none	However , there is still a large difference between the ROUGE scores of a system with a gold DEP-DT and a system with a DEP-DT obtained from an automatically parsed RST-DT .	The evaluation results showed that the TKP with our parser outperformed that with the state-of-the-art RST-DT parser , and achieved almost equivalent ROUGE scores to the TKP with the gold DEP-DT .	CL_D14-1196_ab_3	CL_D14-1196_ab_5	CL_D14-1196_ab_3_5
CL	D14-1196	ab	4	5	proposal	result	elaboration	support	secondary	secondary	none	none	To improve the ROUGE score , we propose a novel discourse parser that directly generates the DEP-DT .	The evaluation results showed that the TKP with our parser outperformed that with the state-of-the-art RST-DT parser , and achieved almost equivalent ROUGE scores to the TKP with the gold DEP-DT .	CL_D14-1196_ab_4	CL_D14-1196_ab_5	CL_D14-1196_ab_4_5
CL	D14-1197	ab	1	2	result	proposal	support	none	secondary	main	forw	support	We show that semantic relationships can be used to improve word alignment , in addition to the lexical and syntactic features that are typically used .	In this paper , we present a method based on a neural network to automatically derive word similarity from monolingual data .	CL_D14-1197_ab_1	CL_D14-1197_ab_2	CL_D14-1197_ab_1_2
CL	D14-1197	ab	2	1	proposal	result	none	support	main	secondary	back	support	In this paper , we present a method based on a neural network to automatically derive word similarity from monolingual data .	We show that semantic relationships can be used to improve word alignment , in addition to the lexical and syntactic features that are typically used .	CL_D14-1197_ab_2	CL_D14-1197_ab_1	CL_D14-1197_ab_1_2
CL	D14-1197	ab	1	3	result	proposal	support	elaboration	secondary	secondary	none	none	We show that semantic relationships can be used to improve word alignment , in addition to the lexical and syntactic features that are typically used .	We present an extension to word alignment models that exploits word similarity .	CL_D14-1197_ab_1	CL_D14-1197_ab_3	CL_D14-1197_ab_1_3
CL	D14-1197	ab	1	4	result	result	support	elaboration	secondary	secondary	back	elaboration	We show that semantic relationships can be used to improve word alignment , in addition to the lexical and syntactic features that are typically used .	Our experiments , in both large-scale and resource-limited settings , show improvements in word alignment tasks as well as translation tasks .	CL_D14-1197_ab_1	CL_D14-1197_ab_4	CL_D14-1197_ab_1_4
CL	D14-1197	ab	4	1	result	result	elaboration	support	secondary	secondary	forw	elaboration	Our experiments , in both large-scale and resource-limited settings , show improvements in word alignment tasks as well as translation tasks .	We show that semantic relationships can be used to improve word alignment , in addition to the lexical and syntactic features that are typically used .	CL_D14-1197_ab_4	CL_D14-1197_ab_1	CL_D14-1197_ab_1_4
CL	D14-1197	ab	2	3	proposal	proposal	none	elaboration	main	secondary	back	elaboration	In this paper , we present a method based on a neural network to automatically derive word similarity from monolingual data .	We present an extension to word alignment models that exploits word similarity .	CL_D14-1197_ab_2	CL_D14-1197_ab_3	CL_D14-1197_ab_2_3
CL	D14-1197	ab	3	2	proposal	proposal	elaboration	none	secondary	main	forw	elaboration	We present an extension to word alignment models that exploits word similarity .	In this paper , we present a method based on a neural network to automatically derive word similarity from monolingual data .	CL_D14-1197_ab_3	CL_D14-1197_ab_2	CL_D14-1197_ab_2_3
CL	D14-1197	ab	2	4	proposal	result	none	elaboration	main	secondary	none	none	In this paper , we present a method based on a neural network to automatically derive word similarity from monolingual data .	Our experiments , in both large-scale and resource-limited settings , show improvements in word alignment tasks as well as translation tasks .	CL_D14-1197_ab_2	CL_D14-1197_ab_4	CL_D14-1197_ab_2_4
CL	D14-1197	ab	3	4	proposal	result	elaboration	elaboration	secondary	secondary	none	none	We present an extension to word alignment models that exploits word similarity .	Our experiments , in both large-scale and resource-limited settings , show improvements in word alignment tasks as well as translation tasks .	CL_D14-1197_ab_3	CL_D14-1197_ab_4	CL_D14-1197_ab_3_4
CL	D14-1198	ab	1	2	proposal	proposal	none	elaboration	main	secondary	back	elaboration	In this paper , we propose a new framework that unifies the output of three information extraction ( IE ) tasks - entity mentions , relations and events as an information network representation , and extracts all of them using one single joint model based on structured prediction .	This novel formulation allows different parts of the information network fully interact with each other .	CL_D14-1198_ab_1	CL_D14-1198_ab_2	CL_D14-1198_ab_1_2
CL	D14-1198	ab	2	1	proposal	proposal	elaboration	none	secondary	main	forw	elaboration	This novel formulation allows different parts of the information network fully interact with each other .	In this paper , we propose a new framework that unifies the output of three information extraction ( IE ) tasks - entity mentions , relations and events as an information network representation , and extracts all of them using one single joint model based on structured prediction .	CL_D14-1198_ab_2	CL_D14-1198_ab_1	CL_D14-1198_ab_1_2
CL	D14-1198	ab	1	3	proposal	information_additional	none	info-optional	main	secondary	none	none	In this paper , we propose a new framework that unifies the output of three information extraction ( IE ) tasks - entity mentions , relations and events as an information network representation , and extracts all of them using one single joint model based on structured prediction .	For example , many relations can now be considered as the resultant states of events .	CL_D14-1198_ab_1	CL_D14-1198_ab_3	CL_D14-1198_ab_1_3
CL	D14-1198	ab	1	4	proposal	result	none	support	main	secondary	back	support	In this paper , we propose a new framework that unifies the output of three information extraction ( IE ) tasks - entity mentions , relations and events as an information network representation , and extracts all of them using one single joint model based on structured prediction .	Our approach achieves substantial improvements over traditional pipelined approaches , and significantly advances state-of-the-art end-to-end event argument extraction .	CL_D14-1198_ab_1	CL_D14-1198_ab_4	CL_D14-1198_ab_1_4
CL	D14-1198	ab	4	1	result	proposal	support	none	secondary	main	forw	support	Our approach achieves substantial improvements over traditional pipelined approaches , and significantly advances state-of-the-art end-to-end event argument extraction .	In this paper , we propose a new framework that unifies the output of three information extraction ( IE ) tasks - entity mentions , relations and events as an information network representation , and extracts all of them using one single joint model based on structured prediction .	CL_D14-1198_ab_4	CL_D14-1198_ab_1	CL_D14-1198_ab_1_4
CL	D14-1198	ab	2	3	proposal	information_additional	elaboration	info-optional	secondary	secondary	back	info-optional	This novel formulation allows different parts of the information network fully interact with each other .	For example , many relations can now be considered as the resultant states of events .	CL_D14-1198_ab_2	CL_D14-1198_ab_3	CL_D14-1198_ab_2_3
CL	D14-1198	ab	3	2	information_additional	proposal	info-optional	elaboration	secondary	secondary	forw	info-optional	For example , many relations can now be considered as the resultant states of events .	This novel formulation allows different parts of the information network fully interact with each other .	CL_D14-1198_ab_3	CL_D14-1198_ab_2	CL_D14-1198_ab_2_3
CL	D14-1198	ab	2	4	proposal	result	elaboration	support	secondary	secondary	none	none	This novel formulation allows different parts of the information network fully interact with each other .	Our approach achieves substantial improvements over traditional pipelined approaches , and significantly advances state-of-the-art end-to-end event argument extraction .	CL_D14-1198_ab_2	CL_D14-1198_ab_4	CL_D14-1198_ab_2_4
CL	D14-1198	ab	3	4	information_additional	result	info-optional	support	secondary	secondary	none	none	For example , many relations can now be considered as the resultant states of events .	Our approach achieves substantial improvements over traditional pipelined approaches , and significantly advances state-of-the-art end-to-end event argument extraction .	CL_D14-1198_ab_3	CL_D14-1198_ab_4	CL_D14-1198_ab_3_4
CL	D14-1199	ab	1	2	motivation_background	proposal	support	none	secondary	main	forw	support	The efficiency of Information Extraction systems is known to be heavily influenced by domain-specific knowledge but the cost of developing such systems is considerably high .	In this article , we consider the problem of event extraction and show that learning word representations from unlabeled domain-specific data and using them for representing event roles enable to outperform previous state-of-the-art event extraction models on the MUC-4 data set .	CL_D14-1199_ab_1	CL_D14-1199_ab_2	CL_D14-1199_ab_1_2
CL	D14-1199	ab	2	1	proposal	motivation_background	none	support	main	secondary	back	support	In this article , we consider the problem of event extraction and show that learning word representations from unlabeled domain-specific data and using them for representing event roles enable to outperform previous state-of-the-art event extraction models on the MUC-4 data set .	The efficiency of Information Extraction systems is known to be heavily influenced by domain-specific knowledge but the cost of developing such systems is considerably high .	CL_D14-1199_ab_2	CL_D14-1199_ab_1	CL_D14-1199_ab_1_2
CL	D14-1200	ab	1	2	proposal	proposal	none	elaboration	main	secondary	back	elaboration	This paper proposes a history-based structured learning approach that jointly extracts entities and relations in a sentence .	We introduce a novel simple and flexible table representation of entities and relations .	CL_D14-1200_ab_1	CL_D14-1200_ab_2	CL_D14-1200_ab_1_2
CL	D14-1200	ab	2	1	proposal	proposal	elaboration	none	secondary	main	forw	elaboration	We introduce a novel simple and flexible table representation of entities and relations .	This paper proposes a history-based structured learning approach that jointly extracts entities and relations in a sentence .	CL_D14-1200_ab_2	CL_D14-1200_ab_1	CL_D14-1200_ab_1_2
CL	D14-1200	ab	1	3	proposal	proposal	none	elaboration	main	secondary	none	none	This paper proposes a history-based structured learning approach that jointly extracts entities and relations in a sentence .	We investigate several feature settings , search orders , and learning methods with inexact search on the table .	CL_D14-1200_ab_1	CL_D14-1200_ab_3	CL_D14-1200_ab_1_3
CL	D14-1200	ab	1	4	proposal	result	none	support	main	secondary	back	support	This paper proposes a history-based structured learning approach that jointly extracts entities and relations in a sentence .	The experimental results demonstrate that a joint learning approach significantly outperforms a pipeline approach by incorporating global features and by selecting appropriate learning methods and search orders .	CL_D14-1200_ab_1	CL_D14-1200_ab_4	CL_D14-1200_ab_1_4
CL	D14-1200	ab	4	1	result	proposal	support	none	secondary	main	forw	support	The experimental results demonstrate that a joint learning approach significantly outperforms a pipeline approach by incorporating global features and by selecting appropriate learning methods and search orders .	This paper proposes a history-based structured learning approach that jointly extracts entities and relations in a sentence .	CL_D14-1200_ab_4	CL_D14-1200_ab_1	CL_D14-1200_ab_1_4
CL	D14-1200	ab	2	3	proposal	proposal	elaboration	elaboration	secondary	secondary	back	elaboration	We introduce a novel simple and flexible table representation of entities and relations .	We investigate several feature settings , search orders , and learning methods with inexact search on the table .	CL_D14-1200_ab_2	CL_D14-1200_ab_3	CL_D14-1200_ab_2_3
CL	D14-1200	ab	3	2	proposal	proposal	elaboration	elaboration	secondary	secondary	forw	elaboration	We investigate several feature settings , search orders , and learning methods with inexact search on the table .	We introduce a novel simple and flexible table representation of entities and relations .	CL_D14-1200_ab_3	CL_D14-1200_ab_2	CL_D14-1200_ab_2_3
CL	D14-1200	ab	2	4	proposal	result	elaboration	support	secondary	secondary	none	none	We introduce a novel simple and flexible table representation of entities and relations .	The experimental results demonstrate that a joint learning approach significantly outperforms a pipeline approach by incorporating global features and by selecting appropriate learning methods and search orders .	CL_D14-1200_ab_2	CL_D14-1200_ab_4	CL_D14-1200_ab_2_4
CL	D14-1200	ab	3	4	proposal	result	elaboration	support	secondary	secondary	none	none	We investigate several feature settings , search orders , and learning methods with inexact search on the table .	The experimental results demonstrate that a joint learning approach significantly outperforms a pipeline approach by incorporating global features and by selecting appropriate learning methods and search orders .	CL_D14-1200_ab_3	CL_D14-1200_ab_4	CL_D14-1200_ab_3_4
CL	D14-1201	ab	1	2	motivation_background	information_additional	info-required	info-optional	secondary	secondary	back	info-optional	Open Relation Extraction ( ORE ) overcomes the limitations of traditional IE techniques , which train individual extractors for every single relation type .	Systems such as ReVerb , PATTY , OLLIE , and Exemplar have attracted much attention on English ORE .	CL_D14-1201_ab_1	CL_D14-1201_ab_2	CL_D14-1201_ab_1_2
CL	D14-1201	ab	2	1	information_additional	motivation_background	info-optional	info-required	secondary	secondary	forw	info-optional	Systems such as ReVerb , PATTY , OLLIE , and Exemplar have attracted much attention on English ORE .	Open Relation Extraction ( ORE ) overcomes the limitations of traditional IE techniques , which train individual extractors for every single relation type .	CL_D14-1201_ab_2	CL_D14-1201_ab_1	CL_D14-1201_ab_1_2
CL	D14-1201	ab	1	3	motivation_background	motivation_problem	info-required	support	secondary	secondary	forw	info-required	Open Relation Extraction ( ORE ) overcomes the limitations of traditional IE techniques , which train individual extractors for every single relation type .	However , few studies have been reported on ORE for languages beyond English .	CL_D14-1201_ab_1	CL_D14-1201_ab_3	CL_D14-1201_ab_1_3
CL	D14-1201	ab	3	1	motivation_problem	motivation_background	support	info-required	secondary	secondary	back	info-required	However , few studies have been reported on ORE for languages beyond English .	Open Relation Extraction ( ORE ) overcomes the limitations of traditional IE techniques , which train individual extractors for every single relation type .	CL_D14-1201_ab_3	CL_D14-1201_ab_1	CL_D14-1201_ab_1_3
CL	D14-1201	ab	1	4	motivation_background	proposal	info-required	none	secondary	main	none	none	Open Relation Extraction ( ORE ) overcomes the limitations of traditional IE techniques , which train individual extractors for every single relation type .	This paper presents a syntax-based Chinese ( Zh ) ORE system , ZORE , for extracting relations and semantic patterns from Chinese text .	CL_D14-1201_ab_1	CL_D14-1201_ab_4	CL_D14-1201_ab_1_4
CL	D14-1201	ab	1	5	motivation_background	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	Open Relation Extraction ( ORE ) overcomes the limitations of traditional IE techniques , which train individual extractors for every single relation type .	ZORE identifies relation candidates from automatically parsed dependency trees , and then extracts relations with their semantic patterns iteratively through a novel double propagation algorithm .	CL_D14-1201_ab_1	CL_D14-1201_ab_5	CL_D14-1201_ab_1_5
CL	D14-1201	ab	1	6	motivation_background	result	info-required	support	secondary	secondary	none	none	Open Relation Extraction ( ORE ) overcomes the limitations of traditional IE techniques , which train individual extractors for every single relation type .	Empirical results on two data sets show the effectiveness of the proposed system .	CL_D14-1201_ab_1	CL_D14-1201_ab_6	CL_D14-1201_ab_1_6
CL	D14-1201	ab	2	3	information_additional	motivation_problem	info-optional	support	secondary	secondary	none	none	Systems such as ReVerb , PATTY , OLLIE , and Exemplar have attracted much attention on English ORE .	However , few studies have been reported on ORE for languages beyond English .	CL_D14-1201_ab_2	CL_D14-1201_ab_3	CL_D14-1201_ab_2_3
CL	D14-1201	ab	2	4	information_additional	proposal	info-optional	none	secondary	main	none	none	Systems such as ReVerb , PATTY , OLLIE , and Exemplar have attracted much attention on English ORE .	This paper presents a syntax-based Chinese ( Zh ) ORE system , ZORE , for extracting relations and semantic patterns from Chinese text .	CL_D14-1201_ab_2	CL_D14-1201_ab_4	CL_D14-1201_ab_2_4
CL	D14-1201	ab	2	5	information_additional	proposal_implementation	info-optional	elaboration	secondary	secondary	none	none	Systems such as ReVerb , PATTY , OLLIE , and Exemplar have attracted much attention on English ORE .	ZORE identifies relation candidates from automatically parsed dependency trees , and then extracts relations with their semantic patterns iteratively through a novel double propagation algorithm .	CL_D14-1201_ab_2	CL_D14-1201_ab_5	CL_D14-1201_ab_2_5
CL	D14-1201	ab	2	6	information_additional	result	info-optional	support	secondary	secondary	none	none	Systems such as ReVerb , PATTY , OLLIE , and Exemplar have attracted much attention on English ORE .	Empirical results on two data sets show the effectiveness of the proposed system .	CL_D14-1201_ab_2	CL_D14-1201_ab_6	CL_D14-1201_ab_2_6
CL	D14-1201	ab	3	4	motivation_problem	proposal	support	none	secondary	main	forw	support	However , few studies have been reported on ORE for languages beyond English .	This paper presents a syntax-based Chinese ( Zh ) ORE system , ZORE , for extracting relations and semantic patterns from Chinese text .	CL_D14-1201_ab_3	CL_D14-1201_ab_4	CL_D14-1201_ab_3_4
CL	D14-1201	ab	4	3	proposal	motivation_problem	none	support	main	secondary	back	support	This paper presents a syntax-based Chinese ( Zh ) ORE system , ZORE , for extracting relations and semantic patterns from Chinese text .	However , few studies have been reported on ORE for languages beyond English .	CL_D14-1201_ab_4	CL_D14-1201_ab_3	CL_D14-1201_ab_3_4
CL	D14-1201	ab	3	5	motivation_problem	proposal_implementation	support	elaboration	secondary	secondary	none	none	However , few studies have been reported on ORE for languages beyond English .	ZORE identifies relation candidates from automatically parsed dependency trees , and then extracts relations with their semantic patterns iteratively through a novel double propagation algorithm .	CL_D14-1201_ab_3	CL_D14-1201_ab_5	CL_D14-1201_ab_3_5
CL	D14-1201	ab	3	6	motivation_problem	result	support	support	secondary	secondary	none	none	However , few studies have been reported on ORE for languages beyond English .	Empirical results on two data sets show the effectiveness of the proposed system .	CL_D14-1201_ab_3	CL_D14-1201_ab_6	CL_D14-1201_ab_3_6
CL	D14-1201	ab	4	5	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	This paper presents a syntax-based Chinese ( Zh ) ORE system , ZORE , for extracting relations and semantic patterns from Chinese text .	ZORE identifies relation candidates from automatically parsed dependency trees , and then extracts relations with their semantic patterns iteratively through a novel double propagation algorithm .	CL_D14-1201_ab_4	CL_D14-1201_ab_5	CL_D14-1201_ab_4_5
CL	D14-1201	ab	5	4	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	ZORE identifies relation candidates from automatically parsed dependency trees , and then extracts relations with their semantic patterns iteratively through a novel double propagation algorithm .	This paper presents a syntax-based Chinese ( Zh ) ORE system , ZORE , for extracting relations and semantic patterns from Chinese text .	CL_D14-1201_ab_5	CL_D14-1201_ab_4	CL_D14-1201_ab_4_5
CL	D14-1201	ab	4	6	proposal	result	none	support	main	secondary	back	support	This paper presents a syntax-based Chinese ( Zh ) ORE system , ZORE , for extracting relations and semantic patterns from Chinese text .	Empirical results on two data sets show the effectiveness of the proposed system .	CL_D14-1201_ab_4	CL_D14-1201_ab_6	CL_D14-1201_ab_4_6
CL	D14-1201	ab	6	4	result	proposal	support	none	secondary	main	forw	support	Empirical results on two data sets show the effectiveness of the proposed system .	This paper presents a syntax-based Chinese ( Zh ) ORE system , ZORE , for extracting relations and semantic patterns from Chinese text .	CL_D14-1201_ab_6	CL_D14-1201_ab_4	CL_D14-1201_ab_4_6
CL	D14-1201	ab	5	6	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	ZORE identifies relation candidates from automatically parsed dependency trees , and then extracts relations with their semantic patterns iteratively through a novel double propagation algorithm .	Empirical results on two data sets show the effectiveness of the proposed system .	CL_D14-1201_ab_5	CL_D14-1201_ab_6	CL_D14-1201_ab_5_6
CL	D14-1202	ab	1	2	motivation_background	proposal	support	none	secondary	main	forw	support	Correctly predicting abbreviations given the full forms is important in many natural language processing systems .	In this paper we propose a two-stage method to find the corresponding abbreviation given its full form .	CL_D14-1202_ab_1	CL_D14-1202_ab_2	CL_D14-1202_ab_1_2
CL	D14-1202	ab	2	1	proposal	motivation_background	none	support	main	secondary	back	support	In this paper we propose a two-stage method to find the corresponding abbreviation given its full form .	Correctly predicting abbreviations given the full forms is important in many natural language processing systems .	CL_D14-1202_ab_2	CL_D14-1202_ab_1	CL_D14-1202_ab_1_2
CL	D14-1202	ab	1	3	motivation_background	proposal_implementation	support	elaboration	secondary	secondary	none	none	Correctly predicting abbreviations given the full forms is important in many natural language processing systems .	We first use the contextual information given a large corpus to get abbreviation candidates for each full form and get a coarse-grained ranking through graph random walk .	CL_D14-1202_ab_1	CL_D14-1202_ab_3	CL_D14-1202_ab_1_3
CL	D14-1202	ab	1	4	motivation_background	proposal_implementation	support	sequence	secondary	secondary	none	none	Correctly predicting abbreviations given the full forms is important in many natural language processing systems .	This coarse-grained rank list fixes the search space inside the top-ranked candidates .	CL_D14-1202_ab_1	CL_D14-1202_ab_4	CL_D14-1202_ab_1_4
CL	D14-1202	ab	1	5	motivation_background	proposal_implementation	support	sequence	secondary	secondary	none	none	Correctly predicting abbreviations given the full forms is important in many natural language processing systems .	Then we use a similarity sensitive re-ranking strategy which can utilize the features of the candidates to give a fine-grained re-ranking and select the final result .	CL_D14-1202_ab_1	CL_D14-1202_ab_5	CL_D14-1202_ab_1_5
CL	D14-1202	ab	1	6	motivation_background	result	support	support	secondary	secondary	none	none	Correctly predicting abbreviations given the full forms is important in many natural language processing systems .	Our method achieves good results and outperforms the state-of-the-art systems .	CL_D14-1202_ab_1	CL_D14-1202_ab_6	CL_D14-1202_ab_1_6
CL	D14-1202	ab	1	7	motivation_background	conclusion	support	support	secondary	secondary	none	none	Correctly predicting abbreviations given the full forms is important in many natural language processing systems .	One advantage of our method is that it only needs weak supervision and can get competitive results with fewer training data .	CL_D14-1202_ab_1	CL_D14-1202_ab_7	CL_D14-1202_ab_1_7
CL	D14-1202	ab	1	8	motivation_background	conclusion	support	elaboration	secondary	secondary	none	none	Correctly predicting abbreviations given the full forms is important in many natural language processing systems .	The candidate generation and coarse-grained ranking is totally unsupervised .	CL_D14-1202_ab_1	CL_D14-1202_ab_8	CL_D14-1202_ab_1_8
CL	D14-1202	ab	1	9	motivation_background	conclusion	support	elaboration	secondary	secondary	none	none	Correctly predicting abbreviations given the full forms is important in many natural language processing systems .	The re-ranking phase can use a very small amount of training data to get a reasonably good result .	CL_D14-1202_ab_1	CL_D14-1202_ab_9	CL_D14-1202_ab_1_9
CL	D14-1202	ab	2	3	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	In this paper we propose a two-stage method to find the corresponding abbreviation given its full form .	We first use the contextual information given a large corpus to get abbreviation candidates for each full form and get a coarse-grained ranking through graph random walk .	CL_D14-1202_ab_2	CL_D14-1202_ab_3	CL_D14-1202_ab_2_3
CL	D14-1202	ab	3	2	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	We first use the contextual information given a large corpus to get abbreviation candidates for each full form and get a coarse-grained ranking through graph random walk .	In this paper we propose a two-stage method to find the corresponding abbreviation given its full form .	CL_D14-1202_ab_3	CL_D14-1202_ab_2	CL_D14-1202_ab_2_3
CL	D14-1202	ab	2	4	proposal	proposal_implementation	none	sequence	main	secondary	none	none	In this paper we propose a two-stage method to find the corresponding abbreviation given its full form .	This coarse-grained rank list fixes the search space inside the top-ranked candidates .	CL_D14-1202_ab_2	CL_D14-1202_ab_4	CL_D14-1202_ab_2_4
CL	D14-1202	ab	2	5	proposal	proposal_implementation	none	sequence	main	secondary	none	none	In this paper we propose a two-stage method to find the corresponding abbreviation given its full form .	Then we use a similarity sensitive re-ranking strategy which can utilize the features of the candidates to give a fine-grained re-ranking and select the final result .	CL_D14-1202_ab_2	CL_D14-1202_ab_5	CL_D14-1202_ab_2_5
CL	D14-1202	ab	2	6	proposal	result	none	support	main	secondary	back	support	In this paper we propose a two-stage method to find the corresponding abbreviation given its full form .	Our method achieves good results and outperforms the state-of-the-art systems .	CL_D14-1202_ab_2	CL_D14-1202_ab_6	CL_D14-1202_ab_2_6
CL	D14-1202	ab	6	2	result	proposal	support	none	secondary	main	forw	support	Our method achieves good results and outperforms the state-of-the-art systems .	In this paper we propose a two-stage method to find the corresponding abbreviation given its full form .	CL_D14-1202_ab_6	CL_D14-1202_ab_2	CL_D14-1202_ab_2_6
CL	D14-1202	ab	2	7	proposal	conclusion	none	support	main	secondary	back	support	In this paper we propose a two-stage method to find the corresponding abbreviation given its full form .	One advantage of our method is that it only needs weak supervision and can get competitive results with fewer training data .	CL_D14-1202_ab_2	CL_D14-1202_ab_7	CL_D14-1202_ab_2_7
CL	D14-1202	ab	7	2	conclusion	proposal	support	none	secondary	main	forw	support	One advantage of our method is that it only needs weak supervision and can get competitive results with fewer training data .	In this paper we propose a two-stage method to find the corresponding abbreviation given its full form .	CL_D14-1202_ab_7	CL_D14-1202_ab_2	CL_D14-1202_ab_2_7
CL	D14-1202	ab	2	8	proposal	conclusion	none	elaboration	main	secondary	none	none	In this paper we propose a two-stage method to find the corresponding abbreviation given its full form .	The candidate generation and coarse-grained ranking is totally unsupervised .	CL_D14-1202_ab_2	CL_D14-1202_ab_8	CL_D14-1202_ab_2_8
CL	D14-1202	ab	2	9	proposal	conclusion	none	elaboration	main	secondary	none	none	In this paper we propose a two-stage method to find the corresponding abbreviation given its full form .	The re-ranking phase can use a very small amount of training data to get a reasonably good result .	CL_D14-1202_ab_2	CL_D14-1202_ab_9	CL_D14-1202_ab_2_9
CL	D14-1202	ab	3	4	proposal_implementation	proposal_implementation	elaboration	sequence	secondary	secondary	back	sequence	We first use the contextual information given a large corpus to get abbreviation candidates for each full form and get a coarse-grained ranking through graph random walk .	This coarse-grained rank list fixes the search space inside the top-ranked candidates .	CL_D14-1202_ab_3	CL_D14-1202_ab_4	CL_D14-1202_ab_3_4
CL	D14-1202	ab	4	3	proposal_implementation	proposal_implementation	sequence	elaboration	secondary	secondary	forw	sequence	This coarse-grained rank list fixes the search space inside the top-ranked candidates .	We first use the contextual information given a large corpus to get abbreviation candidates for each full form and get a coarse-grained ranking through graph random walk .	CL_D14-1202_ab_4	CL_D14-1202_ab_3	CL_D14-1202_ab_3_4
CL	D14-1202	ab	3	5	proposal_implementation	proposal_implementation	elaboration	sequence	secondary	secondary	none	none	We first use the contextual information given a large corpus to get abbreviation candidates for each full form and get a coarse-grained ranking through graph random walk .	Then we use a similarity sensitive re-ranking strategy which can utilize the features of the candidates to give a fine-grained re-ranking and select the final result .	CL_D14-1202_ab_3	CL_D14-1202_ab_5	CL_D14-1202_ab_3_5
CL	D14-1202	ab	3	6	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	We first use the contextual information given a large corpus to get abbreviation candidates for each full form and get a coarse-grained ranking through graph random walk .	Our method achieves good results and outperforms the state-of-the-art systems .	CL_D14-1202_ab_3	CL_D14-1202_ab_6	CL_D14-1202_ab_3_6
CL	D14-1202	ab	3	7	proposal_implementation	conclusion	elaboration	support	secondary	secondary	none	none	We first use the contextual information given a large corpus to get abbreviation candidates for each full form and get a coarse-grained ranking through graph random walk .	One advantage of our method is that it only needs weak supervision and can get competitive results with fewer training data .	CL_D14-1202_ab_3	CL_D14-1202_ab_7	CL_D14-1202_ab_3_7
CL	D14-1202	ab	3	8	proposal_implementation	conclusion	elaboration	elaboration	secondary	secondary	none	none	We first use the contextual information given a large corpus to get abbreviation candidates for each full form and get a coarse-grained ranking through graph random walk .	The candidate generation and coarse-grained ranking is totally unsupervised .	CL_D14-1202_ab_3	CL_D14-1202_ab_8	CL_D14-1202_ab_3_8
CL	D14-1202	ab	3	9	proposal_implementation	conclusion	elaboration	elaboration	secondary	secondary	none	none	We first use the contextual information given a large corpus to get abbreviation candidates for each full form and get a coarse-grained ranking through graph random walk .	The re-ranking phase can use a very small amount of training data to get a reasonably good result .	CL_D14-1202_ab_3	CL_D14-1202_ab_9	CL_D14-1202_ab_3_9
CL	D14-1202	ab	4	5	proposal_implementation	proposal_implementation	sequence	sequence	secondary	secondary	back	sequence	This coarse-grained rank list fixes the search space inside the top-ranked candidates .	Then we use a similarity sensitive re-ranking strategy which can utilize the features of the candidates to give a fine-grained re-ranking and select the final result .	CL_D14-1202_ab_4	CL_D14-1202_ab_5	CL_D14-1202_ab_4_5
CL	D14-1202	ab	5	4	proposal_implementation	proposal_implementation	sequence	sequence	secondary	secondary	forw	sequence	Then we use a similarity sensitive re-ranking strategy which can utilize the features of the candidates to give a fine-grained re-ranking and select the final result .	This coarse-grained rank list fixes the search space inside the top-ranked candidates .	CL_D14-1202_ab_5	CL_D14-1202_ab_4	CL_D14-1202_ab_4_5
CL	D14-1202	ab	4	6	proposal_implementation	result	sequence	support	secondary	secondary	none	none	This coarse-grained rank list fixes the search space inside the top-ranked candidates .	Our method achieves good results and outperforms the state-of-the-art systems .	CL_D14-1202_ab_4	CL_D14-1202_ab_6	CL_D14-1202_ab_4_6
CL	D14-1202	ab	4	7	proposal_implementation	conclusion	sequence	support	secondary	secondary	none	none	This coarse-grained rank list fixes the search space inside the top-ranked candidates .	One advantage of our method is that it only needs weak supervision and can get competitive results with fewer training data .	CL_D14-1202_ab_4	CL_D14-1202_ab_7	CL_D14-1202_ab_4_7
CL	D14-1202	ab	4	8	proposal_implementation	conclusion	sequence	elaboration	secondary	secondary	none	none	This coarse-grained rank list fixes the search space inside the top-ranked candidates .	The candidate generation and coarse-grained ranking is totally unsupervised .	CL_D14-1202_ab_4	CL_D14-1202_ab_8	CL_D14-1202_ab_4_8
CL	D14-1202	ab	4	9	proposal_implementation	conclusion	sequence	elaboration	secondary	secondary	none	none	This coarse-grained rank list fixes the search space inside the top-ranked candidates .	The re-ranking phase can use a very small amount of training data to get a reasonably good result .	CL_D14-1202_ab_4	CL_D14-1202_ab_9	CL_D14-1202_ab_4_9
CL	D14-1202	ab	5	6	proposal_implementation	result	sequence	support	secondary	secondary	none	none	Then we use a similarity sensitive re-ranking strategy which can utilize the features of the candidates to give a fine-grained re-ranking and select the final result .	Our method achieves good results and outperforms the state-of-the-art systems .	CL_D14-1202_ab_5	CL_D14-1202_ab_6	CL_D14-1202_ab_5_6
CL	D14-1202	ab	5	7	proposal_implementation	conclusion	sequence	support	secondary	secondary	none	none	Then we use a similarity sensitive re-ranking strategy which can utilize the features of the candidates to give a fine-grained re-ranking and select the final result .	One advantage of our method is that it only needs weak supervision and can get competitive results with fewer training data .	CL_D14-1202_ab_5	CL_D14-1202_ab_7	CL_D14-1202_ab_5_7
CL	D14-1202	ab	5	8	proposal_implementation	conclusion	sequence	elaboration	secondary	secondary	none	none	Then we use a similarity sensitive re-ranking strategy which can utilize the features of the candidates to give a fine-grained re-ranking and select the final result .	The candidate generation and coarse-grained ranking is totally unsupervised .	CL_D14-1202_ab_5	CL_D14-1202_ab_8	CL_D14-1202_ab_5_8
CL	D14-1202	ab	5	9	proposal_implementation	conclusion	sequence	elaboration	secondary	secondary	none	none	Then we use a similarity sensitive re-ranking strategy which can utilize the features of the candidates to give a fine-grained re-ranking and select the final result .	The re-ranking phase can use a very small amount of training data to get a reasonably good result .	CL_D14-1202_ab_5	CL_D14-1202_ab_9	CL_D14-1202_ab_5_9
CL	D14-1202	ab	6	7	result	conclusion	support	support	secondary	secondary	none	none	Our method achieves good results and outperforms the state-of-the-art systems .	One advantage of our method is that it only needs weak supervision and can get competitive results with fewer training data .	CL_D14-1202_ab_6	CL_D14-1202_ab_7	CL_D14-1202_ab_6_7
CL	D14-1202	ab	6	8	result	conclusion	support	elaboration	secondary	secondary	none	none	Our method achieves good results and outperforms the state-of-the-art systems .	The candidate generation and coarse-grained ranking is totally unsupervised .	CL_D14-1202_ab_6	CL_D14-1202_ab_8	CL_D14-1202_ab_6_8
CL	D14-1202	ab	6	9	result	conclusion	support	elaboration	secondary	secondary	none	none	Our method achieves good results and outperforms the state-of-the-art systems .	The re-ranking phase can use a very small amount of training data to get a reasonably good result .	CL_D14-1202_ab_6	CL_D14-1202_ab_9	CL_D14-1202_ab_6_9
CL	D14-1202	ab	7	8	conclusion	conclusion	support	elaboration	secondary	secondary	back	elaboration	One advantage of our method is that it only needs weak supervision and can get competitive results with fewer training data .	The candidate generation and coarse-grained ranking is totally unsupervised .	CL_D14-1202_ab_7	CL_D14-1202_ab_8	CL_D14-1202_ab_7_8
CL	D14-1202	ab	8	7	conclusion	conclusion	elaboration	support	secondary	secondary	forw	elaboration	The candidate generation and coarse-grained ranking is totally unsupervised .	One advantage of our method is that it only needs weak supervision and can get competitive results with fewer training data .	CL_D14-1202_ab_8	CL_D14-1202_ab_7	CL_D14-1202_ab_7_8
CL	D14-1202	ab	7	9	conclusion	conclusion	support	elaboration	secondary	secondary	none	none	One advantage of our method is that it only needs weak supervision and can get competitive results with fewer training data .	The re-ranking phase can use a very small amount of training data to get a reasonably good result .	CL_D14-1202_ab_7	CL_D14-1202_ab_9	CL_D14-1202_ab_7_9
CL	D14-1202	ab	8	9	conclusion	conclusion	elaboration	elaboration	secondary	secondary	back	elaboration	The candidate generation and coarse-grained ranking is totally unsupervised .	The re-ranking phase can use a very small amount of training data to get a reasonably good result .	CL_D14-1202_ab_8	CL_D14-1202_ab_9	CL_D14-1202_ab_8_9
CL	D14-1202	ab	9	8	conclusion	conclusion	elaboration	elaboration	secondary	secondary	forw	elaboration	The re-ranking phase can use a very small amount of training data to get a reasonably good result .	The candidate generation and coarse-grained ranking is totally unsupervised .	CL_D14-1202_ab_9	CL_D14-1202_ab_8	CL_D14-1202_ab_8_9
CL	D14-1203	ab	1	2	motivation_background	motivation_problem	info-required	support	secondary	secondary	forw	info-required	Distant supervision has become the leading method for training large-scale relation extractors , with nearly universal adoption in recent TAC knowledge-base population competitions .	However , there are still many questions about the best way to learn such extractors .	CL_D14-1203_ab_1	CL_D14-1203_ab_2	CL_D14-1203_ab_1_2
CL	D14-1203	ab	2	1	motivation_problem	motivation_background	support	info-required	secondary	secondary	back	info-required	However , there are still many questions about the best way to learn such extractors .	Distant supervision has become the leading method for training large-scale relation extractors , with nearly universal adoption in recent TAC knowledge-base population competitions .	CL_D14-1203_ab_2	CL_D14-1203_ab_1	CL_D14-1203_ab_1_2
CL	D14-1203	ab	1	3	motivation_background	proposal	info-required	none	secondary	main	none	none	Distant supervision has become the leading method for training large-scale relation extractors , with nearly universal adoption in recent TAC knowledge-base population competitions .	In this paper we investigate four orthogonal improvements : integrating named entity linking ( NEL ) and coreference resolution into argument identification for training and extraction , enforcing type constraints of linked arguments , and partitioning the model by relation type signature .	CL_D14-1203_ab_1	CL_D14-1203_ab_3	CL_D14-1203_ab_1_3
CL	D14-1203	ab	1	4	motivation_background	means	info-required	by-means	secondary	secondary	none	none	Distant supervision has become the leading method for training large-scale relation extractors , with nearly universal adoption in recent TAC knowledge-base population competitions .	We evaluate sentential extraction performance on two datasets : the popular set of NY Times articles partially annotated by Hoffmann et al. ( 2011 ) and a new dataset , called GORECO , that is comprehensively annotated for 48 common relations .	CL_D14-1203_ab_1	CL_D14-1203_ab_4	CL_D14-1203_ab_1_4
CL	D14-1203	ab	1	5	motivation_background	result	info-required	support	secondary	secondary	none	none	Distant supervision has become the leading method for training large-scale relation extractors , with nearly universal adoption in recent TAC knowledge-base population competitions .	We find that using NEL for argument identification boosts performance over the traditional approach ( named entity recognition with string match ) , and there is further improvement from using argument types .	CL_D14-1203_ab_1	CL_D14-1203_ab_5	CL_D14-1203_ab_1_5
CL	D14-1203	ab	1	6	motivation_background	observation	info-required	support	secondary	secondary	none	none	Distant supervision has become the leading method for training large-scale relation extractors , with nearly universal adoption in recent TAC knowledge-base population competitions .	Our best system boosts precision by 44 % and recall by 70 % .	CL_D14-1203_ab_1	CL_D14-1203_ab_6	CL_D14-1203_ab_1_6
CL	D14-1203	ab	2	3	motivation_problem	proposal	support	none	secondary	main	forw	support	However , there are still many questions about the best way to learn such extractors .	In this paper we investigate four orthogonal improvements : integrating named entity linking ( NEL ) and coreference resolution into argument identification for training and extraction , enforcing type constraints of linked arguments , and partitioning the model by relation type signature .	CL_D14-1203_ab_2	CL_D14-1203_ab_3	CL_D14-1203_ab_2_3
CL	D14-1203	ab	3	2	proposal	motivation_problem	none	support	main	secondary	back	support	In this paper we investigate four orthogonal improvements : integrating named entity linking ( NEL ) and coreference resolution into argument identification for training and extraction , enforcing type constraints of linked arguments , and partitioning the model by relation type signature .	However , there are still many questions about the best way to learn such extractors .	CL_D14-1203_ab_3	CL_D14-1203_ab_2	CL_D14-1203_ab_2_3
CL	D14-1203	ab	2	4	motivation_problem	means	support	by-means	secondary	secondary	none	none	However , there are still many questions about the best way to learn such extractors .	We evaluate sentential extraction performance on two datasets : the popular set of NY Times articles partially annotated by Hoffmann et al. ( 2011 ) and a new dataset , called GORECO , that is comprehensively annotated for 48 common relations .	CL_D14-1203_ab_2	CL_D14-1203_ab_4	CL_D14-1203_ab_2_4
CL	D14-1203	ab	2	5	motivation_problem	result	support	support	secondary	secondary	none	none	However , there are still many questions about the best way to learn such extractors .	We find that using NEL for argument identification boosts performance over the traditional approach ( named entity recognition with string match ) , and there is further improvement from using argument types .	CL_D14-1203_ab_2	CL_D14-1203_ab_5	CL_D14-1203_ab_2_5
CL	D14-1203	ab	2	6	motivation_problem	observation	support	support	secondary	secondary	none	none	However , there are still many questions about the best way to learn such extractors .	Our best system boosts precision by 44 % and recall by 70 % .	CL_D14-1203_ab_2	CL_D14-1203_ab_6	CL_D14-1203_ab_2_6
CL	D14-1203	ab	3	4	proposal	means	none	by-means	main	secondary	none	none	In this paper we investigate four orthogonal improvements : integrating named entity linking ( NEL ) and coreference resolution into argument identification for training and extraction , enforcing type constraints of linked arguments , and partitioning the model by relation type signature .	We evaluate sentential extraction performance on two datasets : the popular set of NY Times articles partially annotated by Hoffmann et al. ( 2011 ) and a new dataset , called GORECO , that is comprehensively annotated for 48 common relations .	CL_D14-1203_ab_3	CL_D14-1203_ab_4	CL_D14-1203_ab_3_4
CL	D14-1203	ab	3	5	proposal	result	none	support	main	secondary	back	support	In this paper we investigate four orthogonal improvements : integrating named entity linking ( NEL ) and coreference resolution into argument identification for training and extraction , enforcing type constraints of linked arguments , and partitioning the model by relation type signature .	We find that using NEL for argument identification boosts performance over the traditional approach ( named entity recognition with string match ) , and there is further improvement from using argument types .	CL_D14-1203_ab_3	CL_D14-1203_ab_5	CL_D14-1203_ab_3_5
CL	D14-1203	ab	5	3	result	proposal	support	none	secondary	main	forw	support	We find that using NEL for argument identification boosts performance over the traditional approach ( named entity recognition with string match ) , and there is further improvement from using argument types .	In this paper we investigate four orthogonal improvements : integrating named entity linking ( NEL ) and coreference resolution into argument identification for training and extraction , enforcing type constraints of linked arguments , and partitioning the model by relation type signature .	CL_D14-1203_ab_5	CL_D14-1203_ab_3	CL_D14-1203_ab_3_5
CL	D14-1203	ab	3	6	proposal	observation	none	support	main	secondary	none	none	In this paper we investigate four orthogonal improvements : integrating named entity linking ( NEL ) and coreference resolution into argument identification for training and extraction , enforcing type constraints of linked arguments , and partitioning the model by relation type signature .	Our best system boosts precision by 44 % and recall by 70 % .	CL_D14-1203_ab_3	CL_D14-1203_ab_6	CL_D14-1203_ab_3_6
CL	D14-1203	ab	4	5	means	result	by-means	support	secondary	secondary	forw	by-means	We evaluate sentential extraction performance on two datasets : the popular set of NY Times articles partially annotated by Hoffmann et al. ( 2011 ) and a new dataset , called GORECO , that is comprehensively annotated for 48 common relations .	We find that using NEL for argument identification boosts performance over the traditional approach ( named entity recognition with string match ) , and there is further improvement from using argument types .	CL_D14-1203_ab_4	CL_D14-1203_ab_5	CL_D14-1203_ab_4_5
CL	D14-1203	ab	5	4	result	means	support	by-means	secondary	secondary	back	by-means	We find that using NEL for argument identification boosts performance over the traditional approach ( named entity recognition with string match ) , and there is further improvement from using argument types .	We evaluate sentential extraction performance on two datasets : the popular set of NY Times articles partially annotated by Hoffmann et al. ( 2011 ) and a new dataset , called GORECO , that is comprehensively annotated for 48 common relations .	CL_D14-1203_ab_5	CL_D14-1203_ab_4	CL_D14-1203_ab_4_5
CL	D14-1203	ab	4	6	means	observation	by-means	support	secondary	secondary	none	none	We evaluate sentential extraction performance on two datasets : the popular set of NY Times articles partially annotated by Hoffmann et al. ( 2011 ) and a new dataset , called GORECO , that is comprehensively annotated for 48 common relations .	Our best system boosts precision by 44 % and recall by 70 % .	CL_D14-1203_ab_4	CL_D14-1203_ab_6	CL_D14-1203_ab_4_6
CL	D14-1203	ab	5	6	result	observation	support	support	secondary	secondary	back	support	We find that using NEL for argument identification boosts performance over the traditional approach ( named entity recognition with string match ) , and there is further improvement from using argument types .	Our best system boosts precision by 44 % and recall by 70 % .	CL_D14-1203_ab_5	CL_D14-1203_ab_6	CL_D14-1203_ab_5_6
CL	D14-1203	ab	6	5	observation	result	support	support	secondary	secondary	forw	support	Our best system boosts precision by 44 % and recall by 70 % .	We find that using NEL for argument identification boosts performance over the traditional approach ( named entity recognition with string match ) , and there is further improvement from using argument types .	CL_D14-1203_ab_6	CL_D14-1203_ab_5	CL_D14-1203_ab_5_6
CL	D14-1204	ab	1	2	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	We address the problem of automatically inferring the tense of events in Chinese text .	We use a new corpus annotated with Chinese semantic tense information and other implicit Chinese linguistic information using a "distant annotation" method .	CL_D14-1204_ab_1	CL_D14-1204_ab_2	CL_D14-1204_ab_1_2
CL	D14-1204	ab	2	1	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	We use a new corpus annotated with Chinese semantic tense information and other implicit Chinese linguistic information using a "distant annotation" method .	We address the problem of automatically inferring the tense of events in Chinese text .	CL_D14-1204_ab_2	CL_D14-1204_ab_1	CL_D14-1204_ab_1_2
CL	D14-1204	ab	1	3	proposal	proposal	none	elaboration	main	secondary	none	none	We address the problem of automatically inferring the tense of events in Chinese text .	We propose three improvements over a relatively strong baseline method - a statistical learning method with extensive feature engineering .	CL_D14-1204_ab_1	CL_D14-1204_ab_3	CL_D14-1204_ab_1_3
CL	D14-1204	ab	1	4	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	We address the problem of automatically inferring the tense of events in Chinese text .	First , we add two sources of implicit linguistic information as features - eventuality type and modality of an event , which are also inferred automatically .	CL_D14-1204_ab_1	CL_D14-1204_ab_4	CL_D14-1204_ab_1_4
CL	D14-1204	ab	1	5	proposal	proposal_implementation	none	sequence	main	secondary	none	none	We address the problem of automatically inferring the tense of events in Chinese text .	Second , we perform joint learning on semantic tense , eventuality type , and modality of an event .	CL_D14-1204_ab_1	CL_D14-1204_ab_5	CL_D14-1204_ab_1_5
CL	D14-1204	ab	1	6	proposal	proposal_implementation	none	sequence	main	secondary	none	none	We address the problem of automatically inferring the tense of events in Chinese text .	Third , we train artificial neural network models for this problem and compare its performance with feature-based approaches .	CL_D14-1204_ab_1	CL_D14-1204_ab_6	CL_D14-1204_ab_1_6
CL	D14-1204	ab	1	7	proposal	result	none	support	main	secondary	back	support	We address the problem of automatically inferring the tense of events in Chinese text .	Experimental results show considerable improvements on Chinese tense inference .	CL_D14-1204_ab_1	CL_D14-1204_ab_7	CL_D14-1204_ab_1_7
CL	D14-1204	ab	7	1	result	proposal	support	none	secondary	main	forw	support	Experimental results show considerable improvements on Chinese tense inference .	We address the problem of automatically inferring the tense of events in Chinese text .	CL_D14-1204_ab_7	CL_D14-1204_ab_1	CL_D14-1204_ab_1_7
CL	D14-1204	ab	1	8	proposal	result	none	elaboration	main	secondary	none	none	We address the problem of automatically inferring the tense of events in Chinese text .	Our best performance reaches 68.6 % in accuracy , outperforming a strong baseline method .	CL_D14-1204_ab_1	CL_D14-1204_ab_8	CL_D14-1204_ab_1_8
CL	D14-1204	ab	2	3	proposal_implementation	proposal	elaboration	elaboration	secondary	secondary	back	elaboration	We use a new corpus annotated with Chinese semantic tense information and other implicit Chinese linguistic information using a "distant annotation" method .	We propose three improvements over a relatively strong baseline method - a statistical learning method with extensive feature engineering .	CL_D14-1204_ab_2	CL_D14-1204_ab_3	CL_D14-1204_ab_2_3
CL	D14-1204	ab	3	2	proposal	proposal_implementation	elaboration	elaboration	secondary	secondary	forw	elaboration	We propose three improvements over a relatively strong baseline method - a statistical learning method with extensive feature engineering .	We use a new corpus annotated with Chinese semantic tense information and other implicit Chinese linguistic information using a "distant annotation" method .	CL_D14-1204_ab_3	CL_D14-1204_ab_2	CL_D14-1204_ab_2_3
CL	D14-1204	ab	2	4	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	none	none	We use a new corpus annotated with Chinese semantic tense information and other implicit Chinese linguistic information using a "distant annotation" method .	First , we add two sources of implicit linguistic information as features - eventuality type and modality of an event , which are also inferred automatically .	CL_D14-1204_ab_2	CL_D14-1204_ab_4	CL_D14-1204_ab_2_4
CL	D14-1204	ab	2	5	proposal_implementation	proposal_implementation	elaboration	sequence	secondary	secondary	none	none	We use a new corpus annotated with Chinese semantic tense information and other implicit Chinese linguistic information using a "distant annotation" method .	Second , we perform joint learning on semantic tense , eventuality type , and modality of an event .	CL_D14-1204_ab_2	CL_D14-1204_ab_5	CL_D14-1204_ab_2_5
CL	D14-1204	ab	2	6	proposal_implementation	proposal_implementation	elaboration	sequence	secondary	secondary	none	none	We use a new corpus annotated with Chinese semantic tense information and other implicit Chinese linguistic information using a "distant annotation" method .	Third , we train artificial neural network models for this problem and compare its performance with feature-based approaches .	CL_D14-1204_ab_2	CL_D14-1204_ab_6	CL_D14-1204_ab_2_6
CL	D14-1204	ab	2	7	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	We use a new corpus annotated with Chinese semantic tense information and other implicit Chinese linguistic information using a "distant annotation" method .	Experimental results show considerable improvements on Chinese tense inference .	CL_D14-1204_ab_2	CL_D14-1204_ab_7	CL_D14-1204_ab_2_7
CL	D14-1204	ab	2	8	proposal_implementation	result	elaboration	elaboration	secondary	secondary	none	none	We use a new corpus annotated with Chinese semantic tense information and other implicit Chinese linguistic information using a "distant annotation" method .	Our best performance reaches 68.6 % in accuracy , outperforming a strong baseline method .	CL_D14-1204_ab_2	CL_D14-1204_ab_8	CL_D14-1204_ab_2_8
CL	D14-1204	ab	3	4	proposal	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	We propose three improvements over a relatively strong baseline method - a statistical learning method with extensive feature engineering .	First , we add two sources of implicit linguistic information as features - eventuality type and modality of an event , which are also inferred automatically .	CL_D14-1204_ab_3	CL_D14-1204_ab_4	CL_D14-1204_ab_3_4
CL	D14-1204	ab	4	3	proposal_implementation	proposal	elaboration	elaboration	secondary	secondary	forw	elaboration	First , we add two sources of implicit linguistic information as features - eventuality type and modality of an event , which are also inferred automatically .	We propose three improvements over a relatively strong baseline method - a statistical learning method with extensive feature engineering .	CL_D14-1204_ab_4	CL_D14-1204_ab_3	CL_D14-1204_ab_3_4
CL	D14-1204	ab	3	5	proposal	proposal_implementation	elaboration	sequence	secondary	secondary	none	none	We propose three improvements over a relatively strong baseline method - a statistical learning method with extensive feature engineering .	Second , we perform joint learning on semantic tense , eventuality type , and modality of an event .	CL_D14-1204_ab_3	CL_D14-1204_ab_5	CL_D14-1204_ab_3_5
CL	D14-1204	ab	3	6	proposal	proposal_implementation	elaboration	sequence	secondary	secondary	none	none	We propose three improvements over a relatively strong baseline method - a statistical learning method with extensive feature engineering .	Third , we train artificial neural network models for this problem and compare its performance with feature-based approaches .	CL_D14-1204_ab_3	CL_D14-1204_ab_6	CL_D14-1204_ab_3_6
CL	D14-1204	ab	3	7	proposal	result	elaboration	support	secondary	secondary	none	none	We propose three improvements over a relatively strong baseline method - a statistical learning method with extensive feature engineering .	Experimental results show considerable improvements on Chinese tense inference .	CL_D14-1204_ab_3	CL_D14-1204_ab_7	CL_D14-1204_ab_3_7
CL	D14-1204	ab	3	8	proposal	result	elaboration	elaboration	secondary	secondary	none	none	We propose three improvements over a relatively strong baseline method - a statistical learning method with extensive feature engineering .	Our best performance reaches 68.6 % in accuracy , outperforming a strong baseline method .	CL_D14-1204_ab_3	CL_D14-1204_ab_8	CL_D14-1204_ab_3_8
CL	D14-1204	ab	4	5	proposal_implementation	proposal_implementation	elaboration	sequence	secondary	secondary	back	sequence	First , we add two sources of implicit linguistic information as features - eventuality type and modality of an event , which are also inferred automatically .	Second , we perform joint learning on semantic tense , eventuality type , and modality of an event .	CL_D14-1204_ab_4	CL_D14-1204_ab_5	CL_D14-1204_ab_4_5
CL	D14-1204	ab	5	4	proposal_implementation	proposal_implementation	sequence	elaboration	secondary	secondary	forw	sequence	Second , we perform joint learning on semantic tense , eventuality type , and modality of an event .	First , we add two sources of implicit linguistic information as features - eventuality type and modality of an event , which are also inferred automatically .	CL_D14-1204_ab_5	CL_D14-1204_ab_4	CL_D14-1204_ab_4_5
CL	D14-1204	ab	4	6	proposal_implementation	proposal_implementation	elaboration	sequence	secondary	secondary	none	none	First , we add two sources of implicit linguistic information as features - eventuality type and modality of an event , which are also inferred automatically .	Third , we train artificial neural network models for this problem and compare its performance with feature-based approaches .	CL_D14-1204_ab_4	CL_D14-1204_ab_6	CL_D14-1204_ab_4_6
CL	D14-1204	ab	4	7	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	First , we add two sources of implicit linguistic information as features - eventuality type and modality of an event , which are also inferred automatically .	Experimental results show considerable improvements on Chinese tense inference .	CL_D14-1204_ab_4	CL_D14-1204_ab_7	CL_D14-1204_ab_4_7
CL	D14-1204	ab	4	8	proposal_implementation	result	elaboration	elaboration	secondary	secondary	none	none	First , we add two sources of implicit linguistic information as features - eventuality type and modality of an event , which are also inferred automatically .	Our best performance reaches 68.6 % in accuracy , outperforming a strong baseline method .	CL_D14-1204_ab_4	CL_D14-1204_ab_8	CL_D14-1204_ab_4_8
CL	D14-1204	ab	5	6	proposal_implementation	proposal_implementation	sequence	sequence	secondary	secondary	back	sequence	Second , we perform joint learning on semantic tense , eventuality type , and modality of an event .	Third , we train artificial neural network models for this problem and compare its performance with feature-based approaches .	CL_D14-1204_ab_5	CL_D14-1204_ab_6	CL_D14-1204_ab_5_6
CL	D14-1204	ab	6	5	proposal_implementation	proposal_implementation	sequence	sequence	secondary	secondary	forw	sequence	Third , we train artificial neural network models for this problem and compare its performance with feature-based approaches .	Second , we perform joint learning on semantic tense , eventuality type , and modality of an event .	CL_D14-1204_ab_6	CL_D14-1204_ab_5	CL_D14-1204_ab_5_6
CL	D14-1204	ab	5	7	proposal_implementation	result	sequence	support	secondary	secondary	none	none	Second , we perform joint learning on semantic tense , eventuality type , and modality of an event .	Experimental results show considerable improvements on Chinese tense inference .	CL_D14-1204_ab_5	CL_D14-1204_ab_7	CL_D14-1204_ab_5_7
CL	D14-1204	ab	5	8	proposal_implementation	result	sequence	elaboration	secondary	secondary	none	none	Second , we perform joint learning on semantic tense , eventuality type , and modality of an event .	Our best performance reaches 68.6 % in accuracy , outperforming a strong baseline method .	CL_D14-1204_ab_5	CL_D14-1204_ab_8	CL_D14-1204_ab_5_8
CL	D14-1204	ab	6	7	proposal_implementation	result	sequence	support	secondary	secondary	none	none	Third , we train artificial neural network models for this problem and compare its performance with feature-based approaches .	Experimental results show considerable improvements on Chinese tense inference .	CL_D14-1204_ab_6	CL_D14-1204_ab_7	CL_D14-1204_ab_6_7
CL	D14-1204	ab	6	8	proposal_implementation	result	sequence	elaboration	secondary	secondary	none	none	Third , we train artificial neural network models for this problem and compare its performance with feature-based approaches .	Our best performance reaches 68.6 % in accuracy , outperforming a strong baseline method .	CL_D14-1204_ab_6	CL_D14-1204_ab_8	CL_D14-1204_ab_6_8
CL	D14-1204	ab	7	8	result	result	support	elaboration	secondary	secondary	back	elaboration	Experimental results show considerable improvements on Chinese tense inference .	Our best performance reaches 68.6 % in accuracy , outperforming a strong baseline method .	CL_D14-1204_ab_7	CL_D14-1204_ab_8	CL_D14-1204_ab_7_8
CL	D14-1204	ab	8	7	result	result	elaboration	support	secondary	secondary	forw	elaboration	Our best performance reaches 68.6 % in accuracy , outperforming a strong baseline method .	Experimental results show considerable improvements on Chinese tense inference .	CL_D14-1204_ab_8	CL_D14-1204_ab_7	CL_D14-1204_ab_7_8
CL	D14-1205	ab	1	2	motivation_background	motivation_problem	info-required	support	secondary	secondary	forw	info-required	Populating Knowledge Base ( KB ) with new knowledge facts from reliable text re-sources usually consists of linking name mentions to KB entities and identifying relationship between entity pairs .	However , the task often suffers from errors propagating from upstream entity linkers to downstream relation extractors .	CL_D14-1205_ab_1	CL_D14-1205_ab_2	CL_D14-1205_ab_1_2
CL	D14-1205	ab	2	1	motivation_problem	motivation_background	support	info-required	secondary	secondary	back	info-required	However , the task often suffers from errors propagating from upstream entity linkers to downstream relation extractors .	Populating Knowledge Base ( KB ) with new knowledge facts from reliable text re-sources usually consists of linking name mentions to KB entities and identifying relationship between entity pairs .	CL_D14-1205_ab_2	CL_D14-1205_ab_1	CL_D14-1205_ab_1_2
CL	D14-1205	ab	1	3	motivation_background	proposal_implementation	info-required	none	secondary	main	none	none	Populating Knowledge Base ( KB ) with new knowledge facts from reliable text re-sources usually consists of linking name mentions to KB entities and identifying relationship between entity pairs .	In this paper , we propose a novel joint inference framework to allow interactions between the two subtasks and find an optimal assignment by addressing the coherence among preliminary local predictions : whether the types of entities meet the expectations of relations explicitly or implicitly , and whether the local predictions are globally compatible .	CL_D14-1205_ab_1	CL_D14-1205_ab_3	CL_D14-1205_ab_1_3
CL	D14-1205	ab	1	4	motivation_background	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	Populating Knowledge Base ( KB ) with new knowledge facts from reliable text re-sources usually consists of linking name mentions to KB entities and identifying relationship between entity pairs .	We further measure the confidence of the extracted triples by looking at the details of the complete extraction process .	CL_D14-1205_ab_1	CL_D14-1205_ab_4	CL_D14-1205_ab_1_4
CL	D14-1205	ab	1	5	motivation_background	result	info-required	support	secondary	secondary	none	none	Populating Knowledge Base ( KB ) with new knowledge facts from reliable text re-sources usually consists of linking name mentions to KB entities and identifying relationship between entity pairs .	Experiments show that the proposed framework can significantly reduce the error propagations thus obtain more reliable facts , and outperforms competitive baselines with state-of-the-art relation extraction models .	CL_D14-1205_ab_1	CL_D14-1205_ab_5	CL_D14-1205_ab_1_5
CL	D14-1205	ab	2	3	motivation_problem	proposal_implementation	support	none	secondary	main	forw	support	However , the task often suffers from errors propagating from upstream entity linkers to downstream relation extractors .	In this paper , we propose a novel joint inference framework to allow interactions between the two subtasks and find an optimal assignment by addressing the coherence among preliminary local predictions : whether the types of entities meet the expectations of relations explicitly or implicitly , and whether the local predictions are globally compatible .	CL_D14-1205_ab_2	CL_D14-1205_ab_3	CL_D14-1205_ab_2_3
CL	D14-1205	ab	3	2	proposal_implementation	motivation_problem	none	support	main	secondary	back	support	In this paper , we propose a novel joint inference framework to allow interactions between the two subtasks and find an optimal assignment by addressing the coherence among preliminary local predictions : whether the types of entities meet the expectations of relations explicitly or implicitly , and whether the local predictions are globally compatible .	However , the task often suffers from errors propagating from upstream entity linkers to downstream relation extractors .	CL_D14-1205_ab_3	CL_D14-1205_ab_2	CL_D14-1205_ab_2_3
CL	D14-1205	ab	2	4	motivation_problem	proposal_implementation	support	elaboration	secondary	secondary	none	none	However , the task often suffers from errors propagating from upstream entity linkers to downstream relation extractors .	We further measure the confidence of the extracted triples by looking at the details of the complete extraction process .	CL_D14-1205_ab_2	CL_D14-1205_ab_4	CL_D14-1205_ab_2_4
CL	D14-1205	ab	2	5	motivation_problem	result	support	support	secondary	secondary	none	none	However , the task often suffers from errors propagating from upstream entity linkers to downstream relation extractors .	Experiments show that the proposed framework can significantly reduce the error propagations thus obtain more reliable facts , and outperforms competitive baselines with state-of-the-art relation extraction models .	CL_D14-1205_ab_2	CL_D14-1205_ab_5	CL_D14-1205_ab_2_5
CL	D14-1205	ab	3	4	proposal_implementation	proposal_implementation	none	elaboration	main	secondary	back	elaboration	In this paper , we propose a novel joint inference framework to allow interactions between the two subtasks and find an optimal assignment by addressing the coherence among preliminary local predictions : whether the types of entities meet the expectations of relations explicitly or implicitly , and whether the local predictions are globally compatible .	We further measure the confidence of the extracted triples by looking at the details of the complete extraction process .	CL_D14-1205_ab_3	CL_D14-1205_ab_4	CL_D14-1205_ab_3_4
CL	D14-1205	ab	4	3	proposal_implementation	proposal_implementation	elaboration	none	secondary	main	forw	elaboration	We further measure the confidence of the extracted triples by looking at the details of the complete extraction process .	In this paper , we propose a novel joint inference framework to allow interactions between the two subtasks and find an optimal assignment by addressing the coherence among preliminary local predictions : whether the types of entities meet the expectations of relations explicitly or implicitly , and whether the local predictions are globally compatible .	CL_D14-1205_ab_4	CL_D14-1205_ab_3	CL_D14-1205_ab_3_4
CL	D14-1205	ab	3	5	proposal_implementation	result	none	support	main	secondary	back	support	In this paper , we propose a novel joint inference framework to allow interactions between the two subtasks and find an optimal assignment by addressing the coherence among preliminary local predictions : whether the types of entities meet the expectations of relations explicitly or implicitly , and whether the local predictions are globally compatible .	Experiments show that the proposed framework can significantly reduce the error propagations thus obtain more reliable facts , and outperforms competitive baselines with state-of-the-art relation extraction models .	CL_D14-1205_ab_3	CL_D14-1205_ab_5	CL_D14-1205_ab_3_5
CL	D14-1205	ab	5	3	result	proposal_implementation	support	none	secondary	main	forw	support	Experiments show that the proposed framework can significantly reduce the error propagations thus obtain more reliable facts , and outperforms competitive baselines with state-of-the-art relation extraction models .	In this paper , we propose a novel joint inference framework to allow interactions between the two subtasks and find an optimal assignment by addressing the coherence among preliminary local predictions : whether the types of entities meet the expectations of relations explicitly or implicitly , and whether the local predictions are globally compatible .	CL_D14-1205_ab_5	CL_D14-1205_ab_3	CL_D14-1205_ab_3_5
CL	D14-1205	ab	4	5	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	We further measure the confidence of the extracted triples by looking at the details of the complete extraction process .	Experiments show that the proposed framework can significantly reduce the error propagations thus obtain more reliable facts , and outperforms competitive baselines with state-of-the-art relation extraction models .	CL_D14-1205_ab_4	CL_D14-1205_ab_5	CL_D14-1205_ab_4_5
CL	D14-1206	ab	1	2	motivation_background	motivation_background	info-required	elaboration	secondary	secondary	back	elaboration	Information in visually rich formats such as PDF and HTML is often conveyed by a combination of textual and visual features .	In particular , genres such as marketing flyers and info-graphics often augment textual information by its color , size , positioning , etc. .	CL_D14-1206_ab_1	CL_D14-1206_ab_2	CL_D14-1206_ab_1_2
CL	D14-1206	ab	2	1	motivation_background	motivation_background	elaboration	info-required	secondary	secondary	forw	elaboration	In particular , genres such as marketing flyers and info-graphics often augment textual information by its color , size , positioning , etc. .	Information in visually rich formats such as PDF and HTML is often conveyed by a combination of textual and visual features .	CL_D14-1206_ab_2	CL_D14-1206_ab_1	CL_D14-1206_ab_1_2
CL	D14-1206	ab	1	3	motivation_background	motivation_problem	info-required	support	secondary	secondary	forw	info-required	Information in visually rich formats such as PDF and HTML is often conveyed by a combination of textual and visual features .	As a result , traditional text-based approaches to information extraction ( IE ) could underperform .	CL_D14-1206_ab_1	CL_D14-1206_ab_3	CL_D14-1206_ab_1_3
CL	D14-1206	ab	3	1	motivation_problem	motivation_background	support	info-required	secondary	secondary	back	info-required	As a result , traditional text-based approaches to information extraction ( IE ) could underperform .	Information in visually rich formats such as PDF and HTML is often conveyed by a combination of textual and visual features .	CL_D14-1206_ab_3	CL_D14-1206_ab_1	CL_D14-1206_ab_1_3
CL	D14-1206	ab	1	4	motivation_background	proposal	info-required	none	secondary	main	none	none	Information in visually rich formats such as PDF and HTML is often conveyed by a combination of textual and visual features .	In this study , we present a supervised machine learning approach to IE from online commercial real estate flyers .	CL_D14-1206_ab_1	CL_D14-1206_ab_4	CL_D14-1206_ab_1_4
CL	D14-1206	ab	1	5	motivation_background	means	info-required	by-means	secondary	secondary	none	none	Information in visually rich formats such as PDF and HTML is often conveyed by a combination of textual and visual features .	We evaluated the performance of SVM classifiers on the task of identifying 12 types of named entities using a combination of textual and visual features .	CL_D14-1206_ab_1	CL_D14-1206_ab_5	CL_D14-1206_ab_1_5
CL	D14-1206	ab	1	6	motivation_background	result	info-required	support	secondary	secondary	none	none	Information in visually rich formats such as PDF and HTML is often conveyed by a combination of textual and visual features .	Results show that the addition of visual features such as color , size , and positioning significantly increased classifier performance .	CL_D14-1206_ab_1	CL_D14-1206_ab_6	CL_D14-1206_ab_1_6
CL	D14-1206	ab	2	3	motivation_background	motivation_problem	elaboration	support	secondary	secondary	none	none	In particular , genres such as marketing flyers and info-graphics often augment textual information by its color , size , positioning , etc. .	As a result , traditional text-based approaches to information extraction ( IE ) could underperform .	CL_D14-1206_ab_2	CL_D14-1206_ab_3	CL_D14-1206_ab_2_3
CL	D14-1206	ab	2	4	motivation_background	proposal	elaboration	none	secondary	main	none	none	In particular , genres such as marketing flyers and info-graphics often augment textual information by its color , size , positioning , etc. .	In this study , we present a supervised machine learning approach to IE from online commercial real estate flyers .	CL_D14-1206_ab_2	CL_D14-1206_ab_4	CL_D14-1206_ab_2_4
CL	D14-1206	ab	2	5	motivation_background	means	elaboration	by-means	secondary	secondary	none	none	In particular , genres such as marketing flyers and info-graphics often augment textual information by its color , size , positioning , etc. .	We evaluated the performance of SVM classifiers on the task of identifying 12 types of named entities using a combination of textual and visual features .	CL_D14-1206_ab_2	CL_D14-1206_ab_5	CL_D14-1206_ab_2_5
CL	D14-1206	ab	2	6	motivation_background	result	elaboration	support	secondary	secondary	none	none	In particular , genres such as marketing flyers and info-graphics often augment textual information by its color , size , positioning , etc. .	Results show that the addition of visual features such as color , size , and positioning significantly increased classifier performance .	CL_D14-1206_ab_2	CL_D14-1206_ab_6	CL_D14-1206_ab_2_6
CL	D14-1206	ab	3	4	motivation_problem	proposal	support	none	secondary	main	forw	support	As a result , traditional text-based approaches to information extraction ( IE ) could underperform .	In this study , we present a supervised machine learning approach to IE from online commercial real estate flyers .	CL_D14-1206_ab_3	CL_D14-1206_ab_4	CL_D14-1206_ab_3_4
CL	D14-1206	ab	4	3	proposal	motivation_problem	none	support	main	secondary	back	support	In this study , we present a supervised machine learning approach to IE from online commercial real estate flyers .	As a result , traditional text-based approaches to information extraction ( IE ) could underperform .	CL_D14-1206_ab_4	CL_D14-1206_ab_3	CL_D14-1206_ab_3_4
CL	D14-1206	ab	3	5	motivation_problem	means	support	by-means	secondary	secondary	none	none	As a result , traditional text-based approaches to information extraction ( IE ) could underperform .	We evaluated the performance of SVM classifiers on the task of identifying 12 types of named entities using a combination of textual and visual features .	CL_D14-1206_ab_3	CL_D14-1206_ab_5	CL_D14-1206_ab_3_5
CL	D14-1206	ab	3	6	motivation_problem	result	support	support	secondary	secondary	none	none	As a result , traditional text-based approaches to information extraction ( IE ) could underperform .	Results show that the addition of visual features such as color , size , and positioning significantly increased classifier performance .	CL_D14-1206_ab_3	CL_D14-1206_ab_6	CL_D14-1206_ab_3_6
CL	D14-1206	ab	4	5	proposal	means	none	by-means	main	secondary	none	none	In this study , we present a supervised machine learning approach to IE from online commercial real estate flyers .	We evaluated the performance of SVM classifiers on the task of identifying 12 types of named entities using a combination of textual and visual features .	CL_D14-1206_ab_4	CL_D14-1206_ab_5	CL_D14-1206_ab_4_5
CL	D14-1206	ab	4	6	proposal	result	none	support	main	secondary	back	support	In this study , we present a supervised machine learning approach to IE from online commercial real estate flyers .	Results show that the addition of visual features such as color , size , and positioning significantly increased classifier performance .	CL_D14-1206_ab_4	CL_D14-1206_ab_6	CL_D14-1206_ab_4_6
CL	D14-1206	ab	6	4	result	proposal	support	none	secondary	main	forw	support	Results show that the addition of visual features such as color , size , and positioning significantly increased classifier performance .	In this study , we present a supervised machine learning approach to IE from online commercial real estate flyers .	CL_D14-1206_ab_6	CL_D14-1206_ab_4	CL_D14-1206_ab_4_6
CL	D14-1206	ab	5	6	means	result	by-means	support	secondary	secondary	forw	by-means	We evaluated the performance of SVM classifiers on the task of identifying 12 types of named entities using a combination of textual and visual features .	Results show that the addition of visual features such as color , size , and positioning significantly increased classifier performance .	CL_D14-1206_ab_5	CL_D14-1206_ab_6	CL_D14-1206_ab_5_6
CL	D14-1206	ab	6	5	result	means	support	by-means	secondary	secondary	back	by-means	Results show that the addition of visual features such as color , size , and positioning significantly increased classifier performance .	We evaluated the performance of SVM classifiers on the task of identifying 12 types of named entities using a combination of textual and visual features .	CL_D14-1206_ab_6	CL_D14-1206_ab_5	CL_D14-1206_ab_5_6
CL	D14-1207	ab	1	2	motivation_background	motivation_background	info-required	info-required	secondary	secondary	forw	info-required	Temporal scope adds a time dimension to facts in Knowledge Bases ( KBs ) .	These time scopes specify the time periods when a given fact was valid in real life .	CL_D14-1207_ab_1	CL_D14-1207_ab_2	CL_D14-1207_ab_1_2
CL	D14-1207	ab	2	1	motivation_background	motivation_background	info-required	info-required	secondary	secondary	back	info-required	These time scopes specify the time periods when a given fact was valid in real life .	Temporal scope adds a time dimension to facts in Knowledge Bases ( KBs ) .	CL_D14-1207_ab_2	CL_D14-1207_ab_1	CL_D14-1207_ab_1_2
CL	D14-1207	ab	1	3	motivation_background	motivation_problem	info-required	info-required	secondary	secondary	none	none	Temporal scope adds a time dimension to facts in Knowledge Bases ( KBs ) .	Without temporal scope , many facts are under-specified , reducing the usefulness of the data for upper level applications such as Question Answering .	CL_D14-1207_ab_1	CL_D14-1207_ab_3	CL_D14-1207_ab_1_3
CL	D14-1207	ab	1	4	motivation_background	motivation_problem	info-required	support	secondary	secondary	none	none	Temporal scope adds a time dimension to facts in Knowledge Bases ( KBs ) .	Existing methods for temporal scope inference and extraction still suffer from low accuracy .	CL_D14-1207_ab_1	CL_D14-1207_ab_4	CL_D14-1207_ab_1_4
CL	D14-1207	ab	1	5	motivation_background	proposal	info-required	none	secondary	main	none	none	Temporal scope adds a time dimension to facts in Knowledge Bases ( KBs ) .	In this paper , we present a new method that leverages temporal profiles augmented with context Contextual Temporal Profiles ( CTPs ) of entities .	CL_D14-1207_ab_1	CL_D14-1207_ab_5	CL_D14-1207_ab_1_5
CL	D14-1207	ab	1	6	motivation_background	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	Temporal scope adds a time dimension to facts in Knowledge Bases ( KBs ) .	Through change patterns in an entity's CTP , we model the entity's state change brought about by real world events that happen to the entity ( e.g , hired , fired , divorced , etc. ) .	CL_D14-1207_ab_1	CL_D14-1207_ab_6	CL_D14-1207_ab_1_6
CL	D14-1207	ab	1	7	motivation_background	conclusion	info-required	support	secondary	secondary	none	none	Temporal scope adds a time dimension to facts in Knowledge Bases ( KBs ) .	This leads to a new formulation of the temporal scoping problem as a state change detection problem .	CL_D14-1207_ab_1	CL_D14-1207_ab_7	CL_D14-1207_ab_1_7
CL	D14-1207	ab	1	8	motivation_background	result	info-required	support	secondary	secondary	none	none	Temporal scope adds a time dimension to facts in Knowledge Bases ( KBs ) .	Our experiments show that this formulation of the problem , and the resulting solution are highly effective for inferring temporal scope of facts .	CL_D14-1207_ab_1	CL_D14-1207_ab_8	CL_D14-1207_ab_1_8
CL	D14-1207	ab	2	3	motivation_background	motivation_problem	info-required	info-required	secondary	secondary	forw	info-required	These time scopes specify the time periods when a given fact was valid in real life .	Without temporal scope , many facts are under-specified , reducing the usefulness of the data for upper level applications such as Question Answering .	CL_D14-1207_ab_2	CL_D14-1207_ab_3	CL_D14-1207_ab_2_3
CL	D14-1207	ab	3	2	motivation_problem	motivation_background	info-required	info-required	secondary	secondary	back	info-required	Without temporal scope , many facts are under-specified , reducing the usefulness of the data for upper level applications such as Question Answering .	These time scopes specify the time periods when a given fact was valid in real life .	CL_D14-1207_ab_3	CL_D14-1207_ab_2	CL_D14-1207_ab_2_3
CL	D14-1207	ab	2	4	motivation_background	motivation_problem	info-required	support	secondary	secondary	none	none	These time scopes specify the time periods when a given fact was valid in real life .	Existing methods for temporal scope inference and extraction still suffer from low accuracy .	CL_D14-1207_ab_2	CL_D14-1207_ab_4	CL_D14-1207_ab_2_4
CL	D14-1207	ab	2	5	motivation_background	proposal	info-required	none	secondary	main	none	none	These time scopes specify the time periods when a given fact was valid in real life .	In this paper , we present a new method that leverages temporal profiles augmented with context Contextual Temporal Profiles ( CTPs ) of entities .	CL_D14-1207_ab_2	CL_D14-1207_ab_5	CL_D14-1207_ab_2_5
CL	D14-1207	ab	2	6	motivation_background	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	These time scopes specify the time periods when a given fact was valid in real life .	Through change patterns in an entity's CTP , we model the entity's state change brought about by real world events that happen to the entity ( e.g , hired , fired , divorced , etc. ) .	CL_D14-1207_ab_2	CL_D14-1207_ab_6	CL_D14-1207_ab_2_6
CL	D14-1207	ab	2	7	motivation_background	conclusion	info-required	support	secondary	secondary	none	none	These time scopes specify the time periods when a given fact was valid in real life .	This leads to a new formulation of the temporal scoping problem as a state change detection problem .	CL_D14-1207_ab_2	CL_D14-1207_ab_7	CL_D14-1207_ab_2_7
CL	D14-1207	ab	2	8	motivation_background	result	info-required	support	secondary	secondary	none	none	These time scopes specify the time periods when a given fact was valid in real life .	Our experiments show that this formulation of the problem , and the resulting solution are highly effective for inferring temporal scope of facts .	CL_D14-1207_ab_2	CL_D14-1207_ab_8	CL_D14-1207_ab_2_8
CL	D14-1207	ab	3	4	motivation_problem	motivation_problem	info-required	support	secondary	secondary	forw	info-required	Without temporal scope , many facts are under-specified , reducing the usefulness of the data for upper level applications such as Question Answering .	Existing methods for temporal scope inference and extraction still suffer from low accuracy .	CL_D14-1207_ab_3	CL_D14-1207_ab_4	CL_D14-1207_ab_3_4
CL	D14-1207	ab	4	3	motivation_problem	motivation_problem	support	info-required	secondary	secondary	back	info-required	Existing methods for temporal scope inference and extraction still suffer from low accuracy .	Without temporal scope , many facts are under-specified , reducing the usefulness of the data for upper level applications such as Question Answering .	CL_D14-1207_ab_4	CL_D14-1207_ab_3	CL_D14-1207_ab_3_4
CL	D14-1207	ab	3	5	motivation_problem	proposal	info-required	none	secondary	main	none	none	Without temporal scope , many facts are under-specified , reducing the usefulness of the data for upper level applications such as Question Answering .	In this paper , we present a new method that leverages temporal profiles augmented with context Contextual Temporal Profiles ( CTPs ) of entities .	CL_D14-1207_ab_3	CL_D14-1207_ab_5	CL_D14-1207_ab_3_5
CL	D14-1207	ab	3	6	motivation_problem	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	Without temporal scope , many facts are under-specified , reducing the usefulness of the data for upper level applications such as Question Answering .	Through change patterns in an entity's CTP , we model the entity's state change brought about by real world events that happen to the entity ( e.g , hired , fired , divorced , etc. ) .	CL_D14-1207_ab_3	CL_D14-1207_ab_6	CL_D14-1207_ab_3_6
CL	D14-1207	ab	3	7	motivation_problem	conclusion	info-required	support	secondary	secondary	none	none	Without temporal scope , many facts are under-specified , reducing the usefulness of the data for upper level applications such as Question Answering .	This leads to a new formulation of the temporal scoping problem as a state change detection problem .	CL_D14-1207_ab_3	CL_D14-1207_ab_7	CL_D14-1207_ab_3_7
CL	D14-1207	ab	3	8	motivation_problem	result	info-required	support	secondary	secondary	none	none	Without temporal scope , many facts are under-specified , reducing the usefulness of the data for upper level applications such as Question Answering .	Our experiments show that this formulation of the problem , and the resulting solution are highly effective for inferring temporal scope of facts .	CL_D14-1207_ab_3	CL_D14-1207_ab_8	CL_D14-1207_ab_3_8
CL	D14-1207	ab	4	5	motivation_problem	proposal	support	none	secondary	main	forw	support	Existing methods for temporal scope inference and extraction still suffer from low accuracy .	In this paper , we present a new method that leverages temporal profiles augmented with context Contextual Temporal Profiles ( CTPs ) of entities .	CL_D14-1207_ab_4	CL_D14-1207_ab_5	CL_D14-1207_ab_4_5
CL	D14-1207	ab	5	4	proposal	motivation_problem	none	support	main	secondary	back	support	In this paper , we present a new method that leverages temporal profiles augmented with context Contextual Temporal Profiles ( CTPs ) of entities .	Existing methods for temporal scope inference and extraction still suffer from low accuracy .	CL_D14-1207_ab_5	CL_D14-1207_ab_4	CL_D14-1207_ab_4_5
CL	D14-1207	ab	4	6	motivation_problem	proposal_implementation	support	elaboration	secondary	secondary	none	none	Existing methods for temporal scope inference and extraction still suffer from low accuracy .	Through change patterns in an entity's CTP , we model the entity's state change brought about by real world events that happen to the entity ( e.g , hired , fired , divorced , etc. ) .	CL_D14-1207_ab_4	CL_D14-1207_ab_6	CL_D14-1207_ab_4_6
CL	D14-1207	ab	4	7	motivation_problem	conclusion	support	support	secondary	secondary	none	none	Existing methods for temporal scope inference and extraction still suffer from low accuracy .	This leads to a new formulation of the temporal scoping problem as a state change detection problem .	CL_D14-1207_ab_4	CL_D14-1207_ab_7	CL_D14-1207_ab_4_7
CL	D14-1207	ab	4	8	motivation_problem	result	support	support	secondary	secondary	none	none	Existing methods for temporal scope inference and extraction still suffer from low accuracy .	Our experiments show that this formulation of the problem , and the resulting solution are highly effective for inferring temporal scope of facts .	CL_D14-1207_ab_4	CL_D14-1207_ab_8	CL_D14-1207_ab_4_8
CL	D14-1207	ab	5	6	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	In this paper , we present a new method that leverages temporal profiles augmented with context Contextual Temporal Profiles ( CTPs ) of entities .	Through change patterns in an entity's CTP , we model the entity's state change brought about by real world events that happen to the entity ( e.g , hired , fired , divorced , etc. ) .	CL_D14-1207_ab_5	CL_D14-1207_ab_6	CL_D14-1207_ab_5_6
CL	D14-1207	ab	6	5	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	Through change patterns in an entity's CTP , we model the entity's state change brought about by real world events that happen to the entity ( e.g , hired , fired , divorced , etc. ) .	In this paper , we present a new method that leverages temporal profiles augmented with context Contextual Temporal Profiles ( CTPs ) of entities .	CL_D14-1207_ab_6	CL_D14-1207_ab_5	CL_D14-1207_ab_5_6
CL	D14-1207	ab	5	7	proposal	conclusion	none	support	main	secondary	none	none	In this paper , we present a new method that leverages temporal profiles augmented with context Contextual Temporal Profiles ( CTPs ) of entities .	This leads to a new formulation of the temporal scoping problem as a state change detection problem .	CL_D14-1207_ab_5	CL_D14-1207_ab_7	CL_D14-1207_ab_5_7
CL	D14-1207	ab	5	8	proposal	result	none	support	main	secondary	back	support	In this paper , we present a new method that leverages temporal profiles augmented with context Contextual Temporal Profiles ( CTPs ) of entities .	Our experiments show that this formulation of the problem , and the resulting solution are highly effective for inferring temporal scope of facts .	CL_D14-1207_ab_5	CL_D14-1207_ab_8	CL_D14-1207_ab_5_8
CL	D14-1207	ab	8	5	result	proposal	support	none	secondary	main	forw	support	Our experiments show that this formulation of the problem , and the resulting solution are highly effective for inferring temporal scope of facts .	In this paper , we present a new method that leverages temporal profiles augmented with context Contextual Temporal Profiles ( CTPs ) of entities .	CL_D14-1207_ab_8	CL_D14-1207_ab_5	CL_D14-1207_ab_5_8
CL	D14-1207	ab	6	7	proposal_implementation	conclusion	elaboration	support	secondary	secondary	back	support	Through change patterns in an entity's CTP , we model the entity's state change brought about by real world events that happen to the entity ( e.g , hired , fired , divorced , etc. ) .	This leads to a new formulation of the temporal scoping problem as a state change detection problem .	CL_D14-1207_ab_6	CL_D14-1207_ab_7	CL_D14-1207_ab_6_7
CL	D14-1207	ab	7	6	conclusion	proposal_implementation	support	elaboration	secondary	secondary	forw	support	This leads to a new formulation of the temporal scoping problem as a state change detection problem .	Through change patterns in an entity's CTP , we model the entity's state change brought about by real world events that happen to the entity ( e.g , hired , fired , divorced , etc. ) .	CL_D14-1207_ab_7	CL_D14-1207_ab_6	CL_D14-1207_ab_6_7
CL	D14-1207	ab	6	8	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	Through change patterns in an entity's CTP , we model the entity's state change brought about by real world events that happen to the entity ( e.g , hired , fired , divorced , etc. ) .	Our experiments show that this formulation of the problem , and the resulting solution are highly effective for inferring temporal scope of facts .	CL_D14-1207_ab_6	CL_D14-1207_ab_8	CL_D14-1207_ab_6_8
CL	D14-1207	ab	7	8	conclusion	result	support	support	secondary	secondary	none	none	This leads to a new formulation of the temporal scoping problem as a state change detection problem .	Our experiments show that this formulation of the problem , and the resulting solution are highly effective for inferring temporal scope of facts .	CL_D14-1207_ab_7	CL_D14-1207_ab_8	CL_D14-1207_ab_7_8
CL	D14-1208	ab	1	2	motivation_background	motivation_background	info-required	support	secondary	secondary	forw	info-required	Distant supervision , a paradigm of relation extraction where training data is created by aligning facts in a database with a large unannotated corpus , is an attractive approach for training relation extractors .	Various models are proposed in recent literature to align the facts in the database to their mentions in the corpus .	CL_D14-1208_ab_1	CL_D14-1208_ab_2	CL_D14-1208_ab_1_2
CL	D14-1208	ab	2	1	motivation_background	motivation_background	support	info-required	secondary	secondary	back	info-required	Various models are proposed in recent literature to align the facts in the database to their mentions in the corpus .	Distant supervision , a paradigm of relation extraction where training data is created by aligning facts in a database with a large unannotated corpus , is an attractive approach for training relation extractors .	CL_D14-1208_ab_2	CL_D14-1208_ab_1	CL_D14-1208_ab_1_2
CL	D14-1208	ab	1	3	motivation_background	proposal	info-required	none	secondary	main	none	none	Distant supervision , a paradigm of relation extraction where training data is created by aligning facts in a database with a large unannotated corpus , is an attractive approach for training relation extractors .	In this paper , we discuss and critically analyse a popular alignment strategy called the "at least one" heuristic .	CL_D14-1208_ab_1	CL_D14-1208_ab_3	CL_D14-1208_ab_1_3
CL	D14-1208	ab	1	4	motivation_background	proposal	info-required	elaboration	secondary	secondary	none	none	Distant supervision , a paradigm of relation extraction where training data is created by aligning facts in a database with a large unannotated corpus , is an attractive approach for training relation extractors .	We provide a simple , yet effective relaxation to this strategy .	CL_D14-1208_ab_1	CL_D14-1208_ab_4	CL_D14-1208_ab_1_4
CL	D14-1208	ab	1	5	motivation_background	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	Distant supervision , a paradigm of relation extraction where training data is created by aligning facts in a database with a large unannotated corpus , is an attractive approach for training relation extractors .	We formulate the inference procedures in training as integer linear programming ( ILP ) problems and implement the relaxation to the "at least one " heuristic via a soft constraint in this formulation .	CL_D14-1208_ab_1	CL_D14-1208_ab_5	CL_D14-1208_ab_1_5
CL	D14-1208	ab	1	6	motivation_background	result	info-required	support	secondary	secondary	none	none	Distant supervision , a paradigm of relation extraction where training data is created by aligning facts in a database with a large unannotated corpus , is an attractive approach for training relation extractors .	Empirically , we demonstrate that this simple strategy leads to a better performance under certain settings over the existing approaches .	CL_D14-1208_ab_1	CL_D14-1208_ab_6	CL_D14-1208_ab_1_6
CL	D14-1208	ab	2	3	motivation_background	proposal	support	none	secondary	main	forw	support	Various models are proposed in recent literature to align the facts in the database to their mentions in the corpus .	In this paper , we discuss and critically analyse a popular alignment strategy called the "at least one" heuristic .	CL_D14-1208_ab_2	CL_D14-1208_ab_3	CL_D14-1208_ab_2_3
CL	D14-1208	ab	3	2	proposal	motivation_background	none	support	main	secondary	back	support	In this paper , we discuss and critically analyse a popular alignment strategy called the "at least one" heuristic .	Various models are proposed in recent literature to align the facts in the database to their mentions in the corpus .	CL_D14-1208_ab_3	CL_D14-1208_ab_2	CL_D14-1208_ab_2_3
CL	D14-1208	ab	2	4	motivation_background	proposal	support	elaboration	secondary	secondary	none	none	Various models are proposed in recent literature to align the facts in the database to their mentions in the corpus .	We provide a simple , yet effective relaxation to this strategy .	CL_D14-1208_ab_2	CL_D14-1208_ab_4	CL_D14-1208_ab_2_4
CL	D14-1208	ab	2	5	motivation_background	proposal_implementation	support	elaboration	secondary	secondary	none	none	Various models are proposed in recent literature to align the facts in the database to their mentions in the corpus .	We formulate the inference procedures in training as integer linear programming ( ILP ) problems and implement the relaxation to the "at least one " heuristic via a soft constraint in this formulation .	CL_D14-1208_ab_2	CL_D14-1208_ab_5	CL_D14-1208_ab_2_5
CL	D14-1208	ab	2	6	motivation_background	result	support	support	secondary	secondary	none	none	Various models are proposed in recent literature to align the facts in the database to their mentions in the corpus .	Empirically , we demonstrate that this simple strategy leads to a better performance under certain settings over the existing approaches .	CL_D14-1208_ab_2	CL_D14-1208_ab_6	CL_D14-1208_ab_2_6
CL	D14-1208	ab	3	4	proposal	proposal	none	elaboration	main	secondary	back	elaboration	In this paper , we discuss and critically analyse a popular alignment strategy called the "at least one" heuristic .	We provide a simple , yet effective relaxation to this strategy .	CL_D14-1208_ab_3	CL_D14-1208_ab_4	CL_D14-1208_ab_3_4
CL	D14-1208	ab	4	3	proposal	proposal	elaboration	none	secondary	main	forw	elaboration	We provide a simple , yet effective relaxation to this strategy .	In this paper , we discuss and critically analyse a popular alignment strategy called the "at least one" heuristic .	CL_D14-1208_ab_4	CL_D14-1208_ab_3	CL_D14-1208_ab_3_4
CL	D14-1208	ab	3	5	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	In this paper , we discuss and critically analyse a popular alignment strategy called the "at least one" heuristic .	We formulate the inference procedures in training as integer linear programming ( ILP ) problems and implement the relaxation to the "at least one " heuristic via a soft constraint in this formulation .	CL_D14-1208_ab_3	CL_D14-1208_ab_5	CL_D14-1208_ab_3_5
CL	D14-1208	ab	3	6	proposal	result	none	support	main	secondary	back	support	In this paper , we discuss and critically analyse a popular alignment strategy called the "at least one" heuristic .	Empirically , we demonstrate that this simple strategy leads to a better performance under certain settings over the existing approaches .	CL_D14-1208_ab_3	CL_D14-1208_ab_6	CL_D14-1208_ab_3_6
CL	D14-1208	ab	6	3	result	proposal	support	none	secondary	main	forw	support	Empirically , we demonstrate that this simple strategy leads to a better performance under certain settings over the existing approaches .	In this paper , we discuss and critically analyse a popular alignment strategy called the "at least one" heuristic .	CL_D14-1208_ab_6	CL_D14-1208_ab_3	CL_D14-1208_ab_3_6
CL	D14-1208	ab	4	5	proposal	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	We provide a simple , yet effective relaxation to this strategy .	We formulate the inference procedures in training as integer linear programming ( ILP ) problems and implement the relaxation to the "at least one " heuristic via a soft constraint in this formulation .	CL_D14-1208_ab_4	CL_D14-1208_ab_5	CL_D14-1208_ab_4_5
CL	D14-1208	ab	5	4	proposal_implementation	proposal	elaboration	elaboration	secondary	secondary	forw	elaboration	We formulate the inference procedures in training as integer linear programming ( ILP ) problems and implement the relaxation to the "at least one " heuristic via a soft constraint in this formulation .	We provide a simple , yet effective relaxation to this strategy .	CL_D14-1208_ab_5	CL_D14-1208_ab_4	CL_D14-1208_ab_4_5
CL	D14-1208	ab	4	6	proposal	result	elaboration	support	secondary	secondary	none	none	We provide a simple , yet effective relaxation to this strategy .	Empirically , we demonstrate that this simple strategy leads to a better performance under certain settings over the existing approaches .	CL_D14-1208_ab_4	CL_D14-1208_ab_6	CL_D14-1208_ab_4_6
CL	D14-1208	ab	5	6	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	We formulate the inference procedures in training as integer linear programming ( ILP ) problems and implement the relaxation to the "at least one " heuristic via a soft constraint in this formulation .	Empirically , we demonstrate that this simple strategy leads to a better performance under certain settings over the existing approaches .	CL_D14-1208_ab_5	CL_D14-1208_ab_6	CL_D14-1208_ab_5_6
CL	D14-1209	ab	1	2	motivation_problem	proposal	support	none	secondary	main	forw	support	Parameter tuning is an important problem in statistical machine translation , but surprisingly , most existing methods such as MERT , MIRA and PRO are agnostic about search , while search errors could severely degrade translation quality .	We propose a search-aware framework to promote promising partial translations , preventing them from be-ing pruned .	CL_D14-1209_ab_1	CL_D14-1209_ab_2	CL_D14-1209_ab_1_2
CL	D14-1209	ab	2	1	proposal	motivation_problem	none	support	main	secondary	back	support	We propose a search-aware framework to promote promising partial translations , preventing them from be-ing pruned .	Parameter tuning is an important problem in statistical machine translation , but surprisingly , most existing methods such as MERT , MIRA and PRO are agnostic about search , while search errors could severely degrade translation quality .	CL_D14-1209_ab_2	CL_D14-1209_ab_1	CL_D14-1209_ab_1_2
CL	D14-1209	ab	1	3	motivation_problem	proposal	support	elaboration	secondary	secondary	none	none	Parameter tuning is an important problem in statistical machine translation , but surprisingly , most existing methods such as MERT , MIRA and PRO are agnostic about search , while search errors could severely degrade translation quality .	To do so we develop two metrics to evaluate partial derivations .	CL_D14-1209_ab_1	CL_D14-1209_ab_3	CL_D14-1209_ab_1_3
CL	D14-1209	ab	1	4	motivation_problem	observation	support	support	secondary	secondary	none	none	Parameter tuning is an important problem in statistical machine translation , but surprisingly , most existing methods such as MERT , MIRA and PRO are agnostic about search , while search errors could severely degrade translation quality .	Our technique can be applied to all of the three above-mentioned tuning methods , and extensive experiments on Chinese-to-English and English-to-Chinese translation show up to +2.6 BLEU gains over search-agnostic baselines .	CL_D14-1209_ab_1	CL_D14-1209_ab_4	CL_D14-1209_ab_1_4
CL	D14-1209	ab	2	3	proposal	proposal	none	elaboration	main	secondary	back	elaboration	We propose a search-aware framework to promote promising partial translations , preventing them from be-ing pruned .	To do so we develop two metrics to evaluate partial derivations .	CL_D14-1209_ab_2	CL_D14-1209_ab_3	CL_D14-1209_ab_2_3
CL	D14-1209	ab	3	2	proposal	proposal	elaboration	none	secondary	main	forw	elaboration	To do so we develop two metrics to evaluate partial derivations .	We propose a search-aware framework to promote promising partial translations , preventing them from be-ing pruned .	CL_D14-1209_ab_3	CL_D14-1209_ab_2	CL_D14-1209_ab_2_3
CL	D14-1209	ab	2	4	proposal	observation	none	support	main	secondary	back	support	We propose a search-aware framework to promote promising partial translations , preventing them from be-ing pruned .	Our technique can be applied to all of the three above-mentioned tuning methods , and extensive experiments on Chinese-to-English and English-to-Chinese translation show up to +2.6 BLEU gains over search-agnostic baselines .	CL_D14-1209_ab_2	CL_D14-1209_ab_4	CL_D14-1209_ab_2_4
CL	D14-1209	ab	4	2	observation	proposal	support	none	secondary	main	forw	support	Our technique can be applied to all of the three above-mentioned tuning methods , and extensive experiments on Chinese-to-English and English-to-Chinese translation show up to +2.6 BLEU gains over search-agnostic baselines .	We propose a search-aware framework to promote promising partial translations , preventing them from be-ing pruned .	CL_D14-1209_ab_4	CL_D14-1209_ab_2	CL_D14-1209_ab_2_4
CL	D14-1209	ab	3	4	proposal	observation	elaboration	support	secondary	secondary	none	none	To do so we develop two metrics to evaluate partial derivations .	Our technique can be applied to all of the three above-mentioned tuning methods , and extensive experiments on Chinese-to-English and English-to-Chinese translation show up to +2.6 BLEU gains over search-agnostic baselines .	CL_D14-1209_ab_3	CL_D14-1209_ab_4	CL_D14-1209_ab_3_4
CL	D14-1210	ab	1	2	motivation_hypothesis	proposal	support	none	secondary	main	forw	support	Data-driven refinement of non-terminal categories has been demonstrated to be a reliable technique for improving monolingual parsing with PCFGs .	In this paper , we extend these techniques to learn latent refinements of single-category synchronous grammars , so as to improve translation performance .	CL_D14-1210_ab_1	CL_D14-1210_ab_2	CL_D14-1210_ab_1_2
CL	D14-1210	ab	2	1	proposal	motivation_hypothesis	none	support	main	secondary	back	support	In this paper , we extend these techniques to learn latent refinements of single-category synchronous grammars , so as to improve translation performance .	Data-driven refinement of non-terminal categories has been demonstrated to be a reliable technique for improving monolingual parsing with PCFGs .	CL_D14-1210_ab_2	CL_D14-1210_ab_1	CL_D14-1210_ab_1_2
CL	D14-1210	ab	1	3	motivation_hypothesis	proposal	support	elaboration	secondary	secondary	none	none	Data-driven refinement of non-terminal categories has been demonstrated to be a reliable technique for improving monolingual parsing with PCFGs .	We compare two estimators for this latent-variable model : one based on EM and the other is a spectral algorithm based on the method of moments .	CL_D14-1210_ab_1	CL_D14-1210_ab_3	CL_D14-1210_ab_1_3
CL	D14-1210	ab	1	4	motivation_hypothesis	means	support	by-means	secondary	secondary	none	none	Data-driven refinement of non-terminal categories has been demonstrated to be a reliable technique for improving monolingual parsing with PCFGs .	We evaluate their performance on a Chinese-English translation task .	CL_D14-1210_ab_1	CL_D14-1210_ab_4	CL_D14-1210_ab_1_4
CL	D14-1210	ab	1	5	motivation_hypothesis	result	support	support	secondary	secondary	none	none	Data-driven refinement of non-terminal categories has been demonstrated to be a reliable technique for improving monolingual parsing with PCFGs .	The results indicate that we can achieve significant gains over the baseline with both approaches , but in particular the moments-based estimator is both faster and performs better than EM .	CL_D14-1210_ab_1	CL_D14-1210_ab_5	CL_D14-1210_ab_1_5
CL	D14-1210	ab	2	3	proposal	proposal	none	elaboration	main	secondary	back	elaboration	In this paper , we extend these techniques to learn latent refinements of single-category synchronous grammars , so as to improve translation performance .	We compare two estimators for this latent-variable model : one based on EM and the other is a spectral algorithm based on the method of moments .	CL_D14-1210_ab_2	CL_D14-1210_ab_3	CL_D14-1210_ab_2_3
CL	D14-1210	ab	3	2	proposal	proposal	elaboration	none	secondary	main	forw	elaboration	We compare two estimators for this latent-variable model : one based on EM and the other is a spectral algorithm based on the method of moments .	In this paper , we extend these techniques to learn latent refinements of single-category synchronous grammars , so as to improve translation performance .	CL_D14-1210_ab_3	CL_D14-1210_ab_2	CL_D14-1210_ab_2_3
CL	D14-1210	ab	2	4	proposal	means	none	by-means	main	secondary	none	none	In this paper , we extend these techniques to learn latent refinements of single-category synchronous grammars , so as to improve translation performance .	We evaluate their performance on a Chinese-English translation task .	CL_D14-1210_ab_2	CL_D14-1210_ab_4	CL_D14-1210_ab_2_4
CL	D14-1210	ab	2	5	proposal	result	none	support	main	secondary	back	support	In this paper , we extend these techniques to learn latent refinements of single-category synchronous grammars , so as to improve translation performance .	The results indicate that we can achieve significant gains over the baseline with both approaches , but in particular the moments-based estimator is both faster and performs better than EM .	CL_D14-1210_ab_2	CL_D14-1210_ab_5	CL_D14-1210_ab_2_5
CL	D14-1210	ab	5	2	result	proposal	support	none	secondary	main	forw	support	The results indicate that we can achieve significant gains over the baseline with both approaches , but in particular the moments-based estimator is both faster and performs better than EM .	In this paper , we extend these techniques to learn latent refinements of single-category synchronous grammars , so as to improve translation performance .	CL_D14-1210_ab_5	CL_D14-1210_ab_2	CL_D14-1210_ab_2_5
CL	D14-1210	ab	3	4	proposal	means	elaboration	by-means	secondary	secondary	none	none	We compare two estimators for this latent-variable model : one based on EM and the other is a spectral algorithm based on the method of moments .	We evaluate their performance on a Chinese-English translation task .	CL_D14-1210_ab_3	CL_D14-1210_ab_4	CL_D14-1210_ab_3_4
CL	D14-1210	ab	3	5	proposal	result	elaboration	support	secondary	secondary	none	none	We compare two estimators for this latent-variable model : one based on EM and the other is a spectral algorithm based on the method of moments .	The results indicate that we can achieve significant gains over the baseline with both approaches , but in particular the moments-based estimator is both faster and performs better than EM .	CL_D14-1210_ab_3	CL_D14-1210_ab_5	CL_D14-1210_ab_3_5
CL	D14-1210	ab	4	5	means	result	by-means	support	secondary	secondary	forw	by-means	We evaluate their performance on a Chinese-English translation task .	The results indicate that we can achieve significant gains over the baseline with both approaches , but in particular the moments-based estimator is both faster and performs better than EM .	CL_D14-1210_ab_4	CL_D14-1210_ab_5	CL_D14-1210_ab_4_5
CL	D14-1210	ab	5	4	result	means	support	by-means	secondary	secondary	back	by-means	The results indicate that we can achieve significant gains over the baseline with both approaches , but in particular the moments-based estimator is both faster and performs better than EM .	We evaluate their performance on a Chinese-English translation task .	CL_D14-1210_ab_5	CL_D14-1210_ab_4	CL_D14-1210_ab_4_5
CL	D14-1211	ab	1	2	proposal	proposal	none	elaboration	main	secondary	back	elaboration	We investigate the interaction of power , gender , and language use in the Enron email corpus .	We present a freely available extension to the Enron corpus , with the gender of senders of 87 % messages reliably identified .	CL_D14-1211_ab_1	CL_D14-1211_ab_2	CL_D14-1211_ab_1_2
CL	D14-1211	ab	2	1	proposal	proposal	elaboration	none	secondary	main	forw	elaboration	We present a freely available extension to the Enron corpus , with the gender of senders of 87 % messages reliably identified .	We investigate the interaction of power , gender , and language use in the Enron email corpus .	CL_D14-1211_ab_2	CL_D14-1211_ab_1	CL_D14-1211_ab_1_2
CL	D14-1211	ab	1	3	proposal	proposal	none	elaboration	main	secondary	none	none	We investigate the interaction of power , gender , and language use in the Enron email corpus .	Using this data , we test two specific hypotheses drawn from the sociolinguistic literature pertaining to gender and power : women managers use face-saving communicative strategies , and women use language more explicitly than men to create and maintain social relations .	CL_D14-1211_ab_1	CL_D14-1211_ab_3	CL_D14-1211_ab_1_3
CL	D14-1211	ab	1	4	proposal	proposal	none	elaboration	main	secondary	none	none	We investigate the interaction of power , gender , and language use in the Enron email corpus .	We introduce the notion of "gender environment" to the computational study of written conversations ; we interpret this notion as the gender makeup of an email thread , and show that some manifestations of power differ significantly between gender environments .	CL_D14-1211_ab_1	CL_D14-1211_ab_4	CL_D14-1211_ab_1_4
CL	D14-1211	ab	1	5	proposal	proposal	none	elaboration	main	secondary	none	none	We investigate the interaction of power , gender , and language use in the Enron email corpus .	Finally , we show the utility of gender information in the problem of automatically predicting the direction of power between pairs of participants in email interactions .	CL_D14-1211_ab_1	CL_D14-1211_ab_5	CL_D14-1211_ab_1_5
CL	D14-1211	ab	2	3	proposal	proposal	elaboration	elaboration	secondary	secondary	back	elaboration	We present a freely available extension to the Enron corpus , with the gender of senders of 87 % messages reliably identified .	Using this data , we test two specific hypotheses drawn from the sociolinguistic literature pertaining to gender and power : women managers use face-saving communicative strategies , and women use language more explicitly than men to create and maintain social relations .	CL_D14-1211_ab_2	CL_D14-1211_ab_3	CL_D14-1211_ab_2_3
CL	D14-1211	ab	3	2	proposal	proposal	elaboration	elaboration	secondary	secondary	forw	elaboration	Using this data , we test two specific hypotheses drawn from the sociolinguistic literature pertaining to gender and power : women managers use face-saving communicative strategies , and women use language more explicitly than men to create and maintain social relations .	We present a freely available extension to the Enron corpus , with the gender of senders of 87 % messages reliably identified .	CL_D14-1211_ab_3	CL_D14-1211_ab_2	CL_D14-1211_ab_2_3
CL	D14-1211	ab	2	4	proposal	proposal	elaboration	elaboration	secondary	secondary	none	none	We present a freely available extension to the Enron corpus , with the gender of senders of 87 % messages reliably identified .	We introduce the notion of "gender environment" to the computational study of written conversations ; we interpret this notion as the gender makeup of an email thread , and show that some manifestations of power differ significantly between gender environments .	CL_D14-1211_ab_2	CL_D14-1211_ab_4	CL_D14-1211_ab_2_4
CL	D14-1211	ab	2	5	proposal	proposal	elaboration	elaboration	secondary	secondary	none	none	We present a freely available extension to the Enron corpus , with the gender of senders of 87 % messages reliably identified .	Finally , we show the utility of gender information in the problem of automatically predicting the direction of power between pairs of participants in email interactions .	CL_D14-1211_ab_2	CL_D14-1211_ab_5	CL_D14-1211_ab_2_5
CL	D14-1211	ab	3	4	proposal	proposal	elaboration	elaboration	secondary	secondary	back	elaboration	Using this data , we test two specific hypotheses drawn from the sociolinguistic literature pertaining to gender and power : women managers use face-saving communicative strategies , and women use language more explicitly than men to create and maintain social relations .	We introduce the notion of "gender environment" to the computational study of written conversations ; we interpret this notion as the gender makeup of an email thread , and show that some manifestations of power differ significantly between gender environments .	CL_D14-1211_ab_3	CL_D14-1211_ab_4	CL_D14-1211_ab_3_4
CL	D14-1211	ab	4	3	proposal	proposal	elaboration	elaboration	secondary	secondary	forw	elaboration	We introduce the notion of "gender environment" to the computational study of written conversations ; we interpret this notion as the gender makeup of an email thread , and show that some manifestations of power differ significantly between gender environments .	Using this data , we test two specific hypotheses drawn from the sociolinguistic literature pertaining to gender and power : women managers use face-saving communicative strategies , and women use language more explicitly than men to create and maintain social relations .	CL_D14-1211_ab_4	CL_D14-1211_ab_3	CL_D14-1211_ab_3_4
CL	D14-1211	ab	3	5	proposal	proposal	elaboration	elaboration	secondary	secondary	none	none	Using this data , we test two specific hypotheses drawn from the sociolinguistic literature pertaining to gender and power : women managers use face-saving communicative strategies , and women use language more explicitly than men to create and maintain social relations .	Finally , we show the utility of gender information in the problem of automatically predicting the direction of power between pairs of participants in email interactions .	CL_D14-1211_ab_3	CL_D14-1211_ab_5	CL_D14-1211_ab_3_5
CL	D14-1211	ab	4	5	proposal	proposal	elaboration	elaboration	secondary	secondary	back	elaboration	We introduce the notion of "gender environment" to the computational study of written conversations ; we interpret this notion as the gender makeup of an email thread , and show that some manifestations of power differ significantly between gender environments .	Finally , we show the utility of gender information in the problem of automatically predicting the direction of power between pairs of participants in email interactions .	CL_D14-1211_ab_4	CL_D14-1211_ab_5	CL_D14-1211_ab_4_5
CL	D14-1211	ab	5	4	proposal	proposal	elaboration	elaboration	secondary	secondary	forw	elaboration	Finally , we show the utility of gender information in the problem of automatically predicting the direction of power between pairs of participants in email interactions .	We introduce the notion of "gender environment" to the computational study of written conversations ; we interpret this notion as the gender makeup of an email thread , and show that some manifestations of power differ significantly between gender environments .	CL_D14-1211_ab_5	CL_D14-1211_ab_4	CL_D14-1211_ab_4_5
CL	D14-1212	ab	1	2	motivation_background	motivation_background	info-required	info-required	secondary	secondary	forw	info-required	Latent Dirichlet allocation ( LDA ) is a topic model that has been applied to various fields , including user profiling and event summarization on Twitter .	When LDA is applied to tweet collections , it generally treats all aggregated tweets of a user as a single document .	CL_D14-1212_ab_1	CL_D14-1212_ab_2	CL_D14-1212_ab_1_2
CL	D14-1212	ab	2	1	motivation_background	motivation_background	info-required	info-required	secondary	secondary	back	info-required	When LDA is applied to tweet collections , it generally treats all aggregated tweets of a user as a single document .	Latent Dirichlet allocation ( LDA ) is a topic model that has been applied to various fields , including user profiling and event summarization on Twitter .	CL_D14-1212_ab_2	CL_D14-1212_ab_1	CL_D14-1212_ab_1_2
CL	D14-1212	ab	1	3	motivation_background	motivation_background	info-required	info-required	secondary	secondary	none	none	Latent Dirichlet allocation ( LDA ) is a topic model that has been applied to various fields , including user profiling and event summarization on Twitter .	Twitter-LDA , which assumes a single tweet consists of a single topic , has been proposed and has shown that it is superior in topic semantic coherence .	CL_D14-1212_ab_1	CL_D14-1212_ab_3	CL_D14-1212_ab_1_3
CL	D14-1212	ab	1	4	motivation_background	motivation_problem	info-required	support	secondary	secondary	none	none	Latent Dirichlet allocation ( LDA ) is a topic model that has been applied to various fields , including user profiling and event summarization on Twitter .	However , Twitter-LDA is not capable of online inference .	CL_D14-1212_ab_1	CL_D14-1212_ab_4	CL_D14-1212_ab_1_4
CL	D14-1212	ab	1	5	motivation_background	proposal	info-required	none	secondary	main	none	none	Latent Dirichlet allocation ( LDA ) is a topic model that has been applied to various fields , including user profiling and event summarization on Twitter .	In this study , we extend Twitter-LDA in the following two ways .	CL_D14-1212_ab_1	CL_D14-1212_ab_5	CL_D14-1212_ab_1_5
CL	D14-1212	ab	1	6	motivation_background	proposal	info-required	elaboration	secondary	secondary	none	none	Latent Dirichlet allocation ( LDA ) is a topic model that has been applied to various fields , including user profiling and event summarization on Twitter .	First , we model the generation process of tweets more accurately by estimating the ratio between topic words and general words for each user .	CL_D14-1212_ab_1	CL_D14-1212_ab_6	CL_D14-1212_ab_1_6
CL	D14-1212	ab	1	7	motivation_background	proposal	info-required	elaboration	secondary	secondary	none	none	Latent Dirichlet allocation ( LDA ) is a topic model that has been applied to various fields , including user profiling and event summarization on Twitter .	Second , we enable it to estimate the dynamics of user interests and topic trends online based on the topic tracking model ( TTM ) , which models consumer purchase behaviors .	CL_D14-1212_ab_1	CL_D14-1212_ab_7	CL_D14-1212_ab_1_7
CL	D14-1212	ab	2	3	motivation_background	motivation_background	info-required	info-required	secondary	secondary	forw	info-required	When LDA is applied to tweet collections , it generally treats all aggregated tweets of a user as a single document .	Twitter-LDA , which assumes a single tweet consists of a single topic , has been proposed and has shown that it is superior in topic semantic coherence .	CL_D14-1212_ab_2	CL_D14-1212_ab_3	CL_D14-1212_ab_2_3
CL	D14-1212	ab	3	2	motivation_background	motivation_background	info-required	info-required	secondary	secondary	back	info-required	Twitter-LDA , which assumes a single tweet consists of a single topic , has been proposed and has shown that it is superior in topic semantic coherence .	When LDA is applied to tweet collections , it generally treats all aggregated tweets of a user as a single document .	CL_D14-1212_ab_3	CL_D14-1212_ab_2	CL_D14-1212_ab_2_3
CL	D14-1212	ab	2	4	motivation_background	motivation_problem	info-required	support	secondary	secondary	none	none	When LDA is applied to tweet collections , it generally treats all aggregated tweets of a user as a single document .	However , Twitter-LDA is not capable of online inference .	CL_D14-1212_ab_2	CL_D14-1212_ab_4	CL_D14-1212_ab_2_4
CL	D14-1212	ab	2	5	motivation_background	proposal	info-required	none	secondary	main	none	none	When LDA is applied to tweet collections , it generally treats all aggregated tweets of a user as a single document .	In this study , we extend Twitter-LDA in the following two ways .	CL_D14-1212_ab_2	CL_D14-1212_ab_5	CL_D14-1212_ab_2_5
CL	D14-1212	ab	2	6	motivation_background	proposal	info-required	elaboration	secondary	secondary	none	none	When LDA is applied to tweet collections , it generally treats all aggregated tweets of a user as a single document .	First , we model the generation process of tweets more accurately by estimating the ratio between topic words and general words for each user .	CL_D14-1212_ab_2	CL_D14-1212_ab_6	CL_D14-1212_ab_2_6
CL	D14-1212	ab	2	7	motivation_background	proposal	info-required	elaboration	secondary	secondary	none	none	When LDA is applied to tweet collections , it generally treats all aggregated tweets of a user as a single document .	Second , we enable it to estimate the dynamics of user interests and topic trends online based on the topic tracking model ( TTM ) , which models consumer purchase behaviors .	CL_D14-1212_ab_2	CL_D14-1212_ab_7	CL_D14-1212_ab_2_7
CL	D14-1212	ab	3	4	motivation_background	motivation_problem	info-required	support	secondary	secondary	forw	info-required	Twitter-LDA , which assumes a single tweet consists of a single topic , has been proposed and has shown that it is superior in topic semantic coherence .	However , Twitter-LDA is not capable of online inference .	CL_D14-1212_ab_3	CL_D14-1212_ab_4	CL_D14-1212_ab_3_4
CL	D14-1212	ab	4	3	motivation_problem	motivation_background	support	info-required	secondary	secondary	back	info-required	However , Twitter-LDA is not capable of online inference .	Twitter-LDA , which assumes a single tweet consists of a single topic , has been proposed and has shown that it is superior in topic semantic coherence .	CL_D14-1212_ab_4	CL_D14-1212_ab_3	CL_D14-1212_ab_3_4
CL	D14-1212	ab	3	5	motivation_background	proposal	info-required	none	secondary	main	none	none	Twitter-LDA , which assumes a single tweet consists of a single topic , has been proposed and has shown that it is superior in topic semantic coherence .	In this study , we extend Twitter-LDA in the following two ways .	CL_D14-1212_ab_3	CL_D14-1212_ab_5	CL_D14-1212_ab_3_5
CL	D14-1212	ab	3	6	motivation_background	proposal	info-required	elaboration	secondary	secondary	none	none	Twitter-LDA , which assumes a single tweet consists of a single topic , has been proposed and has shown that it is superior in topic semantic coherence .	First , we model the generation process of tweets more accurately by estimating the ratio between topic words and general words for each user .	CL_D14-1212_ab_3	CL_D14-1212_ab_6	CL_D14-1212_ab_3_6
CL	D14-1212	ab	3	7	motivation_background	proposal	info-required	elaboration	secondary	secondary	none	none	Twitter-LDA , which assumes a single tweet consists of a single topic , has been proposed and has shown that it is superior in topic semantic coherence .	Second , we enable it to estimate the dynamics of user interests and topic trends online based on the topic tracking model ( TTM ) , which models consumer purchase behaviors .	CL_D14-1212_ab_3	CL_D14-1212_ab_7	CL_D14-1212_ab_3_7
CL	D14-1212	ab	4	5	motivation_problem	proposal	support	none	secondary	main	forw	support	However , Twitter-LDA is not capable of online inference .	In this study , we extend Twitter-LDA in the following two ways .	CL_D14-1212_ab_4	CL_D14-1212_ab_5	CL_D14-1212_ab_4_5
CL	D14-1212	ab	5	4	proposal	motivation_problem	none	support	main	secondary	back	support	In this study , we extend Twitter-LDA in the following two ways .	However , Twitter-LDA is not capable of online inference .	CL_D14-1212_ab_5	CL_D14-1212_ab_4	CL_D14-1212_ab_4_5
CL	D14-1212	ab	4	6	motivation_problem	proposal	support	elaboration	secondary	secondary	none	none	However , Twitter-LDA is not capable of online inference .	First , we model the generation process of tweets more accurately by estimating the ratio between topic words and general words for each user .	CL_D14-1212_ab_4	CL_D14-1212_ab_6	CL_D14-1212_ab_4_6
CL	D14-1212	ab	4	7	motivation_problem	proposal	support	elaboration	secondary	secondary	none	none	However , Twitter-LDA is not capable of online inference .	Second , we enable it to estimate the dynamics of user interests and topic trends online based on the topic tracking model ( TTM ) , which models consumer purchase behaviors .	CL_D14-1212_ab_4	CL_D14-1212_ab_7	CL_D14-1212_ab_4_7
CL	D14-1212	ab	5	6	proposal	proposal	none	elaboration	main	secondary	back	elaboration	In this study , we extend Twitter-LDA in the following two ways .	First , we model the generation process of tweets more accurately by estimating the ratio between topic words and general words for each user .	CL_D14-1212_ab_5	CL_D14-1212_ab_6	CL_D14-1212_ab_5_6
CL	D14-1212	ab	6	5	proposal	proposal	elaboration	none	secondary	main	forw	elaboration	First , we model the generation process of tweets more accurately by estimating the ratio between topic words and general words for each user .	In this study , we extend Twitter-LDA in the following two ways .	CL_D14-1212_ab_6	CL_D14-1212_ab_5	CL_D14-1212_ab_5_6
CL	D14-1212	ab	5	7	proposal	proposal	none	elaboration	main	secondary	none	none	In this study , we extend Twitter-LDA in the following two ways .	Second , we enable it to estimate the dynamics of user interests and topic trends online based on the topic tracking model ( TTM ) , which models consumer purchase behaviors .	CL_D14-1212_ab_5	CL_D14-1212_ab_7	CL_D14-1212_ab_5_7
CL	D14-1212	ab	6	7	proposal	proposal	elaboration	elaboration	secondary	secondary	back	elaboration	First , we model the generation process of tweets more accurately by estimating the ratio between topic words and general words for each user .	Second , we enable it to estimate the dynamics of user interests and topic trends online based on the topic tracking model ( TTM ) , which models consumer purchase behaviors .	CL_D14-1212_ab_6	CL_D14-1212_ab_7	CL_D14-1212_ab_6_7
CL	D14-1212	ab	7	6	proposal	proposal	elaboration	elaboration	secondary	secondary	forw	elaboration	Second , we enable it to estimate the dynamics of user interests and topic trends online based on the topic tracking model ( TTM ) , which models consumer purchase behaviors .	First , we model the generation process of tweets more accurately by estimating the ratio between topic words and general words for each user .	CL_D14-1212_ab_7	CL_D14-1212_ab_6	CL_D14-1212_ab_6_7
CL	D14-1213	ab	1	2	information_additional	motivation_problem	info-required	support	secondary	secondary	forw	info-required	Self-disclosure , the act of revealing one-self to others , is an important social behavior that strengthens interpersonal relationships and increases social support .	Although there are many social science studies of self-disclosure , they are based on manual coding of small datasets and questionnaires .	CL_D14-1213_ab_1	CL_D14-1213_ab_2	CL_D14-1213_ab_1_2
CL	D14-1213	ab	2	1	motivation_problem	information_additional	support	info-required	secondary	secondary	back	info-required	Although there are many social science studies of self-disclosure , they are based on manual coding of small datasets and questionnaires .	Self-disclosure , the act of revealing one-self to others , is an important social behavior that strengthens interpersonal relationships and increases social support .	CL_D14-1213_ab_2	CL_D14-1213_ab_1	CL_D14-1213_ab_1_2
CL	D14-1213	ab	1	3	information_additional	proposal_implementation	info-required	none	secondary	main	none	none	Self-disclosure , the act of revealing one-self to others , is an important social behavior that strengthens interpersonal relationships and increases social support .	We conduct a computational analysis of self-disclosure with a large dataset of naturally-occurring conversations , a semi-supervised machine learning algorithm , and a computational analysis of the effects of self-disclosure on subsequent conversations .	CL_D14-1213_ab_1	CL_D14-1213_ab_3	CL_D14-1213_ab_1_3
CL	D14-1213	ab	1	4	information_additional	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	Self-disclosure , the act of revealing one-self to others , is an important social behavior that strengthens interpersonal relationships and increases social support .	We use a longitudinal dataset of 17 million tweets , all of which occurred in conversations that consist of five or more tweets directly replying to the previous tweet , and from dyads with twenty of more conversations each .	CL_D14-1213_ab_1	CL_D14-1213_ab_4	CL_D14-1213_ab_1_4
CL	D14-1213	ab	1	5	information_additional	proposal_implementation	info-required	sequence	secondary	secondary	none	none	Self-disclosure , the act of revealing one-self to others , is an important social behavior that strengthens interpersonal relationships and increases social support .	We develop self-disclosure topic model ( SDTM ) , a variant of latent Dirichlet allocation ( LDA ) for automatically classifying the level of self-disclosure for each tweet .	CL_D14-1213_ab_1	CL_D14-1213_ab_5	CL_D14-1213_ab_1_5
CL	D14-1213	ab	1	6	information_additional	proposal_implementation	info-required	sequence	secondary	secondary	none	none	Self-disclosure , the act of revealing one-self to others , is an important social behavior that strengthens interpersonal relationships and increases social support .	We take the results of SDTM and analyze the effects of self-disclosure on subsequent conversations .	CL_D14-1213_ab_1	CL_D14-1213_ab_6	CL_D14-1213_ab_1_6
CL	D14-1213	ab	1	7	information_additional	result	info-required	support	secondary	secondary	none	none	Self-disclosure , the act of revealing one-self to others , is an important social behavior that strengthens interpersonal relationships and increases social support .	Our model significantly outperforms several comparable methods on classifying the level of self-disclosure , and the analysis of the longitudinal data using SDTM uncovers significant and positive correlation between self-disclosure and conversation frequency and length .	CL_D14-1213_ab_1	CL_D14-1213_ab_7	CL_D14-1213_ab_1_7
CL	D14-1213	ab	2	3	motivation_problem	proposal_implementation	support	none	secondary	main	forw	support	Although there are many social science studies of self-disclosure , they are based on manual coding of small datasets and questionnaires .	We conduct a computational analysis of self-disclosure with a large dataset of naturally-occurring conversations , a semi-supervised machine learning algorithm , and a computational analysis of the effects of self-disclosure on subsequent conversations .	CL_D14-1213_ab_2	CL_D14-1213_ab_3	CL_D14-1213_ab_2_3
CL	D14-1213	ab	3	2	proposal_implementation	motivation_problem	none	support	main	secondary	back	support	We conduct a computational analysis of self-disclosure with a large dataset of naturally-occurring conversations , a semi-supervised machine learning algorithm , and a computational analysis of the effects of self-disclosure on subsequent conversations .	Although there are many social science studies of self-disclosure , they are based on manual coding of small datasets and questionnaires .	CL_D14-1213_ab_3	CL_D14-1213_ab_2	CL_D14-1213_ab_2_3
CL	D14-1213	ab	2	4	motivation_problem	proposal_implementation	support	elaboration	secondary	secondary	none	none	Although there are many social science studies of self-disclosure , they are based on manual coding of small datasets and questionnaires .	We use a longitudinal dataset of 17 million tweets , all of which occurred in conversations that consist of five or more tweets directly replying to the previous tweet , and from dyads with twenty of more conversations each .	CL_D14-1213_ab_2	CL_D14-1213_ab_4	CL_D14-1213_ab_2_4
CL	D14-1213	ab	2	5	motivation_problem	proposal_implementation	support	sequence	secondary	secondary	none	none	Although there are many social science studies of self-disclosure , they are based on manual coding of small datasets and questionnaires .	We develop self-disclosure topic model ( SDTM ) , a variant of latent Dirichlet allocation ( LDA ) for automatically classifying the level of self-disclosure for each tweet .	CL_D14-1213_ab_2	CL_D14-1213_ab_5	CL_D14-1213_ab_2_5
CL	D14-1213	ab	2	6	motivation_problem	proposal_implementation	support	sequence	secondary	secondary	none	none	Although there are many social science studies of self-disclosure , they are based on manual coding of small datasets and questionnaires .	We take the results of SDTM and analyze the effects of self-disclosure on subsequent conversations .	CL_D14-1213_ab_2	CL_D14-1213_ab_6	CL_D14-1213_ab_2_6
CL	D14-1213	ab	2	7	motivation_problem	result	support	support	secondary	secondary	none	none	Although there are many social science studies of self-disclosure , they are based on manual coding of small datasets and questionnaires .	Our model significantly outperforms several comparable methods on classifying the level of self-disclosure , and the analysis of the longitudinal data using SDTM uncovers significant and positive correlation between self-disclosure and conversation frequency and length .	CL_D14-1213_ab_2	CL_D14-1213_ab_7	CL_D14-1213_ab_2_7
CL	D14-1213	ab	3	4	proposal_implementation	proposal_implementation	none	elaboration	main	secondary	back	elaboration	We conduct a computational analysis of self-disclosure with a large dataset of naturally-occurring conversations , a semi-supervised machine learning algorithm , and a computational analysis of the effects of self-disclosure on subsequent conversations .	We use a longitudinal dataset of 17 million tweets , all of which occurred in conversations that consist of five or more tweets directly replying to the previous tweet , and from dyads with twenty of more conversations each .	CL_D14-1213_ab_3	CL_D14-1213_ab_4	CL_D14-1213_ab_3_4
CL	D14-1213	ab	4	3	proposal_implementation	proposal_implementation	elaboration	none	secondary	main	forw	elaboration	We use a longitudinal dataset of 17 million tweets , all of which occurred in conversations that consist of five or more tweets directly replying to the previous tweet , and from dyads with twenty of more conversations each .	We conduct a computational analysis of self-disclosure with a large dataset of naturally-occurring conversations , a semi-supervised machine learning algorithm , and a computational analysis of the effects of self-disclosure on subsequent conversations .	CL_D14-1213_ab_4	CL_D14-1213_ab_3	CL_D14-1213_ab_3_4
CL	D14-1213	ab	3	5	proposal_implementation	proposal_implementation	none	sequence	main	secondary	none	none	We conduct a computational analysis of self-disclosure with a large dataset of naturally-occurring conversations , a semi-supervised machine learning algorithm , and a computational analysis of the effects of self-disclosure on subsequent conversations .	We develop self-disclosure topic model ( SDTM ) , a variant of latent Dirichlet allocation ( LDA ) for automatically classifying the level of self-disclosure for each tweet .	CL_D14-1213_ab_3	CL_D14-1213_ab_5	CL_D14-1213_ab_3_5
CL	D14-1213	ab	3	6	proposal_implementation	proposal_implementation	none	sequence	main	secondary	none	none	We conduct a computational analysis of self-disclosure with a large dataset of naturally-occurring conversations , a semi-supervised machine learning algorithm , and a computational analysis of the effects of self-disclosure on subsequent conversations .	We take the results of SDTM and analyze the effects of self-disclosure on subsequent conversations .	CL_D14-1213_ab_3	CL_D14-1213_ab_6	CL_D14-1213_ab_3_6
CL	D14-1213	ab	3	7	proposal_implementation	result	none	support	main	secondary	none	none	We conduct a computational analysis of self-disclosure with a large dataset of naturally-occurring conversations , a semi-supervised machine learning algorithm , and a computational analysis of the effects of self-disclosure on subsequent conversations .	Our model significantly outperforms several comparable methods on classifying the level of self-disclosure , and the analysis of the longitudinal data using SDTM uncovers significant and positive correlation between self-disclosure and conversation frequency and length .	CL_D14-1213_ab_3	CL_D14-1213_ab_7	CL_D14-1213_ab_3_7
CL	D14-1213	ab	4	5	proposal_implementation	proposal_implementation	elaboration	sequence	secondary	secondary	back	sequence	We use a longitudinal dataset of 17 million tweets , all of which occurred in conversations that consist of five or more tweets directly replying to the previous tweet , and from dyads with twenty of more conversations each .	We develop self-disclosure topic model ( SDTM ) , a variant of latent Dirichlet allocation ( LDA ) for automatically classifying the level of self-disclosure for each tweet .	CL_D14-1213_ab_4	CL_D14-1213_ab_5	CL_D14-1213_ab_4_5
CL	D14-1213	ab	5	4	proposal_implementation	proposal_implementation	sequence	elaboration	secondary	secondary	forw	sequence	We develop self-disclosure topic model ( SDTM ) , a variant of latent Dirichlet allocation ( LDA ) for automatically classifying the level of self-disclosure for each tweet .	We use a longitudinal dataset of 17 million tweets , all of which occurred in conversations that consist of five or more tweets directly replying to the previous tweet , and from dyads with twenty of more conversations each .	CL_D14-1213_ab_5	CL_D14-1213_ab_4	CL_D14-1213_ab_4_5
CL	D14-1213	ab	4	6	proposal_implementation	proposal_implementation	elaboration	sequence	secondary	secondary	none	none	We use a longitudinal dataset of 17 million tweets , all of which occurred in conversations that consist of five or more tweets directly replying to the previous tweet , and from dyads with twenty of more conversations each .	We take the results of SDTM and analyze the effects of self-disclosure on subsequent conversations .	CL_D14-1213_ab_4	CL_D14-1213_ab_6	CL_D14-1213_ab_4_6
CL	D14-1213	ab	4	7	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	We use a longitudinal dataset of 17 million tweets , all of which occurred in conversations that consist of five or more tweets directly replying to the previous tweet , and from dyads with twenty of more conversations each .	Our model significantly outperforms several comparable methods on classifying the level of self-disclosure , and the analysis of the longitudinal data using SDTM uncovers significant and positive correlation between self-disclosure and conversation frequency and length .	CL_D14-1213_ab_4	CL_D14-1213_ab_7	CL_D14-1213_ab_4_7
CL	D14-1213	ab	5	6	proposal_implementation	proposal_implementation	sequence	sequence	secondary	secondary	back	sequence	We develop self-disclosure topic model ( SDTM ) , a variant of latent Dirichlet allocation ( LDA ) for automatically classifying the level of self-disclosure for each tweet .	We take the results of SDTM and analyze the effects of self-disclosure on subsequent conversations .	CL_D14-1213_ab_5	CL_D14-1213_ab_6	CL_D14-1213_ab_5_6
CL	D14-1213	ab	6	5	proposal_implementation	proposal_implementation	sequence	sequence	secondary	secondary	forw	sequence	We take the results of SDTM and analyze the effects of self-disclosure on subsequent conversations .	We develop self-disclosure topic model ( SDTM ) , a variant of latent Dirichlet allocation ( LDA ) for automatically classifying the level of self-disclosure for each tweet .	CL_D14-1213_ab_6	CL_D14-1213_ab_5	CL_D14-1213_ab_5_6
CL	D14-1213	ab	5	7	proposal_implementation	result	sequence	support	secondary	secondary	back	support	We develop self-disclosure topic model ( SDTM ) , a variant of latent Dirichlet allocation ( LDA ) for automatically classifying the level of self-disclosure for each tweet .	Our model significantly outperforms several comparable methods on classifying the level of self-disclosure , and the analysis of the longitudinal data using SDTM uncovers significant and positive correlation between self-disclosure and conversation frequency and length .	CL_D14-1213_ab_5	CL_D14-1213_ab_7	CL_D14-1213_ab_5_7
CL	D14-1213	ab	7	5	result	proposal_implementation	support	sequence	secondary	secondary	forw	support	Our model significantly outperforms several comparable methods on classifying the level of self-disclosure , and the analysis of the longitudinal data using SDTM uncovers significant and positive correlation between self-disclosure and conversation frequency and length .	We develop self-disclosure topic model ( SDTM ) , a variant of latent Dirichlet allocation ( LDA ) for automatically classifying the level of self-disclosure for each tweet .	CL_D14-1213_ab_7	CL_D14-1213_ab_5	CL_D14-1213_ab_5_7
CL	D14-1213	ab	6	7	proposal_implementation	result	sequence	support	secondary	secondary	none	none	We take the results of SDTM and analyze the effects of self-disclosure on subsequent conversations .	Our model significantly outperforms several comparable methods on classifying the level of self-disclosure , and the analysis of the longitudinal data using SDTM uncovers significant and positive correlation between self-disclosure and conversation frequency and length .	CL_D14-1213_ab_6	CL_D14-1213_ab_7	CL_D14-1213_ab_6_7
CL	D14-1214	ab	1	2	motivation_background	motivation_background	info-required	support	secondary	secondary	forw	info-required	Social media websites provide a platform for anyone to describe significant events taking place in their lives in realtime .	Currently , the majority of personal news and life events are published in a textual format , motivating information extraction systems that can provide a structured representations of major life events ( weddings , graduation , etc. ) .	CL_D14-1214_ab_1	CL_D14-1214_ab_2	CL_D14-1214_ab_1_2
CL	D14-1214	ab	2	1	motivation_background	motivation_background	support	info-required	secondary	secondary	back	info-required	Currently , the majority of personal news and life events are published in a textual format , motivating information extraction systems that can provide a structured representations of major life events ( weddings , graduation , etc. ) .	Social media websites provide a platform for anyone to describe significant events taking place in their lives in realtime .	CL_D14-1214_ab_2	CL_D14-1214_ab_1	CL_D14-1214_ab_1_2
CL	D14-1214	ab	1	3	motivation_background	proposal	info-required	none	secondary	main	none	none	Social media websites provide a platform for anyone to describe significant events taking place in their lives in realtime .	This paper demonstrates the feasibility of accurately extracting major life events .	CL_D14-1214_ab_1	CL_D14-1214_ab_3	CL_D14-1214_ab_1_3
CL	D14-1214	ab	1	4	motivation_background	proposal	info-required	elaboration	secondary	secondary	none	none	Social media websites provide a platform for anyone to describe significant events taking place in their lives in realtime .	Our system extracts a fine-grained description of users' life events based on their published tweets .	CL_D14-1214_ab_1	CL_D14-1214_ab_4	CL_D14-1214_ab_1_4
CL	D14-1214	ab	1	5	motivation_background	conclusion	info-required	support	secondary	secondary	none	none	Social media websites provide a platform for anyone to describe significant events taking place in their lives in realtime .	We are optimistic that our system can help Twitter users more easily grasp information from users they take interest in following and also facilitate many downstream applications , for example realtime friend recommendation .	CL_D14-1214_ab_1	CL_D14-1214_ab_5	CL_D14-1214_ab_1_5
CL	D14-1214	ab	2	3	motivation_background	proposal	support	none	secondary	main	forw	support	Currently , the majority of personal news and life events are published in a textual format , motivating information extraction systems that can provide a structured representations of major life events ( weddings , graduation , etc. ) .	This paper demonstrates the feasibility of accurately extracting major life events .	CL_D14-1214_ab_2	CL_D14-1214_ab_3	CL_D14-1214_ab_2_3
CL	D14-1214	ab	3	2	proposal	motivation_background	none	support	main	secondary	back	support	This paper demonstrates the feasibility of accurately extracting major life events .	Currently , the majority of personal news and life events are published in a textual format , motivating information extraction systems that can provide a structured representations of major life events ( weddings , graduation , etc. ) .	CL_D14-1214_ab_3	CL_D14-1214_ab_2	CL_D14-1214_ab_2_3
CL	D14-1214	ab	2	4	motivation_background	proposal	support	elaboration	secondary	secondary	none	none	Currently , the majority of personal news and life events are published in a textual format , motivating information extraction systems that can provide a structured representations of major life events ( weddings , graduation , etc. ) .	Our system extracts a fine-grained description of users' life events based on their published tweets .	CL_D14-1214_ab_2	CL_D14-1214_ab_4	CL_D14-1214_ab_2_4
CL	D14-1214	ab	2	5	motivation_background	conclusion	support	support	secondary	secondary	none	none	Currently , the majority of personal news and life events are published in a textual format , motivating information extraction systems that can provide a structured representations of major life events ( weddings , graduation , etc. ) .	We are optimistic that our system can help Twitter users more easily grasp information from users they take interest in following and also facilitate many downstream applications , for example realtime friend recommendation .	CL_D14-1214_ab_2	CL_D14-1214_ab_5	CL_D14-1214_ab_2_5
CL	D14-1214	ab	3	4	proposal	proposal	none	elaboration	main	secondary	back	elaboration	This paper demonstrates the feasibility of accurately extracting major life events .	Our system extracts a fine-grained description of users' life events based on their published tweets .	CL_D14-1214_ab_3	CL_D14-1214_ab_4	CL_D14-1214_ab_3_4
CL	D14-1214	ab	4	3	proposal	proposal	elaboration	none	secondary	main	forw	elaboration	Our system extracts a fine-grained description of users' life events based on their published tweets .	This paper demonstrates the feasibility of accurately extracting major life events .	CL_D14-1214_ab_4	CL_D14-1214_ab_3	CL_D14-1214_ab_3_4
CL	D14-1214	ab	3	5	proposal	conclusion	none	support	main	secondary	back	support	This paper demonstrates the feasibility of accurately extracting major life events .	We are optimistic that our system can help Twitter users more easily grasp information from users they take interest in following and also facilitate many downstream applications , for example realtime friend recommendation .	CL_D14-1214_ab_3	CL_D14-1214_ab_5	CL_D14-1214_ab_3_5
CL	D14-1214	ab	5	3	conclusion	proposal	support	none	secondary	main	forw	support	We are optimistic that our system can help Twitter users more easily grasp information from users they take interest in following and also facilitate many downstream applications , for example realtime friend recommendation .	This paper demonstrates the feasibility of accurately extracting major life events .	CL_D14-1214_ab_5	CL_D14-1214_ab_3	CL_D14-1214_ab_3_5
CL	D14-1214	ab	4	5	proposal	conclusion	elaboration	support	secondary	secondary	none	none	Our system extracts a fine-grained description of users' life events based on their published tweets .	We are optimistic that our system can help Twitter users more easily grasp information from users they take interest in following and also facilitate many downstream applications , for example realtime friend recommendation .	CL_D14-1214_ab_4	CL_D14-1214_ab_5	CL_D14-1214_ab_4_5
CL	D14-1215	ab	1	2	information_additional	motivation_hypothesis	info-required	support	secondary	secondary	forw	info-required	Comparisons are common linguistic devices used to indicate the likeness of two things .	Often , this likeness is not meant in the literal sense for example , I slept like a log does not imply that logs actually sleep .	CL_D14-1215_ab_1	CL_D14-1215_ab_2	CL_D14-1215_ab_1_2
CL	D14-1215	ab	2	1	motivation_hypothesis	information_additional	support	info-required	secondary	secondary	back	info-required	Often , this likeness is not meant in the literal sense for example , I slept like a log does not imply that logs actually sleep .	Comparisons are common linguistic devices used to indicate the likeness of two things .	CL_D14-1215_ab_2	CL_D14-1215_ab_1	CL_D14-1215_ab_1_2
CL	D14-1215	ab	1	3	information_additional	proposal	info-required	none	secondary	main	none	none	Comparisons are common linguistic devices used to indicate the likeness of two things .	In this paper we propose a computational study of figurative comparisons , or similes .	CL_D14-1215_ab_1	CL_D14-1215_ab_3	CL_D14-1215_ab_1_3
CL	D14-1215	ab	1	4	information_additional	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	Comparisons are common linguistic devices used to indicate the likeness of two things .	Our starting point is a new large dataset of comparisons extracted from product reviews and annotated for figurativeness .	CL_D14-1215_ab_1	CL_D14-1215_ab_4	CL_D14-1215_ab_1_4
CL	D14-1215	ab	1	5	information_additional	proposal_implementation	info-required	sequence	secondary	secondary	none	none	Comparisons are common linguistic devices used to indicate the likeness of two things .	We use this dataset to characterize figurative language in naturally occurring comparisons and reveal linguistic patterns indicative of this phenomenon .	CL_D14-1215_ab_1	CL_D14-1215_ab_5	CL_D14-1215_ab_1_5
CL	D14-1215	ab	1	6	information_additional	proposal_implementation	info-required	sequence	secondary	secondary	none	none	Comparisons are common linguistic devices used to indicate the likeness of two things .	We operationalize these insights and apply them to a new task with high relevance to text understanding : distinguishing between figurative and literal comparisons .	CL_D14-1215_ab_1	CL_D14-1215_ab_6	CL_D14-1215_ab_1_6
CL	D14-1215	ab	1	7	information_additional	proposal_implementation	info-required	sequence	secondary	secondary	none	none	Comparisons are common linguistic devices used to indicate the likeness of two things .	Finally , we apply this framework to explore the social context in which figurative language is produced , showing that similes are more likely to accompany opinions showing extreme sentiment , and that they are uncommon in reviews deemed helpful .	CL_D14-1215_ab_1	CL_D14-1215_ab_7	CL_D14-1215_ab_1_7
CL	D14-1215	ab	2	3	motivation_hypothesis	proposal	support	none	secondary	main	forw	support	Often , this likeness is not meant in the literal sense for example , I slept like a log does not imply that logs actually sleep .	In this paper we propose a computational study of figurative comparisons , or similes .	CL_D14-1215_ab_2	CL_D14-1215_ab_3	CL_D14-1215_ab_2_3
CL	D14-1215	ab	3	2	proposal	motivation_hypothesis	none	support	main	secondary	back	support	In this paper we propose a computational study of figurative comparisons , or similes .	Often , this likeness is not meant in the literal sense for example , I slept like a log does not imply that logs actually sleep .	CL_D14-1215_ab_3	CL_D14-1215_ab_2	CL_D14-1215_ab_2_3
CL	D14-1215	ab	2	4	motivation_hypothesis	proposal_implementation	support	elaboration	secondary	secondary	none	none	Often , this likeness is not meant in the literal sense for example , I slept like a log does not imply that logs actually sleep .	Our starting point is a new large dataset of comparisons extracted from product reviews and annotated for figurativeness .	CL_D14-1215_ab_2	CL_D14-1215_ab_4	CL_D14-1215_ab_2_4
CL	D14-1215	ab	2	5	motivation_hypothesis	proposal_implementation	support	sequence	secondary	secondary	none	none	Often , this likeness is not meant in the literal sense for example , I slept like a log does not imply that logs actually sleep .	We use this dataset to characterize figurative language in naturally occurring comparisons and reveal linguistic patterns indicative of this phenomenon .	CL_D14-1215_ab_2	CL_D14-1215_ab_5	CL_D14-1215_ab_2_5
CL	D14-1215	ab	2	6	motivation_hypothesis	proposal_implementation	support	sequence	secondary	secondary	none	none	Often , this likeness is not meant in the literal sense for example , I slept like a log does not imply that logs actually sleep .	We operationalize these insights and apply them to a new task with high relevance to text understanding : distinguishing between figurative and literal comparisons .	CL_D14-1215_ab_2	CL_D14-1215_ab_6	CL_D14-1215_ab_2_6
CL	D14-1215	ab	2	7	motivation_hypothesis	proposal_implementation	support	sequence	secondary	secondary	none	none	Often , this likeness is not meant in the literal sense for example , I slept like a log does not imply that logs actually sleep .	Finally , we apply this framework to explore the social context in which figurative language is produced , showing that similes are more likely to accompany opinions showing extreme sentiment , and that they are uncommon in reviews deemed helpful .	CL_D14-1215_ab_2	CL_D14-1215_ab_7	CL_D14-1215_ab_2_7
CL	D14-1215	ab	3	4	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	In this paper we propose a computational study of figurative comparisons , or similes .	Our starting point is a new large dataset of comparisons extracted from product reviews and annotated for figurativeness .	CL_D14-1215_ab_3	CL_D14-1215_ab_4	CL_D14-1215_ab_3_4
CL	D14-1215	ab	4	3	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	Our starting point is a new large dataset of comparisons extracted from product reviews and annotated for figurativeness .	In this paper we propose a computational study of figurative comparisons , or similes .	CL_D14-1215_ab_4	CL_D14-1215_ab_3	CL_D14-1215_ab_3_4
CL	D14-1215	ab	3	5	proposal	proposal_implementation	none	sequence	main	secondary	none	none	In this paper we propose a computational study of figurative comparisons , or similes .	We use this dataset to characterize figurative language in naturally occurring comparisons and reveal linguistic patterns indicative of this phenomenon .	CL_D14-1215_ab_3	CL_D14-1215_ab_5	CL_D14-1215_ab_3_5
CL	D14-1215	ab	3	6	proposal	proposal_implementation	none	sequence	main	secondary	none	none	In this paper we propose a computational study of figurative comparisons , or similes .	We operationalize these insights and apply them to a new task with high relevance to text understanding : distinguishing between figurative and literal comparisons .	CL_D14-1215_ab_3	CL_D14-1215_ab_6	CL_D14-1215_ab_3_6
CL	D14-1215	ab	3	7	proposal	proposal_implementation	none	sequence	main	secondary	none	none	In this paper we propose a computational study of figurative comparisons , or similes .	Finally , we apply this framework to explore the social context in which figurative language is produced , showing that similes are more likely to accompany opinions showing extreme sentiment , and that they are uncommon in reviews deemed helpful .	CL_D14-1215_ab_3	CL_D14-1215_ab_7	CL_D14-1215_ab_3_7
CL	D14-1215	ab	4	5	proposal_implementation	proposal_implementation	elaboration	sequence	secondary	secondary	back	sequence	Our starting point is a new large dataset of comparisons extracted from product reviews and annotated for figurativeness .	We use this dataset to characterize figurative language in naturally occurring comparisons and reveal linguistic patterns indicative of this phenomenon .	CL_D14-1215_ab_4	CL_D14-1215_ab_5	CL_D14-1215_ab_4_5
CL	D14-1215	ab	5	4	proposal_implementation	proposal_implementation	sequence	elaboration	secondary	secondary	forw	sequence	We use this dataset to characterize figurative language in naturally occurring comparisons and reveal linguistic patterns indicative of this phenomenon .	Our starting point is a new large dataset of comparisons extracted from product reviews and annotated for figurativeness .	CL_D14-1215_ab_5	CL_D14-1215_ab_4	CL_D14-1215_ab_4_5
CL	D14-1215	ab	4	6	proposal_implementation	proposal_implementation	elaboration	sequence	secondary	secondary	none	none	Our starting point is a new large dataset of comparisons extracted from product reviews and annotated for figurativeness .	We operationalize these insights and apply them to a new task with high relevance to text understanding : distinguishing between figurative and literal comparisons .	CL_D14-1215_ab_4	CL_D14-1215_ab_6	CL_D14-1215_ab_4_6
CL	D14-1215	ab	4	7	proposal_implementation	proposal_implementation	elaboration	sequence	secondary	secondary	none	none	Our starting point is a new large dataset of comparisons extracted from product reviews and annotated for figurativeness .	Finally , we apply this framework to explore the social context in which figurative language is produced , showing that similes are more likely to accompany opinions showing extreme sentiment , and that they are uncommon in reviews deemed helpful .	CL_D14-1215_ab_4	CL_D14-1215_ab_7	CL_D14-1215_ab_4_7
CL	D14-1215	ab	5	6	proposal_implementation	proposal_implementation	sequence	sequence	secondary	secondary	back	sequence	We use this dataset to characterize figurative language in naturally occurring comparisons and reveal linguistic patterns indicative of this phenomenon .	We operationalize these insights and apply them to a new task with high relevance to text understanding : distinguishing between figurative and literal comparisons .	CL_D14-1215_ab_5	CL_D14-1215_ab_6	CL_D14-1215_ab_5_6
CL	D14-1215	ab	6	5	proposal_implementation	proposal_implementation	sequence	sequence	secondary	secondary	forw	sequence	We operationalize these insights and apply them to a new task with high relevance to text understanding : distinguishing between figurative and literal comparisons .	We use this dataset to characterize figurative language in naturally occurring comparisons and reveal linguistic patterns indicative of this phenomenon .	CL_D14-1215_ab_6	CL_D14-1215_ab_5	CL_D14-1215_ab_5_6
CL	D14-1215	ab	5	7	proposal_implementation	proposal_implementation	sequence	sequence	secondary	secondary	none	none	We use this dataset to characterize figurative language in naturally occurring comparisons and reveal linguistic patterns indicative of this phenomenon .	Finally , we apply this framework to explore the social context in which figurative language is produced , showing that similes are more likely to accompany opinions showing extreme sentiment , and that they are uncommon in reviews deemed helpful .	CL_D14-1215_ab_5	CL_D14-1215_ab_7	CL_D14-1215_ab_5_7
CL	D14-1215	ab	6	7	proposal_implementation	proposal_implementation	sequence	sequence	secondary	secondary	back	sequence	We operationalize these insights and apply them to a new task with high relevance to text understanding : distinguishing between figurative and literal comparisons .	Finally , we apply this framework to explore the social context in which figurative language is produced , showing that similes are more likely to accompany opinions showing extreme sentiment , and that they are uncommon in reviews deemed helpful .	CL_D14-1215_ab_6	CL_D14-1215_ab_7	CL_D14-1215_ab_6_7
CL	D14-1215	ab	7	6	proposal_implementation	proposal_implementation	sequence	sequence	secondary	secondary	forw	sequence	Finally , we apply this framework to explore the social context in which figurative language is produced , showing that similes are more likely to accompany opinions showing extreme sentiment , and that they are uncommon in reviews deemed helpful .	We operationalize these insights and apply them to a new task with high relevance to text understanding : distinguishing between figurative and literal comparisons .	CL_D14-1215_ab_7	CL_D14-1215_ab_6	CL_D14-1215_ab_6_7
CL	D14-1216	ab	1	2	proposal	motivation_hypothesis	none	support	main	secondary	back	support	We describe an algorithm for automatic classification of idiomatic and literal expressions .	Our starting point is that words in a given text segment , such as a paragraph , that are high-ranking representatives of a common topic of discussion are less likely to be a part of an idiomatic expression .	CL_D14-1216_ab_1	CL_D14-1216_ab_2	CL_D14-1216_ab_1_2
CL	D14-1216	ab	2	1	motivation_hypothesis	proposal	support	none	secondary	main	forw	support	Our starting point is that words in a given text segment , such as a paragraph , that are high-ranking representatives of a common topic of discussion are less likely to be a part of an idiomatic expression .	We describe an algorithm for automatic classification of idiomatic and literal expressions .	CL_D14-1216_ab_2	CL_D14-1216_ab_1	CL_D14-1216_ab_1_2
CL	D14-1216	ab	1	3	proposal	motivation_hypothesis	none	support	main	secondary	back	support	We describe an algorithm for automatic classification of idiomatic and literal expressions .	Our additional hypothesis is that contexts in which idioms occur , typically , are more affective and therefore , we incorporate a simple analysis of the intensity of the emotions expressed by the contexts .	CL_D14-1216_ab_1	CL_D14-1216_ab_3	CL_D14-1216_ab_1_3
CL	D14-1216	ab	3	1	motivation_hypothesis	proposal	support	none	secondary	main	forw	support	Our additional hypothesis is that contexts in which idioms occur , typically , are more affective and therefore , we incorporate a simple analysis of the intensity of the emotions expressed by the contexts .	We describe an algorithm for automatic classification of idiomatic and literal expressions .	CL_D14-1216_ab_3	CL_D14-1216_ab_1	CL_D14-1216_ab_1_3
CL	D14-1216	ab	1	4	proposal	proposal	none	elaboration	main	secondary	back	elaboration	We describe an algorithm for automatic classification of idiomatic and literal expressions .	We investigate the bag of words topic representation of one to three paragraphs containing an expression that should be classified as idiomatic or literal ( a target phrase ) .	CL_D14-1216_ab_1	CL_D14-1216_ab_4	CL_D14-1216_ab_1_4
CL	D14-1216	ab	4	1	proposal	proposal	elaboration	none	secondary	main	forw	elaboration	We investigate the bag of words topic representation of one to three paragraphs containing an expression that should be classified as idiomatic or literal ( a target phrase ) .	We describe an algorithm for automatic classification of idiomatic and literal expressions .	CL_D14-1216_ab_4	CL_D14-1216_ab_1	CL_D14-1216_ab_1_4
CL	D14-1216	ab	1	5	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	We describe an algorithm for automatic classification of idiomatic and literal expressions .	We extract topics from paragraphs containing idioms and from paragraphs containing literals using an unsupervised clustering method , Latent Dirichlet Allocation ( LDA ) ( Blei et al. , 2003 ) .	CL_D14-1216_ab_1	CL_D14-1216_ab_5	CL_D14-1216_ab_1_5
CL	D14-1216	ab	1	6	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	We describe an algorithm for automatic classification of idiomatic and literal expressions .	Since idiomatic expressions exhibit the property of non-compositionality , we assume that they usually present different semantics than the words used in the local topic .	CL_D14-1216_ab_1	CL_D14-1216_ab_6	CL_D14-1216_ab_1_6
CL	D14-1216	ab	1	7	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	We describe an algorithm for automatic classification of idiomatic and literal expressions .	We treat idioms as semantic outliers , and the identification of a semantic shift as outlier detection .	CL_D14-1216_ab_1	CL_D14-1216_ab_7	CL_D14-1216_ab_1_7
CL	D14-1216	ab	1	8	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	We describe an algorithm for automatic classification of idiomatic and literal expressions .	Thus , this topic representation allows us to differentiate idioms from literals using local semantic contexts .	CL_D14-1216_ab_1	CL_D14-1216_ab_8	CL_D14-1216_ab_1_8
CL	D14-1216	ab	1	9	proposal	result	none	support	main	secondary	back	support	We describe an algorithm for automatic classification of idiomatic and literal expressions .	Our results are encouraging .	CL_D14-1216_ab_1	CL_D14-1216_ab_9	CL_D14-1216_ab_1_9
CL	D14-1216	ab	9	1	result	proposal	support	none	secondary	main	forw	support	Our results are encouraging .	We describe an algorithm for automatic classification of idiomatic and literal expressions .	CL_D14-1216_ab_9	CL_D14-1216_ab_1	CL_D14-1216_ab_1_9
CL	D14-1216	ab	2	3	motivation_hypothesis	motivation_hypothesis	support	support	secondary	secondary	none	none	Our starting point is that words in a given text segment , such as a paragraph , that are high-ranking representatives of a common topic of discussion are less likely to be a part of an idiomatic expression .	Our additional hypothesis is that contexts in which idioms occur , typically , are more affective and therefore , we incorporate a simple analysis of the intensity of the emotions expressed by the contexts .	CL_D14-1216_ab_2	CL_D14-1216_ab_3	CL_D14-1216_ab_2_3
CL	D14-1216	ab	2	4	motivation_hypothesis	proposal	support	elaboration	secondary	secondary	none	none	Our starting point is that words in a given text segment , such as a paragraph , that are high-ranking representatives of a common topic of discussion are less likely to be a part of an idiomatic expression .	We investigate the bag of words topic representation of one to three paragraphs containing an expression that should be classified as idiomatic or literal ( a target phrase ) .	CL_D14-1216_ab_2	CL_D14-1216_ab_4	CL_D14-1216_ab_2_4
CL	D14-1216	ab	2	5	motivation_hypothesis	proposal_implementation	support	elaboration	secondary	secondary	none	none	Our starting point is that words in a given text segment , such as a paragraph , that are high-ranking representatives of a common topic of discussion are less likely to be a part of an idiomatic expression .	We extract topics from paragraphs containing idioms and from paragraphs containing literals using an unsupervised clustering method , Latent Dirichlet Allocation ( LDA ) ( Blei et al. , 2003 ) .	CL_D14-1216_ab_2	CL_D14-1216_ab_5	CL_D14-1216_ab_2_5
CL	D14-1216	ab	2	6	motivation_hypothesis	proposal_implementation	support	elaboration	secondary	secondary	none	none	Our starting point is that words in a given text segment , such as a paragraph , that are high-ranking representatives of a common topic of discussion are less likely to be a part of an idiomatic expression .	Since idiomatic expressions exhibit the property of non-compositionality , we assume that they usually present different semantics than the words used in the local topic .	CL_D14-1216_ab_2	CL_D14-1216_ab_6	CL_D14-1216_ab_2_6
CL	D14-1216	ab	2	7	motivation_hypothesis	proposal_implementation	support	elaboration	secondary	secondary	none	none	Our starting point is that words in a given text segment , such as a paragraph , that are high-ranking representatives of a common topic of discussion are less likely to be a part of an idiomatic expression .	We treat idioms as semantic outliers , and the identification of a semantic shift as outlier detection .	CL_D14-1216_ab_2	CL_D14-1216_ab_7	CL_D14-1216_ab_2_7
CL	D14-1216	ab	2	8	motivation_hypothesis	proposal_implementation	support	elaboration	secondary	secondary	none	none	Our starting point is that words in a given text segment , such as a paragraph , that are high-ranking representatives of a common topic of discussion are less likely to be a part of an idiomatic expression .	Thus , this topic representation allows us to differentiate idioms from literals using local semantic contexts .	CL_D14-1216_ab_2	CL_D14-1216_ab_8	CL_D14-1216_ab_2_8
CL	D14-1216	ab	2	9	motivation_hypothesis	result	support	support	secondary	secondary	none	none	Our starting point is that words in a given text segment , such as a paragraph , that are high-ranking representatives of a common topic of discussion are less likely to be a part of an idiomatic expression .	Our results are encouraging .	CL_D14-1216_ab_2	CL_D14-1216_ab_9	CL_D14-1216_ab_2_9
CL	D14-1216	ab	3	4	motivation_hypothesis	proposal	support	elaboration	secondary	secondary	none	none	Our additional hypothesis is that contexts in which idioms occur , typically , are more affective and therefore , we incorporate a simple analysis of the intensity of the emotions expressed by the contexts .	We investigate the bag of words topic representation of one to three paragraphs containing an expression that should be classified as idiomatic or literal ( a target phrase ) .	CL_D14-1216_ab_3	CL_D14-1216_ab_4	CL_D14-1216_ab_3_4
CL	D14-1216	ab	3	5	motivation_hypothesis	proposal_implementation	support	elaboration	secondary	secondary	none	none	Our additional hypothesis is that contexts in which idioms occur , typically , are more affective and therefore , we incorporate a simple analysis of the intensity of the emotions expressed by the contexts .	We extract topics from paragraphs containing idioms and from paragraphs containing literals using an unsupervised clustering method , Latent Dirichlet Allocation ( LDA ) ( Blei et al. , 2003 ) .	CL_D14-1216_ab_3	CL_D14-1216_ab_5	CL_D14-1216_ab_3_5
CL	D14-1216	ab	3	6	motivation_hypothesis	proposal_implementation	support	elaboration	secondary	secondary	none	none	Our additional hypothesis is that contexts in which idioms occur , typically , are more affective and therefore , we incorporate a simple analysis of the intensity of the emotions expressed by the contexts .	Since idiomatic expressions exhibit the property of non-compositionality , we assume that they usually present different semantics than the words used in the local topic .	CL_D14-1216_ab_3	CL_D14-1216_ab_6	CL_D14-1216_ab_3_6
CL	D14-1216	ab	3	7	motivation_hypothesis	proposal_implementation	support	elaboration	secondary	secondary	none	none	Our additional hypothesis is that contexts in which idioms occur , typically , are more affective and therefore , we incorporate a simple analysis of the intensity of the emotions expressed by the contexts .	We treat idioms as semantic outliers , and the identification of a semantic shift as outlier detection .	CL_D14-1216_ab_3	CL_D14-1216_ab_7	CL_D14-1216_ab_3_7
CL	D14-1216	ab	3	8	motivation_hypothesis	proposal_implementation	support	elaboration	secondary	secondary	none	none	Our additional hypothesis is that contexts in which idioms occur , typically , are more affective and therefore , we incorporate a simple analysis of the intensity of the emotions expressed by the contexts .	Thus , this topic representation allows us to differentiate idioms from literals using local semantic contexts .	CL_D14-1216_ab_3	CL_D14-1216_ab_8	CL_D14-1216_ab_3_8
CL	D14-1216	ab	3	9	motivation_hypothesis	result	support	support	secondary	secondary	none	none	Our additional hypothesis is that contexts in which idioms occur , typically , are more affective and therefore , we incorporate a simple analysis of the intensity of the emotions expressed by the contexts .	Our results are encouraging .	CL_D14-1216_ab_3	CL_D14-1216_ab_9	CL_D14-1216_ab_3_9
CL	D14-1216	ab	4	5	proposal	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	We investigate the bag of words topic representation of one to three paragraphs containing an expression that should be classified as idiomatic or literal ( a target phrase ) .	We extract topics from paragraphs containing idioms and from paragraphs containing literals using an unsupervised clustering method , Latent Dirichlet Allocation ( LDA ) ( Blei et al. , 2003 ) .	CL_D14-1216_ab_4	CL_D14-1216_ab_5	CL_D14-1216_ab_4_5
CL	D14-1216	ab	5	4	proposal_implementation	proposal	elaboration	elaboration	secondary	secondary	forw	elaboration	We extract topics from paragraphs containing idioms and from paragraphs containing literals using an unsupervised clustering method , Latent Dirichlet Allocation ( LDA ) ( Blei et al. , 2003 ) .	We investigate the bag of words topic representation of one to three paragraphs containing an expression that should be classified as idiomatic or literal ( a target phrase ) .	CL_D14-1216_ab_5	CL_D14-1216_ab_4	CL_D14-1216_ab_4_5
CL	D14-1216	ab	4	6	proposal	proposal_implementation	elaboration	elaboration	secondary	secondary	none	none	We investigate the bag of words topic representation of one to three paragraphs containing an expression that should be classified as idiomatic or literal ( a target phrase ) .	Since idiomatic expressions exhibit the property of non-compositionality , we assume that they usually present different semantics than the words used in the local topic .	CL_D14-1216_ab_4	CL_D14-1216_ab_6	CL_D14-1216_ab_4_6
CL	D14-1216	ab	4	7	proposal	proposal_implementation	elaboration	elaboration	secondary	secondary	none	none	We investigate the bag of words topic representation of one to three paragraphs containing an expression that should be classified as idiomatic or literal ( a target phrase ) .	We treat idioms as semantic outliers , and the identification of a semantic shift as outlier detection .	CL_D14-1216_ab_4	CL_D14-1216_ab_7	CL_D14-1216_ab_4_7
CL	D14-1216	ab	4	8	proposal	proposal_implementation	elaboration	elaboration	secondary	secondary	none	none	We investigate the bag of words topic representation of one to three paragraphs containing an expression that should be classified as idiomatic or literal ( a target phrase ) .	Thus , this topic representation allows us to differentiate idioms from literals using local semantic contexts .	CL_D14-1216_ab_4	CL_D14-1216_ab_8	CL_D14-1216_ab_4_8
CL	D14-1216	ab	4	9	proposal	result	elaboration	support	secondary	secondary	none	none	We investigate the bag of words topic representation of one to three paragraphs containing an expression that should be classified as idiomatic or literal ( a target phrase ) .	Our results are encouraging .	CL_D14-1216_ab_4	CL_D14-1216_ab_9	CL_D14-1216_ab_4_9
CL	D14-1216	ab	5	6	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	We extract topics from paragraphs containing idioms and from paragraphs containing literals using an unsupervised clustering method , Latent Dirichlet Allocation ( LDA ) ( Blei et al. , 2003 ) .	Since idiomatic expressions exhibit the property of non-compositionality , we assume that they usually present different semantics than the words used in the local topic .	CL_D14-1216_ab_5	CL_D14-1216_ab_6	CL_D14-1216_ab_5_6
CL	D14-1216	ab	6	5	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	forw	elaboration	Since idiomatic expressions exhibit the property of non-compositionality , we assume that they usually present different semantics than the words used in the local topic .	We extract topics from paragraphs containing idioms and from paragraphs containing literals using an unsupervised clustering method , Latent Dirichlet Allocation ( LDA ) ( Blei et al. , 2003 ) .	CL_D14-1216_ab_6	CL_D14-1216_ab_5	CL_D14-1216_ab_5_6
CL	D14-1216	ab	5	7	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	none	none	We extract topics from paragraphs containing idioms and from paragraphs containing literals using an unsupervised clustering method , Latent Dirichlet Allocation ( LDA ) ( Blei et al. , 2003 ) .	We treat idioms as semantic outliers , and the identification of a semantic shift as outlier detection .	CL_D14-1216_ab_5	CL_D14-1216_ab_7	CL_D14-1216_ab_5_7
CL	D14-1216	ab	5	8	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	none	none	We extract topics from paragraphs containing idioms and from paragraphs containing literals using an unsupervised clustering method , Latent Dirichlet Allocation ( LDA ) ( Blei et al. , 2003 ) .	Thus , this topic representation allows us to differentiate idioms from literals using local semantic contexts .	CL_D14-1216_ab_5	CL_D14-1216_ab_8	CL_D14-1216_ab_5_8
CL	D14-1216	ab	5	9	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	We extract topics from paragraphs containing idioms and from paragraphs containing literals using an unsupervised clustering method , Latent Dirichlet Allocation ( LDA ) ( Blei et al. , 2003 ) .	Our results are encouraging .	CL_D14-1216_ab_5	CL_D14-1216_ab_9	CL_D14-1216_ab_5_9
CL	D14-1216	ab	6	7	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	Since idiomatic expressions exhibit the property of non-compositionality , we assume that they usually present different semantics than the words used in the local topic .	We treat idioms as semantic outliers , and the identification of a semantic shift as outlier detection .	CL_D14-1216_ab_6	CL_D14-1216_ab_7	CL_D14-1216_ab_6_7
CL	D14-1216	ab	7	6	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	forw	elaboration	We treat idioms as semantic outliers , and the identification of a semantic shift as outlier detection .	Since idiomatic expressions exhibit the property of non-compositionality , we assume that they usually present different semantics than the words used in the local topic .	CL_D14-1216_ab_7	CL_D14-1216_ab_6	CL_D14-1216_ab_6_7
CL	D14-1216	ab	6	8	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	none	none	Since idiomatic expressions exhibit the property of non-compositionality , we assume that they usually present different semantics than the words used in the local topic .	Thus , this topic representation allows us to differentiate idioms from literals using local semantic contexts .	CL_D14-1216_ab_6	CL_D14-1216_ab_8	CL_D14-1216_ab_6_8
CL	D14-1216	ab	6	9	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	Since idiomatic expressions exhibit the property of non-compositionality , we assume that they usually present different semantics than the words used in the local topic .	Our results are encouraging .	CL_D14-1216_ab_6	CL_D14-1216_ab_9	CL_D14-1216_ab_6_9
CL	D14-1216	ab	7	8	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	We treat idioms as semantic outliers , and the identification of a semantic shift as outlier detection .	Thus , this topic representation allows us to differentiate idioms from literals using local semantic contexts .	CL_D14-1216_ab_7	CL_D14-1216_ab_8	CL_D14-1216_ab_7_8
CL	D14-1216	ab	8	7	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	forw	elaboration	Thus , this topic representation allows us to differentiate idioms from literals using local semantic contexts .	We treat idioms as semantic outliers , and the identification of a semantic shift as outlier detection .	CL_D14-1216_ab_8	CL_D14-1216_ab_7	CL_D14-1216_ab_7_8
CL	D14-1216	ab	7	9	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	We treat idioms as semantic outliers , and the identification of a semantic shift as outlier detection .	Our results are encouraging .	CL_D14-1216_ab_7	CL_D14-1216_ab_9	CL_D14-1216_ab_7_9
CL	D14-1216	ab	8	9	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	Thus , this topic representation allows us to differentiate idioms from literals using local semantic contexts .	Our results are encouraging .	CL_D14-1216_ab_8	CL_D14-1216_ab_9	CL_D14-1216_ab_8_9
CL	D14-1217	ab	1	2	proposal	proposal	elaboration	elaboration	secondary	secondary	back	elaboration	We address the grounding of natural language to concrete spatial constraints , and inference of implicit pragmatics in 3D environments .	We apply our approach to the task of text-to-3D scene generation .	CL_D14-1217_ab_1	CL_D14-1217_ab_2	CL_D14-1217_ab_1_2
CL	D14-1217	ab	2	1	proposal	proposal	elaboration	elaboration	secondary	secondary	forw	elaboration	We apply our approach to the task of text-to-3D scene generation .	We address the grounding of natural language to concrete spatial constraints , and inference of implicit pragmatics in 3D environments .	CL_D14-1217_ab_2	CL_D14-1217_ab_1	CL_D14-1217_ab_1_2
CL	D14-1217	ab	1	3	proposal	proposal	elaboration	none	secondary	main	forw	elaboration	We address the grounding of natural language to concrete spatial constraints , and inference of implicit pragmatics in 3D environments .	We present a representation for common sense spatial knowledge and an approach to extract it from 3D scene data .	CL_D14-1217_ab_1	CL_D14-1217_ab_3	CL_D14-1217_ab_1_3
CL	D14-1217	ab	3	1	proposal	proposal	none	elaboration	main	secondary	back	elaboration	We present a representation for common sense spatial knowledge and an approach to extract it from 3D scene data .	We address the grounding of natural language to concrete spatial constraints , and inference of implicit pragmatics in 3D environments .	CL_D14-1217_ab_3	CL_D14-1217_ab_1	CL_D14-1217_ab_1_3
CL	D14-1217	ab	1	4	proposal	information_additional	elaboration	info-optional	secondary	secondary	none	none	We address the grounding of natural language to concrete spatial constraints , and inference of implicit pragmatics in 3D environments .	In text-to-3D scene generation , a user provides as input natural language text from which we extract explicit constraints on the objects that should appear in the scene .	CL_D14-1217_ab_1	CL_D14-1217_ab_4	CL_D14-1217_ab_1_4
CL	D14-1217	ab	1	5	proposal	conclusion	elaboration	support	secondary	secondary	none	none	We address the grounding of natural language to concrete spatial constraints , and inference of implicit pragmatics in 3D environments .	The main innovation of this work is to show how to augment these explicit constraints with learned spatial knowledge to infer missing objects and likely layouts for the objects in the scene .	CL_D14-1217_ab_1	CL_D14-1217_ab_5	CL_D14-1217_ab_1_5
CL	D14-1217	ab	1	6	proposal	conclusion	elaboration	elaboration	secondary	secondary	none	none	We address the grounding of natural language to concrete spatial constraints , and inference of implicit pragmatics in 3D environments .	We demonstrate that spatial knowledge is useful for interpreting natural language and show examples of learned knowledge and generated 3D scenes .	CL_D14-1217_ab_1	CL_D14-1217_ab_6	CL_D14-1217_ab_1_6
CL	D14-1217	ab	2	3	proposal	proposal	elaboration	none	secondary	main	none	none	We apply our approach to the task of text-to-3D scene generation .	We present a representation for common sense spatial knowledge and an approach to extract it from 3D scene data .	CL_D14-1217_ab_2	CL_D14-1217_ab_3	CL_D14-1217_ab_2_3
CL	D14-1217	ab	2	4	proposal	information_additional	elaboration	info-optional	secondary	secondary	back	info-optional	We apply our approach to the task of text-to-3D scene generation .	In text-to-3D scene generation , a user provides as input natural language text from which we extract explicit constraints on the objects that should appear in the scene .	CL_D14-1217_ab_2	CL_D14-1217_ab_4	CL_D14-1217_ab_2_4
CL	D14-1217	ab	4	2	information_additional	proposal	info-optional	elaboration	secondary	secondary	forw	info-optional	In text-to-3D scene generation , a user provides as input natural language text from which we extract explicit constraints on the objects that should appear in the scene .	We apply our approach to the task of text-to-3D scene generation .	CL_D14-1217_ab_4	CL_D14-1217_ab_2	CL_D14-1217_ab_2_4
CL	D14-1217	ab	2	5	proposal	conclusion	elaboration	support	secondary	secondary	none	none	We apply our approach to the task of text-to-3D scene generation .	The main innovation of this work is to show how to augment these explicit constraints with learned spatial knowledge to infer missing objects and likely layouts for the objects in the scene .	CL_D14-1217_ab_2	CL_D14-1217_ab_5	CL_D14-1217_ab_2_5
CL	D14-1217	ab	2	6	proposal	conclusion	elaboration	elaboration	secondary	secondary	none	none	We apply our approach to the task of text-to-3D scene generation .	We demonstrate that spatial knowledge is useful for interpreting natural language and show examples of learned knowledge and generated 3D scenes .	CL_D14-1217_ab_2	CL_D14-1217_ab_6	CL_D14-1217_ab_2_6
CL	D14-1217	ab	3	4	proposal	information_additional	none	info-optional	main	secondary	none	none	We present a representation for common sense spatial knowledge and an approach to extract it from 3D scene data .	In text-to-3D scene generation , a user provides as input natural language text from which we extract explicit constraints on the objects that should appear in the scene .	CL_D14-1217_ab_3	CL_D14-1217_ab_4	CL_D14-1217_ab_3_4
CL	D14-1217	ab	3	5	proposal	conclusion	none	support	main	secondary	back	support	We present a representation for common sense spatial knowledge and an approach to extract it from 3D scene data .	The main innovation of this work is to show how to augment these explicit constraints with learned spatial knowledge to infer missing objects and likely layouts for the objects in the scene .	CL_D14-1217_ab_3	CL_D14-1217_ab_5	CL_D14-1217_ab_3_5
CL	D14-1217	ab	5	3	conclusion	proposal	support	none	secondary	main	forw	support	The main innovation of this work is to show how to augment these explicit constraints with learned spatial knowledge to infer missing objects and likely layouts for the objects in the scene .	We present a representation for common sense spatial knowledge and an approach to extract it from 3D scene data .	CL_D14-1217_ab_5	CL_D14-1217_ab_3	CL_D14-1217_ab_3_5
CL	D14-1217	ab	3	6	proposal	conclusion	none	elaboration	main	secondary	none	none	We present a representation for common sense spatial knowledge and an approach to extract it from 3D scene data .	We demonstrate that spatial knowledge is useful for interpreting natural language and show examples of learned knowledge and generated 3D scenes .	CL_D14-1217_ab_3	CL_D14-1217_ab_6	CL_D14-1217_ab_3_6
CL	D14-1217	ab	4	5	information_additional	conclusion	info-optional	support	secondary	secondary	none	none	In text-to-3D scene generation , a user provides as input natural language text from which we extract explicit constraints on the objects that should appear in the scene .	The main innovation of this work is to show how to augment these explicit constraints with learned spatial knowledge to infer missing objects and likely layouts for the objects in the scene .	CL_D14-1217_ab_4	CL_D14-1217_ab_5	CL_D14-1217_ab_4_5
CL	D14-1217	ab	4	6	information_additional	conclusion	info-optional	elaboration	secondary	secondary	none	none	In text-to-3D scene generation , a user provides as input natural language text from which we extract explicit constraints on the objects that should appear in the scene .	We demonstrate that spatial knowledge is useful for interpreting natural language and show examples of learned knowledge and generated 3D scenes .	CL_D14-1217_ab_4	CL_D14-1217_ab_6	CL_D14-1217_ab_4_6
CL	D14-1217	ab	5	6	conclusion	conclusion	support	elaboration	secondary	secondary	back	elaboration	The main innovation of this work is to show how to augment these explicit constraints with learned spatial knowledge to infer missing objects and likely layouts for the objects in the scene .	We demonstrate that spatial knowledge is useful for interpreting natural language and show examples of learned knowledge and generated 3D scenes .	CL_D14-1217_ab_5	CL_D14-1217_ab_6	CL_D14-1217_ab_5_6
CL	D14-1217	ab	6	5	conclusion	conclusion	elaboration	support	secondary	secondary	forw	elaboration	We demonstrate that spatial knowledge is useful for interpreting natural language and show examples of learned knowledge and generated 3D scenes .	The main innovation of this work is to show how to augment these explicit constraints with learned spatial knowledge to infer missing objects and likely layouts for the objects in the scene .	CL_D14-1217_ab_6	CL_D14-1217_ab_5	CL_D14-1217_ab_5_6
CL	D14-1218	ab	1	2	motivation_background	motivation_background	info-required	info-required	secondary	secondary	forw	info-required	Coherence is what makes a multi-sentence text meaningful , both logically and syntactically .	To solve the challenge of ordering a set of sentences into coherent order , existing approaches focus mostly on defining and using sophisticated features to capture the cross-sentence argumentation logic and syntactic relationships .	CL_D14-1218_ab_1	CL_D14-1218_ab_2	CL_D14-1218_ab_1_2
CL	D14-1218	ab	2	1	motivation_background	motivation_background	info-required	info-required	secondary	secondary	back	info-required	To solve the challenge of ordering a set of sentences into coherent order , existing approaches focus mostly on defining and using sophisticated features to capture the cross-sentence argumentation logic and syntactic relationships .	Coherence is what makes a multi-sentence text meaningful , both logically and syntactically .	CL_D14-1218_ab_2	CL_D14-1218_ab_1	CL_D14-1218_ab_1_2
CL	D14-1218	ab	1	3	motivation_background	motivation_problem	info-required	support	secondary	secondary	none	none	Coherence is what makes a multi-sentence text meaningful , both logically and syntactically .	But both argumentation semantics and cross-sentence syntax ( such as coreference and tense rules ) are very hard to formalize .	CL_D14-1218_ab_1	CL_D14-1218_ab_3	CL_D14-1218_ab_1_3
CL	D14-1218	ab	1	4	motivation_background	proposal	info-required	none	secondary	main	none	none	Coherence is what makes a multi-sentence text meaningful , both logically and syntactically .	In this paper , we introduce a neural network model for the coherence task based on distributed sentence representation .	CL_D14-1218_ab_1	CL_D14-1218_ab_4	CL_D14-1218_ab_1_4
CL	D14-1218	ab	1	5	motivation_background	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	Coherence is what makes a multi-sentence text meaningful , both logically and syntactically .	The proposed approach learns a syntactico-semantic representation for sentences automatically , using either recurrent or recursive neural networks .	CL_D14-1218_ab_1	CL_D14-1218_ab_5	CL_D14-1218_ab_1_5
CL	D14-1218	ab	1	6	motivation_background	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	Coherence is what makes a multi-sentence text meaningful , both logically and syntactically .	The architecture obviated the need for feature engineering , and learns sentence representations , which are to some extent able to capture the "rules" governing coherent sentence structure .	CL_D14-1218_ab_1	CL_D14-1218_ab_6	CL_D14-1218_ab_1_6
CL	D14-1218	ab	1	7	motivation_background	result	info-required	support	secondary	secondary	none	none	Coherence is what makes a multi-sentence text meaningful , both logically and syntactically .	The proposed approach outperforms existing baselines and generates the state-of-art performance in standard coherence evaluation tasks .	CL_D14-1218_ab_1	CL_D14-1218_ab_7	CL_D14-1218_ab_1_7
CL	D14-1218	ab	2	3	motivation_background	motivation_problem	info-required	support	secondary	secondary	forw	info-required	To solve the challenge of ordering a set of sentences into coherent order , existing approaches focus mostly on defining and using sophisticated features to capture the cross-sentence argumentation logic and syntactic relationships .	But both argumentation semantics and cross-sentence syntax ( such as coreference and tense rules ) are very hard to formalize .	CL_D14-1218_ab_2	CL_D14-1218_ab_3	CL_D14-1218_ab_2_3
CL	D14-1218	ab	3	2	motivation_problem	motivation_background	support	info-required	secondary	secondary	back	info-required	But both argumentation semantics and cross-sentence syntax ( such as coreference and tense rules ) are very hard to formalize .	To solve the challenge of ordering a set of sentences into coherent order , existing approaches focus mostly on defining and using sophisticated features to capture the cross-sentence argumentation logic and syntactic relationships .	CL_D14-1218_ab_3	CL_D14-1218_ab_2	CL_D14-1218_ab_2_3
CL	D14-1218	ab	2	4	motivation_background	proposal	info-required	none	secondary	main	none	none	To solve the challenge of ordering a set of sentences into coherent order , existing approaches focus mostly on defining and using sophisticated features to capture the cross-sentence argumentation logic and syntactic relationships .	In this paper , we introduce a neural network model for the coherence task based on distributed sentence representation .	CL_D14-1218_ab_2	CL_D14-1218_ab_4	CL_D14-1218_ab_2_4
CL	D14-1218	ab	2	5	motivation_background	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	To solve the challenge of ordering a set of sentences into coherent order , existing approaches focus mostly on defining and using sophisticated features to capture the cross-sentence argumentation logic and syntactic relationships .	The proposed approach learns a syntactico-semantic representation for sentences automatically , using either recurrent or recursive neural networks .	CL_D14-1218_ab_2	CL_D14-1218_ab_5	CL_D14-1218_ab_2_5
CL	D14-1218	ab	2	6	motivation_background	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	To solve the challenge of ordering a set of sentences into coherent order , existing approaches focus mostly on defining and using sophisticated features to capture the cross-sentence argumentation logic and syntactic relationships .	The architecture obviated the need for feature engineering , and learns sentence representations , which are to some extent able to capture the "rules" governing coherent sentence structure .	CL_D14-1218_ab_2	CL_D14-1218_ab_6	CL_D14-1218_ab_2_6
CL	D14-1218	ab	2	7	motivation_background	result	info-required	support	secondary	secondary	none	none	To solve the challenge of ordering a set of sentences into coherent order , existing approaches focus mostly on defining and using sophisticated features to capture the cross-sentence argumentation logic and syntactic relationships .	The proposed approach outperforms existing baselines and generates the state-of-art performance in standard coherence evaluation tasks .	CL_D14-1218_ab_2	CL_D14-1218_ab_7	CL_D14-1218_ab_2_7
CL	D14-1218	ab	3	4	motivation_problem	proposal	support	none	secondary	main	forw	support	But both argumentation semantics and cross-sentence syntax ( such as coreference and tense rules ) are very hard to formalize .	In this paper , we introduce a neural network model for the coherence task based on distributed sentence representation .	CL_D14-1218_ab_3	CL_D14-1218_ab_4	CL_D14-1218_ab_3_4
CL	D14-1218	ab	4	3	proposal	motivation_problem	none	support	main	secondary	back	support	In this paper , we introduce a neural network model for the coherence task based on distributed sentence representation .	But both argumentation semantics and cross-sentence syntax ( such as coreference and tense rules ) are very hard to formalize .	CL_D14-1218_ab_4	CL_D14-1218_ab_3	CL_D14-1218_ab_3_4
CL	D14-1218	ab	3	5	motivation_problem	proposal_implementation	support	elaboration	secondary	secondary	none	none	But both argumentation semantics and cross-sentence syntax ( such as coreference and tense rules ) are very hard to formalize .	The proposed approach learns a syntactico-semantic representation for sentences automatically , using either recurrent or recursive neural networks .	CL_D14-1218_ab_3	CL_D14-1218_ab_5	CL_D14-1218_ab_3_5
CL	D14-1218	ab	3	6	motivation_problem	proposal_implementation	support	elaboration	secondary	secondary	none	none	But both argumentation semantics and cross-sentence syntax ( such as coreference and tense rules ) are very hard to formalize .	The architecture obviated the need for feature engineering , and learns sentence representations , which are to some extent able to capture the "rules" governing coherent sentence structure .	CL_D14-1218_ab_3	CL_D14-1218_ab_6	CL_D14-1218_ab_3_6
CL	D14-1218	ab	3	7	motivation_problem	result	support	support	secondary	secondary	none	none	But both argumentation semantics and cross-sentence syntax ( such as coreference and tense rules ) are very hard to formalize .	The proposed approach outperforms existing baselines and generates the state-of-art performance in standard coherence evaluation tasks .	CL_D14-1218_ab_3	CL_D14-1218_ab_7	CL_D14-1218_ab_3_7
CL	D14-1218	ab	4	5	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	In this paper , we introduce a neural network model for the coherence task based on distributed sentence representation .	The proposed approach learns a syntactico-semantic representation for sentences automatically , using either recurrent or recursive neural networks .	CL_D14-1218_ab_4	CL_D14-1218_ab_5	CL_D14-1218_ab_4_5
CL	D14-1218	ab	5	4	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	The proposed approach learns a syntactico-semantic representation for sentences automatically , using either recurrent or recursive neural networks .	In this paper , we introduce a neural network model for the coherence task based on distributed sentence representation .	CL_D14-1218_ab_5	CL_D14-1218_ab_4	CL_D14-1218_ab_4_5
CL	D14-1218	ab	4	6	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	In this paper , we introduce a neural network model for the coherence task based on distributed sentence representation .	The architecture obviated the need for feature engineering , and learns sentence representations , which are to some extent able to capture the "rules" governing coherent sentence structure .	CL_D14-1218_ab_4	CL_D14-1218_ab_6	CL_D14-1218_ab_4_6
CL	D14-1218	ab	4	7	proposal	result	none	support	main	secondary	back	support	In this paper , we introduce a neural network model for the coherence task based on distributed sentence representation .	The proposed approach outperforms existing baselines and generates the state-of-art performance in standard coherence evaluation tasks .	CL_D14-1218_ab_4	CL_D14-1218_ab_7	CL_D14-1218_ab_4_7
CL	D14-1218	ab	7	4	result	proposal	support	none	secondary	main	forw	support	The proposed approach outperforms existing baselines and generates the state-of-art performance in standard coherence evaluation tasks .	In this paper , we introduce a neural network model for the coherence task based on distributed sentence representation .	CL_D14-1218_ab_7	CL_D14-1218_ab_4	CL_D14-1218_ab_4_7
CL	D14-1218	ab	5	6	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	The proposed approach learns a syntactico-semantic representation for sentences automatically , using either recurrent or recursive neural networks .	The architecture obviated the need for feature engineering , and learns sentence representations , which are to some extent able to capture the "rules" governing coherent sentence structure .	CL_D14-1218_ab_5	CL_D14-1218_ab_6	CL_D14-1218_ab_5_6
CL	D14-1218	ab	6	5	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	forw	elaboration	The architecture obviated the need for feature engineering , and learns sentence representations , which are to some extent able to capture the "rules" governing coherent sentence structure .	The proposed approach learns a syntactico-semantic representation for sentences automatically , using either recurrent or recursive neural networks .	CL_D14-1218_ab_6	CL_D14-1218_ab_5	CL_D14-1218_ab_5_6
CL	D14-1218	ab	5	7	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	The proposed approach learns a syntactico-semantic representation for sentences automatically , using either recurrent or recursive neural networks .	The proposed approach outperforms existing baselines and generates the state-of-art performance in standard coherence evaluation tasks .	CL_D14-1218_ab_5	CL_D14-1218_ab_7	CL_D14-1218_ab_5_7
CL	D14-1218	ab	6	7	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	The architecture obviated the need for feature engineering , and learns sentence representations , which are to some extent able to capture the "rules" governing coherent sentence structure .	The proposed approach outperforms existing baselines and generates the state-of-art performance in standard coherence evaluation tasks .	CL_D14-1218_ab_6	CL_D14-1218_ab_7	CL_D14-1218_ab_6_7
CL	D14-1219	ab	1	2	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	In this paper , we present a discriminative approach for reranking discourse trees generated by an existing probabilistic discourse parser .	The reranker relies on tree kernels ( TKs ) to capture the global dependencies between discourse units in a tree .	CL_D14-1219_ab_1	CL_D14-1219_ab_2	CL_D14-1219_ab_1_2
CL	D14-1219	ab	1	3	proposal	proposal	none	elaboration	main	secondary	back	elaboration	In this paper , we present a discriminative approach for reranking discourse trees generated by an existing probabilistic discourse parser .	In particular , we design new computational structures of discourse trees , which combined with standard TKs , originate novel discourse TKs .	CL_D14-1219_ab_1	CL_D14-1219_ab_3	CL_D14-1219_ab_1_3
CL	D14-1219	ab	3	1	proposal	proposal	elaboration	none	secondary	main	forw	elaboration	In particular , we design new computational structures of discourse trees , which combined with standard TKs , originate novel discourse TKs .	In this paper , we present a discriminative approach for reranking discourse trees generated by an existing probabilistic discourse parser .	CL_D14-1219_ab_3	CL_D14-1219_ab_1	CL_D14-1219_ab_1_3
CL	D14-1219	ab	1	4	proposal	observation	none	support	main	secondary	back	support	In this paper , we present a discriminative approach for reranking discourse trees generated by an existing probabilistic discourse parser .	The empirical evaluation shows that our reranker can improve the state-of-the-art sentence-level parsing accuracy from 79.77 % to 82.15 % , a relative error reduction of 11.8 % , which in turn pushes the state-of-the-art document-level accuracy from 55.8 % to 57.3 % .	CL_D14-1219_ab_1	CL_D14-1219_ab_4	CL_D14-1219_ab_1_4
CL	D14-1219	ab	4	1	observation	proposal	support	none	secondary	main	forw	support	The empirical evaluation shows that our reranker can improve the state-of-the-art sentence-level parsing accuracy from 79.77 % to 82.15 % , a relative error reduction of 11.8 % , which in turn pushes the state-of-the-art document-level accuracy from 55.8 % to 57.3 % .	In this paper , we present a discriminative approach for reranking discourse trees generated by an existing probabilistic discourse parser .	CL_D14-1219_ab_4	CL_D14-1219_ab_1	CL_D14-1219_ab_1_4
CL	D14-1219	ab	2	3	proposal_implementation	proposal	elaboration	elaboration	secondary	secondary	forw	elaboration	The reranker relies on tree kernels ( TKs ) to capture the global dependencies between discourse units in a tree .	In particular , we design new computational structures of discourse trees , which combined with standard TKs , originate novel discourse TKs .	CL_D14-1219_ab_2	CL_D14-1219_ab_3	CL_D14-1219_ab_2_3
CL	D14-1219	ab	3	2	proposal	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	In particular , we design new computational structures of discourse trees , which combined with standard TKs , originate novel discourse TKs .	The reranker relies on tree kernels ( TKs ) to capture the global dependencies between discourse units in a tree .	CL_D14-1219_ab_3	CL_D14-1219_ab_2	CL_D14-1219_ab_2_3
CL	D14-1219	ab	2	4	proposal_implementation	observation	elaboration	support	secondary	secondary	none	none	The reranker relies on tree kernels ( TKs ) to capture the global dependencies between discourse units in a tree .	The empirical evaluation shows that our reranker can improve the state-of-the-art sentence-level parsing accuracy from 79.77 % to 82.15 % , a relative error reduction of 11.8 % , which in turn pushes the state-of-the-art document-level accuracy from 55.8 % to 57.3 % .	CL_D14-1219_ab_2	CL_D14-1219_ab_4	CL_D14-1219_ab_2_4
CL	D14-1219	ab	3	4	proposal	observation	elaboration	support	secondary	secondary	none	none	In particular , we design new computational structures of discourse trees , which combined with standard TKs , originate novel discourse TKs .	The empirical evaluation shows that our reranker can improve the state-of-the-art sentence-level parsing accuracy from 79.77 % to 82.15 % , a relative error reduction of 11.8 % , which in turn pushes the state-of-the-art document-level accuracy from 55.8 % to 57.3 % .	CL_D14-1219_ab_3	CL_D14-1219_ab_4	CL_D14-1219_ab_3_4
CL	D14-1220	ab	1	2	motivation_problem	proposal	support	none	secondary	main	forw	support	Text-level discourse parsing remains a challenge : most approaches employ features that fail to capture the intentional , semantic , and syntactic aspects that govern discourse coherence .	In this paper , we propose a recursive model for discourse parsing that jointly models distributed representations for clauses , sentences , and entire discourses .	CL_D14-1220_ab_1	CL_D14-1220_ab_2	CL_D14-1220_ab_1_2
CL	D14-1220	ab	2	1	proposal	motivation_problem	none	support	main	secondary	back	support	In this paper , we propose a recursive model for discourse parsing that jointly models distributed representations for clauses , sentences , and entire discourses .	Text-level discourse parsing remains a challenge : most approaches employ features that fail to capture the intentional , semantic , and syntactic aspects that govern discourse coherence .	CL_D14-1220_ab_2	CL_D14-1220_ab_1	CL_D14-1220_ab_1_2
CL	D14-1220	ab	1	3	motivation_problem	proposal	support	elaboration	secondary	secondary	none	none	Text-level discourse parsing remains a challenge : most approaches employ features that fail to capture the intentional , semantic , and syntactic aspects that govern discourse coherence .	The learned representations can to some extent learn the semantic and intentional import of words and larger discourse units automatically , .	CL_D14-1220_ab_1	CL_D14-1220_ab_3	CL_D14-1220_ab_1_3
CL	D14-1220	ab	1	4	motivation_problem	result	support	support	secondary	secondary	none	none	Text-level discourse parsing remains a challenge : most approaches employ features that fail to capture the intentional , semantic , and syntactic aspects that govern discourse coherence .	The proposed framework obtains comparable performance regarding standard discours-ing parsing evaluations when compared against current state-of-art systems .	CL_D14-1220_ab_1	CL_D14-1220_ab_4	CL_D14-1220_ab_1_4
CL	D14-1220	ab	2	3	proposal	proposal	none	elaboration	main	secondary	back	elaboration	In this paper , we propose a recursive model for discourse parsing that jointly models distributed representations for clauses , sentences , and entire discourses .	The learned representations can to some extent learn the semantic and intentional import of words and larger discourse units automatically , .	CL_D14-1220_ab_2	CL_D14-1220_ab_3	CL_D14-1220_ab_2_3
CL	D14-1220	ab	3	2	proposal	proposal	elaboration	none	secondary	main	forw	elaboration	The learned representations can to some extent learn the semantic and intentional import of words and larger discourse units automatically , .	In this paper , we propose a recursive model for discourse parsing that jointly models distributed representations for clauses , sentences , and entire discourses .	CL_D14-1220_ab_3	CL_D14-1220_ab_2	CL_D14-1220_ab_2_3
CL	D14-1220	ab	2	4	proposal	result	none	support	main	secondary	back	support	In this paper , we propose a recursive model for discourse parsing that jointly models distributed representations for clauses , sentences , and entire discourses .	The proposed framework obtains comparable performance regarding standard discours-ing parsing evaluations when compared against current state-of-art systems .	CL_D14-1220_ab_2	CL_D14-1220_ab_4	CL_D14-1220_ab_2_4
CL	D14-1220	ab	4	2	result	proposal	support	none	secondary	main	forw	support	The proposed framework obtains comparable performance regarding standard discours-ing parsing evaluations when compared against current state-of-art systems .	In this paper , we propose a recursive model for discourse parsing that jointly models distributed representations for clauses , sentences , and entire discourses .	CL_D14-1220_ab_4	CL_D14-1220_ab_2	CL_D14-1220_ab_2_4
CL	D14-1220	ab	3	4	proposal	result	elaboration	support	secondary	secondary	none	none	The learned representations can to some extent learn the semantic and intentional import of words and larger discourse units automatically , .	The proposed framework obtains comparable performance regarding standard discours-ing parsing evaluations when compared against current state-of-art systems .	CL_D14-1220_ab_3	CL_D14-1220_ab_4	CL_D14-1220_ab_3_4
CL	D14-1221	ab	1	2	proposal	proposal	none	elaboration	main	secondary	back	elaboration	We present a novel method for coreference resolution error analysis which we apply to perform a recall error analysis of four state-of-the-art English coreference resolution systems .	Our analysis highlights differences between the systems and identifies that the majority of recall errors for nouns and names are shared by all systems .	CL_D14-1221_ab_1	CL_D14-1221_ab_2	CL_D14-1221_ab_1_2
CL	D14-1221	ab	2	1	proposal	proposal	elaboration	none	secondary	main	forw	elaboration	Our analysis highlights differences between the systems and identifies that the majority of recall errors for nouns and names are shared by all systems .	We present a novel method for coreference resolution error analysis which we apply to perform a recall error analysis of four state-of-the-art English coreference resolution systems .	CL_D14-1221_ab_2	CL_D14-1221_ab_1	CL_D14-1221_ab_1_2
CL	D14-1221	ab	1	3	proposal	proposal	none	elaboration	main	secondary	none	none	We present a novel method for coreference resolution error analysis which we apply to perform a recall error analysis of four state-of-the-art English coreference resolution systems .	We characterize this set of common challenging errors in terms of a broad range of lexical and semantic properties .	CL_D14-1221_ab_1	CL_D14-1221_ab_3	CL_D14-1221_ab_1_3
CL	D14-1221	ab	2	3	proposal	proposal	elaboration	elaboration	secondary	secondary	back	elaboration	Our analysis highlights differences between the systems and identifies that the majority of recall errors for nouns and names are shared by all systems .	We characterize this set of common challenging errors in terms of a broad range of lexical and semantic properties .	CL_D14-1221_ab_2	CL_D14-1221_ab_3	CL_D14-1221_ab_2_3
CL	D14-1221	ab	3	2	proposal	proposal	elaboration	elaboration	secondary	secondary	forw	elaboration	We characterize this set of common challenging errors in terms of a broad range of lexical and semantic properties .	Our analysis highlights differences between the systems and identifies that the majority of recall errors for nouns and names are shared by all systems .	CL_D14-1221_ab_3	CL_D14-1221_ab_2	CL_D14-1221_ab_2_3
CL	D14-1222	ab	1	2	motivation_background	proposal	support	none	secondary	main	forw	support	Bridging resolution plays an important role in establishing ( local ) entity coherence .	This paper proposes a rule-based approach for the challenging task of unrestricted bridging resolution , where bridging anaphors are not limited to definite NPs and semantic relations between anaphors and their antecedents are not restricted to meronymic relations .	CL_D14-1222_ab_1	CL_D14-1222_ab_2	CL_D14-1222_ab_1_2
CL	D14-1222	ab	2	1	proposal	motivation_background	none	support	main	secondary	back	support	This paper proposes a rule-based approach for the challenging task of unrestricted bridging resolution , where bridging anaphors are not limited to definite NPs and semantic relations between anaphors and their antecedents are not restricted to meronymic relations .	Bridging resolution plays an important role in establishing ( local ) entity coherence .	CL_D14-1222_ab_2	CL_D14-1222_ab_1	CL_D14-1222_ab_1_2
CL	D14-1222	ab	1	3	motivation_background	proposal_implementation	support	elaboration	secondary	secondary	none	none	Bridging resolution plays an important role in establishing ( local ) entity coherence .	The system consists of eight rules which target different relations based on linguistic insights .	CL_D14-1222_ab_1	CL_D14-1222_ab_3	CL_D14-1222_ab_1_3
CL	D14-1222	ab	1	4	motivation_background	result	support	support	secondary	secondary	none	none	Bridging resolution plays an important role in establishing ( local ) entity coherence .	Our rule-based system significantly outperforms a reimplementation of a previous rule-based system ( Vieira and Poesio , 2000 ) .	CL_D14-1222_ab_1	CL_D14-1222_ab_4	CL_D14-1222_ab_1_4
CL	D14-1222	ab	1	5	motivation_background	result	support	elaboration	secondary	secondary	none	none	Bridging resolution plays an important role in establishing ( local ) entity coherence .	Furthermore , it performs better than a learning-based approach which has access to the same knowledge resources as the rule-based system .	CL_D14-1222_ab_1	CL_D14-1222_ab_5	CL_D14-1222_ab_1_5
CL	D14-1222	ab	1	6	motivation_background	result	support	elaboration	secondary	secondary	none	none	Bridging resolution plays an important role in establishing ( local ) entity coherence .	Additionally , incorporating the rules and more features into the learning-based system yields a minor improvement over the rule-based system .	CL_D14-1222_ab_1	CL_D14-1222_ab_6	CL_D14-1222_ab_1_6
CL	D14-1222	ab	2	3	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	This paper proposes a rule-based approach for the challenging task of unrestricted bridging resolution , where bridging anaphors are not limited to definite NPs and semantic relations between anaphors and their antecedents are not restricted to meronymic relations .	The system consists of eight rules which target different relations based on linguistic insights .	CL_D14-1222_ab_2	CL_D14-1222_ab_3	CL_D14-1222_ab_2_3
CL	D14-1222	ab	3	2	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	The system consists of eight rules which target different relations based on linguistic insights .	This paper proposes a rule-based approach for the challenging task of unrestricted bridging resolution , where bridging anaphors are not limited to definite NPs and semantic relations between anaphors and their antecedents are not restricted to meronymic relations .	CL_D14-1222_ab_3	CL_D14-1222_ab_2	CL_D14-1222_ab_2_3
CL	D14-1222	ab	2	4	proposal	result	none	support	main	secondary	back	support	This paper proposes a rule-based approach for the challenging task of unrestricted bridging resolution , where bridging anaphors are not limited to definite NPs and semantic relations between anaphors and their antecedents are not restricted to meronymic relations .	Our rule-based system significantly outperforms a reimplementation of a previous rule-based system ( Vieira and Poesio , 2000 ) .	CL_D14-1222_ab_2	CL_D14-1222_ab_4	CL_D14-1222_ab_2_4
CL	D14-1222	ab	4	2	result	proposal	support	none	secondary	main	forw	support	Our rule-based system significantly outperforms a reimplementation of a previous rule-based system ( Vieira and Poesio , 2000 ) .	This paper proposes a rule-based approach for the challenging task of unrestricted bridging resolution , where bridging anaphors are not limited to definite NPs and semantic relations between anaphors and their antecedents are not restricted to meronymic relations .	CL_D14-1222_ab_4	CL_D14-1222_ab_2	CL_D14-1222_ab_2_4
CL	D14-1222	ab	2	5	proposal	result	none	elaboration	main	secondary	none	none	This paper proposes a rule-based approach for the challenging task of unrestricted bridging resolution , where bridging anaphors are not limited to definite NPs and semantic relations between anaphors and their antecedents are not restricted to meronymic relations .	Furthermore , it performs better than a learning-based approach which has access to the same knowledge resources as the rule-based system .	CL_D14-1222_ab_2	CL_D14-1222_ab_5	CL_D14-1222_ab_2_5
CL	D14-1222	ab	2	6	proposal	result	none	elaboration	main	secondary	none	none	This paper proposes a rule-based approach for the challenging task of unrestricted bridging resolution , where bridging anaphors are not limited to definite NPs and semantic relations between anaphors and their antecedents are not restricted to meronymic relations .	Additionally , incorporating the rules and more features into the learning-based system yields a minor improvement over the rule-based system .	CL_D14-1222_ab_2	CL_D14-1222_ab_6	CL_D14-1222_ab_2_6
CL	D14-1222	ab	3	4	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	The system consists of eight rules which target different relations based on linguistic insights .	Our rule-based system significantly outperforms a reimplementation of a previous rule-based system ( Vieira and Poesio , 2000 ) .	CL_D14-1222_ab_3	CL_D14-1222_ab_4	CL_D14-1222_ab_3_4
CL	D14-1222	ab	3	5	proposal_implementation	result	elaboration	elaboration	secondary	secondary	none	none	The system consists of eight rules which target different relations based on linguistic insights .	Furthermore , it performs better than a learning-based approach which has access to the same knowledge resources as the rule-based system .	CL_D14-1222_ab_3	CL_D14-1222_ab_5	CL_D14-1222_ab_3_5
CL	D14-1222	ab	3	6	proposal_implementation	result	elaboration	elaboration	secondary	secondary	none	none	The system consists of eight rules which target different relations based on linguistic insights .	Additionally , incorporating the rules and more features into the learning-based system yields a minor improvement over the rule-based system .	CL_D14-1222_ab_3	CL_D14-1222_ab_6	CL_D14-1222_ab_3_6
CL	D14-1222	ab	4	5	result	result	support	elaboration	secondary	secondary	back	elaboration	Our rule-based system significantly outperforms a reimplementation of a previous rule-based system ( Vieira and Poesio , 2000 ) .	Furthermore , it performs better than a learning-based approach which has access to the same knowledge resources as the rule-based system .	CL_D14-1222_ab_4	CL_D14-1222_ab_5	CL_D14-1222_ab_4_5
CL	D14-1222	ab	5	4	result	result	elaboration	support	secondary	secondary	forw	elaboration	Furthermore , it performs better than a learning-based approach which has access to the same knowledge resources as the rule-based system .	Our rule-based system significantly outperforms a reimplementation of a previous rule-based system ( Vieira and Poesio , 2000 ) .	CL_D14-1222_ab_5	CL_D14-1222_ab_4	CL_D14-1222_ab_4_5
CL	D14-1222	ab	4	6	result	result	support	elaboration	secondary	secondary	none	none	Our rule-based system significantly outperforms a reimplementation of a previous rule-based system ( Vieira and Poesio , 2000 ) .	Additionally , incorporating the rules and more features into the learning-based system yields a minor improvement over the rule-based system .	CL_D14-1222_ab_4	CL_D14-1222_ab_6	CL_D14-1222_ab_4_6
CL	D14-1222	ab	5	6	result	result	elaboration	elaboration	secondary	secondary	back	elaboration	Furthermore , it performs better than a learning-based approach which has access to the same knowledge resources as the rule-based system .	Additionally , incorporating the rules and more features into the learning-based system yields a minor improvement over the rule-based system .	CL_D14-1222_ab_5	CL_D14-1222_ab_6	CL_D14-1222_ab_5_6
CL	D14-1222	ab	6	5	result	result	elaboration	elaboration	secondary	secondary	forw	elaboration	Additionally , incorporating the rules and more features into the learning-based system yields a minor improvement over the rule-based system .	Furthermore , it performs better than a learning-based approach which has access to the same knowledge resources as the rule-based system .	CL_D14-1222_ab_6	CL_D14-1222_ab_5	CL_D14-1222_ab_5_6
CL	D14-1223	ab	1	2	motivation_background	motivation_background	info-required	info-required	secondary	secondary	forw	info-required	Unlike traditional over-the-phone spoken dialog systems ( SDSs ) , modern dialog systems tend to have visual rendering on the device screen as an additional modality to communicate the system's response to the user .	Visual display of the system's response not only changes human behavior when interacting with devices , but also creates new research areas in SDSs .	CL_D14-1223_ab_1	CL_D14-1223_ab_2	CL_D14-1223_ab_1_2
CL	D14-1223	ab	2	1	motivation_background	motivation_background	info-required	info-required	secondary	secondary	back	info-required	Visual display of the system's response not only changes human behavior when interacting with devices , but also creates new research areas in SDSs .	Unlike traditional over-the-phone spoken dialog systems ( SDSs ) , modern dialog systems tend to have visual rendering on the device screen as an additional modality to communicate the system's response to the user .	CL_D14-1223_ab_2	CL_D14-1223_ab_1	CL_D14-1223_ab_1_2
CL	D14-1223	ab	1	3	motivation_background	motivation_problem	info-required	support	secondary	secondary	none	none	Unlike traditional over-the-phone spoken dialog systems ( SDSs ) , modern dialog systems tend to have visual rendering on the device screen as an additional modality to communicate the system's response to the user .	On-screen item identification and resolution in utterances is one critical problem to achieve a natural and accurate human-machine communication .	CL_D14-1223_ab_1	CL_D14-1223_ab_3	CL_D14-1223_ab_1_3
CL	D14-1223	ab	1	4	motivation_background	proposal	info-required	none	secondary	main	none	none	Unlike traditional over-the-phone spoken dialog systems ( SDSs ) , modern dialog systems tend to have visual rendering on the device screen as an additional modality to communicate the system's response to the user .	We pose the problem as a classification task to correctly identify intended on-screen item ( s ) from user utterances .	CL_D14-1223_ab_1	CL_D14-1223_ab_4	CL_D14-1223_ab_1_4
CL	D14-1223	ab	1	5	motivation_background	observation	info-required	support	secondary	secondary	none	none	Unlike traditional over-the-phone spoken dialog systems ( SDSs ) , modern dialog systems tend to have visual rendering on the device screen as an additional modality to communicate the system's response to the user .	Using syntactic , semantic as well as context features from the display screen , our model can resolve different types of referring expressions with up to 90 % accuracy .	CL_D14-1223_ab_1	CL_D14-1223_ab_5	CL_D14-1223_ab_1_5
CL	D14-1223	ab	1	6	motivation_background	result	info-required	elaboration	secondary	secondary	none	none	Unlike traditional over-the-phone spoken dialog systems ( SDSs ) , modern dialog systems tend to have visual rendering on the device screen as an additional modality to communicate the system's response to the user .	In the experiments we also show that the proposed model is robust to domain and screen layout changes .	CL_D14-1223_ab_1	CL_D14-1223_ab_6	CL_D14-1223_ab_1_6
CL	D14-1223	ab	2	3	motivation_background	motivation_problem	info-required	support	secondary	secondary	forw	info-required	Visual display of the system's response not only changes human behavior when interacting with devices , but also creates new research areas in SDSs .	On-screen item identification and resolution in utterances is one critical problem to achieve a natural and accurate human-machine communication .	CL_D14-1223_ab_2	CL_D14-1223_ab_3	CL_D14-1223_ab_2_3
CL	D14-1223	ab	3	2	motivation_problem	motivation_background	support	info-required	secondary	secondary	back	info-required	On-screen item identification and resolution in utterances is one critical problem to achieve a natural and accurate human-machine communication .	Visual display of the system's response not only changes human behavior when interacting with devices , but also creates new research areas in SDSs .	CL_D14-1223_ab_3	CL_D14-1223_ab_2	CL_D14-1223_ab_2_3
CL	D14-1223	ab	2	4	motivation_background	proposal	info-required	none	secondary	main	none	none	Visual display of the system's response not only changes human behavior when interacting with devices , but also creates new research areas in SDSs .	We pose the problem as a classification task to correctly identify intended on-screen item ( s ) from user utterances .	CL_D14-1223_ab_2	CL_D14-1223_ab_4	CL_D14-1223_ab_2_4
CL	D14-1223	ab	2	5	motivation_background	observation	info-required	support	secondary	secondary	none	none	Visual display of the system's response not only changes human behavior when interacting with devices , but also creates new research areas in SDSs .	Using syntactic , semantic as well as context features from the display screen , our model can resolve different types of referring expressions with up to 90 % accuracy .	CL_D14-1223_ab_2	CL_D14-1223_ab_5	CL_D14-1223_ab_2_5
CL	D14-1223	ab	2	6	motivation_background	result	info-required	elaboration	secondary	secondary	none	none	Visual display of the system's response not only changes human behavior when interacting with devices , but also creates new research areas in SDSs .	In the experiments we also show that the proposed model is robust to domain and screen layout changes .	CL_D14-1223_ab_2	CL_D14-1223_ab_6	CL_D14-1223_ab_2_6
CL	D14-1223	ab	3	4	motivation_problem	proposal	support	none	secondary	main	forw	support	On-screen item identification and resolution in utterances is one critical problem to achieve a natural and accurate human-machine communication .	We pose the problem as a classification task to correctly identify intended on-screen item ( s ) from user utterances .	CL_D14-1223_ab_3	CL_D14-1223_ab_4	CL_D14-1223_ab_3_4
CL	D14-1223	ab	4	3	proposal	motivation_problem	none	support	main	secondary	back	support	We pose the problem as a classification task to correctly identify intended on-screen item ( s ) from user utterances .	On-screen item identification and resolution in utterances is one critical problem to achieve a natural and accurate human-machine communication .	CL_D14-1223_ab_4	CL_D14-1223_ab_3	CL_D14-1223_ab_3_4
CL	D14-1223	ab	3	5	motivation_problem	observation	support	support	secondary	secondary	none	none	On-screen item identification and resolution in utterances is one critical problem to achieve a natural and accurate human-machine communication .	Using syntactic , semantic as well as context features from the display screen , our model can resolve different types of referring expressions with up to 90 % accuracy .	CL_D14-1223_ab_3	CL_D14-1223_ab_5	CL_D14-1223_ab_3_5
CL	D14-1223	ab	3	6	motivation_problem	result	support	elaboration	secondary	secondary	none	none	On-screen item identification and resolution in utterances is one critical problem to achieve a natural and accurate human-machine communication .	In the experiments we also show that the proposed model is robust to domain and screen layout changes .	CL_D14-1223_ab_3	CL_D14-1223_ab_6	CL_D14-1223_ab_3_6
CL	D14-1223	ab	4	5	proposal	observation	none	support	main	secondary	back	support	We pose the problem as a classification task to correctly identify intended on-screen item ( s ) from user utterances .	Using syntactic , semantic as well as context features from the display screen , our model can resolve different types of referring expressions with up to 90 % accuracy .	CL_D14-1223_ab_4	CL_D14-1223_ab_5	CL_D14-1223_ab_4_5
CL	D14-1223	ab	5	4	observation	proposal	support	none	secondary	main	forw	support	Using syntactic , semantic as well as context features from the display screen , our model can resolve different types of referring expressions with up to 90 % accuracy .	We pose the problem as a classification task to correctly identify intended on-screen item ( s ) from user utterances .	CL_D14-1223_ab_5	CL_D14-1223_ab_4	CL_D14-1223_ab_4_5
CL	D14-1223	ab	4	6	proposal	result	none	elaboration	main	secondary	none	none	We pose the problem as a classification task to correctly identify intended on-screen item ( s ) from user utterances .	In the experiments we also show that the proposed model is robust to domain and screen layout changes .	CL_D14-1223_ab_4	CL_D14-1223_ab_6	CL_D14-1223_ab_4_6
CL	D14-1223	ab	5	6	observation	result	support	elaboration	secondary	secondary	back	elaboration	Using syntactic , semantic as well as context features from the display screen , our model can resolve different types of referring expressions with up to 90 % accuracy .	In the experiments we also show that the proposed model is robust to domain and screen layout changes .	CL_D14-1223_ab_5	CL_D14-1223_ab_6	CL_D14-1223_ab_5_6
CL	D14-1223	ab	6	5	result	observation	elaboration	support	secondary	secondary	forw	elaboration	In the experiments we also show that the proposed model is robust to domain and screen layout changes .	Using syntactic , semantic as well as context features from the display screen , our model can resolve different types of referring expressions with up to 90 % accuracy .	CL_D14-1223_ab_6	CL_D14-1223_ab_5	CL_D14-1223_ab_5_6
CL	D14-1224	ab	1	2	proposal_implementation	proposal_implementation	none	elaboration	main	secondary	back	elaboration	In this paper , we propose a Connective-driven Dependency Tree ( CDT ) scheme to represent the discourse rhetorical structure in Chinese language ,  with elementary discourse units as leaf nodes and connectives as non-leaf nodes , largely motivated by the Penn Discourse Treebank and the Rhetorical Structure Theory .	In particular , connectives are employed to directly represent the hierarchy of the tree structure and the rhetorical relation of a discourse , while the nuclei of discourse units are globally determined with reference to the dependency theory .	CL_D14-1224_ab_1	CL_D14-1224_ab_2	CL_D14-1224_ab_1_2
CL	D14-1224	ab	2	1	proposal_implementation	proposal_implementation	elaboration	none	secondary	main	forw	elaboration	In particular , connectives are employed to directly represent the hierarchy of the tree structure and the rhetorical relation of a discourse , while the nuclei of discourse units are globally determined with reference to the dependency theory .	In this paper , we propose a Connective-driven Dependency Tree ( CDT ) scheme to represent the discourse rhetorical structure in Chinese language ,  with elementary discourse units as leaf nodes and connectives as non-leaf nodes , largely motivated by the Penn Discourse Treebank and the Rhetorical Structure Theory .	CL_D14-1224_ab_2	CL_D14-1224_ab_1	CL_D14-1224_ab_1_2
CL	D14-1224	ab	1	3	proposal_implementation	proposal_implementation	none	elaboration	main	secondary	none	none	In this paper , we propose a Connective-driven Dependency Tree ( CDT ) scheme to represent the discourse rhetorical structure in Chinese language ,  with elementary discourse units as leaf nodes and connectives as non-leaf nodes , largely motivated by the Penn Discourse Treebank and the Rhetorical Structure Theory .	Guided by the CDT scheme , we manually annotate a Chinese Discourse Treebank ( CDTB ) of 500 documents .	CL_D14-1224_ab_1	CL_D14-1224_ab_3	CL_D14-1224_ab_1_3
CL	D14-1224	ab	1	4	proposal_implementation	result	none	support	main	secondary	back	support	In this paper , we propose a Connective-driven Dependency Tree ( CDT ) scheme to represent the discourse rhetorical structure in Chinese language ,  with elementary discourse units as leaf nodes and connectives as non-leaf nodes , largely motivated by the Penn Discourse Treebank and the Rhetorical Structure Theory .	Preliminary evaluation justifies the appropriateness of the CDT scheme to Chinese discourse analysis and the usefulness of our manually annotated CDTB corpus .	CL_D14-1224_ab_1	CL_D14-1224_ab_4	CL_D14-1224_ab_1_4
CL	D14-1224	ab	4	1	result	proposal_implementation	support	none	secondary	main	forw	support	Preliminary evaluation justifies the appropriateness of the CDT scheme to Chinese discourse analysis and the usefulness of our manually annotated CDTB corpus .	In this paper , we propose a Connective-driven Dependency Tree ( CDT ) scheme to represent the discourse rhetorical structure in Chinese language ,  with elementary discourse units as leaf nodes and connectives as non-leaf nodes , largely motivated by the Penn Discourse Treebank and the Rhetorical Structure Theory .	CL_D14-1224_ab_4	CL_D14-1224_ab_1	CL_D14-1224_ab_1_4
CL	D14-1224	ab	2	3	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	In particular , connectives are employed to directly represent the hierarchy of the tree structure and the rhetorical relation of a discourse , while the nuclei of discourse units are globally determined with reference to the dependency theory .	Guided by the CDT scheme , we manually annotate a Chinese Discourse Treebank ( CDTB ) of 500 documents .	CL_D14-1224_ab_2	CL_D14-1224_ab_3	CL_D14-1224_ab_2_3
CL	D14-1224	ab	3	2	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	forw	elaboration	Guided by the CDT scheme , we manually annotate a Chinese Discourse Treebank ( CDTB ) of 500 documents .	In particular , connectives are employed to directly represent the hierarchy of the tree structure and the rhetorical relation of a discourse , while the nuclei of discourse units are globally determined with reference to the dependency theory .	CL_D14-1224_ab_3	CL_D14-1224_ab_2	CL_D14-1224_ab_2_3
CL	D14-1224	ab	2	4	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	In particular , connectives are employed to directly represent the hierarchy of the tree structure and the rhetorical relation of a discourse , while the nuclei of discourse units are globally determined with reference to the dependency theory .	Preliminary evaluation justifies the appropriateness of the CDT scheme to Chinese discourse analysis and the usefulness of our manually annotated CDTB corpus .	CL_D14-1224_ab_2	CL_D14-1224_ab_4	CL_D14-1224_ab_2_4
CL	D14-1224	ab	3	4	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	Guided by the CDT scheme , we manually annotate a Chinese Discourse Treebank ( CDTB ) of 500 documents .	Preliminary evaluation justifies the appropriateness of the CDT scheme to Chinese discourse analysis and the usefulness of our manually annotated CDTB corpus .	CL_D14-1224_ab_3	CL_D14-1224_ab_4	CL_D14-1224_ab_3_4
CL	D14-1225	ab	1	2	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	We propose a novel search-based approach for greedy coreference resolution , where the mentions are processed in order and added to previous coreference clusters .	Our method is distinguished by the use of two functions to make each coreference decision : a pruning function that prunes bad coreference decisions from further consideration , and a scoring function that then selects the best among the remaining decisions .	CL_D14-1225_ab_1	CL_D14-1225_ab_2	CL_D14-1225_ab_1_2
CL	D14-1225	ab	2	1	proposal_implementation	proposal	elaboration	none	secondary	main	forw	elaboration	Our method is distinguished by the use of two functions to make each coreference decision : a pruning function that prunes bad coreference decisions from further consideration , and a scoring function that then selects the best among the remaining decisions .	We propose a novel search-based approach for greedy coreference resolution , where the mentions are processed in order and added to previous coreference clusters .	CL_D14-1225_ab_2	CL_D14-1225_ab_1	CL_D14-1225_ab_1_2
CL	D14-1225	ab	1	3	proposal	proposal	none	elaboration	main	secondary	none	none	We propose a novel search-based approach for greedy coreference resolution , where the mentions are processed in order and added to previous coreference clusters .	Our framework reduces learning of these functions to rank learning , which helps leverage powerful off-the-shelf rank-learners .	CL_D14-1225_ab_1	CL_D14-1225_ab_3	CL_D14-1225_ab_1_3
CL	D14-1225	ab	1	4	proposal	result_means	none	support	main	secondary	back	support	We propose a novel search-based approach for greedy coreference resolution , where the mentions are processed in order and added to previous coreference clusters .	We show that our Prune-and-Score approach is superior to using a single scoring function to make both decisions and outperforms several state-of-the-art approaches on multiple benchmark corpora including OntoNotes .	CL_D14-1225_ab_1	CL_D14-1225_ab_4	CL_D14-1225_ab_1_4
CL	D14-1225	ab	4	1	result_means	proposal	support	none	secondary	main	forw	support	We show that our Prune-and-Score approach is superior to using a single scoring function to make both decisions and outperforms several state-of-the-art approaches on multiple benchmark corpora including OntoNotes .	We propose a novel search-based approach for greedy coreference resolution , where the mentions are processed in order and added to previous coreference clusters .	CL_D14-1225_ab_4	CL_D14-1225_ab_1	CL_D14-1225_ab_1_4
CL	D14-1225	ab	2	3	proposal_implementation	proposal	elaboration	elaboration	secondary	secondary	back	elaboration	Our method is distinguished by the use of two functions to make each coreference decision : a pruning function that prunes bad coreference decisions from further consideration , and a scoring function that then selects the best among the remaining decisions .	Our framework reduces learning of these functions to rank learning , which helps leverage powerful off-the-shelf rank-learners .	CL_D14-1225_ab_2	CL_D14-1225_ab_3	CL_D14-1225_ab_2_3
CL	D14-1225	ab	3	2	proposal	proposal_implementation	elaboration	elaboration	secondary	secondary	forw	elaboration	Our framework reduces learning of these functions to rank learning , which helps leverage powerful off-the-shelf rank-learners .	Our method is distinguished by the use of two functions to make each coreference decision : a pruning function that prunes bad coreference decisions from further consideration , and a scoring function that then selects the best among the remaining decisions .	CL_D14-1225_ab_3	CL_D14-1225_ab_2	CL_D14-1225_ab_2_3
CL	D14-1225	ab	2	4	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	Our method is distinguished by the use of two functions to make each coreference decision : a pruning function that prunes bad coreference decisions from further consideration , and a scoring function that then selects the best among the remaining decisions .	We show that our Prune-and-Score approach is superior to using a single scoring function to make both decisions and outperforms several state-of-the-art approaches on multiple benchmark corpora including OntoNotes .	CL_D14-1225_ab_2	CL_D14-1225_ab_4	CL_D14-1225_ab_2_4
CL	D14-1225	ab	3	4	proposal	result_means	elaboration	support	secondary	secondary	none	none	Our framework reduces learning of these functions to rank learning , which helps leverage powerful off-the-shelf rank-learners .	We show that our Prune-and-Score approach is superior to using a single scoring function to make both decisions and outperforms several state-of-the-art approaches on multiple benchmark corpora including OntoNotes .	CL_D14-1225_ab_3	CL_D14-1225_ab_4	CL_D14-1225_ab_3_4
CL	D14-1226	ab	1	2	motivation_background	motivation_hypothesis	info-required	support	secondary	secondary	forw	info-required	A typical discussion thread in an online forum spans multiple pages involving participation from multiple users and thus , may contain multiple view-points and solutions .	A user interested in the topic of discussion or having a problem similar to being discussed in the thread may not want to read all the previous posts but only a few selected posts that provide her a concise summary of the ongoing discussion .	CL_D14-1226_ab_1	CL_D14-1226_ab_2	CL_D14-1226_ab_1_2
CL	D14-1226	ab	2	1	motivation_hypothesis	motivation_background	support	info-required	secondary	secondary	back	info-required	A user interested in the topic of discussion or having a problem similar to being discussed in the thread may not want to read all the previous posts but only a few selected posts that provide her a concise summary of the ongoing discussion .	A typical discussion thread in an online forum spans multiple pages involving participation from multiple users and thus , may contain multiple view-points and solutions .	CL_D14-1226_ab_2	CL_D14-1226_ab_1	CL_D14-1226_ab_1_2
CL	D14-1226	ab	1	3	motivation_background	proposal	info-required	none	secondary	main	none	none	A typical discussion thread in an online forum spans multiple pages involving participation from multiple users and thus , may contain multiple view-points and solutions .	This paper describes an extractive summarization technique that uses textual features and dialog act information of individual messages to select a subset of posts .	CL_D14-1226_ab_1	CL_D14-1226_ab_3	CL_D14-1226_ab_1_3
CL	D14-1226	ab	1	4	motivation_background	means	info-required	by-means	secondary	secondary	none	none	A typical discussion thread in an online forum spans multiple pages involving participation from multiple users and thus , may contain multiple view-points and solutions .	Proposed approach is evaluated using two real life forum datasets .	CL_D14-1226_ab_1	CL_D14-1226_ab_4	CL_D14-1226_ab_1_4
CL	D14-1226	ab	2	3	motivation_hypothesis	proposal	support	none	secondary	main	forw	support	A user interested in the topic of discussion or having a problem similar to being discussed in the thread may not want to read all the previous posts but only a few selected posts that provide her a concise summary of the ongoing discussion .	This paper describes an extractive summarization technique that uses textual features and dialog act information of individual messages to select a subset of posts .	CL_D14-1226_ab_2	CL_D14-1226_ab_3	CL_D14-1226_ab_2_3
CL	D14-1226	ab	3	2	proposal	motivation_hypothesis	none	support	main	secondary	back	support	This paper describes an extractive summarization technique that uses textual features and dialog act information of individual messages to select a subset of posts .	A user interested in the topic of discussion or having a problem similar to being discussed in the thread may not want to read all the previous posts but only a few selected posts that provide her a concise summary of the ongoing discussion .	CL_D14-1226_ab_3	CL_D14-1226_ab_2	CL_D14-1226_ab_2_3
CL	D14-1226	ab	2	4	motivation_hypothesis	means	support	by-means	secondary	secondary	none	none	A user interested in the topic of discussion or having a problem similar to being discussed in the thread may not want to read all the previous posts but only a few selected posts that provide her a concise summary of the ongoing discussion .	Proposed approach is evaluated using two real life forum datasets .	CL_D14-1226_ab_2	CL_D14-1226_ab_4	CL_D14-1226_ab_2_4
CL	D14-1226	ab	3	4	proposal	means	none	by-means	main	secondary	back	by-means	This paper describes an extractive summarization technique that uses textual features and dialog act information of individual messages to select a subset of posts .	Proposed approach is evaluated using two real life forum datasets .	CL_D14-1226_ab_3	CL_D14-1226_ab_4	CL_D14-1226_ab_3_4
CL	D14-1226	ab	4	3	means	proposal	by-means	none	secondary	main	forw	by-means	Proposed approach is evaluated using two real life forum datasets .	This paper describes an extractive summarization technique that uses textual features and dialog act information of individual messages to select a subset of posts .	CL_D14-1226_ab_4	CL_D14-1226_ab_3	CL_D14-1226_ab_3_4
