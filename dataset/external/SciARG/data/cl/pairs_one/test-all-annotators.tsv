domain	doc_id	annotator	adu1_pos	adu2_pos	adu1_aty	adu2_aty	adu1_afu	adu2_afu	adu1_main	adu2_main	rel_direction	relation	adu1_text	adu2_text	adu1_id	adu2_id	rel_id
CL	D14-1040	ab	1	2	proposal	proposal_implementation	none	elaboration	secondary	secondary	none	none	We propose the first probabilistic approach to modeling cross-lingual semantic similarity ( CLSS ) in context which requires only comparable data .	The approach relies on an idea of projecting words and sets of words into a shared latent semantic space spanned by language-pair independent latent semantic concepts ( e.g. , cross-lingual topics obtained by a multilingual topic model ) .	CL_D14-1040_ab_1	CL_D14-1040_ab_2	CL_D14-1040_ab_1_2
CL	D14-1040	ab	1	3	proposal	proposal_implementation	none	elaboration	secondary	secondary	none	none	We propose the first probabilistic approach to modeling cross-lingual semantic similarity ( CLSS ) in context which requires only comparable data .	These latent cross-lingual concepts are induced from a comparable corpus without any additional lexical resources .	CL_D14-1040_ab_1	CL_D14-1040_ab_3	CL_D14-1040_ab_1_3
CL	D14-1040	ab	1	4	proposal	proposal_implementation	none	elaboration	secondary	secondary	none	none	We propose the first probabilistic approach to modeling cross-lingual semantic similarity ( CLSS ) in context which requires only comparable data .	Word meaning is represented as a probability distribution over the latent concepts , and a change in meaning is represented as a change in the distribution over these latent concepts .	CL_D14-1040_ab_1	CL_D14-1040_ab_4	CL_D14-1040_ab_1_4
CL	D14-1040	ab	1	5	proposal	proposal	none	elaboration	secondary	secondary	back	elaboration	We propose the first probabilistic approach to modeling cross-lingual semantic similarity ( CLSS ) in context which requires only comparable data .	We present new models that modulate the isolated out-of-context word representations with contextual knowledge .	CL_D14-1040_ab_1	CL_D14-1040_ab_5	CL_D14-1040_ab_1_5
CL	D14-1040	ab	1	6	proposal	result	none	support	secondary	secondary	none	none	We propose the first probabilistic approach to modeling cross-lingual semantic similarity ( CLSS ) in context which requires only comparable data .	Results on the task of suggesting word translations in context for 3 language pairs reveal the utility of the proposed contextualized models of cross-lingual semantic similarity .	CL_D14-1040_ab_1	CL_D14-1040_ab_6	CL_D14-1040_ab_1_6
CL	D14-1040	ab	2	3	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	The approach relies on an idea of projecting words and sets of words into a shared latent semantic space spanned by language-pair independent latent semantic concepts ( e.g. , cross-lingual topics obtained by a multilingual topic model ) .	These latent cross-lingual concepts are induced from a comparable corpus without any additional lexical resources .	CL_D14-1040_ab_2	CL_D14-1040_ab_3	CL_D14-1040_ab_2_3
CL	D14-1040	ab	2	4	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	none	none	The approach relies on an idea of projecting words and sets of words into a shared latent semantic space spanned by language-pair independent latent semantic concepts ( e.g. , cross-lingual topics obtained by a multilingual topic model ) .	Word meaning is represented as a probability distribution over the latent concepts , and a change in meaning is represented as a change in the distribution over these latent concepts .	CL_D14-1040_ab_2	CL_D14-1040_ab_4	CL_D14-1040_ab_2_4
CL	D14-1040	ab	2	5	proposal_implementation	proposal	elaboration	elaboration	secondary	secondary	forw	elaboration	The approach relies on an idea of projecting words and sets of words into a shared latent semantic space spanned by language-pair independent latent semantic concepts ( e.g. , cross-lingual topics obtained by a multilingual topic model ) .	We present new models that modulate the isolated out-of-context word representations with contextual knowledge .	CL_D14-1040_ab_2	CL_D14-1040_ab_5	CL_D14-1040_ab_2_5
CL	D14-1040	ab	2	6	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	The approach relies on an idea of projecting words and sets of words into a shared latent semantic space spanned by language-pair independent latent semantic concepts ( e.g. , cross-lingual topics obtained by a multilingual topic model ) .	Results on the task of suggesting word translations in context for 3 language pairs reveal the utility of the proposed contextualized models of cross-lingual semantic similarity .	CL_D14-1040_ab_2	CL_D14-1040_ab_6	CL_D14-1040_ab_2_6
CL	D14-1040	ab	3	4	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	These latent cross-lingual concepts are induced from a comparable corpus without any additional lexical resources .	Word meaning is represented as a probability distribution over the latent concepts , and a change in meaning is represented as a change in the distribution over these latent concepts .	CL_D14-1040_ab_3	CL_D14-1040_ab_4	CL_D14-1040_ab_3_4
CL	D14-1040	ab	3	5	proposal_implementation	proposal	elaboration	elaboration	secondary	secondary	none	none	These latent cross-lingual concepts are induced from a comparable corpus without any additional lexical resources .	We present new models that modulate the isolated out-of-context word representations with contextual knowledge .	CL_D14-1040_ab_3	CL_D14-1040_ab_5	CL_D14-1040_ab_3_5
CL	D14-1040	ab	3	6	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	These latent cross-lingual concepts are induced from a comparable corpus without any additional lexical resources .	Results on the task of suggesting word translations in context for 3 language pairs reveal the utility of the proposed contextualized models of cross-lingual semantic similarity .	CL_D14-1040_ab_3	CL_D14-1040_ab_6	CL_D14-1040_ab_3_6
CL	D14-1040	ab	4	5	proposal_implementation	proposal	elaboration	elaboration	secondary	secondary	none	none	Word meaning is represented as a probability distribution over the latent concepts , and a change in meaning is represented as a change in the distribution over these latent concepts .	We present new models that modulate the isolated out-of-context word representations with contextual knowledge .	CL_D14-1040_ab_4	CL_D14-1040_ab_5	CL_D14-1040_ab_4_5
CL	D14-1040	ab	4	6	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	Word meaning is represented as a probability distribution over the latent concepts , and a change in meaning is represented as a change in the distribution over these latent concepts .	Results on the task of suggesting word translations in context for 3 language pairs reveal the utility of the proposed contextualized models of cross-lingual semantic similarity .	CL_D14-1040_ab_4	CL_D14-1040_ab_6	CL_D14-1040_ab_4_6
CL	D14-1040	ab	5	6	proposal	result	elaboration	support	secondary	secondary	back	support	We present new models that modulate the isolated out-of-context word representations with contextual knowledge .	Results on the task of suggesting word translations in context for 3 language pairs reveal the utility of the proposed contextualized models of cross-lingual semantic similarity .	CL_D14-1040_ab_5	CL_D14-1040_ab_6	CL_D14-1040_ab_5_6
CL	D14-1040	ab	6	5	result	proposal	support	elaboration	secondary	secondary	forw	support	Results on the task of suggesting word translations in context for 3 language pairs reveal the utility of the proposed contextualized models of cross-lingual semantic similarity .	We present new models that modulate the isolated out-of-context word representations with contextual knowledge .	CL_D14-1040_ab_6	CL_D14-1040_ab_5	CL_D14-1040_ab_5_6
CL	D14-1040	mn	1	2	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	We propose the first probabilistic approach to modeling cross-lingual semantic similarity ( CLSS ) in context which requires only comparable data .	The approach relies on an idea of projecting words and sets of words into a shared latent semantic space spanned by language-pair independent latent semantic concepts ( e.g. , cross-lingual topics obtained by a multilingual topic model ) .	CL_D14-1040_mn_1	CL_D14-1040_mn_2	CL_D14-1040_mn_1_2
CL	D14-1040	mn	1	3	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	We propose the first probabilistic approach to modeling cross-lingual semantic similarity ( CLSS ) in context which requires only comparable data .	These latent cross-lingual concepts are induced from a comparable corpus without any additional lexical resources .	CL_D14-1040_mn_1	CL_D14-1040_mn_3	CL_D14-1040_mn_1_3
CL	D14-1040	mn	1	4	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	We propose the first probabilistic approach to modeling cross-lingual semantic similarity ( CLSS ) in context which requires only comparable data .	Word meaning is represented as a probability distribution over the latent concepts , and a change in meaning is represented as a change in the distribution over these latent concepts .	CL_D14-1040_mn_1	CL_D14-1040_mn_4	CL_D14-1040_mn_1_4
CL	D14-1040	mn	1	5	proposal	conclusion	none	support	main	secondary	back	support	We propose the first probabilistic approach to modeling cross-lingual semantic similarity ( CLSS ) in context which requires only comparable data .	We present new models that modulate the isolated out-of-context word representations with contextual knowledge .	CL_D14-1040_mn_1	CL_D14-1040_mn_5	CL_D14-1040_mn_1_5
CL	D14-1040	mn	1	6	proposal	result	none	support	main	secondary	none	none	We propose the first probabilistic approach to modeling cross-lingual semantic similarity ( CLSS ) in context which requires only comparable data .	Results on the task of suggesting word translations in context for 3 language pairs reveal the utility of the proposed contextualized models of cross-lingual semantic similarity .	CL_D14-1040_mn_1	CL_D14-1040_mn_6	CL_D14-1040_mn_1_6
CL	D14-1040	mn	2	3	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	The approach relies on an idea of projecting words and sets of words into a shared latent semantic space spanned by language-pair independent latent semantic concepts ( e.g. , cross-lingual topics obtained by a multilingual topic model ) .	These latent cross-lingual concepts are induced from a comparable corpus without any additional lexical resources .	CL_D14-1040_mn_2	CL_D14-1040_mn_3	CL_D14-1040_mn_2_3
CL	D14-1040	mn	2	4	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	The approach relies on an idea of projecting words and sets of words into a shared latent semantic space spanned by language-pair independent latent semantic concepts ( e.g. , cross-lingual topics obtained by a multilingual topic model ) .	Word meaning is represented as a probability distribution over the latent concepts , and a change in meaning is represented as a change in the distribution over these latent concepts .	CL_D14-1040_mn_2	CL_D14-1040_mn_4	CL_D14-1040_mn_2_4
CL	D14-1040	mn	2	5	proposal_implementation	conclusion	elaboration	support	secondary	secondary	none	none	The approach relies on an idea of projecting words and sets of words into a shared latent semantic space spanned by language-pair independent latent semantic concepts ( e.g. , cross-lingual topics obtained by a multilingual topic model ) .	We present new models that modulate the isolated out-of-context word representations with contextual knowledge .	CL_D14-1040_mn_2	CL_D14-1040_mn_5	CL_D14-1040_mn_2_5
CL	D14-1040	mn	2	6	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	The approach relies on an idea of projecting words and sets of words into a shared latent semantic space spanned by language-pair independent latent semantic concepts ( e.g. , cross-lingual topics obtained by a multilingual topic model ) .	Results on the task of suggesting word translations in context for 3 language pairs reveal the utility of the proposed contextualized models of cross-lingual semantic similarity .	CL_D14-1040_mn_2	CL_D14-1040_mn_6	CL_D14-1040_mn_2_6
CL	D14-1040	mn	3	4	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	none	none	These latent cross-lingual concepts are induced from a comparable corpus without any additional lexical resources .	Word meaning is represented as a probability distribution over the latent concepts , and a change in meaning is represented as a change in the distribution over these latent concepts .	CL_D14-1040_mn_3	CL_D14-1040_mn_4	CL_D14-1040_mn_3_4
CL	D14-1040	mn	3	5	proposal_implementation	conclusion	elaboration	support	secondary	secondary	none	none	These latent cross-lingual concepts are induced from a comparable corpus without any additional lexical resources .	We present new models that modulate the isolated out-of-context word representations with contextual knowledge .	CL_D14-1040_mn_3	CL_D14-1040_mn_5	CL_D14-1040_mn_3_5
CL	D14-1040	mn	3	6	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	These latent cross-lingual concepts are induced from a comparable corpus without any additional lexical resources .	Results on the task of suggesting word translations in context for 3 language pairs reveal the utility of the proposed contextualized models of cross-lingual semantic similarity .	CL_D14-1040_mn_3	CL_D14-1040_mn_6	CL_D14-1040_mn_3_6
CL	D14-1040	mn	4	5	proposal_implementation	conclusion	elaboration	support	secondary	secondary	none	none	Word meaning is represented as a probability distribution over the latent concepts , and a change in meaning is represented as a change in the distribution over these latent concepts .	We present new models that modulate the isolated out-of-context word representations with contextual knowledge .	CL_D14-1040_mn_4	CL_D14-1040_mn_5	CL_D14-1040_mn_4_5
CL	D14-1040	mn	4	6	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	Word meaning is represented as a probability distribution over the latent concepts , and a change in meaning is represented as a change in the distribution over these latent concepts .	Results on the task of suggesting word translations in context for 3 language pairs reveal the utility of the proposed contextualized models of cross-lingual semantic similarity .	CL_D14-1040_mn_4	CL_D14-1040_mn_6	CL_D14-1040_mn_4_6
CL	D14-1040	mn	5	6	conclusion	result	support	support	secondary	secondary	back	support	We present new models that modulate the isolated out-of-context word representations with contextual knowledge .	Results on the task of suggesting word translations in context for 3 language pairs reveal the utility of the proposed contextualized models of cross-lingual semantic similarity .	CL_D14-1040_mn_5	CL_D14-1040_mn_6	CL_D14-1040_mn_5_6
CL	D14-1040	mn	6	5	result	conclusion	support	support	secondary	secondary	forw	support	Results on the task of suggesting word translations in context for 3 language pairs reveal the utility of the proposed contextualized models of cross-lingual semantic similarity .	We present new models that modulate the isolated out-of-context word representations with contextual knowledge .	CL_D14-1040_mn_6	CL_D14-1040_mn_5	CL_D14-1040_mn_5_6
CL	D14-1040	pa	1	2	proposal	proposal	none	elaboration	main	secondary	back	elaboration	We propose the first probabilistic approach to modeling cross-lingual semantic similarity ( CLSS ) in context which requires only comparable data .	The approach relies on an idea of projecting words and sets of words into a shared latent semantic space spanned by language-pair independent latent semantic concepts ( e.g. , cross-lingual topics obtained by a multilingual topic model ) .	CL_D14-1040_pa_1	CL_D14-1040_pa_2	CL_D14-1040_pa_1_2
CL	D14-1040	pa	1	3	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	We propose the first probabilistic approach to modeling cross-lingual semantic similarity ( CLSS ) in context which requires only comparable data .	These latent cross-lingual concepts are induced from a comparable corpus without any additional lexical resources .	CL_D14-1040_pa_1	CL_D14-1040_pa_3	CL_D14-1040_pa_1_3
CL	D14-1040	pa	1	4	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	We propose the first probabilistic approach to modeling cross-lingual semantic similarity ( CLSS ) in context which requires only comparable data .	Word meaning is represented as a probability distribution over the latent concepts , and a change in meaning is represented as a change in the distribution over these latent concepts .	CL_D14-1040_pa_1	CL_D14-1040_pa_4	CL_D14-1040_pa_1_4
CL	D14-1040	pa	1	5	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	We propose the first probabilistic approach to modeling cross-lingual semantic similarity ( CLSS ) in context which requires only comparable data .	We present new models that modulate the isolated out-of-context word representations with contextual knowledge .	CL_D14-1040_pa_1	CL_D14-1040_pa_5	CL_D14-1040_pa_1_5
CL	D14-1040	pa	1	6	proposal	result	none	support	main	secondary	back	support	We propose the first probabilistic approach to modeling cross-lingual semantic similarity ( CLSS ) in context which requires only comparable data .	Results on the task of suggesting word translations in context for 3 language pairs reveal the utility of the proposed contextualized models of cross-lingual semantic similarity .	CL_D14-1040_pa_1	CL_D14-1040_pa_6	CL_D14-1040_pa_1_6
CL	D14-1040	pa	2	3	proposal	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	The approach relies on an idea of projecting words and sets of words into a shared latent semantic space spanned by language-pair independent latent semantic concepts ( e.g. , cross-lingual topics obtained by a multilingual topic model ) .	These latent cross-lingual concepts are induced from a comparable corpus without any additional lexical resources .	CL_D14-1040_pa_2	CL_D14-1040_pa_3	CL_D14-1040_pa_2_3
CL	D14-1040	pa	2	4	proposal	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	The approach relies on an idea of projecting words and sets of words into a shared latent semantic space spanned by language-pair independent latent semantic concepts ( e.g. , cross-lingual topics obtained by a multilingual topic model ) .	Word meaning is represented as a probability distribution over the latent concepts , and a change in meaning is represented as a change in the distribution over these latent concepts .	CL_D14-1040_pa_2	CL_D14-1040_pa_4	CL_D14-1040_pa_2_4
CL	D14-1040	pa	2	5	proposal	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	The approach relies on an idea of projecting words and sets of words into a shared latent semantic space spanned by language-pair independent latent semantic concepts ( e.g. , cross-lingual topics obtained by a multilingual topic model ) .	We present new models that modulate the isolated out-of-context word representations with contextual knowledge .	CL_D14-1040_pa_2	CL_D14-1040_pa_5	CL_D14-1040_pa_2_5
CL	D14-1040	pa	2	6	proposal	result	elaboration	support	secondary	secondary	none	none	The approach relies on an idea of projecting words and sets of words into a shared latent semantic space spanned by language-pair independent latent semantic concepts ( e.g. , cross-lingual topics obtained by a multilingual topic model ) .	Results on the task of suggesting word translations in context for 3 language pairs reveal the utility of the proposed contextualized models of cross-lingual semantic similarity .	CL_D14-1040_pa_2	CL_D14-1040_pa_6	CL_D14-1040_pa_2_6
CL	D14-1040	pa	3	4	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	none	none	These latent cross-lingual concepts are induced from a comparable corpus without any additional lexical resources .	Word meaning is represented as a probability distribution over the latent concepts , and a change in meaning is represented as a change in the distribution over these latent concepts .	CL_D14-1040_pa_3	CL_D14-1040_pa_4	CL_D14-1040_pa_3_4
CL	D14-1040	pa	3	5	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	none	none	These latent cross-lingual concepts are induced from a comparable corpus without any additional lexical resources .	We present new models that modulate the isolated out-of-context word representations with contextual knowledge .	CL_D14-1040_pa_3	CL_D14-1040_pa_5	CL_D14-1040_pa_3_5
CL	D14-1040	pa	3	6	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	These latent cross-lingual concepts are induced from a comparable corpus without any additional lexical resources .	Results on the task of suggesting word translations in context for 3 language pairs reveal the utility of the proposed contextualized models of cross-lingual semantic similarity .	CL_D14-1040_pa_3	CL_D14-1040_pa_6	CL_D14-1040_pa_3_6
CL	D14-1040	pa	4	5	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	none	none	Word meaning is represented as a probability distribution over the latent concepts , and a change in meaning is represented as a change in the distribution over these latent concepts .	We present new models that modulate the isolated out-of-context word representations with contextual knowledge .	CL_D14-1040_pa_4	CL_D14-1040_pa_5	CL_D14-1040_pa_4_5
CL	D14-1040	pa	4	6	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	Word meaning is represented as a probability distribution over the latent concepts , and a change in meaning is represented as a change in the distribution over these latent concepts .	Results on the task of suggesting word translations in context for 3 language pairs reveal the utility of the proposed contextualized models of cross-lingual semantic similarity .	CL_D14-1040_pa_4	CL_D14-1040_pa_6	CL_D14-1040_pa_4_6
CL	D14-1040	pa	5	6	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	We present new models that modulate the isolated out-of-context word representations with contextual knowledge .	Results on the task of suggesting word translations in context for 3 language pairs reveal the utility of the proposed contextualized models of cross-lingual semantic similarity .	CL_D14-1040_pa_5	CL_D14-1040_pa_6	CL_D14-1040_pa_5_6
CL	D14-1040	pa	6	5	result	proposal_implementation	support	elaboration	secondary	secondary	none	none	Results on the task of suggesting word translations in context for 3 language pairs reveal the utility of the proposed contextualized models of cross-lingual semantic similarity .	We present new models that modulate the isolated out-of-context word representations with contextual knowledge .	CL_D14-1040_pa_6	CL_D14-1040_pa_5	CL_D14-1040_pa_5_6
CL	D14-1041	ab	1	2	motivation_background	proposal	support	none	secondary	main	forw	support	The current approaches to Semantic Role Labeling ( SRL ) usually perform role classification for each predicate separately and the interaction among individual predicate's role labeling is ignored if there is more than one predicate in a sentence .	In this paper , we prove that different predicates in a sentence could help each other during SRL .	CL_D14-1041_ab_1	CL_D14-1041_ab_2	CL_D14-1041_ab_1_2
CL	D14-1041	ab	1	3	motivation_background	motivation_background	support	elaboration	secondary	secondary	back	elaboration	The current approaches to Semantic Role Labeling ( SRL ) usually perform role classification for each predicate separately and the interaction among individual predicate's role labeling is ignored if there is more than one predicate in a sentence .	In multi-predicate role labeling , there are mainly two key points : argument identification and role labeling of the arguments shared by multiple predicates .	CL_D14-1041_ab_1	CL_D14-1041_ab_3	CL_D14-1041_ab_1_3
CL	D14-1041	ab	1	4	motivation_background	proposal_implementation	support	elaboration	secondary	secondary	none	none	The current approaches to Semantic Role Labeling ( SRL ) usually perform role classification for each predicate separately and the interaction among individual predicate's role labeling is ignored if there is more than one predicate in a sentence .	To address these issues , in the stage of argument identification , we propose novel predicate-related features which help remove many argument identification errors ; in the stage of argument classification , we adopt a discriminative reranking approach to perform role classification of the shared arguments , in which a large set of global features are proposed .	CL_D14-1041_ab_1	CL_D14-1041_ab_4	CL_D14-1041_ab_1_4
CL	D14-1041	ab	1	5	motivation_background	means	support	by-means	secondary	secondary	none	none	The current approaches to Semantic Role Labeling ( SRL ) usually perform role classification for each predicate separately and the interaction among individual predicate's role labeling is ignored if there is more than one predicate in a sentence .	We conducted experiments on two standard benchmarks : Chinese PropBank and English PropBank .	CL_D14-1041_ab_1	CL_D14-1041_ab_5	CL_D14-1041_ab_1_5
CL	D14-1041	ab	1	6	motivation_background	result	support	support	secondary	secondary	none	none	The current approaches to Semantic Role Labeling ( SRL ) usually perform role classification for each predicate separately and the interaction among individual predicate's role labeling is ignored if there is more than one predicate in a sentence .	The experimental results show that our approach can significantly improve SRL performance , especially in Chinese PropBank .	CL_D14-1041_ab_1	CL_D14-1041_ab_6	CL_D14-1041_ab_1_6
CL	D14-1041	ab	2	3	proposal	motivation_background	none	elaboration	main	secondary	none	none	In this paper , we prove that different predicates in a sentence could help each other during SRL .	In multi-predicate role labeling , there are mainly two key points : argument identification and role labeling of the arguments shared by multiple predicates .	CL_D14-1041_ab_2	CL_D14-1041_ab_3	CL_D14-1041_ab_2_3
CL	D14-1041	ab	2	4	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	In this paper , we prove that different predicates in a sentence could help each other during SRL .	To address these issues , in the stage of argument identification , we propose novel predicate-related features which help remove many argument identification errors ; in the stage of argument classification , we adopt a discriminative reranking approach to perform role classification of the shared arguments , in which a large set of global features are proposed .	CL_D14-1041_ab_2	CL_D14-1041_ab_4	CL_D14-1041_ab_2_4
CL	D14-1041	ab	2	5	proposal	means	none	by-means	main	secondary	none	none	In this paper , we prove that different predicates in a sentence could help each other during SRL .	We conducted experiments on two standard benchmarks : Chinese PropBank and English PropBank .	CL_D14-1041_ab_2	CL_D14-1041_ab_5	CL_D14-1041_ab_2_5
CL	D14-1041	ab	2	6	proposal	result	none	support	main	secondary	none	none	In this paper , we prove that different predicates in a sentence could help each other during SRL .	The experimental results show that our approach can significantly improve SRL performance , especially in Chinese PropBank .	CL_D14-1041_ab_2	CL_D14-1041_ab_6	CL_D14-1041_ab_2_6
CL	D14-1041	ab	3	4	motivation_background	proposal_implementation	elaboration	elaboration	secondary	secondary	none	none	In multi-predicate role labeling , there are mainly two key points : argument identification and role labeling of the arguments shared by multiple predicates .	To address these issues , in the stage of argument identification , we propose novel predicate-related features which help remove many argument identification errors ; in the stage of argument classification , we adopt a discriminative reranking approach to perform role classification of the shared arguments , in which a large set of global features are proposed .	CL_D14-1041_ab_3	CL_D14-1041_ab_4	CL_D14-1041_ab_3_4
CL	D14-1041	ab	3	5	motivation_background	means	elaboration	by-means	secondary	secondary	none	none	In multi-predicate role labeling , there are mainly two key points : argument identification and role labeling of the arguments shared by multiple predicates .	We conducted experiments on two standard benchmarks : Chinese PropBank and English PropBank .	CL_D14-1041_ab_3	CL_D14-1041_ab_5	CL_D14-1041_ab_3_5
CL	D14-1041	ab	3	6	motivation_background	result	elaboration	support	secondary	secondary	none	none	In multi-predicate role labeling , there are mainly two key points : argument identification and role labeling of the arguments shared by multiple predicates .	The experimental results show that our approach can significantly improve SRL performance , especially in Chinese PropBank .	CL_D14-1041_ab_3	CL_D14-1041_ab_6	CL_D14-1041_ab_3_6
CL	D14-1041	ab	4	5	proposal_implementation	means	elaboration	by-means	secondary	secondary	none	none	To address these issues , in the stage of argument identification , we propose novel predicate-related features which help remove many argument identification errors ; in the stage of argument classification , we adopt a discriminative reranking approach to perform role classification of the shared arguments , in which a large set of global features are proposed .	We conducted experiments on two standard benchmarks : Chinese PropBank and English PropBank .	CL_D14-1041_ab_4	CL_D14-1041_ab_5	CL_D14-1041_ab_4_5
CL	D14-1041	ab	4	6	proposal_implementation	result	elaboration	support	secondary	secondary	back	support	To address these issues , in the stage of argument identification , we propose novel predicate-related features which help remove many argument identification errors ; in the stage of argument classification , we adopt a discriminative reranking approach to perform role classification of the shared arguments , in which a large set of global features are proposed .	The experimental results show that our approach can significantly improve SRL performance , especially in Chinese PropBank .	CL_D14-1041_ab_4	CL_D14-1041_ab_6	CL_D14-1041_ab_4_6
CL	D14-1041	ab	5	6	means	result	by-means	support	secondary	secondary	forw	by-means	We conducted experiments on two standard benchmarks : Chinese PropBank and English PropBank .	The experimental results show that our approach can significantly improve SRL performance , especially in Chinese PropBank .	CL_D14-1041_ab_5	CL_D14-1041_ab_6	CL_D14-1041_ab_5_6
CL	D14-1041	ab	6	5	result	means	support	by-means	secondary	secondary	back	by-means	The experimental results show that our approach can significantly improve SRL performance , especially in Chinese PropBank .	We conducted experiments on two standard benchmarks : Chinese PropBank and English PropBank .	CL_D14-1041_ab_6	CL_D14-1041_ab_5	CL_D14-1041_ab_5_6
CL	D14-1041	mn	1	2	motivation_problem	proposal	support	elaboration	secondary	secondary	none	none	The current approaches to Semantic Role Labeling ( SRL ) usually perform role classification for each predicate separately and the interaction among individual predicate's role labeling is ignored if there is more than one predicate in a sentence .	In this paper , we prove that different predicates in a sentence could help each other during SRL .	CL_D14-1041_mn_1	CL_D14-1041_mn_2	CL_D14-1041_mn_1_2
CL	D14-1041	mn	1	3	motivation_problem	information_additional	support	info-optional	secondary	secondary	none	none	The current approaches to Semantic Role Labeling ( SRL ) usually perform role classification for each predicate separately and the interaction among individual predicate's role labeling is ignored if there is more than one predicate in a sentence .	In multi-predicate role labeling , there are mainly two key points : argument identification and role labeling of the arguments shared by multiple predicates .	CL_D14-1041_mn_1	CL_D14-1041_mn_3	CL_D14-1041_mn_1_3
CL	D14-1041	mn	1	4	motivation_problem	proposal	support	none	secondary	main	forw	support	The current approaches to Semantic Role Labeling ( SRL ) usually perform role classification for each predicate separately and the interaction among individual predicate's role labeling is ignored if there is more than one predicate in a sentence .	To address these issues , in the stage of argument identification , we propose novel predicate-related features which help remove many argument identification errors ; in the stage of argument classification , we adopt a discriminative reranking approach to perform role classification of the shared arguments , in which a large set of global features are proposed .	CL_D14-1041_mn_1	CL_D14-1041_mn_4	CL_D14-1041_mn_1_4
CL	D14-1041	mn	1	5	motivation_problem	means	support	by-means	secondary	secondary	none	none	The current approaches to Semantic Role Labeling ( SRL ) usually perform role classification for each predicate separately and the interaction among individual predicate's role labeling is ignored if there is more than one predicate in a sentence .	We conducted experiments on two standard benchmarks : Chinese PropBank and English PropBank .	CL_D14-1041_mn_1	CL_D14-1041_mn_5	CL_D14-1041_mn_1_5
CL	D14-1041	mn	1	6	motivation_problem	result	support	support	secondary	secondary	none	none	The current approaches to Semantic Role Labeling ( SRL ) usually perform role classification for each predicate separately and the interaction among individual predicate's role labeling is ignored if there is more than one predicate in a sentence .	The experimental results show that our approach can significantly improve SRL performance , especially in Chinese PropBank .	CL_D14-1041_mn_1	CL_D14-1041_mn_6	CL_D14-1041_mn_1_6
CL	D14-1041	mn	2	3	proposal	information_additional	elaboration	info-optional	secondary	secondary	none	none	In this paper , we prove that different predicates in a sentence could help each other during SRL .	In multi-predicate role labeling , there are mainly two key points : argument identification and role labeling of the arguments shared by multiple predicates .	CL_D14-1041_mn_2	CL_D14-1041_mn_3	CL_D14-1041_mn_2_3
CL	D14-1041	mn	2	4	proposal	proposal	elaboration	none	secondary	main	forw	elaboration	In this paper , we prove that different predicates in a sentence could help each other during SRL .	To address these issues , in the stage of argument identification , we propose novel predicate-related features which help remove many argument identification errors ; in the stage of argument classification , we adopt a discriminative reranking approach to perform role classification of the shared arguments , in which a large set of global features are proposed .	CL_D14-1041_mn_2	CL_D14-1041_mn_4	CL_D14-1041_mn_2_4
CL	D14-1041	mn	2	5	proposal	means	elaboration	by-means	secondary	secondary	none	none	In this paper , we prove that different predicates in a sentence could help each other during SRL .	We conducted experiments on two standard benchmarks : Chinese PropBank and English PropBank .	CL_D14-1041_mn_2	CL_D14-1041_mn_5	CL_D14-1041_mn_2_5
CL	D14-1041	mn	2	6	proposal	result	elaboration	support	secondary	secondary	none	none	In this paper , we prove that different predicates in a sentence could help each other during SRL .	The experimental results show that our approach can significantly improve SRL performance , especially in Chinese PropBank .	CL_D14-1041_mn_2	CL_D14-1041_mn_6	CL_D14-1041_mn_2_6
CL	D14-1041	mn	3	4	information_additional	proposal	info-optional	none	secondary	main	forw	info-optional	In multi-predicate role labeling , there are mainly two key points : argument identification and role labeling of the arguments shared by multiple predicates .	To address these issues , in the stage of argument identification , we propose novel predicate-related features which help remove many argument identification errors ; in the stage of argument classification , we adopt a discriminative reranking approach to perform role classification of the shared arguments , in which a large set of global features are proposed .	CL_D14-1041_mn_3	CL_D14-1041_mn_4	CL_D14-1041_mn_3_4
CL	D14-1041	mn	3	5	information_additional	means	info-optional	by-means	secondary	secondary	none	none	In multi-predicate role labeling , there are mainly two key points : argument identification and role labeling of the arguments shared by multiple predicates .	We conducted experiments on two standard benchmarks : Chinese PropBank and English PropBank .	CL_D14-1041_mn_3	CL_D14-1041_mn_5	CL_D14-1041_mn_3_5
CL	D14-1041	mn	3	6	information_additional	result	info-optional	support	secondary	secondary	none	none	In multi-predicate role labeling , there are mainly two key points : argument identification and role labeling of the arguments shared by multiple predicates .	The experimental results show that our approach can significantly improve SRL performance , especially in Chinese PropBank .	CL_D14-1041_mn_3	CL_D14-1041_mn_6	CL_D14-1041_mn_3_6
CL	D14-1041	mn	4	5	proposal	means	none	by-means	main	secondary	none	none	To address these issues , in the stage of argument identification , we propose novel predicate-related features which help remove many argument identification errors ; in the stage of argument classification , we adopt a discriminative reranking approach to perform role classification of the shared arguments , in which a large set of global features are proposed .	We conducted experiments on two standard benchmarks : Chinese PropBank and English PropBank .	CL_D14-1041_mn_4	CL_D14-1041_mn_5	CL_D14-1041_mn_4_5
CL	D14-1041	mn	4	6	proposal	result	none	support	main	secondary	back	support	To address these issues , in the stage of argument identification , we propose novel predicate-related features which help remove many argument identification errors ; in the stage of argument classification , we adopt a discriminative reranking approach to perform role classification of the shared arguments , in which a large set of global features are proposed .	The experimental results show that our approach can significantly improve SRL performance , especially in Chinese PropBank .	CL_D14-1041_mn_4	CL_D14-1041_mn_6	CL_D14-1041_mn_4_6
CL	D14-1041	mn	5	6	means	result	by-means	support	secondary	secondary	forw	by-means	We conducted experiments on two standard benchmarks : Chinese PropBank and English PropBank .	The experimental results show that our approach can significantly improve SRL performance , especially in Chinese PropBank .	CL_D14-1041_mn_5	CL_D14-1041_mn_6	CL_D14-1041_mn_5_6
CL	D14-1041	mn	6	5	result	means	support	by-means	secondary	secondary	back	by-means	The experimental results show that our approach can significantly improve SRL performance , especially in Chinese PropBank .	We conducted experiments on two standard benchmarks : Chinese PropBank and English PropBank .	CL_D14-1041_mn_6	CL_D14-1041_mn_5	CL_D14-1041_mn_5_6
CL	D14-1041	pa	1	2	motivation_problem	proposal	support	none	secondary	main	forw	support	The current approaches to Semantic Role Labeling ( SRL ) usually perform role classification for each predicate separately and the interaction among individual predicate's role labeling is ignored if there is more than one predicate in a sentence .	In this paper , we prove that different predicates in a sentence could help each other during SRL .	CL_D14-1041_pa_1	CL_D14-1041_pa_2	CL_D14-1041_pa_1_2
CL	D14-1041	pa	1	3	motivation_problem	motivation_background	support	info-required	secondary	secondary	back	info-required	The current approaches to Semantic Role Labeling ( SRL ) usually perform role classification for each predicate separately and the interaction among individual predicate's role labeling is ignored if there is more than one predicate in a sentence .	In multi-predicate role labeling , there are mainly two key points : argument identification and role labeling of the arguments shared by multiple predicates .	CL_D14-1041_pa_1	CL_D14-1041_pa_3	CL_D14-1041_pa_1_3
CL	D14-1041	pa	1	4	motivation_problem	proposal_implementation	support	elaboration	secondary	secondary	none	none	The current approaches to Semantic Role Labeling ( SRL ) usually perform role classification for each predicate separately and the interaction among individual predicate's role labeling is ignored if there is more than one predicate in a sentence .	To address these issues , in the stage of argument identification , we propose novel predicate-related features which help remove many argument identification errors ; in the stage of argument classification , we adopt a discriminative reranking approach to perform role classification of the shared arguments , in which a large set of global features are proposed .	CL_D14-1041_pa_1	CL_D14-1041_pa_4	CL_D14-1041_pa_1_4
CL	D14-1041	pa	1	5	motivation_problem	means	support	by-means	secondary	secondary	none	none	The current approaches to Semantic Role Labeling ( SRL ) usually perform role classification for each predicate separately and the interaction among individual predicate's role labeling is ignored if there is more than one predicate in a sentence .	We conducted experiments on two standard benchmarks : Chinese PropBank and English PropBank .	CL_D14-1041_pa_1	CL_D14-1041_pa_5	CL_D14-1041_pa_1_5
CL	D14-1041	pa	1	6	motivation_problem	result	support	support	secondary	secondary	none	none	The current approaches to Semantic Role Labeling ( SRL ) usually perform role classification for each predicate separately and the interaction among individual predicate's role labeling is ignored if there is more than one predicate in a sentence .	The experimental results show that our approach can significantly improve SRL performance , especially in Chinese PropBank .	CL_D14-1041_pa_1	CL_D14-1041_pa_6	CL_D14-1041_pa_1_6
CL	D14-1041	pa	2	3	proposal	motivation_background	none	info-required	main	secondary	none	none	In this paper , we prove that different predicates in a sentence could help each other during SRL .	In multi-predicate role labeling , there are mainly two key points : argument identification and role labeling of the arguments shared by multiple predicates .	CL_D14-1041_pa_2	CL_D14-1041_pa_3	CL_D14-1041_pa_2_3
CL	D14-1041	pa	2	4	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	In this paper , we prove that different predicates in a sentence could help each other during SRL .	To address these issues , in the stage of argument identification , we propose novel predicate-related features which help remove many argument identification errors ; in the stage of argument classification , we adopt a discriminative reranking approach to perform role classification of the shared arguments , in which a large set of global features are proposed .	CL_D14-1041_pa_2	CL_D14-1041_pa_4	CL_D14-1041_pa_2_4
CL	D14-1041	pa	2	5	proposal	means	none	by-means	main	secondary	none	none	In this paper , we prove that different predicates in a sentence could help each other during SRL .	We conducted experiments on two standard benchmarks : Chinese PropBank and English PropBank .	CL_D14-1041_pa_2	CL_D14-1041_pa_5	CL_D14-1041_pa_2_5
CL	D14-1041	pa	2	6	proposal	result	none	support	main	secondary	back	support	In this paper , we prove that different predicates in a sentence could help each other during SRL .	The experimental results show that our approach can significantly improve SRL performance , especially in Chinese PropBank .	CL_D14-1041_pa_2	CL_D14-1041_pa_6	CL_D14-1041_pa_2_6
CL	D14-1041	pa	3	4	motivation_background	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	In multi-predicate role labeling , there are mainly two key points : argument identification and role labeling of the arguments shared by multiple predicates .	To address these issues , in the stage of argument identification , we propose novel predicate-related features which help remove many argument identification errors ; in the stage of argument classification , we adopt a discriminative reranking approach to perform role classification of the shared arguments , in which a large set of global features are proposed .	CL_D14-1041_pa_3	CL_D14-1041_pa_4	CL_D14-1041_pa_3_4
CL	D14-1041	pa	3	5	motivation_background	means	info-required	by-means	secondary	secondary	none	none	In multi-predicate role labeling , there are mainly two key points : argument identification and role labeling of the arguments shared by multiple predicates .	We conducted experiments on two standard benchmarks : Chinese PropBank and English PropBank .	CL_D14-1041_pa_3	CL_D14-1041_pa_5	CL_D14-1041_pa_3_5
CL	D14-1041	pa	3	6	motivation_background	result	info-required	support	secondary	secondary	none	none	In multi-predicate role labeling , there are mainly two key points : argument identification and role labeling of the arguments shared by multiple predicates .	The experimental results show that our approach can significantly improve SRL performance , especially in Chinese PropBank .	CL_D14-1041_pa_3	CL_D14-1041_pa_6	CL_D14-1041_pa_3_6
CL	D14-1041	pa	4	5	proposal_implementation	means	elaboration	by-means	secondary	secondary	none	none	To address these issues , in the stage of argument identification , we propose novel predicate-related features which help remove many argument identification errors ; in the stage of argument classification , we adopt a discriminative reranking approach to perform role classification of the shared arguments , in which a large set of global features are proposed .	We conducted experiments on two standard benchmarks : Chinese PropBank and English PropBank .	CL_D14-1041_pa_4	CL_D14-1041_pa_5	CL_D14-1041_pa_4_5
CL	D14-1041	pa	4	6	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	To address these issues , in the stage of argument identification , we propose novel predicate-related features which help remove many argument identification errors ; in the stage of argument classification , we adopt a discriminative reranking approach to perform role classification of the shared arguments , in which a large set of global features are proposed .	The experimental results show that our approach can significantly improve SRL performance , especially in Chinese PropBank .	CL_D14-1041_pa_4	CL_D14-1041_pa_6	CL_D14-1041_pa_4_6
CL	D14-1041	pa	5	6	means	result	by-means	support	secondary	secondary	forw	by-means	We conducted experiments on two standard benchmarks : Chinese PropBank and English PropBank .	The experimental results show that our approach can significantly improve SRL performance , especially in Chinese PropBank .	CL_D14-1041_pa_5	CL_D14-1041_pa_6	CL_D14-1041_pa_5_6
CL	D14-1041	pa	6	5	result	means	support	by-means	secondary	secondary	back	by-means	The experimental results show that our approach can significantly improve SRL performance , especially in Chinese PropBank .	We conducted experiments on two standard benchmarks : Chinese PropBank and English PropBank .	CL_D14-1041_pa_6	CL_D14-1041_pa_5	CL_D14-1041_pa_5_6
CL	D14-1042	ab	1	2	motivation_background	motivation_problem	info-required	support	secondary	secondary	forw	info-required	Word-sense recognition and disambiguation ( WERD ) is the task of identifying word phrases and their senses in natural language text .	Though it is well understood how to disambiguate noun phrases , this task is much less studied for verbs and verbal phrases .	CL_D14-1042_ab_1	CL_D14-1042_ab_2	CL_D14-1042_ab_1_2
CL	D14-1042	ab	1	3	motivation_background	proposal	info-required	none	secondary	main	none	none	Word-sense recognition and disambiguation ( WERD ) is the task of identifying word phrases and their senses in natural language text .	We present Werdy , a framework for WERD with particular focus on verbs and verbal phrases .	CL_D14-1042_ab_1	CL_D14-1042_ab_3	CL_D14-1042_ab_1_3
CL	D14-1042	ab	1	4	motivation_background	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	Word-sense recognition and disambiguation ( WERD ) is the task of identifying word phrases and their senses in natural language text .	Our framework first identifies multi-word expressions based on the syntactic structure of the sentence ; this allows us to recognize both contiguous and non-contiguous phrases .	CL_D14-1042_ab_1	CL_D14-1042_ab_4	CL_D14-1042_ab_1_4
CL	D14-1042	ab	1	5	motivation_background	proposal_implementation	info-required	sequence	secondary	secondary	none	none	Word-sense recognition and disambiguation ( WERD ) is the task of identifying word phrases and their senses in natural language text .	We then generate a list of candidate senses for each word or phrase , using novel syntactic and semantic pruning techniques .	CL_D14-1042_ab_1	CL_D14-1042_ab_5	CL_D14-1042_ab_1_5
CL	D14-1042	ab	1	6	motivation_background	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	Word-sense recognition and disambiguation ( WERD ) is the task of identifying word phrases and their senses in natural language text .	We also construct and leverage a new resource of pairs of senses for verbs and their object arguments .	CL_D14-1042_ab_1	CL_D14-1042_ab_6	CL_D14-1042_ab_1_6
CL	D14-1042	ab	1	7	motivation_background	proposal_implementation	info-required	sequence	secondary	secondary	none	none	Word-sense recognition and disambiguation ( WERD ) is the task of identifying word phrases and their senses in natural language text .	Finally , we feed the so-obtained candidate senses into standard word-sense disambiguation ( WSD ) methods , and boost their precision and recall .	CL_D14-1042_ab_1	CL_D14-1042_ab_7	CL_D14-1042_ab_1_7
CL	D14-1042	ab	1	8	motivation_background	result	info-required	support	secondary	secondary	none	none	Word-sense recognition and disambiguation ( WERD ) is the task of identifying word phrases and their senses in natural language text .	Our experiments indicate that Werdy significantly increases the performance of existing WSD methods .	CL_D14-1042_ab_1	CL_D14-1042_ab_8	CL_D14-1042_ab_1_8
CL	D14-1042	ab	2	3	motivation_problem	proposal	support	none	secondary	main	forw	support	Though it is well understood how to disambiguate noun phrases , this task is much less studied for verbs and verbal phrases .	We present Werdy , a framework for WERD with particular focus on verbs and verbal phrases .	CL_D14-1042_ab_2	CL_D14-1042_ab_3	CL_D14-1042_ab_2_3
CL	D14-1042	ab	2	4	motivation_problem	proposal_implementation	support	elaboration	secondary	secondary	none	none	Though it is well understood how to disambiguate noun phrases , this task is much less studied for verbs and verbal phrases .	Our framework first identifies multi-word expressions based on the syntactic structure of the sentence ; this allows us to recognize both contiguous and non-contiguous phrases .	CL_D14-1042_ab_2	CL_D14-1042_ab_4	CL_D14-1042_ab_2_4
CL	D14-1042	ab	2	5	motivation_problem	proposal_implementation	support	sequence	secondary	secondary	none	none	Though it is well understood how to disambiguate noun phrases , this task is much less studied for verbs and verbal phrases .	We then generate a list of candidate senses for each word or phrase , using novel syntactic and semantic pruning techniques .	CL_D14-1042_ab_2	CL_D14-1042_ab_5	CL_D14-1042_ab_2_5
CL	D14-1042	ab	2	6	motivation_problem	proposal_implementation	support	elaboration	secondary	secondary	none	none	Though it is well understood how to disambiguate noun phrases , this task is much less studied for verbs and verbal phrases .	We also construct and leverage a new resource of pairs of senses for verbs and their object arguments .	CL_D14-1042_ab_2	CL_D14-1042_ab_6	CL_D14-1042_ab_2_6
CL	D14-1042	ab	2	7	motivation_problem	proposal_implementation	support	sequence	secondary	secondary	none	none	Though it is well understood how to disambiguate noun phrases , this task is much less studied for verbs and verbal phrases .	Finally , we feed the so-obtained candidate senses into standard word-sense disambiguation ( WSD ) methods , and boost their precision and recall .	CL_D14-1042_ab_2	CL_D14-1042_ab_7	CL_D14-1042_ab_2_7
CL	D14-1042	ab	2	8	motivation_problem	result	support	support	secondary	secondary	none	none	Though it is well understood how to disambiguate noun phrases , this task is much less studied for verbs and verbal phrases .	Our experiments indicate that Werdy significantly increases the performance of existing WSD methods .	CL_D14-1042_ab_2	CL_D14-1042_ab_8	CL_D14-1042_ab_2_8
CL	D14-1042	ab	3	4	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	We present Werdy , a framework for WERD with particular focus on verbs and verbal phrases .	Our framework first identifies multi-word expressions based on the syntactic structure of the sentence ; this allows us to recognize both contiguous and non-contiguous phrases .	CL_D14-1042_ab_3	CL_D14-1042_ab_4	CL_D14-1042_ab_3_4
CL	D14-1042	ab	3	5	proposal	proposal_implementation	none	sequence	main	secondary	none	none	We present Werdy , a framework for WERD with particular focus on verbs and verbal phrases .	We then generate a list of candidate senses for each word or phrase , using novel syntactic and semantic pruning techniques .	CL_D14-1042_ab_3	CL_D14-1042_ab_5	CL_D14-1042_ab_3_5
CL	D14-1042	ab	3	6	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	We present Werdy , a framework for WERD with particular focus on verbs and verbal phrases .	We also construct and leverage a new resource of pairs of senses for verbs and their object arguments .	CL_D14-1042_ab_3	CL_D14-1042_ab_6	CL_D14-1042_ab_3_6
CL	D14-1042	ab	3	7	proposal	proposal_implementation	none	sequence	main	secondary	none	none	We present Werdy , a framework for WERD with particular focus on verbs and verbal phrases .	Finally , we feed the so-obtained candidate senses into standard word-sense disambiguation ( WSD ) methods , and boost their precision and recall .	CL_D14-1042_ab_3	CL_D14-1042_ab_7	CL_D14-1042_ab_3_7
CL	D14-1042	ab	3	8	proposal	result	none	support	main	secondary	back	support	We present Werdy , a framework for WERD with particular focus on verbs and verbal phrases .	Our experiments indicate that Werdy significantly increases the performance of existing WSD methods .	CL_D14-1042_ab_3	CL_D14-1042_ab_8	CL_D14-1042_ab_3_8
CL	D14-1042	ab	4	5	proposal_implementation	proposal_implementation	elaboration	sequence	secondary	secondary	back	sequence	Our framework first identifies multi-word expressions based on the syntactic structure of the sentence ; this allows us to recognize both contiguous and non-contiguous phrases .	We then generate a list of candidate senses for each word or phrase , using novel syntactic and semantic pruning techniques .	CL_D14-1042_ab_4	CL_D14-1042_ab_5	CL_D14-1042_ab_4_5
CL	D14-1042	ab	4	6	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	Our framework first identifies multi-word expressions based on the syntactic structure of the sentence ; this allows us to recognize both contiguous and non-contiguous phrases .	We also construct and leverage a new resource of pairs of senses for verbs and their object arguments .	CL_D14-1042_ab_4	CL_D14-1042_ab_6	CL_D14-1042_ab_4_6
CL	D14-1042	ab	4	7	proposal_implementation	proposal_implementation	elaboration	sequence	secondary	secondary	none	none	Our framework first identifies multi-word expressions based on the syntactic structure of the sentence ; this allows us to recognize both contiguous and non-contiguous phrases .	Finally , we feed the so-obtained candidate senses into standard word-sense disambiguation ( WSD ) methods , and boost their precision and recall .	CL_D14-1042_ab_4	CL_D14-1042_ab_7	CL_D14-1042_ab_4_7
CL	D14-1042	ab	4	8	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	Our framework first identifies multi-word expressions based on the syntactic structure of the sentence ; this allows us to recognize both contiguous and non-contiguous phrases .	Our experiments indicate that Werdy significantly increases the performance of existing WSD methods .	CL_D14-1042_ab_4	CL_D14-1042_ab_8	CL_D14-1042_ab_4_8
CL	D14-1042	ab	5	6	proposal_implementation	proposal_implementation	sequence	elaboration	secondary	secondary	none	none	We then generate a list of candidate senses for each word or phrase , using novel syntactic and semantic pruning techniques .	We also construct and leverage a new resource of pairs of senses for verbs and their object arguments .	CL_D14-1042_ab_5	CL_D14-1042_ab_6	CL_D14-1042_ab_5_6
CL	D14-1042	ab	5	7	proposal_implementation	proposal_implementation	sequence	sequence	secondary	secondary	back	sequence	We then generate a list of candidate senses for each word or phrase , using novel syntactic and semantic pruning techniques .	Finally , we feed the so-obtained candidate senses into standard word-sense disambiguation ( WSD ) methods , and boost their precision and recall .	CL_D14-1042_ab_5	CL_D14-1042_ab_7	CL_D14-1042_ab_5_7
CL	D14-1042	ab	5	8	proposal_implementation	result	sequence	support	secondary	secondary	none	none	We then generate a list of candidate senses for each word or phrase , using novel syntactic and semantic pruning techniques .	Our experiments indicate that Werdy significantly increases the performance of existing WSD methods .	CL_D14-1042_ab_5	CL_D14-1042_ab_8	CL_D14-1042_ab_5_8
CL	D14-1042	ab	6	7	proposal_implementation	proposal_implementation	elaboration	sequence	secondary	secondary	none	none	We also construct and leverage a new resource of pairs of senses for verbs and their object arguments .	Finally , we feed the so-obtained candidate senses into standard word-sense disambiguation ( WSD ) methods , and boost their precision and recall .	CL_D14-1042_ab_6	CL_D14-1042_ab_7	CL_D14-1042_ab_6_7
CL	D14-1042	ab	6	8	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	We also construct and leverage a new resource of pairs of senses for verbs and their object arguments .	Our experiments indicate that Werdy significantly increases the performance of existing WSD methods .	CL_D14-1042_ab_6	CL_D14-1042_ab_8	CL_D14-1042_ab_6_8
CL	D14-1042	ab	7	8	proposal_implementation	result	sequence	support	secondary	secondary	none	none	Finally , we feed the so-obtained candidate senses into standard word-sense disambiguation ( WSD ) methods , and boost their precision and recall .	Our experiments indicate that Werdy significantly increases the performance of existing WSD methods .	CL_D14-1042_ab_7	CL_D14-1042_ab_8	CL_D14-1042_ab_7_8
CL	D14-1042	ab	8	7	result	proposal_implementation	support	sequence	secondary	secondary	none	none	Our experiments indicate that Werdy significantly increases the performance of existing WSD methods .	Finally , we feed the so-obtained candidate senses into standard word-sense disambiguation ( WSD ) methods , and boost their precision and recall .	CL_D14-1042_ab_8	CL_D14-1042_ab_7	CL_D14-1042_ab_7_8
CL	D14-1042	mn	1	2	motivation_background	motivation_problem	info-required	support	secondary	secondary	forw	info-required	Word-sense recognition and disambiguation ( WERD ) is the task of identifying word phrases and their senses in natural language text .	Though it is well understood how to disambiguate noun phrases , this task is much less studied for verbs and verbal phrases .	CL_D14-1042_mn_1	CL_D14-1042_mn_2	CL_D14-1042_mn_1_2
CL	D14-1042	mn	1	3	motivation_background	proposal	info-required	none	secondary	main	none	none	Word-sense recognition and disambiguation ( WERD ) is the task of identifying word phrases and their senses in natural language text .	We present Werdy , a framework for WERD with particular focus on verbs and verbal phrases .	CL_D14-1042_mn_1	CL_D14-1042_mn_3	CL_D14-1042_mn_1_3
CL	D14-1042	mn	1	4	motivation_background	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	Word-sense recognition and disambiguation ( WERD ) is the task of identifying word phrases and their senses in natural language text .	Our framework first identifies multi-word expressions based on the syntactic structure of the sentence ; this allows us to recognize both contiguous and non-contiguous phrases .	CL_D14-1042_mn_1	CL_D14-1042_mn_4	CL_D14-1042_mn_1_4
CL	D14-1042	mn	1	5	motivation_background	proposal_implementation	info-required	sequence	secondary	secondary	none	none	Word-sense recognition and disambiguation ( WERD ) is the task of identifying word phrases and their senses in natural language text .	We then generate a list of candidate senses for each word or phrase , using novel syntactic and semantic pruning techniques .	CL_D14-1042_mn_1	CL_D14-1042_mn_5	CL_D14-1042_mn_1_5
CL	D14-1042	mn	1	6	motivation_background	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	Word-sense recognition and disambiguation ( WERD ) is the task of identifying word phrases and their senses in natural language text .	We also construct and leverage a new resource of pairs of senses for verbs and their object arguments .	CL_D14-1042_mn_1	CL_D14-1042_mn_6	CL_D14-1042_mn_1_6
CL	D14-1042	mn	1	7	motivation_background	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	Word-sense recognition and disambiguation ( WERD ) is the task of identifying word phrases and their senses in natural language text .	Finally , we feed the so-obtained candidate senses into standard word-sense disambiguation ( WSD ) methods , and boost their precision and recall .	CL_D14-1042_mn_1	CL_D14-1042_mn_7	CL_D14-1042_mn_1_7
CL	D14-1042	mn	1	8	motivation_background	result	info-required	support	secondary	secondary	none	none	Word-sense recognition and disambiguation ( WERD ) is the task of identifying word phrases and their senses in natural language text .	Our experiments indicate that Werdy significantly increases the performance of existing WSD methods .	CL_D14-1042_mn_1	CL_D14-1042_mn_8	CL_D14-1042_mn_1_8
CL	D14-1042	mn	2	3	motivation_problem	proposal	support	none	secondary	main	forw	support	Though it is well understood how to disambiguate noun phrases , this task is much less studied for verbs and verbal phrases .	We present Werdy , a framework for WERD with particular focus on verbs and verbal phrases .	CL_D14-1042_mn_2	CL_D14-1042_mn_3	CL_D14-1042_mn_2_3
CL	D14-1042	mn	2	4	motivation_problem	proposal_implementation	support	elaboration	secondary	secondary	none	none	Though it is well understood how to disambiguate noun phrases , this task is much less studied for verbs and verbal phrases .	Our framework first identifies multi-word expressions based on the syntactic structure of the sentence ; this allows us to recognize both contiguous and non-contiguous phrases .	CL_D14-1042_mn_2	CL_D14-1042_mn_4	CL_D14-1042_mn_2_4
CL	D14-1042	mn	2	5	motivation_problem	proposal_implementation	support	sequence	secondary	secondary	none	none	Though it is well understood how to disambiguate noun phrases , this task is much less studied for verbs and verbal phrases .	We then generate a list of candidate senses for each word or phrase , using novel syntactic and semantic pruning techniques .	CL_D14-1042_mn_2	CL_D14-1042_mn_5	CL_D14-1042_mn_2_5
CL	D14-1042	mn	2	6	motivation_problem	proposal_implementation	support	elaboration	secondary	secondary	none	none	Though it is well understood how to disambiguate noun phrases , this task is much less studied for verbs and verbal phrases .	We also construct and leverage a new resource of pairs of senses for verbs and their object arguments .	CL_D14-1042_mn_2	CL_D14-1042_mn_6	CL_D14-1042_mn_2_6
CL	D14-1042	mn	2	7	motivation_problem	proposal_implementation	support	elaboration	secondary	secondary	none	none	Though it is well understood how to disambiguate noun phrases , this task is much less studied for verbs and verbal phrases .	Finally , we feed the so-obtained candidate senses into standard word-sense disambiguation ( WSD ) methods , and boost their precision and recall .	CL_D14-1042_mn_2	CL_D14-1042_mn_7	CL_D14-1042_mn_2_7
CL	D14-1042	mn	2	8	motivation_problem	result	support	support	secondary	secondary	none	none	Though it is well understood how to disambiguate noun phrases , this task is much less studied for verbs and verbal phrases .	Our experiments indicate that Werdy significantly increases the performance of existing WSD methods .	CL_D14-1042_mn_2	CL_D14-1042_mn_8	CL_D14-1042_mn_2_8
CL	D14-1042	mn	3	4	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	We present Werdy , a framework for WERD with particular focus on verbs and verbal phrases .	Our framework first identifies multi-word expressions based on the syntactic structure of the sentence ; this allows us to recognize both contiguous and non-contiguous phrases .	CL_D14-1042_mn_3	CL_D14-1042_mn_4	CL_D14-1042_mn_3_4
CL	D14-1042	mn	3	5	proposal	proposal_implementation	none	sequence	main	secondary	none	none	We present Werdy , a framework for WERD with particular focus on verbs and verbal phrases .	We then generate a list of candidate senses for each word or phrase , using novel syntactic and semantic pruning techniques .	CL_D14-1042_mn_3	CL_D14-1042_mn_5	CL_D14-1042_mn_3_5
CL	D14-1042	mn	3	6	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	We present Werdy , a framework for WERD with particular focus on verbs and verbal phrases .	We also construct and leverage a new resource of pairs of senses for verbs and their object arguments .	CL_D14-1042_mn_3	CL_D14-1042_mn_6	CL_D14-1042_mn_3_6
CL	D14-1042	mn	3	7	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	We present Werdy , a framework for WERD with particular focus on verbs and verbal phrases .	Finally , we feed the so-obtained candidate senses into standard word-sense disambiguation ( WSD ) methods , and boost their precision and recall .	CL_D14-1042_mn_3	CL_D14-1042_mn_7	CL_D14-1042_mn_3_7
CL	D14-1042	mn	3	8	proposal	result	none	support	main	secondary	back	support	We present Werdy , a framework for WERD with particular focus on verbs and verbal phrases .	Our experiments indicate that Werdy significantly increases the performance of existing WSD methods .	CL_D14-1042_mn_3	CL_D14-1042_mn_8	CL_D14-1042_mn_3_8
CL	D14-1042	mn	4	5	proposal_implementation	proposal_implementation	elaboration	sequence	secondary	secondary	back	sequence	Our framework first identifies multi-word expressions based on the syntactic structure of the sentence ; this allows us to recognize both contiguous and non-contiguous phrases .	We then generate a list of candidate senses for each word or phrase , using novel syntactic and semantic pruning techniques .	CL_D14-1042_mn_4	CL_D14-1042_mn_5	CL_D14-1042_mn_4_5
CL	D14-1042	mn	4	6	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	none	none	Our framework first identifies multi-word expressions based on the syntactic structure of the sentence ; this allows us to recognize both contiguous and non-contiguous phrases .	We also construct and leverage a new resource of pairs of senses for verbs and their object arguments .	CL_D14-1042_mn_4	CL_D14-1042_mn_6	CL_D14-1042_mn_4_6
CL	D14-1042	mn	4	7	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	none	none	Our framework first identifies multi-word expressions based on the syntactic structure of the sentence ; this allows us to recognize both contiguous and non-contiguous phrases .	Finally , we feed the so-obtained candidate senses into standard word-sense disambiguation ( WSD ) methods , and boost their precision and recall .	CL_D14-1042_mn_4	CL_D14-1042_mn_7	CL_D14-1042_mn_4_7
CL	D14-1042	mn	4	8	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	Our framework first identifies multi-word expressions based on the syntactic structure of the sentence ; this allows us to recognize both contiguous and non-contiguous phrases .	Our experiments indicate that Werdy significantly increases the performance of existing WSD methods .	CL_D14-1042_mn_4	CL_D14-1042_mn_8	CL_D14-1042_mn_4_8
CL	D14-1042	mn	5	6	proposal_implementation	proposal_implementation	sequence	elaboration	secondary	secondary	back	elaboration	We then generate a list of candidate senses for each word or phrase , using novel syntactic and semantic pruning techniques .	We also construct and leverage a new resource of pairs of senses for verbs and their object arguments .	CL_D14-1042_mn_5	CL_D14-1042_mn_6	CL_D14-1042_mn_5_6
CL	D14-1042	mn	5	7	proposal_implementation	proposal_implementation	sequence	elaboration	secondary	secondary	back	elaboration	We then generate a list of candidate senses for each word or phrase , using novel syntactic and semantic pruning techniques .	Finally , we feed the so-obtained candidate senses into standard word-sense disambiguation ( WSD ) methods , and boost their precision and recall .	CL_D14-1042_mn_5	CL_D14-1042_mn_7	CL_D14-1042_mn_5_7
CL	D14-1042	mn	5	8	proposal_implementation	result	sequence	support	secondary	secondary	none	none	We then generate a list of candidate senses for each word or phrase , using novel syntactic and semantic pruning techniques .	Our experiments indicate that Werdy significantly increases the performance of existing WSD methods .	CL_D14-1042_mn_5	CL_D14-1042_mn_8	CL_D14-1042_mn_5_8
CL	D14-1042	mn	6	7	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	none	none	We also construct and leverage a new resource of pairs of senses for verbs and their object arguments .	Finally , we feed the so-obtained candidate senses into standard word-sense disambiguation ( WSD ) methods , and boost their precision and recall .	CL_D14-1042_mn_6	CL_D14-1042_mn_7	CL_D14-1042_mn_6_7
CL	D14-1042	mn	6	8	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	We also construct and leverage a new resource of pairs of senses for verbs and their object arguments .	Our experiments indicate that Werdy significantly increases the performance of existing WSD methods .	CL_D14-1042_mn_6	CL_D14-1042_mn_8	CL_D14-1042_mn_6_8
CL	D14-1042	mn	7	8	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	Finally , we feed the so-obtained candidate senses into standard word-sense disambiguation ( WSD ) methods , and boost their precision and recall .	Our experiments indicate that Werdy significantly increases the performance of existing WSD methods .	CL_D14-1042_mn_7	CL_D14-1042_mn_8	CL_D14-1042_mn_7_8
CL	D14-1042	mn	8	7	result	proposal_implementation	support	elaboration	secondary	secondary	none	none	Our experiments indicate that Werdy significantly increases the performance of existing WSD methods .	Finally , we feed the so-obtained candidate senses into standard word-sense disambiguation ( WSD ) methods , and boost their precision and recall .	CL_D14-1042_mn_8	CL_D14-1042_mn_7	CL_D14-1042_mn_7_8
CL	D14-1042	pa	1	2	information_additional	motivation_background	info-optional	support	secondary	secondary	forw	info-optional	Word-sense recognition and disambiguation ( WERD ) is the task of identifying word phrases and their senses in natural language text .	Though it is well understood how to disambiguate noun phrases , this task is much less studied for verbs and verbal phrases .	CL_D14-1042_pa_1	CL_D14-1042_pa_2	CL_D14-1042_pa_1_2
CL	D14-1042	pa	1	3	information_additional	proposal	info-optional	none	secondary	main	none	none	Word-sense recognition and disambiguation ( WERD ) is the task of identifying word phrases and their senses in natural language text .	We present Werdy , a framework for WERD with particular focus on verbs and verbal phrases .	CL_D14-1042_pa_1	CL_D14-1042_pa_3	CL_D14-1042_pa_1_3
CL	D14-1042	pa	1	4	information_additional	proposal_implementation	info-optional	elaboration	secondary	secondary	none	none	Word-sense recognition and disambiguation ( WERD ) is the task of identifying word phrases and their senses in natural language text .	Our framework first identifies multi-word expressions based on the syntactic structure of the sentence ; this allows us to recognize both contiguous and non-contiguous phrases .	CL_D14-1042_pa_1	CL_D14-1042_pa_4	CL_D14-1042_pa_1_4
CL	D14-1042	pa	1	5	information_additional	proposal_implementation	info-optional	sequence	secondary	secondary	none	none	Word-sense recognition and disambiguation ( WERD ) is the task of identifying word phrases and their senses in natural language text .	We then generate a list of candidate senses for each word or phrase , using novel syntactic and semantic pruning techniques .	CL_D14-1042_pa_1	CL_D14-1042_pa_5	CL_D14-1042_pa_1_5
CL	D14-1042	pa	1	6	information_additional	proposal_implementation	info-optional	sequence	secondary	secondary	none	none	Word-sense recognition and disambiguation ( WERD ) is the task of identifying word phrases and their senses in natural language text .	We also construct and leverage a new resource of pairs of senses for verbs and their object arguments .	CL_D14-1042_pa_1	CL_D14-1042_pa_6	CL_D14-1042_pa_1_6
CL	D14-1042	pa	1	7	information_additional	proposal_implementation	info-optional	sequence	secondary	secondary	none	none	Word-sense recognition and disambiguation ( WERD ) is the task of identifying word phrases and their senses in natural language text .	Finally , we feed the so-obtained candidate senses into standard word-sense disambiguation ( WSD ) methods , and boost their precision and recall .	CL_D14-1042_pa_1	CL_D14-1042_pa_7	CL_D14-1042_pa_1_7
CL	D14-1042	pa	1	8	information_additional	result	info-optional	support	secondary	secondary	none	none	Word-sense recognition and disambiguation ( WERD ) is the task of identifying word phrases and their senses in natural language text .	Our experiments indicate that Werdy significantly increases the performance of existing WSD methods .	CL_D14-1042_pa_1	CL_D14-1042_pa_8	CL_D14-1042_pa_1_8
CL	D14-1042	pa	2	3	motivation_background	proposal	support	none	secondary	main	forw	support	Though it is well understood how to disambiguate noun phrases , this task is much less studied for verbs and verbal phrases .	We present Werdy , a framework for WERD with particular focus on verbs and verbal phrases .	CL_D14-1042_pa_2	CL_D14-1042_pa_3	CL_D14-1042_pa_2_3
CL	D14-1042	pa	2	4	motivation_background	proposal_implementation	support	elaboration	secondary	secondary	none	none	Though it is well understood how to disambiguate noun phrases , this task is much less studied for verbs and verbal phrases .	Our framework first identifies multi-word expressions based on the syntactic structure of the sentence ; this allows us to recognize both contiguous and non-contiguous phrases .	CL_D14-1042_pa_2	CL_D14-1042_pa_4	CL_D14-1042_pa_2_4
CL	D14-1042	pa	2	5	motivation_background	proposal_implementation	support	sequence	secondary	secondary	none	none	Though it is well understood how to disambiguate noun phrases , this task is much less studied for verbs and verbal phrases .	We then generate a list of candidate senses for each word or phrase , using novel syntactic and semantic pruning techniques .	CL_D14-1042_pa_2	CL_D14-1042_pa_5	CL_D14-1042_pa_2_5
CL	D14-1042	pa	2	6	motivation_background	proposal_implementation	support	sequence	secondary	secondary	none	none	Though it is well understood how to disambiguate noun phrases , this task is much less studied for verbs and verbal phrases .	We also construct and leverage a new resource of pairs of senses for verbs and their object arguments .	CL_D14-1042_pa_2	CL_D14-1042_pa_6	CL_D14-1042_pa_2_6
CL	D14-1042	pa	2	7	motivation_background	proposal_implementation	support	sequence	secondary	secondary	none	none	Though it is well understood how to disambiguate noun phrases , this task is much less studied for verbs and verbal phrases .	Finally , we feed the so-obtained candidate senses into standard word-sense disambiguation ( WSD ) methods , and boost their precision and recall .	CL_D14-1042_pa_2	CL_D14-1042_pa_7	CL_D14-1042_pa_2_7
CL	D14-1042	pa	2	8	motivation_background	result	support	support	secondary	secondary	none	none	Though it is well understood how to disambiguate noun phrases , this task is much less studied for verbs and verbal phrases .	Our experiments indicate that Werdy significantly increases the performance of existing WSD methods .	CL_D14-1042_pa_2	CL_D14-1042_pa_8	CL_D14-1042_pa_2_8
CL	D14-1042	pa	3	4	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	We present Werdy , a framework for WERD with particular focus on verbs and verbal phrases .	Our framework first identifies multi-word expressions based on the syntactic structure of the sentence ; this allows us to recognize both contiguous and non-contiguous phrases .	CL_D14-1042_pa_3	CL_D14-1042_pa_4	CL_D14-1042_pa_3_4
CL	D14-1042	pa	3	5	proposal	proposal_implementation	none	sequence	main	secondary	none	none	We present Werdy , a framework for WERD with particular focus on verbs and verbal phrases .	We then generate a list of candidate senses for each word or phrase , using novel syntactic and semantic pruning techniques .	CL_D14-1042_pa_3	CL_D14-1042_pa_5	CL_D14-1042_pa_3_5
CL	D14-1042	pa	3	6	proposal	proposal_implementation	none	sequence	main	secondary	none	none	We present Werdy , a framework for WERD with particular focus on verbs and verbal phrases .	We also construct and leverage a new resource of pairs of senses for verbs and their object arguments .	CL_D14-1042_pa_3	CL_D14-1042_pa_6	CL_D14-1042_pa_3_6
CL	D14-1042	pa	3	7	proposal	proposal_implementation	none	sequence	main	secondary	none	none	We present Werdy , a framework for WERD with particular focus on verbs and verbal phrases .	Finally , we feed the so-obtained candidate senses into standard word-sense disambiguation ( WSD ) methods , and boost their precision and recall .	CL_D14-1042_pa_3	CL_D14-1042_pa_7	CL_D14-1042_pa_3_7
CL	D14-1042	pa	3	8	proposal	result	none	support	main	secondary	back	support	We present Werdy , a framework for WERD with particular focus on verbs and verbal phrases .	Our experiments indicate that Werdy significantly increases the performance of existing WSD methods .	CL_D14-1042_pa_3	CL_D14-1042_pa_8	CL_D14-1042_pa_3_8
CL	D14-1042	pa	4	5	proposal_implementation	proposal_implementation	elaboration	sequence	secondary	secondary	back	sequence	Our framework first identifies multi-word expressions based on the syntactic structure of the sentence ; this allows us to recognize both contiguous and non-contiguous phrases .	We then generate a list of candidate senses for each word or phrase , using novel syntactic and semantic pruning techniques .	CL_D14-1042_pa_4	CL_D14-1042_pa_5	CL_D14-1042_pa_4_5
CL	D14-1042	pa	4	6	proposal_implementation	proposal_implementation	elaboration	sequence	secondary	secondary	none	none	Our framework first identifies multi-word expressions based on the syntactic structure of the sentence ; this allows us to recognize both contiguous and non-contiguous phrases .	We also construct and leverage a new resource of pairs of senses for verbs and their object arguments .	CL_D14-1042_pa_4	CL_D14-1042_pa_6	CL_D14-1042_pa_4_6
CL	D14-1042	pa	4	7	proposal_implementation	proposal_implementation	elaboration	sequence	secondary	secondary	none	none	Our framework first identifies multi-word expressions based on the syntactic structure of the sentence ; this allows us to recognize both contiguous and non-contiguous phrases .	Finally , we feed the so-obtained candidate senses into standard word-sense disambiguation ( WSD ) methods , and boost their precision and recall .	CL_D14-1042_pa_4	CL_D14-1042_pa_7	CL_D14-1042_pa_4_7
CL	D14-1042	pa	4	8	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	Our framework first identifies multi-word expressions based on the syntactic structure of the sentence ; this allows us to recognize both contiguous and non-contiguous phrases .	Our experiments indicate that Werdy significantly increases the performance of existing WSD methods .	CL_D14-1042_pa_4	CL_D14-1042_pa_8	CL_D14-1042_pa_4_8
CL	D14-1042	pa	5	6	proposal_implementation	proposal_implementation	sequence	sequence	secondary	secondary	back	sequence	We then generate a list of candidate senses for each word or phrase , using novel syntactic and semantic pruning techniques .	We also construct and leverage a new resource of pairs of senses for verbs and their object arguments .	CL_D14-1042_pa_5	CL_D14-1042_pa_6	CL_D14-1042_pa_5_6
CL	D14-1042	pa	5	7	proposal_implementation	proposal_implementation	sequence	sequence	secondary	secondary	none	none	We then generate a list of candidate senses for each word or phrase , using novel syntactic and semantic pruning techniques .	Finally , we feed the so-obtained candidate senses into standard word-sense disambiguation ( WSD ) methods , and boost their precision and recall .	CL_D14-1042_pa_5	CL_D14-1042_pa_7	CL_D14-1042_pa_5_7
CL	D14-1042	pa	5	8	proposal_implementation	result	sequence	support	secondary	secondary	none	none	We then generate a list of candidate senses for each word or phrase , using novel syntactic and semantic pruning techniques .	Our experiments indicate that Werdy significantly increases the performance of existing WSD methods .	CL_D14-1042_pa_5	CL_D14-1042_pa_8	CL_D14-1042_pa_5_8
CL	D14-1042	pa	6	7	proposal_implementation	proposal_implementation	sequence	sequence	secondary	secondary	back	sequence	We also construct and leverage a new resource of pairs of senses for verbs and their object arguments .	Finally , we feed the so-obtained candidate senses into standard word-sense disambiguation ( WSD ) methods , and boost their precision and recall .	CL_D14-1042_pa_6	CL_D14-1042_pa_7	CL_D14-1042_pa_6_7
CL	D14-1042	pa	6	8	proposal_implementation	result	sequence	support	secondary	secondary	none	none	We also construct and leverage a new resource of pairs of senses for verbs and their object arguments .	Our experiments indicate that Werdy significantly increases the performance of existing WSD methods .	CL_D14-1042_pa_6	CL_D14-1042_pa_8	CL_D14-1042_pa_6_8
CL	D14-1042	pa	7	8	proposal_implementation	result	sequence	support	secondary	secondary	none	none	Finally , we feed the so-obtained candidate senses into standard word-sense disambiguation ( WSD ) methods , and boost their precision and recall .	Our experiments indicate that Werdy significantly increases the performance of existing WSD methods .	CL_D14-1042_pa_7	CL_D14-1042_pa_8	CL_D14-1042_pa_7_8
CL	D14-1042	pa	8	7	result	proposal_implementation	support	sequence	secondary	secondary	none	none	Our experiments indicate that Werdy significantly increases the performance of existing WSD methods .	Finally , we feed the so-obtained candidate senses into standard word-sense disambiguation ( WSD ) methods , and boost their precision and recall .	CL_D14-1042_pa_8	CL_D14-1042_pa_7	CL_D14-1042_pa_7_8
CL	D14-1043	ab	1	2	motivation_background	motivation_background	info-required	support	secondary	secondary	forw	info-required	Language is given meaning through its correspondence with a world representation .	This correspondence can be at multiple levels of granularity or resolutions .	CL_D14-1043_ab_1	CL_D14-1043_ab_2	CL_D14-1043_ab_1_2
CL	D14-1043	ab	1	3	motivation_background	proposal	info-required	none	secondary	main	none	none	Language is given meaning through its correspondence with a world representation .	In this paper , we introduce an approach to multi-resolution language grounding in the extremely challenging domain of professional soccer commentaries .	CL_D14-1043_ab_1	CL_D14-1043_ab_3	CL_D14-1043_ab_1_3
CL	D14-1043	ab	1	4	motivation_background	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	Language is given meaning through its correspondence with a world representation .	We define and optimize a factored objective function that allows us to leverage discourse structure and the compositional nature of both language and game events .	CL_D14-1043_ab_1	CL_D14-1043_ab_4	CL_D14-1043_ab_1_4
CL	D14-1043	ab	1	5	motivation_background	result	info-required	support	secondary	secondary	none	none	Language is given meaning through its correspondence with a world representation .	We show that finer resolution grounding helps coarser resolution grounding , and vice versa .	CL_D14-1043_ab_1	CL_D14-1043_ab_5	CL_D14-1043_ab_1_5
CL	D14-1043	ab	1	6	motivation_background	observation	info-required	support	secondary	secondary	none	none	Language is given meaning through its correspondence with a world representation .	Our method results in an F1 improvement of more than 48 % versus the previous state of the art for fine-resolution grounding .	CL_D14-1043_ab_1	CL_D14-1043_ab_6	CL_D14-1043_ab_1_6
CL	D14-1043	ab	2	3	motivation_background	proposal	support	none	secondary	main	forw	support	This correspondence can be at multiple levels of granularity or resolutions .	In this paper , we introduce an approach to multi-resolution language grounding in the extremely challenging domain of professional soccer commentaries .	CL_D14-1043_ab_2	CL_D14-1043_ab_3	CL_D14-1043_ab_2_3
CL	D14-1043	ab	2	4	motivation_background	proposal_implementation	support	elaboration	secondary	secondary	none	none	This correspondence can be at multiple levels of granularity or resolutions .	We define and optimize a factored objective function that allows us to leverage discourse structure and the compositional nature of both language and game events .	CL_D14-1043_ab_2	CL_D14-1043_ab_4	CL_D14-1043_ab_2_4
CL	D14-1043	ab	2	5	motivation_background	result	support	support	secondary	secondary	none	none	This correspondence can be at multiple levels of granularity or resolutions .	We show that finer resolution grounding helps coarser resolution grounding , and vice versa .	CL_D14-1043_ab_2	CL_D14-1043_ab_5	CL_D14-1043_ab_2_5
CL	D14-1043	ab	2	6	motivation_background	observation	support	support	secondary	secondary	none	none	This correspondence can be at multiple levels of granularity or resolutions .	Our method results in an F1 improvement of more than 48 % versus the previous state of the art for fine-resolution grounding .	CL_D14-1043_ab_2	CL_D14-1043_ab_6	CL_D14-1043_ab_2_6
CL	D14-1043	ab	3	4	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	In this paper , we introduce an approach to multi-resolution language grounding in the extremely challenging domain of professional soccer commentaries .	We define and optimize a factored objective function that allows us to leverage discourse structure and the compositional nature of both language and game events .	CL_D14-1043_ab_3	CL_D14-1043_ab_4	CL_D14-1043_ab_3_4
CL	D14-1043	ab	3	5	proposal	result	none	support	main	secondary	back	support	In this paper , we introduce an approach to multi-resolution language grounding in the extremely challenging domain of professional soccer commentaries .	We show that finer resolution grounding helps coarser resolution grounding , and vice versa .	CL_D14-1043_ab_3	CL_D14-1043_ab_5	CL_D14-1043_ab_3_5
CL	D14-1043	ab	3	6	proposal	observation	none	support	main	secondary	none	none	In this paper , we introduce an approach to multi-resolution language grounding in the extremely challenging domain of professional soccer commentaries .	Our method results in an F1 improvement of more than 48 % versus the previous state of the art for fine-resolution grounding .	CL_D14-1043_ab_3	CL_D14-1043_ab_6	CL_D14-1043_ab_3_6
CL	D14-1043	ab	4	5	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	We define and optimize a factored objective function that allows us to leverage discourse structure and the compositional nature of both language and game events .	We show that finer resolution grounding helps coarser resolution grounding , and vice versa .	CL_D14-1043_ab_4	CL_D14-1043_ab_5	CL_D14-1043_ab_4_5
CL	D14-1043	ab	4	6	proposal_implementation	observation	elaboration	support	secondary	secondary	none	none	We define and optimize a factored objective function that allows us to leverage discourse structure and the compositional nature of both language and game events .	Our method results in an F1 improvement of more than 48 % versus the previous state of the art for fine-resolution grounding .	CL_D14-1043_ab_4	CL_D14-1043_ab_6	CL_D14-1043_ab_4_6
CL	D14-1043	ab	5	6	result	observation	support	support	secondary	secondary	back	support	We show that finer resolution grounding helps coarser resolution grounding , and vice versa .	Our method results in an F1 improvement of more than 48 % versus the previous state of the art for fine-resolution grounding .	CL_D14-1043_ab_5	CL_D14-1043_ab_6	CL_D14-1043_ab_5_6
CL	D14-1043	ab	6	5	observation	result	support	support	secondary	secondary	forw	support	Our method results in an F1 improvement of more than 48 % versus the previous state of the art for fine-resolution grounding .	We show that finer resolution grounding helps coarser resolution grounding , and vice versa .	CL_D14-1043_ab_6	CL_D14-1043_ab_5	CL_D14-1043_ab_5_6
CL	D14-1043	mn	1	2	motivation_background	motivation_background	info-required	support	secondary	secondary	forw	info-required	Language is given meaning through its correspondence with a world representation .	This correspondence can be at multiple levels of granularity or resolutions .	CL_D14-1043_mn_1	CL_D14-1043_mn_2	CL_D14-1043_mn_1_2
CL	D14-1043	mn	1	3	motivation_background	proposal	info-required	none	secondary	main	none	none	Language is given meaning through its correspondence with a world representation .	In this paper , we introduce an approach to multi-resolution language grounding in the extremely challenging domain of professional soccer commentaries .	CL_D14-1043_mn_1	CL_D14-1043_mn_3	CL_D14-1043_mn_1_3
CL	D14-1043	mn	1	4	motivation_background	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	Language is given meaning through its correspondence with a world representation .	We define and optimize a factored objective function that allows us to leverage discourse structure and the compositional nature of both language and game events .	CL_D14-1043_mn_1	CL_D14-1043_mn_4	CL_D14-1043_mn_1_4
CL	D14-1043	mn	1	5	motivation_background	result	info-required	support	secondary	secondary	none	none	Language is given meaning through its correspondence with a world representation .	We show that finer resolution grounding helps coarser resolution grounding , and vice versa .	CL_D14-1043_mn_1	CL_D14-1043_mn_5	CL_D14-1043_mn_1_5
CL	D14-1043	mn	1	6	motivation_background	result	info-required	elaboration	secondary	secondary	none	none	Language is given meaning through its correspondence with a world representation .	Our method results in an F1 improvement of more than 48 % versus the previous state of the art for fine-resolution grounding .	CL_D14-1043_mn_1	CL_D14-1043_mn_6	CL_D14-1043_mn_1_6
CL	D14-1043	mn	2	3	motivation_background	proposal	support	none	secondary	main	forw	support	This correspondence can be at multiple levels of granularity or resolutions .	In this paper , we introduce an approach to multi-resolution language grounding in the extremely challenging domain of professional soccer commentaries .	CL_D14-1043_mn_2	CL_D14-1043_mn_3	CL_D14-1043_mn_2_3
CL	D14-1043	mn	2	4	motivation_background	proposal_implementation	support	elaboration	secondary	secondary	none	none	This correspondence can be at multiple levels of granularity or resolutions .	We define and optimize a factored objective function that allows us to leverage discourse structure and the compositional nature of both language and game events .	CL_D14-1043_mn_2	CL_D14-1043_mn_4	CL_D14-1043_mn_2_4
CL	D14-1043	mn	2	5	motivation_background	result	support	support	secondary	secondary	none	none	This correspondence can be at multiple levels of granularity or resolutions .	We show that finer resolution grounding helps coarser resolution grounding , and vice versa .	CL_D14-1043_mn_2	CL_D14-1043_mn_5	CL_D14-1043_mn_2_5
CL	D14-1043	mn	2	6	motivation_background	result	support	elaboration	secondary	secondary	none	none	This correspondence can be at multiple levels of granularity or resolutions .	Our method results in an F1 improvement of more than 48 % versus the previous state of the art for fine-resolution grounding .	CL_D14-1043_mn_2	CL_D14-1043_mn_6	CL_D14-1043_mn_2_6
CL	D14-1043	mn	3	4	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	In this paper , we introduce an approach to multi-resolution language grounding in the extremely challenging domain of professional soccer commentaries .	We define and optimize a factored objective function that allows us to leverage discourse structure and the compositional nature of both language and game events .	CL_D14-1043_mn_3	CL_D14-1043_mn_4	CL_D14-1043_mn_3_4
CL	D14-1043	mn	3	5	proposal	result	none	support	main	secondary	back	support	In this paper , we introduce an approach to multi-resolution language grounding in the extremely challenging domain of professional soccer commentaries .	We show that finer resolution grounding helps coarser resolution grounding , and vice versa .	CL_D14-1043_mn_3	CL_D14-1043_mn_5	CL_D14-1043_mn_3_5
CL	D14-1043	mn	3	6	proposal	result	none	elaboration	main	secondary	none	none	In this paper , we introduce an approach to multi-resolution language grounding in the extremely challenging domain of professional soccer commentaries .	Our method results in an F1 improvement of more than 48 % versus the previous state of the art for fine-resolution grounding .	CL_D14-1043_mn_3	CL_D14-1043_mn_6	CL_D14-1043_mn_3_6
CL	D14-1043	mn	4	5	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	We define and optimize a factored objective function that allows us to leverage discourse structure and the compositional nature of both language and game events .	We show that finer resolution grounding helps coarser resolution grounding , and vice versa .	CL_D14-1043_mn_4	CL_D14-1043_mn_5	CL_D14-1043_mn_4_5
CL	D14-1043	mn	4	6	proposal_implementation	result	elaboration	elaboration	secondary	secondary	none	none	We define and optimize a factored objective function that allows us to leverage discourse structure and the compositional nature of both language and game events .	Our method results in an F1 improvement of more than 48 % versus the previous state of the art for fine-resolution grounding .	CL_D14-1043_mn_4	CL_D14-1043_mn_6	CL_D14-1043_mn_4_6
CL	D14-1043	mn	5	6	result	result	support	elaboration	secondary	secondary	back	elaboration	We show that finer resolution grounding helps coarser resolution grounding , and vice versa .	Our method results in an F1 improvement of more than 48 % versus the previous state of the art for fine-resolution grounding .	CL_D14-1043_mn_5	CL_D14-1043_mn_6	CL_D14-1043_mn_5_6
CL	D14-1043	mn	6	5	result	result	elaboration	support	secondary	secondary	forw	elaboration	Our method results in an F1 improvement of more than 48 % versus the previous state of the art for fine-resolution grounding .	We show that finer resolution grounding helps coarser resolution grounding , and vice versa .	CL_D14-1043_mn_6	CL_D14-1043_mn_5	CL_D14-1043_mn_5_6
CL	D14-1043	pa	1	2	information_additional	motivation_background	info-required	support	secondary	secondary	forw	info-required	Language is given meaning through its correspondence with a world representation .	This correspondence can be at multiple levels of granularity or resolutions .	CL_D14-1043_pa_1	CL_D14-1043_pa_2	CL_D14-1043_pa_1_2
CL	D14-1043	pa	1	3	information_additional	proposal	info-required	none	secondary	main	none	none	Language is given meaning through its correspondence with a world representation .	In this paper , we introduce an approach to multi-resolution language grounding in the extremely challenging domain of professional soccer commentaries .	CL_D14-1043_pa_1	CL_D14-1043_pa_3	CL_D14-1043_pa_1_3
CL	D14-1043	pa	1	4	information_additional	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	Language is given meaning through its correspondence with a world representation .	We define and optimize a factored objective function that allows us to leverage discourse structure and the compositional nature of both language and game events .	CL_D14-1043_pa_1	CL_D14-1043_pa_4	CL_D14-1043_pa_1_4
CL	D14-1043	pa	1	5	information_additional	result	info-required	support	secondary	secondary	none	none	Language is given meaning through its correspondence with a world representation .	We show that finer resolution grounding helps coarser resolution grounding , and vice versa .	CL_D14-1043_pa_1	CL_D14-1043_pa_5	CL_D14-1043_pa_1_5
CL	D14-1043	pa	1	6	information_additional	observation	info-required	support	secondary	secondary	none	none	Language is given meaning through its correspondence with a world representation .	Our method results in an F1 improvement of more than 48 % versus the previous state of the art for fine-resolution grounding .	CL_D14-1043_pa_1	CL_D14-1043_pa_6	CL_D14-1043_pa_1_6
CL	D14-1043	pa	2	3	motivation_background	proposal	support	none	secondary	main	forw	support	This correspondence can be at multiple levels of granularity or resolutions .	In this paper , we introduce an approach to multi-resolution language grounding in the extremely challenging domain of professional soccer commentaries .	CL_D14-1043_pa_2	CL_D14-1043_pa_3	CL_D14-1043_pa_2_3
CL	D14-1043	pa	2	4	motivation_background	proposal_implementation	support	elaboration	secondary	secondary	none	none	This correspondence can be at multiple levels of granularity or resolutions .	We define and optimize a factored objective function that allows us to leverage discourse structure and the compositional nature of both language and game events .	CL_D14-1043_pa_2	CL_D14-1043_pa_4	CL_D14-1043_pa_2_4
CL	D14-1043	pa	2	5	motivation_background	result	support	support	secondary	secondary	none	none	This correspondence can be at multiple levels of granularity or resolutions .	We show that finer resolution grounding helps coarser resolution grounding , and vice versa .	CL_D14-1043_pa_2	CL_D14-1043_pa_5	CL_D14-1043_pa_2_5
CL	D14-1043	pa	2	6	motivation_background	observation	support	support	secondary	secondary	none	none	This correspondence can be at multiple levels of granularity or resolutions .	Our method results in an F1 improvement of more than 48 % versus the previous state of the art for fine-resolution grounding .	CL_D14-1043_pa_2	CL_D14-1043_pa_6	CL_D14-1043_pa_2_6
CL	D14-1043	pa	3	4	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	In this paper , we introduce an approach to multi-resolution language grounding in the extremely challenging domain of professional soccer commentaries .	We define and optimize a factored objective function that allows us to leverage discourse structure and the compositional nature of both language and game events .	CL_D14-1043_pa_3	CL_D14-1043_pa_4	CL_D14-1043_pa_3_4
CL	D14-1043	pa	3	5	proposal	result	none	support	main	secondary	back	support	In this paper , we introduce an approach to multi-resolution language grounding in the extremely challenging domain of professional soccer commentaries .	We show that finer resolution grounding helps coarser resolution grounding , and vice versa .	CL_D14-1043_pa_3	CL_D14-1043_pa_5	CL_D14-1043_pa_3_5
CL	D14-1043	pa	3	6	proposal	observation	none	support	main	secondary	none	none	In this paper , we introduce an approach to multi-resolution language grounding in the extremely challenging domain of professional soccer commentaries .	Our method results in an F1 improvement of more than 48 % versus the previous state of the art for fine-resolution grounding .	CL_D14-1043_pa_3	CL_D14-1043_pa_6	CL_D14-1043_pa_3_6
CL	D14-1043	pa	4	5	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	We define and optimize a factored objective function that allows us to leverage discourse structure and the compositional nature of both language and game events .	We show that finer resolution grounding helps coarser resolution grounding , and vice versa .	CL_D14-1043_pa_4	CL_D14-1043_pa_5	CL_D14-1043_pa_4_5
CL	D14-1043	pa	4	6	proposal_implementation	observation	elaboration	support	secondary	secondary	none	none	We define and optimize a factored objective function that allows us to leverage discourse structure and the compositional nature of both language and game events .	Our method results in an F1 improvement of more than 48 % versus the previous state of the art for fine-resolution grounding .	CL_D14-1043_pa_4	CL_D14-1043_pa_6	CL_D14-1043_pa_4_6
CL	D14-1043	pa	5	6	result	observation	support	support	secondary	secondary	back	support	We show that finer resolution grounding helps coarser resolution grounding , and vice versa .	Our method results in an F1 improvement of more than 48 % versus the previous state of the art for fine-resolution grounding .	CL_D14-1043_pa_5	CL_D14-1043_pa_6	CL_D14-1043_pa_5_6
CL	D14-1043	pa	6	5	observation	result	support	support	secondary	secondary	forw	support	Our method results in an F1 improvement of more than 48 % versus the previous state of the art for fine-resolution grounding .	We show that finer resolution grounding helps coarser resolution grounding , and vice versa .	CL_D14-1043_pa_6	CL_D14-1043_pa_5	CL_D14-1043_pa_5_6
CL	D14-1044	ab	1	2	motivation_background	motivation_problem	info-required	support	secondary	secondary	none	none	Much work in recent years has gone into the construction of large knowledge bases ( KBs ) , such as Freebase , DBPedia , NELL , and YAGO .	While these KBs are very large , they are still very incomplete , necessitating the use of inference to fill in gaps .	CL_D14-1044_ab_1	CL_D14-1044_ab_2	CL_D14-1044_ab_1_2
CL	D14-1044	ab	1	3	motivation_background	motivation_background	info-required	support	secondary	secondary	forw	info-required	Much work in recent years has gone into the construction of large knowledge bases ( KBs ) , such as Freebase , DBPedia , NELL , and YAGO .	Prior work has shown how to make use of a large text corpus to augment random walk inference over KBs .	CL_D14-1044_ab_1	CL_D14-1044_ab_3	CL_D14-1044_ab_1_3
CL	D14-1044	ab	1	4	motivation_background	proposal	info-required	none	secondary	main	none	none	Much work in recent years has gone into the construction of large knowledge bases ( KBs ) , such as Freebase , DBPedia , NELL , and YAGO .	We present two improvements to the use of such large corpora to augment KB inference .	CL_D14-1044_ab_1	CL_D14-1044_ab_4	CL_D14-1044_ab_1_4
CL	D14-1044	ab	1	5	motivation_background	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	Much work in recent years has gone into the construction of large knowledge bases ( KBs ) , such as Freebase , DBPedia , NELL , and YAGO .	First , we present a new technique for combining KB relations and surface text into a single graph representation that is much more compact than graphs used in prior work .	CL_D14-1044_ab_1	CL_D14-1044_ab_5	CL_D14-1044_ab_1_5
CL	D14-1044	ab	1	6	motivation_background	proposal_implementation	info-required	sequence	secondary	secondary	none	none	Much work in recent years has gone into the construction of large knowledge bases ( KBs ) , such as Freebase , DBPedia , NELL , and YAGO .	Second , we describe how to incorporate vector space similarity into random walk inference over KBs , reducing the feature sparsity inherent in using surface text .	CL_D14-1044_ab_1	CL_D14-1044_ab_6	CL_D14-1044_ab_1_6
CL	D14-1044	ab	1	7	motivation_background	proposal	info-required	elaboration	secondary	secondary	none	none	Much work in recent years has gone into the construction of large knowledge bases ( KBs ) , such as Freebase , DBPedia , NELL , and YAGO .	This allows us to combine distributional similarity with symbolic logical inference in novel and effective ways .	CL_D14-1044_ab_1	CL_D14-1044_ab_7	CL_D14-1044_ab_1_7
CL	D14-1044	ab	1	8	motivation_background	result_means	info-required	support	secondary	secondary	none	none	Much work in recent years has gone into the construction of large knowledge bases ( KBs ) , such as Freebase , DBPedia , NELL , and YAGO .	With experiments on many relations from two separate KBs , we show that our methods significantly outperform prior work on KB inference , both in the size of problem our methods can handle and in the quality of predictions made .	CL_D14-1044_ab_1	CL_D14-1044_ab_8	CL_D14-1044_ab_1_8
CL	D14-1044	ab	2	3	motivation_problem	motivation_background	support	support	secondary	secondary	back	support	While these KBs are very large , they are still very incomplete , necessitating the use of inference to fill in gaps .	Prior work has shown how to make use of a large text corpus to augment random walk inference over KBs .	CL_D14-1044_ab_2	CL_D14-1044_ab_3	CL_D14-1044_ab_2_3
CL	D14-1044	ab	2	4	motivation_problem	proposal	support	none	secondary	main	forw	support	While these KBs are very large , they are still very incomplete , necessitating the use of inference to fill in gaps .	We present two improvements to the use of such large corpora to augment KB inference .	CL_D14-1044_ab_2	CL_D14-1044_ab_4	CL_D14-1044_ab_2_4
CL	D14-1044	ab	2	5	motivation_problem	proposal_implementation	support	elaboration	secondary	secondary	none	none	While these KBs are very large , they are still very incomplete , necessitating the use of inference to fill in gaps .	First , we present a new technique for combining KB relations and surface text into a single graph representation that is much more compact than graphs used in prior work .	CL_D14-1044_ab_2	CL_D14-1044_ab_5	CL_D14-1044_ab_2_5
CL	D14-1044	ab	2	6	motivation_problem	proposal_implementation	support	sequence	secondary	secondary	none	none	While these KBs are very large , they are still very incomplete , necessitating the use of inference to fill in gaps .	Second , we describe how to incorporate vector space similarity into random walk inference over KBs , reducing the feature sparsity inherent in using surface text .	CL_D14-1044_ab_2	CL_D14-1044_ab_6	CL_D14-1044_ab_2_6
CL	D14-1044	ab	2	7	motivation_problem	proposal	support	elaboration	secondary	secondary	none	none	While these KBs are very large , they are still very incomplete , necessitating the use of inference to fill in gaps .	This allows us to combine distributional similarity with symbolic logical inference in novel and effective ways .	CL_D14-1044_ab_2	CL_D14-1044_ab_7	CL_D14-1044_ab_2_7
CL	D14-1044	ab	2	8	motivation_problem	result_means	support	support	secondary	secondary	none	none	While these KBs are very large , they are still very incomplete , necessitating the use of inference to fill in gaps .	With experiments on many relations from two separate KBs , we show that our methods significantly outperform prior work on KB inference , both in the size of problem our methods can handle and in the quality of predictions made .	CL_D14-1044_ab_2	CL_D14-1044_ab_8	CL_D14-1044_ab_2_8
CL	D14-1044	ab	3	4	motivation_background	proposal	support	none	secondary	main	none	none	Prior work has shown how to make use of a large text corpus to augment random walk inference over KBs .	We present two improvements to the use of such large corpora to augment KB inference .	CL_D14-1044_ab_3	CL_D14-1044_ab_4	CL_D14-1044_ab_3_4
CL	D14-1044	ab	3	5	motivation_background	proposal_implementation	support	elaboration	secondary	secondary	none	none	Prior work has shown how to make use of a large text corpus to augment random walk inference over KBs .	First , we present a new technique for combining KB relations and surface text into a single graph representation that is much more compact than graphs used in prior work .	CL_D14-1044_ab_3	CL_D14-1044_ab_5	CL_D14-1044_ab_3_5
CL	D14-1044	ab	3	6	motivation_background	proposal_implementation	support	sequence	secondary	secondary	none	none	Prior work has shown how to make use of a large text corpus to augment random walk inference over KBs .	Second , we describe how to incorporate vector space similarity into random walk inference over KBs , reducing the feature sparsity inherent in using surface text .	CL_D14-1044_ab_3	CL_D14-1044_ab_6	CL_D14-1044_ab_3_6
CL	D14-1044	ab	3	7	motivation_background	proposal	support	elaboration	secondary	secondary	none	none	Prior work has shown how to make use of a large text corpus to augment random walk inference over KBs .	This allows us to combine distributional similarity with symbolic logical inference in novel and effective ways .	CL_D14-1044_ab_3	CL_D14-1044_ab_7	CL_D14-1044_ab_3_7
CL	D14-1044	ab	3	8	motivation_background	result_means	support	support	secondary	secondary	none	none	Prior work has shown how to make use of a large text corpus to augment random walk inference over KBs .	With experiments on many relations from two separate KBs , we show that our methods significantly outperform prior work on KB inference , both in the size of problem our methods can handle and in the quality of predictions made .	CL_D14-1044_ab_3	CL_D14-1044_ab_8	CL_D14-1044_ab_3_8
CL	D14-1044	ab	4	5	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	We present two improvements to the use of such large corpora to augment KB inference .	First , we present a new technique for combining KB relations and surface text into a single graph representation that is much more compact than graphs used in prior work .	CL_D14-1044_ab_4	CL_D14-1044_ab_5	CL_D14-1044_ab_4_5
CL	D14-1044	ab	4	6	proposal	proposal_implementation	none	sequence	main	secondary	none	none	We present two improvements to the use of such large corpora to augment KB inference .	Second , we describe how to incorporate vector space similarity into random walk inference over KBs , reducing the feature sparsity inherent in using surface text .	CL_D14-1044_ab_4	CL_D14-1044_ab_6	CL_D14-1044_ab_4_6
CL	D14-1044	ab	4	7	proposal	proposal	none	elaboration	main	secondary	none	none	We present two improvements to the use of such large corpora to augment KB inference .	This allows us to combine distributional similarity with symbolic logical inference in novel and effective ways .	CL_D14-1044_ab_4	CL_D14-1044_ab_7	CL_D14-1044_ab_4_7
CL	D14-1044	ab	4	8	proposal	result_means	none	support	main	secondary	back	support	We present two improvements to the use of such large corpora to augment KB inference .	With experiments on many relations from two separate KBs , we show that our methods significantly outperform prior work on KB inference , both in the size of problem our methods can handle and in the quality of predictions made .	CL_D14-1044_ab_4	CL_D14-1044_ab_8	CL_D14-1044_ab_4_8
CL	D14-1044	ab	5	6	proposal_implementation	proposal_implementation	elaboration	sequence	secondary	secondary	back	sequence	First , we present a new technique for combining KB relations and surface text into a single graph representation that is much more compact than graphs used in prior work .	Second , we describe how to incorporate vector space similarity into random walk inference over KBs , reducing the feature sparsity inherent in using surface text .	CL_D14-1044_ab_5	CL_D14-1044_ab_6	CL_D14-1044_ab_5_6
CL	D14-1044	ab	5	7	proposal_implementation	proposal	elaboration	elaboration	secondary	secondary	none	none	First , we present a new technique for combining KB relations and surface text into a single graph representation that is much more compact than graphs used in prior work .	This allows us to combine distributional similarity with symbolic logical inference in novel and effective ways .	CL_D14-1044_ab_5	CL_D14-1044_ab_7	CL_D14-1044_ab_5_7
CL	D14-1044	ab	5	8	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	First , we present a new technique for combining KB relations and surface text into a single graph representation that is much more compact than graphs used in prior work .	With experiments on many relations from two separate KBs , we show that our methods significantly outperform prior work on KB inference , both in the size of problem our methods can handle and in the quality of predictions made .	CL_D14-1044_ab_5	CL_D14-1044_ab_8	CL_D14-1044_ab_5_8
CL	D14-1044	ab	6	7	proposal_implementation	proposal	sequence	elaboration	secondary	secondary	back	elaboration	Second , we describe how to incorporate vector space similarity into random walk inference over KBs , reducing the feature sparsity inherent in using surface text .	This allows us to combine distributional similarity with symbolic logical inference in novel and effective ways .	CL_D14-1044_ab_6	CL_D14-1044_ab_7	CL_D14-1044_ab_6_7
CL	D14-1044	ab	6	8	proposal_implementation	result_means	sequence	support	secondary	secondary	none	none	Second , we describe how to incorporate vector space similarity into random walk inference over KBs , reducing the feature sparsity inherent in using surface text .	With experiments on many relations from two separate KBs , we show that our methods significantly outperform prior work on KB inference , both in the size of problem our methods can handle and in the quality of predictions made .	CL_D14-1044_ab_6	CL_D14-1044_ab_8	CL_D14-1044_ab_6_8
CL	D14-1044	ab	7	8	proposal	result_means	elaboration	support	secondary	secondary	none	none	This allows us to combine distributional similarity with symbolic logical inference in novel and effective ways .	With experiments on many relations from two separate KBs , we show that our methods significantly outperform prior work on KB inference , both in the size of problem our methods can handle and in the quality of predictions made .	CL_D14-1044_ab_7	CL_D14-1044_ab_8	CL_D14-1044_ab_7_8
CL	D14-1044	ab	8	7	result_means	proposal	support	elaboration	secondary	secondary	none	none	With experiments on many relations from two separate KBs , we show that our methods significantly outperform prior work on KB inference , both in the size of problem our methods can handle and in the quality of predictions made .	This allows us to combine distributional similarity with symbolic logical inference in novel and effective ways .	CL_D14-1044_ab_8	CL_D14-1044_ab_7	CL_D14-1044_ab_7_8
CL	D14-1044	mn	1	2	motivation_background	motivation_problem	info-required	support	secondary	secondary	forw	info-required	Much work in recent years has gone into the construction of large knowledge bases ( KBs ) , such as Freebase , DBPedia , NELL , and YAGO .	While these KBs are very large , they are still very incomplete , necessitating the use of inference to fill in gaps .	CL_D14-1044_mn_1	CL_D14-1044_mn_2	CL_D14-1044_mn_1_2
CL	D14-1044	mn	1	3	motivation_background	motivation_background	info-required	info-required	secondary	secondary	none	none	Much work in recent years has gone into the construction of large knowledge bases ( KBs ) , such as Freebase , DBPedia , NELL , and YAGO .	Prior work has shown how to make use of a large text corpus to augment random walk inference over KBs .	CL_D14-1044_mn_1	CL_D14-1044_mn_3	CL_D14-1044_mn_1_3
CL	D14-1044	mn	1	4	motivation_background	proposal	info-required	none	secondary	main	none	none	Much work in recent years has gone into the construction of large knowledge bases ( KBs ) , such as Freebase , DBPedia , NELL , and YAGO .	We present two improvements to the use of such large corpora to augment KB inference .	CL_D14-1044_mn_1	CL_D14-1044_mn_4	CL_D14-1044_mn_1_4
CL	D14-1044	mn	1	5	motivation_background	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	Much work in recent years has gone into the construction of large knowledge bases ( KBs ) , such as Freebase , DBPedia , NELL , and YAGO .	First , we present a new technique for combining KB relations and surface text into a single graph representation that is much more compact than graphs used in prior work .	CL_D14-1044_mn_1	CL_D14-1044_mn_5	CL_D14-1044_mn_1_5
CL	D14-1044	mn	1	6	motivation_background	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	Much work in recent years has gone into the construction of large knowledge bases ( KBs ) , such as Freebase , DBPedia , NELL , and YAGO .	Second , we describe how to incorporate vector space similarity into random walk inference over KBs , reducing the feature sparsity inherent in using surface text .	CL_D14-1044_mn_1	CL_D14-1044_mn_6	CL_D14-1044_mn_1_6
CL	D14-1044	mn	1	7	motivation_background	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	Much work in recent years has gone into the construction of large knowledge bases ( KBs ) , such as Freebase , DBPedia , NELL , and YAGO .	This allows us to combine distributional similarity with symbolic logical inference in novel and effective ways .	CL_D14-1044_mn_1	CL_D14-1044_mn_7	CL_D14-1044_mn_1_7
CL	D14-1044	mn	1	8	motivation_background	result_means	info-required	support	secondary	secondary	none	none	Much work in recent years has gone into the construction of large knowledge bases ( KBs ) , such as Freebase , DBPedia , NELL , and YAGO .	With experiments on many relations from two separate KBs , we show that our methods significantly outperform prior work on KB inference , both in the size of problem our methods can handle and in the quality of predictions made .	CL_D14-1044_mn_1	CL_D14-1044_mn_8	CL_D14-1044_mn_1_8
CL	D14-1044	mn	2	3	motivation_problem	motivation_background	support	info-required	secondary	secondary	back	info-required	While these KBs are very large , they are still very incomplete , necessitating the use of inference to fill in gaps .	Prior work has shown how to make use of a large text corpus to augment random walk inference over KBs .	CL_D14-1044_mn_2	CL_D14-1044_mn_3	CL_D14-1044_mn_2_3
CL	D14-1044	mn	2	4	motivation_problem	proposal	support	none	secondary	main	forw	support	While these KBs are very large , they are still very incomplete , necessitating the use of inference to fill in gaps .	We present two improvements to the use of such large corpora to augment KB inference .	CL_D14-1044_mn_2	CL_D14-1044_mn_4	CL_D14-1044_mn_2_4
CL	D14-1044	mn	2	5	motivation_problem	proposal_implementation	support	elaboration	secondary	secondary	none	none	While these KBs are very large , they are still very incomplete , necessitating the use of inference to fill in gaps .	First , we present a new technique for combining KB relations and surface text into a single graph representation that is much more compact than graphs used in prior work .	CL_D14-1044_mn_2	CL_D14-1044_mn_5	CL_D14-1044_mn_2_5
CL	D14-1044	mn	2	6	motivation_problem	proposal_implementation	support	elaboration	secondary	secondary	none	none	While these KBs are very large , they are still very incomplete , necessitating the use of inference to fill in gaps .	Second , we describe how to incorporate vector space similarity into random walk inference over KBs , reducing the feature sparsity inherent in using surface text .	CL_D14-1044_mn_2	CL_D14-1044_mn_6	CL_D14-1044_mn_2_6
CL	D14-1044	mn	2	7	motivation_problem	proposal_implementation	support	elaboration	secondary	secondary	none	none	While these KBs are very large , they are still very incomplete , necessitating the use of inference to fill in gaps .	This allows us to combine distributional similarity with symbolic logical inference in novel and effective ways .	CL_D14-1044_mn_2	CL_D14-1044_mn_7	CL_D14-1044_mn_2_7
CL	D14-1044	mn	2	8	motivation_problem	result_means	support	support	secondary	secondary	none	none	While these KBs are very large , they are still very incomplete , necessitating the use of inference to fill in gaps .	With experiments on many relations from two separate KBs , we show that our methods significantly outperform prior work on KB inference , both in the size of problem our methods can handle and in the quality of predictions made .	CL_D14-1044_mn_2	CL_D14-1044_mn_8	CL_D14-1044_mn_2_8
CL	D14-1044	mn	3	4	motivation_background	proposal	info-required	none	secondary	main	none	none	Prior work has shown how to make use of a large text corpus to augment random walk inference over KBs .	We present two improvements to the use of such large corpora to augment KB inference .	CL_D14-1044_mn_3	CL_D14-1044_mn_4	CL_D14-1044_mn_3_4
CL	D14-1044	mn	3	5	motivation_background	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	Prior work has shown how to make use of a large text corpus to augment random walk inference over KBs .	First , we present a new technique for combining KB relations and surface text into a single graph representation that is much more compact than graphs used in prior work .	CL_D14-1044_mn_3	CL_D14-1044_mn_5	CL_D14-1044_mn_3_5
CL	D14-1044	mn	3	6	motivation_background	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	Prior work has shown how to make use of a large text corpus to augment random walk inference over KBs .	Second , we describe how to incorporate vector space similarity into random walk inference over KBs , reducing the feature sparsity inherent in using surface text .	CL_D14-1044_mn_3	CL_D14-1044_mn_6	CL_D14-1044_mn_3_6
CL	D14-1044	mn	3	7	motivation_background	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	Prior work has shown how to make use of a large text corpus to augment random walk inference over KBs .	This allows us to combine distributional similarity with symbolic logical inference in novel and effective ways .	CL_D14-1044_mn_3	CL_D14-1044_mn_7	CL_D14-1044_mn_3_7
CL	D14-1044	mn	3	8	motivation_background	result_means	info-required	support	secondary	secondary	none	none	Prior work has shown how to make use of a large text corpus to augment random walk inference over KBs .	With experiments on many relations from two separate KBs , we show that our methods significantly outperform prior work on KB inference , both in the size of problem our methods can handle and in the quality of predictions made .	CL_D14-1044_mn_3	CL_D14-1044_mn_8	CL_D14-1044_mn_3_8
CL	D14-1044	mn	4	5	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	We present two improvements to the use of such large corpora to augment KB inference .	First , we present a new technique for combining KB relations and surface text into a single graph representation that is much more compact than graphs used in prior work .	CL_D14-1044_mn_4	CL_D14-1044_mn_5	CL_D14-1044_mn_4_5
CL	D14-1044	mn	4	6	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	We present two improvements to the use of such large corpora to augment KB inference .	Second , we describe how to incorporate vector space similarity into random walk inference over KBs , reducing the feature sparsity inherent in using surface text .	CL_D14-1044_mn_4	CL_D14-1044_mn_6	CL_D14-1044_mn_4_6
CL	D14-1044	mn	4	7	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	We present two improvements to the use of such large corpora to augment KB inference .	This allows us to combine distributional similarity with symbolic logical inference in novel and effective ways .	CL_D14-1044_mn_4	CL_D14-1044_mn_7	CL_D14-1044_mn_4_7
CL	D14-1044	mn	4	8	proposal	result_means	none	support	main	secondary	back	support	We present two improvements to the use of such large corpora to augment KB inference .	With experiments on many relations from two separate KBs , we show that our methods significantly outperform prior work on KB inference , both in the size of problem our methods can handle and in the quality of predictions made .	CL_D14-1044_mn_4	CL_D14-1044_mn_8	CL_D14-1044_mn_4_8
CL	D14-1044	mn	5	6	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	First , we present a new technique for combining KB relations and surface text into a single graph representation that is much more compact than graphs used in prior work .	Second , we describe how to incorporate vector space similarity into random walk inference over KBs , reducing the feature sparsity inherent in using surface text .	CL_D14-1044_mn_5	CL_D14-1044_mn_6	CL_D14-1044_mn_5_6
CL	D14-1044	mn	5	7	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	none	none	First , we present a new technique for combining KB relations and surface text into a single graph representation that is much more compact than graphs used in prior work .	This allows us to combine distributional similarity with symbolic logical inference in novel and effective ways .	CL_D14-1044_mn_5	CL_D14-1044_mn_7	CL_D14-1044_mn_5_7
CL	D14-1044	mn	5	8	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	First , we present a new technique for combining KB relations and surface text into a single graph representation that is much more compact than graphs used in prior work .	With experiments on many relations from two separate KBs , we show that our methods significantly outperform prior work on KB inference , both in the size of problem our methods can handle and in the quality of predictions made .	CL_D14-1044_mn_5	CL_D14-1044_mn_8	CL_D14-1044_mn_5_8
CL	D14-1044	mn	6	7	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	Second , we describe how to incorporate vector space similarity into random walk inference over KBs , reducing the feature sparsity inherent in using surface text .	This allows us to combine distributional similarity with symbolic logical inference in novel and effective ways .	CL_D14-1044_mn_6	CL_D14-1044_mn_7	CL_D14-1044_mn_6_7
CL	D14-1044	mn	6	8	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	Second , we describe how to incorporate vector space similarity into random walk inference over KBs , reducing the feature sparsity inherent in using surface text .	With experiments on many relations from two separate KBs , we show that our methods significantly outperform prior work on KB inference , both in the size of problem our methods can handle and in the quality of predictions made .	CL_D14-1044_mn_6	CL_D14-1044_mn_8	CL_D14-1044_mn_6_8
CL	D14-1044	mn	7	8	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	This allows us to combine distributional similarity with symbolic logical inference in novel and effective ways .	With experiments on many relations from two separate KBs , we show that our methods significantly outperform prior work on KB inference , both in the size of problem our methods can handle and in the quality of predictions made .	CL_D14-1044_mn_7	CL_D14-1044_mn_8	CL_D14-1044_mn_7_8
CL	D14-1044	mn	8	7	result_means	proposal_implementation	support	elaboration	secondary	secondary	none	none	With experiments on many relations from two separate KBs , we show that our methods significantly outperform prior work on KB inference , both in the size of problem our methods can handle and in the quality of predictions made .	This allows us to combine distributional similarity with symbolic logical inference in novel and effective ways .	CL_D14-1044_mn_8	CL_D14-1044_mn_7	CL_D14-1044_mn_7_8
CL	D14-1044	pa	1	2	motivation_background	motivation_problem	info-required	info-required	secondary	secondary	forw	info-required	Much work in recent years has gone into the construction of large knowledge bases ( KBs ) , such as Freebase , DBPedia , NELL , and YAGO .	While these KBs are very large , they are still very incomplete , necessitating the use of inference to fill in gaps .	CL_D14-1044_pa_1	CL_D14-1044_pa_2	CL_D14-1044_pa_1_2
CL	D14-1044	pa	1	3	motivation_background	motivation_background	info-required	support	secondary	secondary	none	none	Much work in recent years has gone into the construction of large knowledge bases ( KBs ) , such as Freebase , DBPedia , NELL , and YAGO .	Prior work has shown how to make use of a large text corpus to augment random walk inference over KBs .	CL_D14-1044_pa_1	CL_D14-1044_pa_3	CL_D14-1044_pa_1_3
CL	D14-1044	pa	1	4	motivation_background	proposal	info-required	none	secondary	main	none	none	Much work in recent years has gone into the construction of large knowledge bases ( KBs ) , such as Freebase , DBPedia , NELL , and YAGO .	We present two improvements to the use of such large corpora to augment KB inference .	CL_D14-1044_pa_1	CL_D14-1044_pa_4	CL_D14-1044_pa_1_4
CL	D14-1044	pa	1	5	motivation_background	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	Much work in recent years has gone into the construction of large knowledge bases ( KBs ) , such as Freebase , DBPedia , NELL , and YAGO .	First , we present a new technique for combining KB relations and surface text into a single graph representation that is much more compact than graphs used in prior work .	CL_D14-1044_pa_1	CL_D14-1044_pa_5	CL_D14-1044_pa_1_5
CL	D14-1044	pa	1	6	motivation_background	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	Much work in recent years has gone into the construction of large knowledge bases ( KBs ) , such as Freebase , DBPedia , NELL , and YAGO .	Second , we describe how to incorporate vector space similarity into random walk inference over KBs , reducing the feature sparsity inherent in using surface text .	CL_D14-1044_pa_1	CL_D14-1044_pa_6	CL_D14-1044_pa_1_6
CL	D14-1044	pa	1	7	motivation_background	proposal	info-required	elaboration	secondary	secondary	none	none	Much work in recent years has gone into the construction of large knowledge bases ( KBs ) , such as Freebase , DBPedia , NELL , and YAGO .	This allows us to combine distributional similarity with symbolic logical inference in novel and effective ways .	CL_D14-1044_pa_1	CL_D14-1044_pa_7	CL_D14-1044_pa_1_7
CL	D14-1044	pa	1	8	motivation_background	result_means	info-required	support	secondary	secondary	none	none	Much work in recent years has gone into the construction of large knowledge bases ( KBs ) , such as Freebase , DBPedia , NELL , and YAGO .	With experiments on many relations from two separate KBs , we show that our methods significantly outperform prior work on KB inference , both in the size of problem our methods can handle and in the quality of predictions made .	CL_D14-1044_pa_1	CL_D14-1044_pa_8	CL_D14-1044_pa_1_8
CL	D14-1044	pa	2	3	motivation_problem	motivation_background	info-required	support	secondary	secondary	forw	info-required	While these KBs are very large , they are still very incomplete , necessitating the use of inference to fill in gaps .	Prior work has shown how to make use of a large text corpus to augment random walk inference over KBs .	CL_D14-1044_pa_2	CL_D14-1044_pa_3	CL_D14-1044_pa_2_3
CL	D14-1044	pa	2	4	motivation_problem	proposal	info-required	none	secondary	main	none	none	While these KBs are very large , they are still very incomplete , necessitating the use of inference to fill in gaps .	We present two improvements to the use of such large corpora to augment KB inference .	CL_D14-1044_pa_2	CL_D14-1044_pa_4	CL_D14-1044_pa_2_4
CL	D14-1044	pa	2	5	motivation_problem	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	While these KBs are very large , they are still very incomplete , necessitating the use of inference to fill in gaps .	First , we present a new technique for combining KB relations and surface text into a single graph representation that is much more compact than graphs used in prior work .	CL_D14-1044_pa_2	CL_D14-1044_pa_5	CL_D14-1044_pa_2_5
CL	D14-1044	pa	2	6	motivation_problem	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	While these KBs are very large , they are still very incomplete , necessitating the use of inference to fill in gaps .	Second , we describe how to incorporate vector space similarity into random walk inference over KBs , reducing the feature sparsity inherent in using surface text .	CL_D14-1044_pa_2	CL_D14-1044_pa_6	CL_D14-1044_pa_2_6
CL	D14-1044	pa	2	7	motivation_problem	proposal	info-required	elaboration	secondary	secondary	none	none	While these KBs are very large , they are still very incomplete , necessitating the use of inference to fill in gaps .	This allows us to combine distributional similarity with symbolic logical inference in novel and effective ways .	CL_D14-1044_pa_2	CL_D14-1044_pa_7	CL_D14-1044_pa_2_7
CL	D14-1044	pa	2	8	motivation_problem	result_means	info-required	support	secondary	secondary	none	none	While these KBs are very large , they are still very incomplete , necessitating the use of inference to fill in gaps .	With experiments on many relations from two separate KBs , we show that our methods significantly outperform prior work on KB inference , both in the size of problem our methods can handle and in the quality of predictions made .	CL_D14-1044_pa_2	CL_D14-1044_pa_8	CL_D14-1044_pa_2_8
CL	D14-1044	pa	3	4	motivation_background	proposal	support	none	secondary	main	forw	support	Prior work has shown how to make use of a large text corpus to augment random walk inference over KBs .	We present two improvements to the use of such large corpora to augment KB inference .	CL_D14-1044_pa_3	CL_D14-1044_pa_4	CL_D14-1044_pa_3_4
CL	D14-1044	pa	3	5	motivation_background	proposal_implementation	support	elaboration	secondary	secondary	none	none	Prior work has shown how to make use of a large text corpus to augment random walk inference over KBs .	First , we present a new technique for combining KB relations and surface text into a single graph representation that is much more compact than graphs used in prior work .	CL_D14-1044_pa_3	CL_D14-1044_pa_5	CL_D14-1044_pa_3_5
CL	D14-1044	pa	3	6	motivation_background	proposal_implementation	support	elaboration	secondary	secondary	none	none	Prior work has shown how to make use of a large text corpus to augment random walk inference over KBs .	Second , we describe how to incorporate vector space similarity into random walk inference over KBs , reducing the feature sparsity inherent in using surface text .	CL_D14-1044_pa_3	CL_D14-1044_pa_6	CL_D14-1044_pa_3_6
CL	D14-1044	pa	3	7	motivation_background	proposal	support	elaboration	secondary	secondary	none	none	Prior work has shown how to make use of a large text corpus to augment random walk inference over KBs .	This allows us to combine distributional similarity with symbolic logical inference in novel and effective ways .	CL_D14-1044_pa_3	CL_D14-1044_pa_7	CL_D14-1044_pa_3_7
CL	D14-1044	pa	3	8	motivation_background	result_means	support	support	secondary	secondary	none	none	Prior work has shown how to make use of a large text corpus to augment random walk inference over KBs .	With experiments on many relations from two separate KBs , we show that our methods significantly outperform prior work on KB inference , both in the size of problem our methods can handle and in the quality of predictions made .	CL_D14-1044_pa_3	CL_D14-1044_pa_8	CL_D14-1044_pa_3_8
CL	D14-1044	pa	4	5	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	We present two improvements to the use of such large corpora to augment KB inference .	First , we present a new technique for combining KB relations and surface text into a single graph representation that is much more compact than graphs used in prior work .	CL_D14-1044_pa_4	CL_D14-1044_pa_5	CL_D14-1044_pa_4_5
CL	D14-1044	pa	4	6	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	We present two improvements to the use of such large corpora to augment KB inference .	Second , we describe how to incorporate vector space similarity into random walk inference over KBs , reducing the feature sparsity inherent in using surface text .	CL_D14-1044_pa_4	CL_D14-1044_pa_6	CL_D14-1044_pa_4_6
CL	D14-1044	pa	4	7	proposal	proposal	none	elaboration	main	secondary	back	elaboration	We present two improvements to the use of such large corpora to augment KB inference .	This allows us to combine distributional similarity with symbolic logical inference in novel and effective ways .	CL_D14-1044_pa_4	CL_D14-1044_pa_7	CL_D14-1044_pa_4_7
CL	D14-1044	pa	4	8	proposal	result_means	none	support	main	secondary	back	support	We present two improvements to the use of such large corpora to augment KB inference .	With experiments on many relations from two separate KBs , we show that our methods significantly outperform prior work on KB inference , both in the size of problem our methods can handle and in the quality of predictions made .	CL_D14-1044_pa_4	CL_D14-1044_pa_8	CL_D14-1044_pa_4_8
CL	D14-1044	pa	5	6	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	none	none	First , we present a new technique for combining KB relations and surface text into a single graph representation that is much more compact than graphs used in prior work .	Second , we describe how to incorporate vector space similarity into random walk inference over KBs , reducing the feature sparsity inherent in using surface text .	CL_D14-1044_pa_5	CL_D14-1044_pa_6	CL_D14-1044_pa_5_6
CL	D14-1044	pa	5	7	proposal_implementation	proposal	elaboration	elaboration	secondary	secondary	forw	elaboration	First , we present a new technique for combining KB relations and surface text into a single graph representation that is much more compact than graphs used in prior work .	This allows us to combine distributional similarity with symbolic logical inference in novel and effective ways .	CL_D14-1044_pa_5	CL_D14-1044_pa_7	CL_D14-1044_pa_5_7
CL	D14-1044	pa	5	8	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	First , we present a new technique for combining KB relations and surface text into a single graph representation that is much more compact than graphs used in prior work .	With experiments on many relations from two separate KBs , we show that our methods significantly outperform prior work on KB inference , both in the size of problem our methods can handle and in the quality of predictions made .	CL_D14-1044_pa_5	CL_D14-1044_pa_8	CL_D14-1044_pa_5_8
CL	D14-1044	pa	6	7	proposal_implementation	proposal	elaboration	elaboration	secondary	secondary	forw	elaboration	Second , we describe how to incorporate vector space similarity into random walk inference over KBs , reducing the feature sparsity inherent in using surface text .	This allows us to combine distributional similarity with symbolic logical inference in novel and effective ways .	CL_D14-1044_pa_6	CL_D14-1044_pa_7	CL_D14-1044_pa_6_7
CL	D14-1044	pa	6	8	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	Second , we describe how to incorporate vector space similarity into random walk inference over KBs , reducing the feature sparsity inherent in using surface text .	With experiments on many relations from two separate KBs , we show that our methods significantly outperform prior work on KB inference , both in the size of problem our methods can handle and in the quality of predictions made .	CL_D14-1044_pa_6	CL_D14-1044_pa_8	CL_D14-1044_pa_6_8
CL	D14-1044	pa	7	8	proposal	result_means	elaboration	support	secondary	secondary	none	none	This allows us to combine distributional similarity with symbolic logical inference in novel and effective ways .	With experiments on many relations from two separate KBs , we show that our methods significantly outperform prior work on KB inference , both in the size of problem our methods can handle and in the quality of predictions made .	CL_D14-1044_pa_7	CL_D14-1044_pa_8	CL_D14-1044_pa_7_8
CL	D14-1044	pa	8	7	result_means	proposal	support	elaboration	secondary	secondary	none	none	With experiments on many relations from two separate KBs , we show that our methods significantly outperform prior work on KB inference , both in the size of problem our methods can handle and in the quality of predictions made .	This allows us to combine distributional similarity with symbolic logical inference in novel and effective ways .	CL_D14-1044_pa_8	CL_D14-1044_pa_7	CL_D14-1044_pa_7_8
CL	D14-1045	ab	1	2	motivation_background	motivation_problem	info-required	elaboration	secondary	secondary	back	elaboration	State-of-the-art semantic role labelling systems require large annotated corpora to achieve full performance .	Unfortunately , such corpora are expensive to produce and often do not generalize well across do-mains .	CL_D14-1045_ab_1	CL_D14-1045_ab_2	CL_D14-1045_ab_1_2
CL	D14-1045	ab	1	3	motivation_background	motivation_problem	info-required	elaboration	secondary	secondary	none	none	State-of-the-art semantic role labelling systems require large annotated corpora to achieve full performance .	Even in domain , errors are often made where syntactic information does not provide sufficient cues .	CL_D14-1045_ab_1	CL_D14-1045_ab_3	CL_D14-1045_ab_1_3
CL	D14-1045	ab	1	4	motivation_background	proposal	info-required	none	secondary	main	forw	info-required	State-of-the-art semantic role labelling systems require large annotated corpora to achieve full performance .	In this paper , we mitigate both of these problems by employing distributional word representations gathered from unlabelled data .	CL_D14-1045_ab_1	CL_D14-1045_ab_4	CL_D14-1045_ab_1_4
CL	D14-1045	ab	1	5	motivation_background	result_means	info-required	support	secondary	secondary	none	none	State-of-the-art semantic role labelling systems require large annotated corpora to achieve full performance .	While straight-forward word representations of predicates and arguments improve performance , we show that further gains are achieved by composing representations that model the interaction between predicate and argument , and capture full argument spans .	CL_D14-1045_ab_1	CL_D14-1045_ab_5	CL_D14-1045_ab_1_5
CL	D14-1045	ab	2	3	motivation_problem	motivation_problem	elaboration	elaboration	secondary	secondary	back	elaboration	Unfortunately , such corpora are expensive to produce and often do not generalize well across do-mains .	Even in domain , errors are often made where syntactic information does not provide sufficient cues .	CL_D14-1045_ab_2	CL_D14-1045_ab_3	CL_D14-1045_ab_2_3
CL	D14-1045	ab	2	4	motivation_problem	proposal	elaboration	none	secondary	main	none	none	Unfortunately , such corpora are expensive to produce and often do not generalize well across do-mains .	In this paper , we mitigate both of these problems by employing distributional word representations gathered from unlabelled data .	CL_D14-1045_ab_2	CL_D14-1045_ab_4	CL_D14-1045_ab_2_4
CL	D14-1045	ab	2	5	motivation_problem	result_means	elaboration	support	secondary	secondary	none	none	Unfortunately , such corpora are expensive to produce and often do not generalize well across do-mains .	While straight-forward word representations of predicates and arguments improve performance , we show that further gains are achieved by composing representations that model the interaction between predicate and argument , and capture full argument spans .	CL_D14-1045_ab_2	CL_D14-1045_ab_5	CL_D14-1045_ab_2_5
CL	D14-1045	ab	3	4	motivation_problem	proposal	elaboration	none	secondary	main	none	none	Even in domain , errors are often made where syntactic information does not provide sufficient cues .	In this paper , we mitigate both of these problems by employing distributional word representations gathered from unlabelled data .	CL_D14-1045_ab_3	CL_D14-1045_ab_4	CL_D14-1045_ab_3_4
CL	D14-1045	ab	3	5	motivation_problem	result_means	elaboration	support	secondary	secondary	none	none	Even in domain , errors are often made where syntactic information does not provide sufficient cues .	While straight-forward word representations of predicates and arguments improve performance , we show that further gains are achieved by composing representations that model the interaction between predicate and argument , and capture full argument spans .	CL_D14-1045_ab_3	CL_D14-1045_ab_5	CL_D14-1045_ab_3_5
CL	D14-1045	ab	4	5	proposal	result_means	none	support	main	secondary	back	support	In this paper , we mitigate both of these problems by employing distributional word representations gathered from unlabelled data .	While straight-forward word representations of predicates and arguments improve performance , we show that further gains are achieved by composing representations that model the interaction between predicate and argument , and capture full argument spans .	CL_D14-1045_ab_4	CL_D14-1045_ab_5	CL_D14-1045_ab_4_5
CL	D14-1045	ab	5	4	result_means	proposal	support	none	secondary	main	forw	support	While straight-forward word representations of predicates and arguments improve performance , we show that further gains are achieved by composing representations that model the interaction between predicate and argument , and capture full argument spans .	In this paper , we mitigate both of these problems by employing distributional word representations gathered from unlabelled data .	CL_D14-1045_ab_5	CL_D14-1045_ab_4	CL_D14-1045_ab_4_5
CL	D14-1045	mn	1	2	motivation_background	motivation_problem	info-required	support	secondary	secondary	forw	info-required	State-of-the-art semantic role labelling systems require large annotated corpora to achieve full performance .	Unfortunately , such corpora are expensive to produce and often do not generalize well across do-mains .	CL_D14-1045_mn_1	CL_D14-1045_mn_2	CL_D14-1045_mn_1_2
CL	D14-1045	mn	1	3	motivation_background	motivation_problem	info-required	elaboration	secondary	secondary	none	none	State-of-the-art semantic role labelling systems require large annotated corpora to achieve full performance .	Even in domain , errors are often made where syntactic information does not provide sufficient cues .	CL_D14-1045_mn_1	CL_D14-1045_mn_3	CL_D14-1045_mn_1_3
CL	D14-1045	mn	1	4	motivation_background	proposal	info-required	none	secondary	main	none	none	State-of-the-art semantic role labelling systems require large annotated corpora to achieve full performance .	In this paper , we mitigate both of these problems by employing distributional word representations gathered from unlabelled data .	CL_D14-1045_mn_1	CL_D14-1045_mn_4	CL_D14-1045_mn_1_4
CL	D14-1045	mn	1	5	motivation_background	result	info-required	support	secondary	secondary	none	none	State-of-the-art semantic role labelling systems require large annotated corpora to achieve full performance .	While straight-forward word representations of predicates and arguments improve performance , we show that further gains are achieved by composing representations that model the interaction between predicate and argument , and capture full argument spans .	CL_D14-1045_mn_1	CL_D14-1045_mn_5	CL_D14-1045_mn_1_5
CL	D14-1045	mn	2	3	motivation_problem	motivation_problem	support	elaboration	secondary	secondary	back	elaboration	Unfortunately , such corpora are expensive to produce and often do not generalize well across do-mains .	Even in domain , errors are often made where syntactic information does not provide sufficient cues .	CL_D14-1045_mn_2	CL_D14-1045_mn_3	CL_D14-1045_mn_2_3
CL	D14-1045	mn	2	4	motivation_problem	proposal	support	none	secondary	main	forw	support	Unfortunately , such corpora are expensive to produce and often do not generalize well across do-mains .	In this paper , we mitigate both of these problems by employing distributional word representations gathered from unlabelled data .	CL_D14-1045_mn_2	CL_D14-1045_mn_4	CL_D14-1045_mn_2_4
CL	D14-1045	mn	2	5	motivation_problem	result	support	support	secondary	secondary	none	none	Unfortunately , such corpora are expensive to produce and often do not generalize well across do-mains .	While straight-forward word representations of predicates and arguments improve performance , we show that further gains are achieved by composing representations that model the interaction between predicate and argument , and capture full argument spans .	CL_D14-1045_mn_2	CL_D14-1045_mn_5	CL_D14-1045_mn_2_5
CL	D14-1045	mn	3	4	motivation_problem	proposal	elaboration	none	secondary	main	none	none	Even in domain , errors are often made where syntactic information does not provide sufficient cues .	In this paper , we mitigate both of these problems by employing distributional word representations gathered from unlabelled data .	CL_D14-1045_mn_3	CL_D14-1045_mn_4	CL_D14-1045_mn_3_4
CL	D14-1045	mn	3	5	motivation_problem	result	elaboration	support	secondary	secondary	none	none	Even in domain , errors are often made where syntactic information does not provide sufficient cues .	While straight-forward word representations of predicates and arguments improve performance , we show that further gains are achieved by composing representations that model the interaction between predicate and argument , and capture full argument spans .	CL_D14-1045_mn_3	CL_D14-1045_mn_5	CL_D14-1045_mn_3_5
CL	D14-1045	mn	4	5	proposal	result	none	support	main	secondary	back	support	In this paper , we mitigate both of these problems by employing distributional word representations gathered from unlabelled data .	While straight-forward word representations of predicates and arguments improve performance , we show that further gains are achieved by composing representations that model the interaction between predicate and argument , and capture full argument spans .	CL_D14-1045_mn_4	CL_D14-1045_mn_5	CL_D14-1045_mn_4_5
CL	D14-1045	mn	5	4	result	proposal	support	none	secondary	main	forw	support	While straight-forward word representations of predicates and arguments improve performance , we show that further gains are achieved by composing representations that model the interaction between predicate and argument , and capture full argument spans .	In this paper , we mitigate both of these problems by employing distributional word representations gathered from unlabelled data .	CL_D14-1045_mn_5	CL_D14-1045_mn_4	CL_D14-1045_mn_4_5
CL	D14-1045	pa	1	2	motivation_background	motivation_problem	info-required	support	secondary	secondary	forw	info-required	State-of-the-art semantic role labelling systems require large annotated corpora to achieve full performance .	Unfortunately , such corpora are expensive to produce and often do not generalize well across do-mains .	CL_D14-1045_pa_1	CL_D14-1045_pa_2	CL_D14-1045_pa_1_2
CL	D14-1045	pa	1	3	motivation_background	motivation_problem	info-required	support	secondary	secondary	none	none	State-of-the-art semantic role labelling systems require large annotated corpora to achieve full performance .	Even in domain , errors are often made where syntactic information does not provide sufficient cues .	CL_D14-1045_pa_1	CL_D14-1045_pa_3	CL_D14-1045_pa_1_3
CL	D14-1045	pa	1	4	motivation_background	proposal	info-required	none	secondary	main	none	none	State-of-the-art semantic role labelling systems require large annotated corpora to achieve full performance .	In this paper , we mitigate both of these problems by employing distributional word representations gathered from unlabelled data .	CL_D14-1045_pa_1	CL_D14-1045_pa_4	CL_D14-1045_pa_1_4
CL	D14-1045	pa	1	5	motivation_background	result_means	info-required	support	secondary	secondary	none	none	State-of-the-art semantic role labelling systems require large annotated corpora to achieve full performance .	While straight-forward word representations of predicates and arguments improve performance , we show that further gains are achieved by composing representations that model the interaction between predicate and argument , and capture full argument spans .	CL_D14-1045_pa_1	CL_D14-1045_pa_5	CL_D14-1045_pa_1_5
CL	D14-1045	pa	2	3	motivation_problem	motivation_problem	support	support	secondary	secondary	none	none	Unfortunately , such corpora are expensive to produce and often do not generalize well across do-mains .	Even in domain , errors are often made where syntactic information does not provide sufficient cues .	CL_D14-1045_pa_2	CL_D14-1045_pa_3	CL_D14-1045_pa_2_3
CL	D14-1045	pa	2	4	motivation_problem	proposal	support	none	secondary	main	forw	support	Unfortunately , such corpora are expensive to produce and often do not generalize well across do-mains .	In this paper , we mitigate both of these problems by employing distributional word representations gathered from unlabelled data .	CL_D14-1045_pa_2	CL_D14-1045_pa_4	CL_D14-1045_pa_2_4
CL	D14-1045	pa	2	5	motivation_problem	result_means	support	support	secondary	secondary	none	none	Unfortunately , such corpora are expensive to produce and often do not generalize well across do-mains .	While straight-forward word representations of predicates and arguments improve performance , we show that further gains are achieved by composing representations that model the interaction between predicate and argument , and capture full argument spans .	CL_D14-1045_pa_2	CL_D14-1045_pa_5	CL_D14-1045_pa_2_5
CL	D14-1045	pa	3	4	motivation_problem	proposal	support	none	secondary	main	forw	support	Even in domain , errors are often made where syntactic information does not provide sufficient cues .	In this paper , we mitigate both of these problems by employing distributional word representations gathered from unlabelled data .	CL_D14-1045_pa_3	CL_D14-1045_pa_4	CL_D14-1045_pa_3_4
CL	D14-1045	pa	3	5	motivation_problem	result_means	support	support	secondary	secondary	none	none	Even in domain , errors are often made where syntactic information does not provide sufficient cues .	While straight-forward word representations of predicates and arguments improve performance , we show that further gains are achieved by composing representations that model the interaction between predicate and argument , and capture full argument spans .	CL_D14-1045_pa_3	CL_D14-1045_pa_5	CL_D14-1045_pa_3_5
CL	D14-1045	pa	4	5	proposal	result_means	none	support	main	secondary	back	support	In this paper , we mitigate both of these problems by employing distributional word representations gathered from unlabelled data .	While straight-forward word representations of predicates and arguments improve performance , we show that further gains are achieved by composing representations that model the interaction between predicate and argument , and capture full argument spans .	CL_D14-1045_pa_4	CL_D14-1045_pa_5	CL_D14-1045_pa_4_5
CL	D14-1045	pa	5	4	result_means	proposal	support	none	secondary	main	forw	support	While straight-forward word representations of predicates and arguments improve performance , we show that further gains are achieved by composing representations that model the interaction between predicate and argument , and capture full argument spans .	In this paper , we mitigate both of these problems by employing distributional word representations gathered from unlabelled data .	CL_D14-1045_pa_5	CL_D14-1045_pa_4	CL_D14-1045_pa_4_5
CL	D14-1046	ab	1	2	proposal	observation	none	support	main	secondary	none	none	This paper reports on the development of a hybrid and simple method based on a machine learning classifier ( Naive Bayes ) , Word Sense Disambiguation and rules , for the automatic assignment of WordNet Domains to nominal entries of a lexicographic dictionary , the Senso Comune De Mauro Lexicon .	The system obtained an F1 score of 0.58 , with a Precision of 0.70 .	CL_D14-1046_ab_1	CL_D14-1046_ab_2	CL_D14-1046_ab_1_2
CL	D14-1046	ab	1	3	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	This paper reports on the development of a hybrid and simple method based on a machine learning classifier ( Naive Bayes ) , Word Sense Disambiguation and rules , for the automatic assignment of WordNet Domains to nominal entries of a lexicographic dictionary , the Senso Comune De Mauro Lexicon .	We further used the automatically assigned domains to filter out word sense alignments between MultiWordNet and Senso Comune .	CL_D14-1046_ab_1	CL_D14-1046_ab_3	CL_D14-1046_ab_1_3
CL	D14-1046	ab	1	4	proposal	result	none	support	main	secondary	back	support	This paper reports on the development of a hybrid and simple method based on a machine learning classifier ( Naive Bayes ) , Word Sense Disambiguation and rules , for the automatic assignment of WordNet Domains to nominal entries of a lexicographic dictionary , the Senso Comune De Mauro Lexicon .	This has led to an improvement in the quality of the sense alignments showing the validity of the approach for domain assignment and the importance of domain information for achieving good sense alignments .	CL_D14-1046_ab_1	CL_D14-1046_ab_4	CL_D14-1046_ab_1_4
CL	D14-1046	ab	2	3	observation	proposal_implementation	support	elaboration	secondary	secondary	none	none	The system obtained an F1 score of 0.58 , with a Precision of 0.70 .	We further used the automatically assigned domains to filter out word sense alignments between MultiWordNet and Senso Comune .	CL_D14-1046_ab_2	CL_D14-1046_ab_3	CL_D14-1046_ab_2_3
CL	D14-1046	ab	2	4	observation	result	support	support	secondary	secondary	forw	support	The system obtained an F1 score of 0.58 , with a Precision of 0.70 .	This has led to an improvement in the quality of the sense alignments showing the validity of the approach for domain assignment and the importance of domain information for achieving good sense alignments .	CL_D14-1046_ab_2	CL_D14-1046_ab_4	CL_D14-1046_ab_2_4
CL	D14-1046	ab	3	4	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	We further used the automatically assigned domains to filter out word sense alignments between MultiWordNet and Senso Comune .	This has led to an improvement in the quality of the sense alignments showing the validity of the approach for domain assignment and the importance of domain information for achieving good sense alignments .	CL_D14-1046_ab_3	CL_D14-1046_ab_4	CL_D14-1046_ab_3_4
CL	D14-1046	ab	4	3	result	proposal_implementation	support	elaboration	secondary	secondary	none	none	This has led to an improvement in the quality of the sense alignments showing the validity of the approach for domain assignment and the importance of domain information for achieving good sense alignments .	We further used the automatically assigned domains to filter out word sense alignments between MultiWordNet and Senso Comune .	CL_D14-1046_ab_4	CL_D14-1046_ab_3	CL_D14-1046_ab_3_4
CL	D14-1046	mn	1	2	proposal	observation	none	support	main	secondary	back	support	This paper reports on the development of a hybrid and simple method based on a machine learning classifier ( Naive Bayes ) , Word Sense Disambiguation and rules , for the automatic assignment of WordNet Domains to nominal entries of a lexicographic dictionary , the Senso Comune De Mauro Lexicon .	The system obtained an F1 score of 0.58 , with a Precision of 0.70 .	CL_D14-1046_mn_1	CL_D14-1046_mn_2	CL_D14-1046_mn_1_2
CL	D14-1046	mn	1	3	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	This paper reports on the development of a hybrid and simple method based on a machine learning classifier ( Naive Bayes ) , Word Sense Disambiguation and rules , for the automatic assignment of WordNet Domains to nominal entries of a lexicographic dictionary , the Senso Comune De Mauro Lexicon .	We further used the automatically assigned domains to filter out word sense alignments between MultiWordNet and Senso Comune .	CL_D14-1046_mn_1	CL_D14-1046_mn_3	CL_D14-1046_mn_1_3
CL	D14-1046	mn	1	4	proposal	result	none	support	main	secondary	none	none	This paper reports on the development of a hybrid and simple method based on a machine learning classifier ( Naive Bayes ) , Word Sense Disambiguation and rules , for the automatic assignment of WordNet Domains to nominal entries of a lexicographic dictionary , the Senso Comune De Mauro Lexicon .	This has led to an improvement in the quality of the sense alignments showing the validity of the approach for domain assignment and the importance of domain information for achieving good sense alignments .	CL_D14-1046_mn_1	CL_D14-1046_mn_4	CL_D14-1046_mn_1_4
CL	D14-1046	mn	2	3	observation	proposal_implementation	support	elaboration	secondary	secondary	none	none	The system obtained an F1 score of 0.58 , with a Precision of 0.70 .	We further used the automatically assigned domains to filter out word sense alignments between MultiWordNet and Senso Comune .	CL_D14-1046_mn_2	CL_D14-1046_mn_3	CL_D14-1046_mn_2_3
CL	D14-1046	mn	2	4	observation	result	support	support	secondary	secondary	none	none	The system obtained an F1 score of 0.58 , with a Precision of 0.70 .	This has led to an improvement in the quality of the sense alignments showing the validity of the approach for domain assignment and the importance of domain information for achieving good sense alignments .	CL_D14-1046_mn_2	CL_D14-1046_mn_4	CL_D14-1046_mn_2_4
CL	D14-1046	mn	3	4	proposal_implementation	result	elaboration	support	secondary	secondary	back	support	We further used the automatically assigned domains to filter out word sense alignments between MultiWordNet and Senso Comune .	This has led to an improvement in the quality of the sense alignments showing the validity of the approach for domain assignment and the importance of domain information for achieving good sense alignments .	CL_D14-1046_mn_3	CL_D14-1046_mn_4	CL_D14-1046_mn_3_4
CL	D14-1046	mn	4	3	result	proposal_implementation	support	elaboration	secondary	secondary	forw	support	This has led to an improvement in the quality of the sense alignments showing the validity of the approach for domain assignment and the importance of domain information for achieving good sense alignments .	We further used the automatically assigned domains to filter out word sense alignments between MultiWordNet and Senso Comune .	CL_D14-1046_mn_4	CL_D14-1046_mn_3	CL_D14-1046_mn_3_4
CL	D14-1046	pa	1	2	proposal	observation	none	support	main	secondary	back	support	This paper reports on the development of a hybrid and simple method based on a machine learning classifier ( Naive Bayes ) , Word Sense Disambiguation and rules , for the automatic assignment of WordNet Domains to nominal entries of a lexicographic dictionary , the Senso Comune De Mauro Lexicon .	The system obtained an F1 score of 0.58 , with a Precision of 0.70 .	CL_D14-1046_pa_1	CL_D14-1046_pa_2	CL_D14-1046_pa_1_2
CL	D14-1046	pa	1	3	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	This paper reports on the development of a hybrid and simple method based on a machine learning classifier ( Naive Bayes ) , Word Sense Disambiguation and rules , for the automatic assignment of WordNet Domains to nominal entries of a lexicographic dictionary , the Senso Comune De Mauro Lexicon .	We further used the automatically assigned domains to filter out word sense alignments between MultiWordNet and Senso Comune .	CL_D14-1046_pa_1	CL_D14-1046_pa_3	CL_D14-1046_pa_1_3
CL	D14-1046	pa	1	4	proposal	result	none	support	main	secondary	none	none	This paper reports on the development of a hybrid and simple method based on a machine learning classifier ( Naive Bayes ) , Word Sense Disambiguation and rules , for the automatic assignment of WordNet Domains to nominal entries of a lexicographic dictionary , the Senso Comune De Mauro Lexicon .	This has led to an improvement in the quality of the sense alignments showing the validity of the approach for domain assignment and the importance of domain information for achieving good sense alignments .	CL_D14-1046_pa_1	CL_D14-1046_pa_4	CL_D14-1046_pa_1_4
CL	D14-1046	pa	2	3	observation	proposal_implementation	support	elaboration	secondary	secondary	none	none	The system obtained an F1 score of 0.58 , with a Precision of 0.70 .	We further used the automatically assigned domains to filter out word sense alignments between MultiWordNet and Senso Comune .	CL_D14-1046_pa_2	CL_D14-1046_pa_3	CL_D14-1046_pa_2_3
CL	D14-1046	pa	2	4	observation	result	support	support	secondary	secondary	none	none	The system obtained an F1 score of 0.58 , with a Precision of 0.70 .	This has led to an improvement in the quality of the sense alignments showing the validity of the approach for domain assignment and the importance of domain information for achieving good sense alignments .	CL_D14-1046_pa_2	CL_D14-1046_pa_4	CL_D14-1046_pa_2_4
CL	D14-1046	pa	3	4	proposal_implementation	result	elaboration	support	secondary	secondary	back	support	We further used the automatically assigned domains to filter out word sense alignments between MultiWordNet and Senso Comune .	This has led to an improvement in the quality of the sense alignments showing the validity of the approach for domain assignment and the importance of domain information for achieving good sense alignments .	CL_D14-1046_pa_3	CL_D14-1046_pa_4	CL_D14-1046_pa_3_4
CL	D14-1046	pa	4	3	result	proposal_implementation	support	elaboration	secondary	secondary	forw	support	This has led to an improvement in the quality of the sense alignments showing the validity of the approach for domain assignment and the importance of domain information for achieving good sense alignments .	We further used the automatically assigned domains to filter out word sense alignments between MultiWordNet and Senso Comune .	CL_D14-1046_pa_4	CL_D14-1046_pa_3	CL_D14-1046_pa_3_4
CL	D14-1047	ab	1	2	motivation_background	proposal	support	none	secondary	main	forw	support	Much attention has been given to the impact of informativeness and similarity measures on distributional thesauri .	We investigate the effects of context filters on thesaurus quality and propose the use of cooccurrence frequency as a simple and inexpensive criterion .	CL_D14-1047_ab_1	CL_D14-1047_ab_2	CL_D14-1047_ab_1_2
CL	D14-1047	ab	1	3	motivation_background	means	support	by-means	secondary	secondary	none	none	Much attention has been given to the impact of informativeness and similarity measures on distributional thesauri .	For evaluation , we measure thesaurus agreement with WordNet and performance in answering TOEFL-like questions .	CL_D14-1047_ab_1	CL_D14-1047_ab_3	CL_D14-1047_ab_1_3
CL	D14-1047	ab	1	4	motivation_background	result	support	support	secondary	secondary	none	none	Much attention has been given to the impact of informativeness and similarity measures on distributional thesauri .	Results illustrate the sensitivity of distributional thesauri to filters .	CL_D14-1047_ab_1	CL_D14-1047_ab_4	CL_D14-1047_ab_1_4
CL	D14-1047	ab	2	3	proposal	means	none	by-means	main	secondary	none	none	We investigate the effects of context filters on thesaurus quality and propose the use of cooccurrence frequency as a simple and inexpensive criterion .	For evaluation , we measure thesaurus agreement with WordNet and performance in answering TOEFL-like questions .	CL_D14-1047_ab_2	CL_D14-1047_ab_3	CL_D14-1047_ab_2_3
CL	D14-1047	ab	2	4	proposal	result	none	support	main	secondary	back	support	We investigate the effects of context filters on thesaurus quality and propose the use of cooccurrence frequency as a simple and inexpensive criterion .	Results illustrate the sensitivity of distributional thesauri to filters .	CL_D14-1047_ab_2	CL_D14-1047_ab_4	CL_D14-1047_ab_2_4
CL	D14-1047	ab	3	4	means	result	by-means	support	secondary	secondary	forw	by-means	For evaluation , we measure thesaurus agreement with WordNet and performance in answering TOEFL-like questions .	Results illustrate the sensitivity of distributional thesauri to filters .	CL_D14-1047_ab_3	CL_D14-1047_ab_4	CL_D14-1047_ab_3_4
CL	D14-1047	ab	4	3	result	means	support	by-means	secondary	secondary	back	by-means	Results illustrate the sensitivity of distributional thesauri to filters .	For evaluation , we measure thesaurus agreement with WordNet and performance in answering TOEFL-like questions .	CL_D14-1047_ab_4	CL_D14-1047_ab_3	CL_D14-1047_ab_3_4
CL	D14-1047	mn	1	2	motivation_background	proposal	support	none	secondary	main	forw	support	Much attention has been given to the impact of informativeness and similarity measures on distributional thesauri .	We investigate the effects of context filters on thesaurus quality and propose the use of cooccurrence frequency as a simple and inexpensive criterion .	CL_D14-1047_mn_1	CL_D14-1047_mn_2	CL_D14-1047_mn_1_2
CL	D14-1047	mn	1	3	motivation_background	means	support	by-means	secondary	secondary	none	none	Much attention has been given to the impact of informativeness and similarity measures on distributional thesauri .	For evaluation , we measure thesaurus agreement with WordNet and performance in answering TOEFL-like questions .	CL_D14-1047_mn_1	CL_D14-1047_mn_3	CL_D14-1047_mn_1_3
CL	D14-1047	mn	1	4	motivation_background	result	support	support	secondary	secondary	none	none	Much attention has been given to the impact of informativeness and similarity measures on distributional thesauri .	Results illustrate the sensitivity of distributional thesauri to filters .	CL_D14-1047_mn_1	CL_D14-1047_mn_4	CL_D14-1047_mn_1_4
CL	D14-1047	mn	2	3	proposal	means	none	by-means	main	secondary	none	none	We investigate the effects of context filters on thesaurus quality and propose the use of cooccurrence frequency as a simple and inexpensive criterion .	For evaluation , we measure thesaurus agreement with WordNet and performance in answering TOEFL-like questions .	CL_D14-1047_mn_2	CL_D14-1047_mn_3	CL_D14-1047_mn_2_3
CL	D14-1047	mn	2	4	proposal	result	none	support	main	secondary	back	support	We investigate the effects of context filters on thesaurus quality and propose the use of cooccurrence frequency as a simple and inexpensive criterion .	Results illustrate the sensitivity of distributional thesauri to filters .	CL_D14-1047_mn_2	CL_D14-1047_mn_4	CL_D14-1047_mn_2_4
CL	D14-1047	mn	3	4	means	result	by-means	support	secondary	secondary	forw	by-means	For evaluation , we measure thesaurus agreement with WordNet and performance in answering TOEFL-like questions .	Results illustrate the sensitivity of distributional thesauri to filters .	CL_D14-1047_mn_3	CL_D14-1047_mn_4	CL_D14-1047_mn_3_4
CL	D14-1047	mn	4	3	result	means	support	by-means	secondary	secondary	back	by-means	Results illustrate the sensitivity of distributional thesauri to filters .	For evaluation , we measure thesaurus agreement with WordNet and performance in answering TOEFL-like questions .	CL_D14-1047_mn_4	CL_D14-1047_mn_3	CL_D14-1047_mn_3_4
CL	D14-1047	pa	1	2	motivation_background	proposal	support	none	secondary	main	forw	support	Much attention has been given to the impact of informativeness and similarity measures on distributional thesauri .	We investigate the effects of context filters on thesaurus quality and propose the use of cooccurrence frequency as a simple and inexpensive criterion .	CL_D14-1047_pa_1	CL_D14-1047_pa_2	CL_D14-1047_pa_1_2
CL	D14-1047	pa	1	3	motivation_background	means	support	by-means	secondary	secondary	none	none	Much attention has been given to the impact of informativeness and similarity measures on distributional thesauri .	For evaluation , we measure thesaurus agreement with WordNet and performance in answering TOEFL-like questions .	CL_D14-1047_pa_1	CL_D14-1047_pa_3	CL_D14-1047_pa_1_3
CL	D14-1047	pa	1	4	motivation_background	result	support	support	secondary	secondary	none	none	Much attention has been given to the impact of informativeness and similarity measures on distributional thesauri .	Results illustrate the sensitivity of distributional thesauri to filters .	CL_D14-1047_pa_1	CL_D14-1047_pa_4	CL_D14-1047_pa_1_4
CL	D14-1047	pa	2	3	proposal	means	none	by-means	main	secondary	none	none	We investigate the effects of context filters on thesaurus quality and propose the use of cooccurrence frequency as a simple and inexpensive criterion .	For evaluation , we measure thesaurus agreement with WordNet and performance in answering TOEFL-like questions .	CL_D14-1047_pa_2	CL_D14-1047_pa_3	CL_D14-1047_pa_2_3
CL	D14-1047	pa	2	4	proposal	result	none	support	main	secondary	back	support	We investigate the effects of context filters on thesaurus quality and propose the use of cooccurrence frequency as a simple and inexpensive criterion .	Results illustrate the sensitivity of distributional thesauri to filters .	CL_D14-1047_pa_2	CL_D14-1047_pa_4	CL_D14-1047_pa_2_4
CL	D14-1047	pa	3	4	means	result	by-means	support	secondary	secondary	forw	by-means	For evaluation , we measure thesaurus agreement with WordNet and performance in answering TOEFL-like questions .	Results illustrate the sensitivity of distributional thesauri to filters .	CL_D14-1047_pa_3	CL_D14-1047_pa_4	CL_D14-1047_pa_3_4
CL	D14-1047	pa	4	3	result	means	support	by-means	secondary	secondary	back	by-means	Results illustrate the sensitivity of distributional thesauri to filters .	For evaluation , we measure thesaurus agreement with WordNet and performance in answering TOEFL-like questions .	CL_D14-1047_pa_4	CL_D14-1047_pa_3	CL_D14-1047_pa_3_4
CL	D14-1048	ab	1	2	proposal	motivation_hypothesis	none	support	main	secondary	back	support	We align pairs of English sentences and corresponding Abstract Meaning Representations ( AMR ) , at the token level .	Such alignments will be useful for downstream extraction of semantic interpretation and generation rules .	CL_D14-1048_ab_1	CL_D14-1048_ab_2	CL_D14-1048_ab_1_2
CL	D14-1048	ab	1	3	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	We align pairs of English sentences and corresponding Abstract Meaning Representations ( AMR ) , at the token level .	Our method involves linearizing AMR structures and performing symmetrized EM training .	CL_D14-1048_ab_1	CL_D14-1048_ab_3	CL_D14-1048_ab_1_3
CL	D14-1048	ab	1	4	proposal	observation	none	support	main	secondary	back	support	We align pairs of English sentences and corresponding Abstract Meaning Representations ( AMR ) , at the token level .	We obtain 86.5 % and 83.1 % alignment F score on development and test sets .	CL_D14-1048_ab_1	CL_D14-1048_ab_4	CL_D14-1048_ab_1_4
CL	D14-1048	ab	2	3	motivation_hypothesis	proposal_implementation	support	elaboration	secondary	secondary	none	none	Such alignments will be useful for downstream extraction of semantic interpretation and generation rules .	Our method involves linearizing AMR structures and performing symmetrized EM training .	CL_D14-1048_ab_2	CL_D14-1048_ab_3	CL_D14-1048_ab_2_3
CL	D14-1048	ab	2	4	motivation_hypothesis	observation	support	support	secondary	secondary	none	none	Such alignments will be useful for downstream extraction of semantic interpretation and generation rules .	We obtain 86.5 % and 83.1 % alignment F score on development and test sets .	CL_D14-1048_ab_2	CL_D14-1048_ab_4	CL_D14-1048_ab_2_4
CL	D14-1048	ab	3	4	proposal_implementation	observation	elaboration	support	secondary	secondary	none	none	Our method involves linearizing AMR structures and performing symmetrized EM training .	We obtain 86.5 % and 83.1 % alignment F score on development and test sets .	CL_D14-1048_ab_3	CL_D14-1048_ab_4	CL_D14-1048_ab_3_4
CL	D14-1048	ab	4	3	observation	proposal_implementation	support	elaboration	secondary	secondary	none	none	We obtain 86.5 % and 83.1 % alignment F score on development and test sets .	Our method involves linearizing AMR structures and performing symmetrized EM training .	CL_D14-1048_ab_4	CL_D14-1048_ab_3	CL_D14-1048_ab_3_4
CL	D14-1048	mn	1	2	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	We align pairs of English sentences and corresponding Abstract Meaning Representations ( AMR ) , at the token level .	Such alignments will be useful for downstream extraction of semantic interpretation and generation rules .	CL_D14-1048_mn_1	CL_D14-1048_mn_2	CL_D14-1048_mn_1_2
CL	D14-1048	mn	1	3	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	We align pairs of English sentences and corresponding Abstract Meaning Representations ( AMR ) , at the token level .	Our method involves linearizing AMR structures and performing symmetrized EM training .	CL_D14-1048_mn_1	CL_D14-1048_mn_3	CL_D14-1048_mn_1_3
CL	D14-1048	mn	1	4	proposal	observation	none	support	main	secondary	back	support	We align pairs of English sentences and corresponding Abstract Meaning Representations ( AMR ) , at the token level .	We obtain 86.5 % and 83.1 % alignment F score on development and test sets .	CL_D14-1048_mn_1	CL_D14-1048_mn_4	CL_D14-1048_mn_1_4
CL	D14-1048	mn	2	3	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	none	none	Such alignments will be useful for downstream extraction of semantic interpretation and generation rules .	Our method involves linearizing AMR structures and performing symmetrized EM training .	CL_D14-1048_mn_2	CL_D14-1048_mn_3	CL_D14-1048_mn_2_3
CL	D14-1048	mn	2	4	proposal_implementation	observation	elaboration	support	secondary	secondary	none	none	Such alignments will be useful for downstream extraction of semantic interpretation and generation rules .	We obtain 86.5 % and 83.1 % alignment F score on development and test sets .	CL_D14-1048_mn_2	CL_D14-1048_mn_4	CL_D14-1048_mn_2_4
CL	D14-1048	mn	3	4	proposal_implementation	observation	elaboration	support	secondary	secondary	none	none	Our method involves linearizing AMR structures and performing symmetrized EM training .	We obtain 86.5 % and 83.1 % alignment F score on development and test sets .	CL_D14-1048_mn_3	CL_D14-1048_mn_4	CL_D14-1048_mn_3_4
CL	D14-1048	mn	4	3	observation	proposal_implementation	support	elaboration	secondary	secondary	none	none	We obtain 86.5 % and 83.1 % alignment F score on development and test sets .	Our method involves linearizing AMR structures and performing symmetrized EM training .	CL_D14-1048_mn_4	CL_D14-1048_mn_3	CL_D14-1048_mn_3_4
CL	D14-1048	pa	1	2	proposal	motivation_hypothesis	none	support	main	secondary	back	support	We align pairs of English sentences and corresponding Abstract Meaning Representations ( AMR ) , at the token level .	Such alignments will be useful for downstream extraction of semantic interpretation and generation rules .	CL_D14-1048_pa_1	CL_D14-1048_pa_2	CL_D14-1048_pa_1_2
CL	D14-1048	pa	1	3	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	We align pairs of English sentences and corresponding Abstract Meaning Representations ( AMR ) , at the token level .	Our method involves linearizing AMR structures and performing symmetrized EM training .	CL_D14-1048_pa_1	CL_D14-1048_pa_3	CL_D14-1048_pa_1_3
CL	D14-1048	pa	1	4	proposal	observation	none	support	main	secondary	back	support	We align pairs of English sentences and corresponding Abstract Meaning Representations ( AMR ) , at the token level .	We obtain 86.5 % and 83.1 % alignment F score on development and test sets .	CL_D14-1048_pa_1	CL_D14-1048_pa_4	CL_D14-1048_pa_1_4
CL	D14-1048	pa	2	3	motivation_hypothesis	proposal_implementation	support	elaboration	secondary	secondary	none	none	Such alignments will be useful for downstream extraction of semantic interpretation and generation rules .	Our method involves linearizing AMR structures and performing symmetrized EM training .	CL_D14-1048_pa_2	CL_D14-1048_pa_3	CL_D14-1048_pa_2_3
CL	D14-1048	pa	2	4	motivation_hypothesis	observation	support	support	secondary	secondary	none	none	Such alignments will be useful for downstream extraction of semantic interpretation and generation rules .	We obtain 86.5 % and 83.1 % alignment F score on development and test sets .	CL_D14-1048_pa_2	CL_D14-1048_pa_4	CL_D14-1048_pa_2_4
CL	D14-1048	pa	3	4	proposal_implementation	observation	elaboration	support	secondary	secondary	none	none	Our method involves linearizing AMR structures and performing symmetrized EM training .	We obtain 86.5 % and 83.1 % alignment F score on development and test sets .	CL_D14-1048_pa_3	CL_D14-1048_pa_4	CL_D14-1048_pa_3_4
CL	D14-1048	pa	4	3	observation	proposal_implementation	support	elaboration	secondary	secondary	none	none	We obtain 86.5 % and 83.1 % alignment F score on development and test sets .	Our method involves linearizing AMR structures and performing symmetrized EM training .	CL_D14-1048_pa_4	CL_D14-1048_pa_3	CL_D14-1048_pa_3_4
CL	D14-1049	ab	1	2	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	We introduce a Semantic Role Labeling ( SRL ) parser that finds semantic roles for a predicate together with the syntactic paths linking predicates and arguments .	Our main contribution is to formulate SRL in terms of shortest-path inference , on the assumption that the SRL model is restricted to arc-factored features of the syntactic paths behind semantic roles .	CL_D14-1049_ab_1	CL_D14-1049_ab_2	CL_D14-1049_ab_1_2
CL	D14-1049	ab	1	3	proposal	proposal	none	elaboration	main	secondary	none	none	We introduce a Semantic Role Labeling ( SRL ) parser that finds semantic roles for a predicate together with the syntactic paths linking predicates and arguments .	Overall , our method for SRL is a novel way to exploit larger variability in the syntactic realizations of predicate-argument relations , moving away from pipeline architectures .	CL_D14-1049_ab_1	CL_D14-1049_ab_3	CL_D14-1049_ab_1_3
CL	D14-1049	ab	1	4	proposal	result	none	support	main	secondary	back	support	We introduce a Semantic Role Labeling ( SRL ) parser that finds semantic roles for a predicate together with the syntactic paths linking predicates and arguments .	Experiments show that our approach improves the robustness of the predictions , producing arc-factored models that perform closely to methods using unrestricted features from the syntax .	CL_D14-1049_ab_1	CL_D14-1049_ab_4	CL_D14-1049_ab_1_4
CL	D14-1049	ab	2	3	proposal_implementation	proposal	elaboration	elaboration	secondary	secondary	back	elaboration	Our main contribution is to formulate SRL in terms of shortest-path inference , on the assumption that the SRL model is restricted to arc-factored features of the syntactic paths behind semantic roles .	Overall , our method for SRL is a novel way to exploit larger variability in the syntactic realizations of predicate-argument relations , moving away from pipeline architectures .	CL_D14-1049_ab_2	CL_D14-1049_ab_3	CL_D14-1049_ab_2_3
CL	D14-1049	ab	2	4	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	Our main contribution is to formulate SRL in terms of shortest-path inference , on the assumption that the SRL model is restricted to arc-factored features of the syntactic paths behind semantic roles .	Experiments show that our approach improves the robustness of the predictions , producing arc-factored models that perform closely to methods using unrestricted features from the syntax .	CL_D14-1049_ab_2	CL_D14-1049_ab_4	CL_D14-1049_ab_2_4
CL	D14-1049	ab	3	4	proposal	result	elaboration	support	secondary	secondary	none	none	Overall , our method for SRL is a novel way to exploit larger variability in the syntactic realizations of predicate-argument relations , moving away from pipeline architectures .	Experiments show that our approach improves the robustness of the predictions , producing arc-factored models that perform closely to methods using unrestricted features from the syntax .	CL_D14-1049_ab_3	CL_D14-1049_ab_4	CL_D14-1049_ab_3_4
CL	D14-1049	ab	4	3	result	proposal	support	elaboration	secondary	secondary	none	none	Experiments show that our approach improves the robustness of the predictions , producing arc-factored models that perform closely to methods using unrestricted features from the syntax .	Overall , our method for SRL is a novel way to exploit larger variability in the syntactic realizations of predicate-argument relations , moving away from pipeline architectures .	CL_D14-1049_ab_4	CL_D14-1049_ab_3	CL_D14-1049_ab_3_4
CL	D14-1049	mn	1	2	proposal	proposal_implementation	none	support	main	secondary	back	support	We introduce a Semantic Role Labeling ( SRL ) parser that finds semantic roles for a predicate together with the syntactic paths linking predicates and arguments .	Our main contribution is to formulate SRL in terms of shortest-path inference , on the assumption that the SRL model is restricted to arc-factored features of the syntactic paths behind semantic roles .	CL_D14-1049_mn_1	CL_D14-1049_mn_2	CL_D14-1049_mn_1_2
CL	D14-1049	mn	1	3	proposal	conclusion	none	support	main	secondary	back	support	We introduce a Semantic Role Labeling ( SRL ) parser that finds semantic roles for a predicate together with the syntactic paths linking predicates and arguments .	Overall , our method for SRL is a novel way to exploit larger variability in the syntactic realizations of predicate-argument relations , moving away from pipeline architectures .	CL_D14-1049_mn_1	CL_D14-1049_mn_3	CL_D14-1049_mn_1_3
CL	D14-1049	mn	1	4	proposal	result	none	support	main	secondary	none	none	We introduce a Semantic Role Labeling ( SRL ) parser that finds semantic roles for a predicate together with the syntactic paths linking predicates and arguments .	Experiments show that our approach improves the robustness of the predictions , producing arc-factored models that perform closely to methods using unrestricted features from the syntax .	CL_D14-1049_mn_1	CL_D14-1049_mn_4	CL_D14-1049_mn_1_4
CL	D14-1049	mn	2	3	proposal_implementation	conclusion	support	support	secondary	secondary	none	none	Our main contribution is to formulate SRL in terms of shortest-path inference , on the assumption that the SRL model is restricted to arc-factored features of the syntactic paths behind semantic roles .	Overall , our method for SRL is a novel way to exploit larger variability in the syntactic realizations of predicate-argument relations , moving away from pipeline architectures .	CL_D14-1049_mn_2	CL_D14-1049_mn_3	CL_D14-1049_mn_2_3
CL	D14-1049	mn	2	4	proposal_implementation	result	support	support	secondary	secondary	none	none	Our main contribution is to formulate SRL in terms of shortest-path inference , on the assumption that the SRL model is restricted to arc-factored features of the syntactic paths behind semantic roles .	Experiments show that our approach improves the robustness of the predictions , producing arc-factored models that perform closely to methods using unrestricted features from the syntax .	CL_D14-1049_mn_2	CL_D14-1049_mn_4	CL_D14-1049_mn_2_4
CL	D14-1049	mn	3	4	conclusion	result	support	support	secondary	secondary	back	support	Overall , our method for SRL is a novel way to exploit larger variability in the syntactic realizations of predicate-argument relations , moving away from pipeline architectures .	Experiments show that our approach improves the robustness of the predictions , producing arc-factored models that perform closely to methods using unrestricted features from the syntax .	CL_D14-1049_mn_3	CL_D14-1049_mn_4	CL_D14-1049_mn_3_4
CL	D14-1049	mn	4	3	result	conclusion	support	support	secondary	secondary	forw	support	Experiments show that our approach improves the robustness of the predictions , producing arc-factored models that perform closely to methods using unrestricted features from the syntax .	Overall , our method for SRL is a novel way to exploit larger variability in the syntactic realizations of predicate-argument relations , moving away from pipeline architectures .	CL_D14-1049_mn_4	CL_D14-1049_mn_3	CL_D14-1049_mn_3_4
CL	D14-1049	pa	1	2	proposal	proposal	none	elaboration	main	secondary	back	elaboration	We introduce a Semantic Role Labeling ( SRL ) parser that finds semantic roles for a predicate together with the syntactic paths linking predicates and arguments .	Our main contribution is to formulate SRL in terms of shortest-path inference , on the assumption that the SRL model is restricted to arc-factored features of the syntactic paths behind semantic roles .	CL_D14-1049_pa_1	CL_D14-1049_pa_2	CL_D14-1049_pa_1_2
CL	D14-1049	pa	1	3	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	We introduce a Semantic Role Labeling ( SRL ) parser that finds semantic roles for a predicate together with the syntactic paths linking predicates and arguments .	Overall , our method for SRL is a novel way to exploit larger variability in the syntactic realizations of predicate-argument relations , moving away from pipeline architectures .	CL_D14-1049_pa_1	CL_D14-1049_pa_3	CL_D14-1049_pa_1_3
CL	D14-1049	pa	1	4	proposal	result	none	support	main	secondary	back	support	We introduce a Semantic Role Labeling ( SRL ) parser that finds semantic roles for a predicate together with the syntactic paths linking predicates and arguments .	Experiments show that our approach improves the robustness of the predictions , producing arc-factored models that perform closely to methods using unrestricted features from the syntax .	CL_D14-1049_pa_1	CL_D14-1049_pa_4	CL_D14-1049_pa_1_4
CL	D14-1049	pa	2	3	proposal	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	Our main contribution is to formulate SRL in terms of shortest-path inference , on the assumption that the SRL model is restricted to arc-factored features of the syntactic paths behind semantic roles .	Overall , our method for SRL is a novel way to exploit larger variability in the syntactic realizations of predicate-argument relations , moving away from pipeline architectures .	CL_D14-1049_pa_2	CL_D14-1049_pa_3	CL_D14-1049_pa_2_3
CL	D14-1049	pa	2	4	proposal	result	elaboration	support	secondary	secondary	none	none	Our main contribution is to formulate SRL in terms of shortest-path inference , on the assumption that the SRL model is restricted to arc-factored features of the syntactic paths behind semantic roles .	Experiments show that our approach improves the robustness of the predictions , producing arc-factored models that perform closely to methods using unrestricted features from the syntax .	CL_D14-1049_pa_2	CL_D14-1049_pa_4	CL_D14-1049_pa_2_4
CL	D14-1049	pa	3	4	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	Overall , our method for SRL is a novel way to exploit larger variability in the syntactic realizations of predicate-argument relations , moving away from pipeline architectures .	Experiments show that our approach improves the robustness of the predictions , producing arc-factored models that perform closely to methods using unrestricted features from the syntax .	CL_D14-1049_pa_3	CL_D14-1049_pa_4	CL_D14-1049_pa_3_4
CL	D14-1049	pa	4	3	result	proposal_implementation	support	elaboration	secondary	secondary	none	none	Experiments show that our approach improves the robustness of the predictions , producing arc-factored models that perform closely to methods using unrestricted features from the syntax .	Overall , our method for SRL is a novel way to exploit larger variability in the syntactic realizations of predicate-argument relations , moving away from pipeline architectures .	CL_D14-1049_pa_4	CL_D14-1049_pa_3	CL_D14-1049_pa_3_4
CL	D14-1050	ab	1	2	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	We present an empirical study on the use of semantic information for Concept Segmentation and Labeling ( CSL ) , which is an important step for semantic parsing .	We represent the alternative analyses output by a state-of-the-art CSL parser with tree structures , which we rerank with a classifier trained on two types of semantic tree kernels : one processing structures built with words , concepts and Brown clusters , and another one using semantic similarity among the words composing the structure .	CL_D14-1050_ab_1	CL_D14-1050_ab_2	CL_D14-1050_ab_1_2
CL	D14-1050	ab	1	3	proposal	result	none	support	main	secondary	back	support	We present an empirical study on the use of semantic information for Concept Segmentation and Labeling ( CSL ) , which is an important step for semantic parsing .	The results on a corpus from the restaurant domain show that our semantic kernels exploiting similarity measures outperform state-of-the-art rerankers .	CL_D14-1050_ab_1	CL_D14-1050_ab_3	CL_D14-1050_ab_1_3
CL	D14-1050	ab	2	3	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	We represent the alternative analyses output by a state-of-the-art CSL parser with tree structures , which we rerank with a classifier trained on two types of semantic tree kernels : one processing structures built with words , concepts and Brown clusters , and another one using semantic similarity among the words composing the structure .	The results on a corpus from the restaurant domain show that our semantic kernels exploiting similarity measures outperform state-of-the-art rerankers .	CL_D14-1050_ab_2	CL_D14-1050_ab_3	CL_D14-1050_ab_2_3
CL	D14-1050	ab	3	2	result	proposal_implementation	support	elaboration	secondary	secondary	none	none	The results on a corpus from the restaurant domain show that our semantic kernels exploiting similarity measures outperform state-of-the-art rerankers .	We represent the alternative analyses output by a state-of-the-art CSL parser with tree structures , which we rerank with a classifier trained on two types of semantic tree kernels : one processing structures built with words , concepts and Brown clusters , and another one using semantic similarity among the words composing the structure .	CL_D14-1050_ab_3	CL_D14-1050_ab_2	CL_D14-1050_ab_2_3
CL	D14-1050	mn	1	2	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	We present an empirical study on the use of semantic information for Concept Segmentation and Labeling ( CSL ) , which is an important step for semantic parsing .	We represent the alternative analyses output by a state-of-the-art CSL parser with tree structures , which we rerank with a classifier trained on two types of semantic tree kernels : one processing structures built with words , concepts and Brown clusters , and another one using semantic similarity among the words composing the structure .	CL_D14-1050_mn_1	CL_D14-1050_mn_2	CL_D14-1050_mn_1_2
CL	D14-1050	mn	1	3	proposal	result	none	support	main	secondary	back	support	We present an empirical study on the use of semantic information for Concept Segmentation and Labeling ( CSL ) , which is an important step for semantic parsing .	The results on a corpus from the restaurant domain show that our semantic kernels exploiting similarity measures outperform state-of-the-art rerankers .	CL_D14-1050_mn_1	CL_D14-1050_mn_3	CL_D14-1050_mn_1_3
CL	D14-1050	mn	2	3	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	We represent the alternative analyses output by a state-of-the-art CSL parser with tree structures , which we rerank with a classifier trained on two types of semantic tree kernels : one processing structures built with words , concepts and Brown clusters , and another one using semantic similarity among the words composing the structure .	The results on a corpus from the restaurant domain show that our semantic kernels exploiting similarity measures outperform state-of-the-art rerankers .	CL_D14-1050_mn_2	CL_D14-1050_mn_3	CL_D14-1050_mn_2_3
CL	D14-1050	mn	3	2	result	proposal_implementation	support	elaboration	secondary	secondary	none	none	The results on a corpus from the restaurant domain show that our semantic kernels exploiting similarity measures outperform state-of-the-art rerankers .	We represent the alternative analyses output by a state-of-the-art CSL parser with tree structures , which we rerank with a classifier trained on two types of semantic tree kernels : one processing structures built with words , concepts and Brown clusters , and another one using semantic similarity among the words composing the structure .	CL_D14-1050_mn_3	CL_D14-1050_mn_2	CL_D14-1050_mn_2_3
CL	D14-1050	pa	1	2	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	We present an empirical study on the use of semantic information for Concept Segmentation and Labeling ( CSL ) , which is an important step for semantic parsing .	We represent the alternative analyses output by a state-of-the-art CSL parser with tree structures , which we rerank with a classifier trained on two types of semantic tree kernels : one processing structures built with words , concepts and Brown clusters , and another one using semantic similarity among the words composing the structure .	CL_D14-1050_pa_1	CL_D14-1050_pa_2	CL_D14-1050_pa_1_2
CL	D14-1050	pa	1	3	proposal	result_means	none	support	main	secondary	none	none	We present an empirical study on the use of semantic information for Concept Segmentation and Labeling ( CSL ) , which is an important step for semantic parsing .	The results on a corpus from the restaurant domain show that our semantic kernels exploiting similarity measures outperform state-of-the-art rerankers .	CL_D14-1050_pa_1	CL_D14-1050_pa_3	CL_D14-1050_pa_1_3
CL	D14-1050	pa	2	3	proposal_implementation	result_means	elaboration	support	secondary	secondary	back	support	We represent the alternative analyses output by a state-of-the-art CSL parser with tree structures , which we rerank with a classifier trained on two types of semantic tree kernels : one processing structures built with words , concepts and Brown clusters , and another one using semantic similarity among the words composing the structure .	The results on a corpus from the restaurant domain show that our semantic kernels exploiting similarity measures outperform state-of-the-art rerankers .	CL_D14-1050_pa_2	CL_D14-1050_pa_3	CL_D14-1050_pa_2_3
CL	D14-1050	pa	3	2	result_means	proposal_implementation	support	elaboration	secondary	secondary	forw	support	The results on a corpus from the restaurant domain show that our semantic kernels exploiting similarity measures outperform state-of-the-art rerankers .	We represent the alternative analyses output by a state-of-the-art CSL parser with tree structures , which we rerank with a classifier trained on two types of semantic tree kernels : one processing structures built with words , concepts and Brown clusters , and another one using semantic similarity among the words composing the structure .	CL_D14-1050_pa_3	CL_D14-1050_pa_2	CL_D14-1050_pa_2_3
CL	D14-1051	ab	1	2	motivation_background	motivation_problem	support	elaboration	secondary	secondary	back	elaboration	Various studies highlighted that topic-based approaches give a powerful spoken content representation of documents .	Nonetheless , these documents may contain more than one main theme , and their automatic transcription inevitably contains errors .	CL_D14-1051_ab_1	CL_D14-1051_ab_2	CL_D14-1051_ab_1_2
CL	D14-1051	ab	1	3	motivation_background	proposal	support	none	secondary	main	forw	support	Various studies highlighted that topic-based approaches give a powerful spoken content representation of documents .	In this study , we propose an original and promising framework based on a compact representation of a textual document , to solve issues related to topic space granularity .	CL_D14-1051_ab_1	CL_D14-1051_ab_3	CL_D14-1051_ab_1_3
CL	D14-1051	ab	1	4	motivation_background	proposal_implementation	support	elaboration	secondary	secondary	none	none	Various studies highlighted that topic-based approaches give a powerful spoken content representation of documents .	Firstly , various topic spaces are estimated with different numbers of classes from a Latent Dirichlet Allocation .	CL_D14-1051_ab_1	CL_D14-1051_ab_4	CL_D14-1051_ab_1_4
CL	D14-1051	ab	1	5	motivation_background	proposal_implementation	support	sequence	secondary	secondary	none	none	Various studies highlighted that topic-based approaches give a powerful spoken content representation of documents .	Then , this multiple topic space representation is compacted into an elementary segment , called c-vector , originally developed in the context of speaker recognition .	CL_D14-1051_ab_1	CL_D14-1051_ab_5	CL_D14-1051_ab_1_5
CL	D14-1051	ab	1	6	motivation_background	means	support	by-means	secondary	secondary	none	none	Various studies highlighted that topic-based approaches give a powerful spoken content representation of documents .	Experiments are conducted on the DECODA corpus of conversations .	CL_D14-1051_ab_1	CL_D14-1051_ab_6	CL_D14-1051_ab_1_6
CL	D14-1051	ab	1	7	motivation_background	result	support	support	secondary	secondary	none	none	Various studies highlighted that topic-based approaches give a powerful spoken content representation of documents .	Results show the effectiveness of the proposed multi-view compact representation paradigm .	CL_D14-1051_ab_1	CL_D14-1051_ab_7	CL_D14-1051_ab_1_7
CL	D14-1051	ab	1	8	motivation_background	observation	support	support	secondary	secondary	none	none	Various studies highlighted that topic-based approaches give a powerful spoken content representation of documents .	Our identification system reaches an accuracy of 85 % , with a significant gain of 9 points compared to the baseline ( best single topic space configuration ) .	CL_D14-1051_ab_1	CL_D14-1051_ab_8	CL_D14-1051_ab_1_8
CL	D14-1051	ab	2	3	motivation_problem	proposal	elaboration	none	secondary	main	none	none	Nonetheless , these documents may contain more than one main theme , and their automatic transcription inevitably contains errors .	In this study , we propose an original and promising framework based on a compact representation of a textual document , to solve issues related to topic space granularity .	CL_D14-1051_ab_2	CL_D14-1051_ab_3	CL_D14-1051_ab_2_3
CL	D14-1051	ab	2	4	motivation_problem	proposal_implementation	elaboration	elaboration	secondary	secondary	none	none	Nonetheless , these documents may contain more than one main theme , and their automatic transcription inevitably contains errors .	Firstly , various topic spaces are estimated with different numbers of classes from a Latent Dirichlet Allocation .	CL_D14-1051_ab_2	CL_D14-1051_ab_4	CL_D14-1051_ab_2_4
CL	D14-1051	ab	2	5	motivation_problem	proposal_implementation	elaboration	sequence	secondary	secondary	none	none	Nonetheless , these documents may contain more than one main theme , and their automatic transcription inevitably contains errors .	Then , this multiple topic space representation is compacted into an elementary segment , called c-vector , originally developed in the context of speaker recognition .	CL_D14-1051_ab_2	CL_D14-1051_ab_5	CL_D14-1051_ab_2_5
CL	D14-1051	ab	2	6	motivation_problem	means	elaboration	by-means	secondary	secondary	none	none	Nonetheless , these documents may contain more than one main theme , and their automatic transcription inevitably contains errors .	Experiments are conducted on the DECODA corpus of conversations .	CL_D14-1051_ab_2	CL_D14-1051_ab_6	CL_D14-1051_ab_2_6
CL	D14-1051	ab	2	7	motivation_problem	result	elaboration	support	secondary	secondary	none	none	Nonetheless , these documents may contain more than one main theme , and their automatic transcription inevitably contains errors .	Results show the effectiveness of the proposed multi-view compact representation paradigm .	CL_D14-1051_ab_2	CL_D14-1051_ab_7	CL_D14-1051_ab_2_7
CL	D14-1051	ab	2	8	motivation_problem	observation	elaboration	support	secondary	secondary	none	none	Nonetheless , these documents may contain more than one main theme , and their automatic transcription inevitably contains errors .	Our identification system reaches an accuracy of 85 % , with a significant gain of 9 points compared to the baseline ( best single topic space configuration ) .	CL_D14-1051_ab_2	CL_D14-1051_ab_8	CL_D14-1051_ab_2_8
CL	D14-1051	ab	3	4	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	In this study , we propose an original and promising framework based on a compact representation of a textual document , to solve issues related to topic space granularity .	Firstly , various topic spaces are estimated with different numbers of classes from a Latent Dirichlet Allocation .	CL_D14-1051_ab_3	CL_D14-1051_ab_4	CL_D14-1051_ab_3_4
CL	D14-1051	ab	3	5	proposal	proposal_implementation	none	sequence	main	secondary	none	none	In this study , we propose an original and promising framework based on a compact representation of a textual document , to solve issues related to topic space granularity .	Then , this multiple topic space representation is compacted into an elementary segment , called c-vector , originally developed in the context of speaker recognition .	CL_D14-1051_ab_3	CL_D14-1051_ab_5	CL_D14-1051_ab_3_5
CL	D14-1051	ab	3	6	proposal	means	none	by-means	main	secondary	none	none	In this study , we propose an original and promising framework based on a compact representation of a textual document , to solve issues related to topic space granularity .	Experiments are conducted on the DECODA corpus of conversations .	CL_D14-1051_ab_3	CL_D14-1051_ab_6	CL_D14-1051_ab_3_6
CL	D14-1051	ab	3	7	proposal	result	none	support	main	secondary	back	support	In this study , we propose an original and promising framework based on a compact representation of a textual document , to solve issues related to topic space granularity .	Results show the effectiveness of the proposed multi-view compact representation paradigm .	CL_D14-1051_ab_3	CL_D14-1051_ab_7	CL_D14-1051_ab_3_7
CL	D14-1051	ab	3	8	proposal	observation	none	support	main	secondary	none	none	In this study , we propose an original and promising framework based on a compact representation of a textual document , to solve issues related to topic space granularity .	Our identification system reaches an accuracy of 85 % , with a significant gain of 9 points compared to the baseline ( best single topic space configuration ) .	CL_D14-1051_ab_3	CL_D14-1051_ab_8	CL_D14-1051_ab_3_8
CL	D14-1051	ab	4	5	proposal_implementation	proposal_implementation	elaboration	sequence	secondary	secondary	back	sequence	Firstly , various topic spaces are estimated with different numbers of classes from a Latent Dirichlet Allocation .	Then , this multiple topic space representation is compacted into an elementary segment , called c-vector , originally developed in the context of speaker recognition .	CL_D14-1051_ab_4	CL_D14-1051_ab_5	CL_D14-1051_ab_4_5
CL	D14-1051	ab	4	6	proposal_implementation	means	elaboration	by-means	secondary	secondary	none	none	Firstly , various topic spaces are estimated with different numbers of classes from a Latent Dirichlet Allocation .	Experiments are conducted on the DECODA corpus of conversations .	CL_D14-1051_ab_4	CL_D14-1051_ab_6	CL_D14-1051_ab_4_6
CL	D14-1051	ab	4	7	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	Firstly , various topic spaces are estimated with different numbers of classes from a Latent Dirichlet Allocation .	Results show the effectiveness of the proposed multi-view compact representation paradigm .	CL_D14-1051_ab_4	CL_D14-1051_ab_7	CL_D14-1051_ab_4_7
CL	D14-1051	ab	4	8	proposal_implementation	observation	elaboration	support	secondary	secondary	none	none	Firstly , various topic spaces are estimated with different numbers of classes from a Latent Dirichlet Allocation .	Our identification system reaches an accuracy of 85 % , with a significant gain of 9 points compared to the baseline ( best single topic space configuration ) .	CL_D14-1051_ab_4	CL_D14-1051_ab_8	CL_D14-1051_ab_4_8
CL	D14-1051	ab	5	6	proposal_implementation	means	sequence	by-means	secondary	secondary	none	none	Then , this multiple topic space representation is compacted into an elementary segment , called c-vector , originally developed in the context of speaker recognition .	Experiments are conducted on the DECODA corpus of conversations .	CL_D14-1051_ab_5	CL_D14-1051_ab_6	CL_D14-1051_ab_5_6
CL	D14-1051	ab	5	7	proposal_implementation	result	sequence	support	secondary	secondary	none	none	Then , this multiple topic space representation is compacted into an elementary segment , called c-vector , originally developed in the context of speaker recognition .	Results show the effectiveness of the proposed multi-view compact representation paradigm .	CL_D14-1051_ab_5	CL_D14-1051_ab_7	CL_D14-1051_ab_5_7
CL	D14-1051	ab	5	8	proposal_implementation	observation	sequence	support	secondary	secondary	none	none	Then , this multiple topic space representation is compacted into an elementary segment , called c-vector , originally developed in the context of speaker recognition .	Our identification system reaches an accuracy of 85 % , with a significant gain of 9 points compared to the baseline ( best single topic space configuration ) .	CL_D14-1051_ab_5	CL_D14-1051_ab_8	CL_D14-1051_ab_5_8
CL	D14-1051	ab	6	7	means	result	by-means	support	secondary	secondary	none	none	Experiments are conducted on the DECODA corpus of conversations .	Results show the effectiveness of the proposed multi-view compact representation paradigm .	CL_D14-1051_ab_6	CL_D14-1051_ab_7	CL_D14-1051_ab_6_7
CL	D14-1051	ab	6	8	means	observation	by-means	support	secondary	secondary	forw	by-means	Experiments are conducted on the DECODA corpus of conversations .	Our identification system reaches an accuracy of 85 % , with a significant gain of 9 points compared to the baseline ( best single topic space configuration ) .	CL_D14-1051_ab_6	CL_D14-1051_ab_8	CL_D14-1051_ab_6_8
CL	D14-1051	ab	7	8	result	observation	support	support	secondary	secondary	back	support	Results show the effectiveness of the proposed multi-view compact representation paradigm .	Our identification system reaches an accuracy of 85 % , with a significant gain of 9 points compared to the baseline ( best single topic space configuration ) .	CL_D14-1051_ab_7	CL_D14-1051_ab_8	CL_D14-1051_ab_7_8
CL	D14-1051	ab	8	7	observation	result	support	support	secondary	secondary	forw	support	Our identification system reaches an accuracy of 85 % , with a significant gain of 9 points compared to the baseline ( best single topic space configuration ) .	Results show the effectiveness of the proposed multi-view compact representation paradigm .	CL_D14-1051_ab_8	CL_D14-1051_ab_7	CL_D14-1051_ab_7_8
CL	D14-1051	mn	1	2	motivation_background	motivation_problem	info-required	support	secondary	secondary	forw	info-required	Various studies highlighted that topic-based approaches give a powerful spoken content representation of documents .	Nonetheless , these documents may contain more than one main theme , and their automatic transcription inevitably contains errors .	CL_D14-1051_mn_1	CL_D14-1051_mn_2	CL_D14-1051_mn_1_2
CL	D14-1051	mn	1	3	motivation_background	proposal	info-required	none	secondary	main	none	none	Various studies highlighted that topic-based approaches give a powerful spoken content representation of documents .	In this study , we propose an original and promising framework based on a compact representation of a textual document , to solve issues related to topic space granularity .	CL_D14-1051_mn_1	CL_D14-1051_mn_3	CL_D14-1051_mn_1_3
CL	D14-1051	mn	1	4	motivation_background	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	Various studies highlighted that topic-based approaches give a powerful spoken content representation of documents .	Firstly , various topic spaces are estimated with different numbers of classes from a Latent Dirichlet Allocation .	CL_D14-1051_mn_1	CL_D14-1051_mn_4	CL_D14-1051_mn_1_4
CL	D14-1051	mn	1	5	motivation_background	proposal_implementation	info-required	sequence	secondary	secondary	none	none	Various studies highlighted that topic-based approaches give a powerful spoken content representation of documents .	Then , this multiple topic space representation is compacted into an elementary segment , called c-vector , originally developed in the context of speaker recognition .	CL_D14-1051_mn_1	CL_D14-1051_mn_5	CL_D14-1051_mn_1_5
CL	D14-1051	mn	1	6	motivation_background	means	info-required	by-means	secondary	secondary	none	none	Various studies highlighted that topic-based approaches give a powerful spoken content representation of documents .	Experiments are conducted on the DECODA corpus of conversations .	CL_D14-1051_mn_1	CL_D14-1051_mn_6	CL_D14-1051_mn_1_6
CL	D14-1051	mn	1	7	motivation_background	result	info-required	support	secondary	secondary	none	none	Various studies highlighted that topic-based approaches give a powerful spoken content representation of documents .	Results show the effectiveness of the proposed multi-view compact representation paradigm .	CL_D14-1051_mn_1	CL_D14-1051_mn_7	CL_D14-1051_mn_1_7
CL	D14-1051	mn	1	8	motivation_background	observation	info-required	support	secondary	secondary	none	none	Various studies highlighted that topic-based approaches give a powerful spoken content representation of documents .	Our identification system reaches an accuracy of 85 % , with a significant gain of 9 points compared to the baseline ( best single topic space configuration ) .	CL_D14-1051_mn_1	CL_D14-1051_mn_8	CL_D14-1051_mn_1_8
CL	D14-1051	mn	2	3	motivation_problem	proposal	support	none	secondary	main	forw	support	Nonetheless , these documents may contain more than one main theme , and their automatic transcription inevitably contains errors .	In this study , we propose an original and promising framework based on a compact representation of a textual document , to solve issues related to topic space granularity .	CL_D14-1051_mn_2	CL_D14-1051_mn_3	CL_D14-1051_mn_2_3
CL	D14-1051	mn	2	4	motivation_problem	proposal_implementation	support	elaboration	secondary	secondary	none	none	Nonetheless , these documents may contain more than one main theme , and their automatic transcription inevitably contains errors .	Firstly , various topic spaces are estimated with different numbers of classes from a Latent Dirichlet Allocation .	CL_D14-1051_mn_2	CL_D14-1051_mn_4	CL_D14-1051_mn_2_4
CL	D14-1051	mn	2	5	motivation_problem	proposal_implementation	support	sequence	secondary	secondary	none	none	Nonetheless , these documents may contain more than one main theme , and their automatic transcription inevitably contains errors .	Then , this multiple topic space representation is compacted into an elementary segment , called c-vector , originally developed in the context of speaker recognition .	CL_D14-1051_mn_2	CL_D14-1051_mn_5	CL_D14-1051_mn_2_5
CL	D14-1051	mn	2	6	motivation_problem	means	support	by-means	secondary	secondary	none	none	Nonetheless , these documents may contain more than one main theme , and their automatic transcription inevitably contains errors .	Experiments are conducted on the DECODA corpus of conversations .	CL_D14-1051_mn_2	CL_D14-1051_mn_6	CL_D14-1051_mn_2_6
CL	D14-1051	mn	2	7	motivation_problem	result	support	support	secondary	secondary	none	none	Nonetheless , these documents may contain more than one main theme , and their automatic transcription inevitably contains errors .	Results show the effectiveness of the proposed multi-view compact representation paradigm .	CL_D14-1051_mn_2	CL_D14-1051_mn_7	CL_D14-1051_mn_2_7
CL	D14-1051	mn	2	8	motivation_problem	observation	support	support	secondary	secondary	none	none	Nonetheless , these documents may contain more than one main theme , and their automatic transcription inevitably contains errors .	Our identification system reaches an accuracy of 85 % , with a significant gain of 9 points compared to the baseline ( best single topic space configuration ) .	CL_D14-1051_mn_2	CL_D14-1051_mn_8	CL_D14-1051_mn_2_8
CL	D14-1051	mn	3	4	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	In this study , we propose an original and promising framework based on a compact representation of a textual document , to solve issues related to topic space granularity .	Firstly , various topic spaces are estimated with different numbers of classes from a Latent Dirichlet Allocation .	CL_D14-1051_mn_3	CL_D14-1051_mn_4	CL_D14-1051_mn_3_4
CL	D14-1051	mn	3	5	proposal	proposal_implementation	none	sequence	main	secondary	none	none	In this study , we propose an original and promising framework based on a compact representation of a textual document , to solve issues related to topic space granularity .	Then , this multiple topic space representation is compacted into an elementary segment , called c-vector , originally developed in the context of speaker recognition .	CL_D14-1051_mn_3	CL_D14-1051_mn_5	CL_D14-1051_mn_3_5
CL	D14-1051	mn	3	6	proposal	means	none	by-means	main	secondary	none	none	In this study , we propose an original and promising framework based on a compact representation of a textual document , to solve issues related to topic space granularity .	Experiments are conducted on the DECODA corpus of conversations .	CL_D14-1051_mn_3	CL_D14-1051_mn_6	CL_D14-1051_mn_3_6
CL	D14-1051	mn	3	7	proposal	result	none	support	main	secondary	back	support	In this study , we propose an original and promising framework based on a compact representation of a textual document , to solve issues related to topic space granularity .	Results show the effectiveness of the proposed multi-view compact representation paradigm .	CL_D14-1051_mn_3	CL_D14-1051_mn_7	CL_D14-1051_mn_3_7
CL	D14-1051	mn	3	8	proposal	observation	none	support	main	secondary	none	none	In this study , we propose an original and promising framework based on a compact representation of a textual document , to solve issues related to topic space granularity .	Our identification system reaches an accuracy of 85 % , with a significant gain of 9 points compared to the baseline ( best single topic space configuration ) .	CL_D14-1051_mn_3	CL_D14-1051_mn_8	CL_D14-1051_mn_3_8
CL	D14-1051	mn	4	5	proposal_implementation	proposal_implementation	elaboration	sequence	secondary	secondary	back	sequence	Firstly , various topic spaces are estimated with different numbers of classes from a Latent Dirichlet Allocation .	Then , this multiple topic space representation is compacted into an elementary segment , called c-vector , originally developed in the context of speaker recognition .	CL_D14-1051_mn_4	CL_D14-1051_mn_5	CL_D14-1051_mn_4_5
CL	D14-1051	mn	4	6	proposal_implementation	means	elaboration	by-means	secondary	secondary	none	none	Firstly , various topic spaces are estimated with different numbers of classes from a Latent Dirichlet Allocation .	Experiments are conducted on the DECODA corpus of conversations .	CL_D14-1051_mn_4	CL_D14-1051_mn_6	CL_D14-1051_mn_4_6
CL	D14-1051	mn	4	7	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	Firstly , various topic spaces are estimated with different numbers of classes from a Latent Dirichlet Allocation .	Results show the effectiveness of the proposed multi-view compact representation paradigm .	CL_D14-1051_mn_4	CL_D14-1051_mn_7	CL_D14-1051_mn_4_7
CL	D14-1051	mn	4	8	proposal_implementation	observation	elaboration	support	secondary	secondary	none	none	Firstly , various topic spaces are estimated with different numbers of classes from a Latent Dirichlet Allocation .	Our identification system reaches an accuracy of 85 % , with a significant gain of 9 points compared to the baseline ( best single topic space configuration ) .	CL_D14-1051_mn_4	CL_D14-1051_mn_8	CL_D14-1051_mn_4_8
CL	D14-1051	mn	5	6	proposal_implementation	means	sequence	by-means	secondary	secondary	none	none	Then , this multiple topic space representation is compacted into an elementary segment , called c-vector , originally developed in the context of speaker recognition .	Experiments are conducted on the DECODA corpus of conversations .	CL_D14-1051_mn_5	CL_D14-1051_mn_6	CL_D14-1051_mn_5_6
CL	D14-1051	mn	5	7	proposal_implementation	result	sequence	support	secondary	secondary	none	none	Then , this multiple topic space representation is compacted into an elementary segment , called c-vector , originally developed in the context of speaker recognition .	Results show the effectiveness of the proposed multi-view compact representation paradigm .	CL_D14-1051_mn_5	CL_D14-1051_mn_7	CL_D14-1051_mn_5_7
CL	D14-1051	mn	5	8	proposal_implementation	observation	sequence	support	secondary	secondary	none	none	Then , this multiple topic space representation is compacted into an elementary segment , called c-vector , originally developed in the context of speaker recognition .	Our identification system reaches an accuracy of 85 % , with a significant gain of 9 points compared to the baseline ( best single topic space configuration ) .	CL_D14-1051_mn_5	CL_D14-1051_mn_8	CL_D14-1051_mn_5_8
CL	D14-1051	mn	6	7	means	result	by-means	support	secondary	secondary	forw	by-means	Experiments are conducted on the DECODA corpus of conversations .	Results show the effectiveness of the proposed multi-view compact representation paradigm .	CL_D14-1051_mn_6	CL_D14-1051_mn_7	CL_D14-1051_mn_6_7
CL	D14-1051	mn	6	8	means	observation	by-means	support	secondary	secondary	none	none	Experiments are conducted on the DECODA corpus of conversations .	Our identification system reaches an accuracy of 85 % , with a significant gain of 9 points compared to the baseline ( best single topic space configuration ) .	CL_D14-1051_mn_6	CL_D14-1051_mn_8	CL_D14-1051_mn_6_8
CL	D14-1051	mn	7	8	result	observation	support	support	secondary	secondary	back	support	Results show the effectiveness of the proposed multi-view compact representation paradigm .	Our identification system reaches an accuracy of 85 % , with a significant gain of 9 points compared to the baseline ( best single topic space configuration ) .	CL_D14-1051_mn_7	CL_D14-1051_mn_8	CL_D14-1051_mn_7_8
CL	D14-1051	mn	8	7	observation	result	support	support	secondary	secondary	forw	support	Our identification system reaches an accuracy of 85 % , with a significant gain of 9 points compared to the baseline ( best single topic space configuration ) .	Results show the effectiveness of the proposed multi-view compact representation paradigm .	CL_D14-1051_mn_8	CL_D14-1051_mn_7	CL_D14-1051_mn_7_8
CL	D14-1051	pa	1	2	motivation_background	motivation_problem	info-required	support	secondary	secondary	forw	info-required	Various studies highlighted that topic-based approaches give a powerful spoken content representation of documents .	Nonetheless , these documents may contain more than one main theme , and their automatic transcription inevitably contains errors .	CL_D14-1051_pa_1	CL_D14-1051_pa_2	CL_D14-1051_pa_1_2
CL	D14-1051	pa	1	3	motivation_background	proposal	info-required	none	secondary	main	none	none	Various studies highlighted that topic-based approaches give a powerful spoken content representation of documents .	In this study , we propose an original and promising framework based on a compact representation of a textual document , to solve issues related to topic space granularity .	CL_D14-1051_pa_1	CL_D14-1051_pa_3	CL_D14-1051_pa_1_3
CL	D14-1051	pa	1	4	motivation_background	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	Various studies highlighted that topic-based approaches give a powerful spoken content representation of documents .	Firstly , various topic spaces are estimated with different numbers of classes from a Latent Dirichlet Allocation .	CL_D14-1051_pa_1	CL_D14-1051_pa_4	CL_D14-1051_pa_1_4
CL	D14-1051	pa	1	5	motivation_background	proposal_implementation	info-required	sequence	secondary	secondary	none	none	Various studies highlighted that topic-based approaches give a powerful spoken content representation of documents .	Then , this multiple topic space representation is compacted into an elementary segment , called c-vector , originally developed in the context of speaker recognition .	CL_D14-1051_pa_1	CL_D14-1051_pa_5	CL_D14-1051_pa_1_5
CL	D14-1051	pa	1	6	motivation_background	means	info-required	by-means	secondary	secondary	none	none	Various studies highlighted that topic-based approaches give a powerful spoken content representation of documents .	Experiments are conducted on the DECODA corpus of conversations .	CL_D14-1051_pa_1	CL_D14-1051_pa_6	CL_D14-1051_pa_1_6
CL	D14-1051	pa	1	7	motivation_background	result	info-required	support	secondary	secondary	none	none	Various studies highlighted that topic-based approaches give a powerful spoken content representation of documents .	Results show the effectiveness of the proposed multi-view compact representation paradigm .	CL_D14-1051_pa_1	CL_D14-1051_pa_7	CL_D14-1051_pa_1_7
CL	D14-1051	pa	1	8	motivation_background	observation	info-required	support	secondary	secondary	none	none	Various studies highlighted that topic-based approaches give a powerful spoken content representation of documents .	Our identification system reaches an accuracy of 85 % , with a significant gain of 9 points compared to the baseline ( best single topic space configuration ) .	CL_D14-1051_pa_1	CL_D14-1051_pa_8	CL_D14-1051_pa_1_8
CL	D14-1051	pa	2	3	motivation_problem	proposal	support	none	secondary	main	forw	support	Nonetheless , these documents may contain more than one main theme , and their automatic transcription inevitably contains errors .	In this study , we propose an original and promising framework based on a compact representation of a textual document , to solve issues related to topic space granularity .	CL_D14-1051_pa_2	CL_D14-1051_pa_3	CL_D14-1051_pa_2_3
CL	D14-1051	pa	2	4	motivation_problem	proposal_implementation	support	elaboration	secondary	secondary	none	none	Nonetheless , these documents may contain more than one main theme , and their automatic transcription inevitably contains errors .	Firstly , various topic spaces are estimated with different numbers of classes from a Latent Dirichlet Allocation .	CL_D14-1051_pa_2	CL_D14-1051_pa_4	CL_D14-1051_pa_2_4
CL	D14-1051	pa	2	5	motivation_problem	proposal_implementation	support	sequence	secondary	secondary	none	none	Nonetheless , these documents may contain more than one main theme , and their automatic transcription inevitably contains errors .	Then , this multiple topic space representation is compacted into an elementary segment , called c-vector , originally developed in the context of speaker recognition .	CL_D14-1051_pa_2	CL_D14-1051_pa_5	CL_D14-1051_pa_2_5
CL	D14-1051	pa	2	6	motivation_problem	means	support	by-means	secondary	secondary	none	none	Nonetheless , these documents may contain more than one main theme , and their automatic transcription inevitably contains errors .	Experiments are conducted on the DECODA corpus of conversations .	CL_D14-1051_pa_2	CL_D14-1051_pa_6	CL_D14-1051_pa_2_6
CL	D14-1051	pa	2	7	motivation_problem	result	support	support	secondary	secondary	none	none	Nonetheless , these documents may contain more than one main theme , and their automatic transcription inevitably contains errors .	Results show the effectiveness of the proposed multi-view compact representation paradigm .	CL_D14-1051_pa_2	CL_D14-1051_pa_7	CL_D14-1051_pa_2_7
CL	D14-1051	pa	2	8	motivation_problem	observation	support	support	secondary	secondary	none	none	Nonetheless , these documents may contain more than one main theme , and their automatic transcription inevitably contains errors .	Our identification system reaches an accuracy of 85 % , with a significant gain of 9 points compared to the baseline ( best single topic space configuration ) .	CL_D14-1051_pa_2	CL_D14-1051_pa_8	CL_D14-1051_pa_2_8
CL	D14-1051	pa	3	4	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	In this study , we propose an original and promising framework based on a compact representation of a textual document , to solve issues related to topic space granularity .	Firstly , various topic spaces are estimated with different numbers of classes from a Latent Dirichlet Allocation .	CL_D14-1051_pa_3	CL_D14-1051_pa_4	CL_D14-1051_pa_3_4
CL	D14-1051	pa	3	5	proposal	proposal_implementation	none	sequence	main	secondary	none	none	In this study , we propose an original and promising framework based on a compact representation of a textual document , to solve issues related to topic space granularity .	Then , this multiple topic space representation is compacted into an elementary segment , called c-vector , originally developed in the context of speaker recognition .	CL_D14-1051_pa_3	CL_D14-1051_pa_5	CL_D14-1051_pa_3_5
CL	D14-1051	pa	3	6	proposal	means	none	by-means	main	secondary	none	none	In this study , we propose an original and promising framework based on a compact representation of a textual document , to solve issues related to topic space granularity .	Experiments are conducted on the DECODA corpus of conversations .	CL_D14-1051_pa_3	CL_D14-1051_pa_6	CL_D14-1051_pa_3_6
CL	D14-1051	pa	3	7	proposal	result	none	support	main	secondary	back	support	In this study , we propose an original and promising framework based on a compact representation of a textual document , to solve issues related to topic space granularity .	Results show the effectiveness of the proposed multi-view compact representation paradigm .	CL_D14-1051_pa_3	CL_D14-1051_pa_7	CL_D14-1051_pa_3_7
CL	D14-1051	pa	3	8	proposal	observation	none	support	main	secondary	none	none	In this study , we propose an original and promising framework based on a compact representation of a textual document , to solve issues related to topic space granularity .	Our identification system reaches an accuracy of 85 % , with a significant gain of 9 points compared to the baseline ( best single topic space configuration ) .	CL_D14-1051_pa_3	CL_D14-1051_pa_8	CL_D14-1051_pa_3_8
CL	D14-1051	pa	4	5	proposal_implementation	proposal_implementation	elaboration	sequence	secondary	secondary	back	sequence	Firstly , various topic spaces are estimated with different numbers of classes from a Latent Dirichlet Allocation .	Then , this multiple topic space representation is compacted into an elementary segment , called c-vector , originally developed in the context of speaker recognition .	CL_D14-1051_pa_4	CL_D14-1051_pa_5	CL_D14-1051_pa_4_5
CL	D14-1051	pa	4	6	proposal_implementation	means	elaboration	by-means	secondary	secondary	none	none	Firstly , various topic spaces are estimated with different numbers of classes from a Latent Dirichlet Allocation .	Experiments are conducted on the DECODA corpus of conversations .	CL_D14-1051_pa_4	CL_D14-1051_pa_6	CL_D14-1051_pa_4_6
CL	D14-1051	pa	4	7	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	Firstly , various topic spaces are estimated with different numbers of classes from a Latent Dirichlet Allocation .	Results show the effectiveness of the proposed multi-view compact representation paradigm .	CL_D14-1051_pa_4	CL_D14-1051_pa_7	CL_D14-1051_pa_4_7
CL	D14-1051	pa	4	8	proposal_implementation	observation	elaboration	support	secondary	secondary	none	none	Firstly , various topic spaces are estimated with different numbers of classes from a Latent Dirichlet Allocation .	Our identification system reaches an accuracy of 85 % , with a significant gain of 9 points compared to the baseline ( best single topic space configuration ) .	CL_D14-1051_pa_4	CL_D14-1051_pa_8	CL_D14-1051_pa_4_8
CL	D14-1051	pa	5	6	proposal_implementation	means	sequence	by-means	secondary	secondary	none	none	Then , this multiple topic space representation is compacted into an elementary segment , called c-vector , originally developed in the context of speaker recognition .	Experiments are conducted on the DECODA corpus of conversations .	CL_D14-1051_pa_5	CL_D14-1051_pa_6	CL_D14-1051_pa_5_6
CL	D14-1051	pa	5	7	proposal_implementation	result	sequence	support	secondary	secondary	none	none	Then , this multiple topic space representation is compacted into an elementary segment , called c-vector , originally developed in the context of speaker recognition .	Results show the effectiveness of the proposed multi-view compact representation paradigm .	CL_D14-1051_pa_5	CL_D14-1051_pa_7	CL_D14-1051_pa_5_7
CL	D14-1051	pa	5	8	proposal_implementation	observation	sequence	support	secondary	secondary	none	none	Then , this multiple topic space representation is compacted into an elementary segment , called c-vector , originally developed in the context of speaker recognition .	Our identification system reaches an accuracy of 85 % , with a significant gain of 9 points compared to the baseline ( best single topic space configuration ) .	CL_D14-1051_pa_5	CL_D14-1051_pa_8	CL_D14-1051_pa_5_8
CL	D14-1051	pa	6	7	means	result	by-means	support	secondary	secondary	forw	by-means	Experiments are conducted on the DECODA corpus of conversations .	Results show the effectiveness of the proposed multi-view compact representation paradigm .	CL_D14-1051_pa_6	CL_D14-1051_pa_7	CL_D14-1051_pa_6_7
CL	D14-1051	pa	6	8	means	observation	by-means	support	secondary	secondary	none	none	Experiments are conducted on the DECODA corpus of conversations .	Our identification system reaches an accuracy of 85 % , with a significant gain of 9 points compared to the baseline ( best single topic space configuration ) .	CL_D14-1051_pa_6	CL_D14-1051_pa_8	CL_D14-1051_pa_6_8
CL	D14-1051	pa	7	8	result	observation	support	support	secondary	secondary	back	support	Results show the effectiveness of the proposed multi-view compact representation paradigm .	Our identification system reaches an accuracy of 85 % , with a significant gain of 9 points compared to the baseline ( best single topic space configuration ) .	CL_D14-1051_pa_7	CL_D14-1051_pa_8	CL_D14-1051_pa_7_8
CL	D14-1051	pa	8	7	observation	result	support	support	secondary	secondary	forw	support	Our identification system reaches an accuracy of 85 % , with a significant gain of 9 points compared to the baseline ( best single topic space configuration ) .	Results show the effectiveness of the proposed multi-view compact representation paradigm .	CL_D14-1051_pa_8	CL_D14-1051_pa_7	CL_D14-1051_pa_7_8
CL	D14-1052	ab	1	2	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	This paper introduces a model of multiple-instance learning applied to the prediction of aspect ratings or judgments of specific properties of an item from user-contributed texts such as product reviews .	Each variable-length text is represented by several independent feature vectors ; one word vector per sentence or paragraph .	CL_D14-1052_ab_1	CL_D14-1052_ab_2	CL_D14-1052_ab_1_2
CL	D14-1052	ab	1	3	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	This paper introduces a model of multiple-instance learning applied to the prediction of aspect ratings or judgments of specific properties of an item from user-contributed texts such as product reviews .	For learning from texts with known aspect ratings , the model performs multiple-instance regression ( MIR ) and assigns importance weights to each of the sentences or paragraphs of a text , uncovering their contribution to the aspect ratings .	CL_D14-1052_ab_1	CL_D14-1052_ab_3	CL_D14-1052_ab_1_3
CL	D14-1052	ab	1	4	proposal	proposal_implementation	none	sequence	main	secondary	none	none	This paper introduces a model of multiple-instance learning applied to the prediction of aspect ratings or judgments of specific properties of an item from user-contributed texts such as product reviews .	Next , the model is used to predict aspect ratings in previously unseen texts , demonstrating interpretability and explanatory power for its predictions .	CL_D14-1052_ab_1	CL_D14-1052_ab_4	CL_D14-1052_ab_1_4
CL	D14-1052	ab	1	5	proposal	result_means	none	support	main	secondary	back	support	This paper introduces a model of multiple-instance learning applied to the prediction of aspect ratings or judgments of specific properties of an item from user-contributed texts such as product reviews .	We evaluate the model on seven multi-aspect sentiment analysis data sets , improving over four MIR baselines and two strong bag-of-words linear models , namely SVR and Lasso , by more than 10 % relative in terms of MSE .	CL_D14-1052_ab_1	CL_D14-1052_ab_5	CL_D14-1052_ab_1_5
CL	D14-1052	ab	2	3	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	Each variable-length text is represented by several independent feature vectors ; one word vector per sentence or paragraph .	For learning from texts with known aspect ratings , the model performs multiple-instance regression ( MIR ) and assigns importance weights to each of the sentences or paragraphs of a text , uncovering their contribution to the aspect ratings .	CL_D14-1052_ab_2	CL_D14-1052_ab_3	CL_D14-1052_ab_2_3
CL	D14-1052	ab	2	4	proposal_implementation	proposal_implementation	elaboration	sequence	secondary	secondary	none	none	Each variable-length text is represented by several independent feature vectors ; one word vector per sentence or paragraph .	Next , the model is used to predict aspect ratings in previously unseen texts , demonstrating interpretability and explanatory power for its predictions .	CL_D14-1052_ab_2	CL_D14-1052_ab_4	CL_D14-1052_ab_2_4
CL	D14-1052	ab	2	5	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	Each variable-length text is represented by several independent feature vectors ; one word vector per sentence or paragraph .	We evaluate the model on seven multi-aspect sentiment analysis data sets , improving over four MIR baselines and two strong bag-of-words linear models , namely SVR and Lasso , by more than 10 % relative in terms of MSE .	CL_D14-1052_ab_2	CL_D14-1052_ab_5	CL_D14-1052_ab_2_5
CL	D14-1052	ab	3	4	proposal_implementation	proposal_implementation	elaboration	sequence	secondary	secondary	back	sequence	For learning from texts with known aspect ratings , the model performs multiple-instance regression ( MIR ) and assigns importance weights to each of the sentences or paragraphs of a text , uncovering their contribution to the aspect ratings .	Next , the model is used to predict aspect ratings in previously unseen texts , demonstrating interpretability and explanatory power for its predictions .	CL_D14-1052_ab_3	CL_D14-1052_ab_4	CL_D14-1052_ab_3_4
CL	D14-1052	ab	3	5	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	For learning from texts with known aspect ratings , the model performs multiple-instance regression ( MIR ) and assigns importance weights to each of the sentences or paragraphs of a text , uncovering their contribution to the aspect ratings .	We evaluate the model on seven multi-aspect sentiment analysis data sets , improving over four MIR baselines and two strong bag-of-words linear models , namely SVR and Lasso , by more than 10 % relative in terms of MSE .	CL_D14-1052_ab_3	CL_D14-1052_ab_5	CL_D14-1052_ab_3_5
CL	D14-1052	ab	4	5	proposal_implementation	result_means	sequence	support	secondary	secondary	none	none	Next , the model is used to predict aspect ratings in previously unseen texts , demonstrating interpretability and explanatory power for its predictions .	We evaluate the model on seven multi-aspect sentiment analysis data sets , improving over four MIR baselines and two strong bag-of-words linear models , namely SVR and Lasso , by more than 10 % relative in terms of MSE .	CL_D14-1052_ab_4	CL_D14-1052_ab_5	CL_D14-1052_ab_4_5
CL	D14-1052	ab	5	4	result_means	proposal_implementation	support	sequence	secondary	secondary	none	none	We evaluate the model on seven multi-aspect sentiment analysis data sets , improving over four MIR baselines and two strong bag-of-words linear models , namely SVR and Lasso , by more than 10 % relative in terms of MSE .	Next , the model is used to predict aspect ratings in previously unseen texts , demonstrating interpretability and explanatory power for its predictions .	CL_D14-1052_ab_5	CL_D14-1052_ab_4	CL_D14-1052_ab_4_5
CL	D14-1052	mn	1	2	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	This paper introduces a model of multiple-instance learning applied to the prediction of aspect ratings or judgments of specific properties of an item from user-contributed texts such as product reviews .	Each variable-length text is represented by several independent feature vectors ; one word vector per sentence or paragraph .	CL_D14-1052_mn_1	CL_D14-1052_mn_2	CL_D14-1052_mn_1_2
CL	D14-1052	mn	1	3	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	This paper introduces a model of multiple-instance learning applied to the prediction of aspect ratings or judgments of specific properties of an item from user-contributed texts such as product reviews .	For learning from texts with known aspect ratings , the model performs multiple-instance regression ( MIR ) and assigns importance weights to each of the sentences or paragraphs of a text , uncovering their contribution to the aspect ratings .	CL_D14-1052_mn_1	CL_D14-1052_mn_3	CL_D14-1052_mn_1_3
CL	D14-1052	mn	1	4	proposal	proposal_implementation	none	sequence	main	secondary	none	none	This paper introduces a model of multiple-instance learning applied to the prediction of aspect ratings or judgments of specific properties of an item from user-contributed texts such as product reviews .	Next , the model is used to predict aspect ratings in previously unseen texts , demonstrating interpretability and explanatory power for its predictions .	CL_D14-1052_mn_1	CL_D14-1052_mn_4	CL_D14-1052_mn_1_4
CL	D14-1052	mn	1	5	proposal	result_means	none	support	main	secondary	back	support	This paper introduces a model of multiple-instance learning applied to the prediction of aspect ratings or judgments of specific properties of an item from user-contributed texts such as product reviews .	We evaluate the model on seven multi-aspect sentiment analysis data sets , improving over four MIR baselines and two strong bag-of-words linear models , namely SVR and Lasso , by more than 10 % relative in terms of MSE .	CL_D14-1052_mn_1	CL_D14-1052_mn_5	CL_D14-1052_mn_1_5
CL	D14-1052	mn	2	3	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	Each variable-length text is represented by several independent feature vectors ; one word vector per sentence or paragraph .	For learning from texts with known aspect ratings , the model performs multiple-instance regression ( MIR ) and assigns importance weights to each of the sentences or paragraphs of a text , uncovering their contribution to the aspect ratings .	CL_D14-1052_mn_2	CL_D14-1052_mn_3	CL_D14-1052_mn_2_3
CL	D14-1052	mn	2	4	proposal_implementation	proposal_implementation	elaboration	sequence	secondary	secondary	none	none	Each variable-length text is represented by several independent feature vectors ; one word vector per sentence or paragraph .	Next , the model is used to predict aspect ratings in previously unseen texts , demonstrating interpretability and explanatory power for its predictions .	CL_D14-1052_mn_2	CL_D14-1052_mn_4	CL_D14-1052_mn_2_4
CL	D14-1052	mn	2	5	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	Each variable-length text is represented by several independent feature vectors ; one word vector per sentence or paragraph .	We evaluate the model on seven multi-aspect sentiment analysis data sets , improving over four MIR baselines and two strong bag-of-words linear models , namely SVR and Lasso , by more than 10 % relative in terms of MSE .	CL_D14-1052_mn_2	CL_D14-1052_mn_5	CL_D14-1052_mn_2_5
CL	D14-1052	mn	3	4	proposal_implementation	proposal_implementation	elaboration	sequence	secondary	secondary	back	sequence	For learning from texts with known aspect ratings , the model performs multiple-instance regression ( MIR ) and assigns importance weights to each of the sentences or paragraphs of a text , uncovering their contribution to the aspect ratings .	Next , the model is used to predict aspect ratings in previously unseen texts , demonstrating interpretability and explanatory power for its predictions .	CL_D14-1052_mn_3	CL_D14-1052_mn_4	CL_D14-1052_mn_3_4
CL	D14-1052	mn	3	5	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	For learning from texts with known aspect ratings , the model performs multiple-instance regression ( MIR ) and assigns importance weights to each of the sentences or paragraphs of a text , uncovering their contribution to the aspect ratings .	We evaluate the model on seven multi-aspect sentiment analysis data sets , improving over four MIR baselines and two strong bag-of-words linear models , namely SVR and Lasso , by more than 10 % relative in terms of MSE .	CL_D14-1052_mn_3	CL_D14-1052_mn_5	CL_D14-1052_mn_3_5
CL	D14-1052	mn	4	5	proposal_implementation	result_means	sequence	support	secondary	secondary	none	none	Next , the model is used to predict aspect ratings in previously unseen texts , demonstrating interpretability and explanatory power for its predictions .	We evaluate the model on seven multi-aspect sentiment analysis data sets , improving over four MIR baselines and two strong bag-of-words linear models , namely SVR and Lasso , by more than 10 % relative in terms of MSE .	CL_D14-1052_mn_4	CL_D14-1052_mn_5	CL_D14-1052_mn_4_5
CL	D14-1052	mn	5	4	result_means	proposal_implementation	support	sequence	secondary	secondary	none	none	We evaluate the model on seven multi-aspect sentiment analysis data sets , improving over four MIR baselines and two strong bag-of-words linear models , namely SVR and Lasso , by more than 10 % relative in terms of MSE .	Next , the model is used to predict aspect ratings in previously unseen texts , demonstrating interpretability and explanatory power for its predictions .	CL_D14-1052_mn_5	CL_D14-1052_mn_4	CL_D14-1052_mn_4_5
CL	D14-1052	pa	1	2	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	This paper introduces a model of multiple-instance learning applied to the prediction of aspect ratings or judgments of specific properties of an item from user-contributed texts such as product reviews .	Each variable-length text is represented by several independent feature vectors ; one word vector per sentence or paragraph .	CL_D14-1052_pa_1	CL_D14-1052_pa_2	CL_D14-1052_pa_1_2
CL	D14-1052	pa	1	3	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	This paper introduces a model of multiple-instance learning applied to the prediction of aspect ratings or judgments of specific properties of an item from user-contributed texts such as product reviews .	For learning from texts with known aspect ratings , the model performs multiple-instance regression ( MIR ) and assigns importance weights to each of the sentences or paragraphs of a text , uncovering their contribution to the aspect ratings .	CL_D14-1052_pa_1	CL_D14-1052_pa_3	CL_D14-1052_pa_1_3
CL	D14-1052	pa	1	4	proposal	result_means	none	support	main	secondary	back	support	This paper introduces a model of multiple-instance learning applied to the prediction of aspect ratings or judgments of specific properties of an item from user-contributed texts such as product reviews .	Next , the model is used to predict aspect ratings in previously unseen texts , demonstrating interpretability and explanatory power for its predictions .	CL_D14-1052_pa_1	CL_D14-1052_pa_4	CL_D14-1052_pa_1_4
CL	D14-1052	pa	1	5	proposal	result_means	none	elaboration	main	secondary	none	none	This paper introduces a model of multiple-instance learning applied to the prediction of aspect ratings or judgments of specific properties of an item from user-contributed texts such as product reviews .	We evaluate the model on seven multi-aspect sentiment analysis data sets , improving over four MIR baselines and two strong bag-of-words linear models , namely SVR and Lasso , by more than 10 % relative in terms of MSE .	CL_D14-1052_pa_1	CL_D14-1052_pa_5	CL_D14-1052_pa_1_5
CL	D14-1052	pa	2	3	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	forw	elaboration	Each variable-length text is represented by several independent feature vectors ; one word vector per sentence or paragraph .	For learning from texts with known aspect ratings , the model performs multiple-instance regression ( MIR ) and assigns importance weights to each of the sentences or paragraphs of a text , uncovering their contribution to the aspect ratings .	CL_D14-1052_pa_2	CL_D14-1052_pa_3	CL_D14-1052_pa_2_3
CL	D14-1052	pa	2	4	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	Each variable-length text is represented by several independent feature vectors ; one word vector per sentence or paragraph .	Next , the model is used to predict aspect ratings in previously unseen texts , demonstrating interpretability and explanatory power for its predictions .	CL_D14-1052_pa_2	CL_D14-1052_pa_4	CL_D14-1052_pa_2_4
CL	D14-1052	pa	2	5	proposal_implementation	result_means	elaboration	elaboration	secondary	secondary	none	none	Each variable-length text is represented by several independent feature vectors ; one word vector per sentence or paragraph .	We evaluate the model on seven multi-aspect sentiment analysis data sets , improving over four MIR baselines and two strong bag-of-words linear models , namely SVR and Lasso , by more than 10 % relative in terms of MSE .	CL_D14-1052_pa_2	CL_D14-1052_pa_5	CL_D14-1052_pa_2_5
CL	D14-1052	pa	3	4	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	For learning from texts with known aspect ratings , the model performs multiple-instance regression ( MIR ) and assigns importance weights to each of the sentences or paragraphs of a text , uncovering their contribution to the aspect ratings .	Next , the model is used to predict aspect ratings in previously unseen texts , demonstrating interpretability and explanatory power for its predictions .	CL_D14-1052_pa_3	CL_D14-1052_pa_4	CL_D14-1052_pa_3_4
CL	D14-1052	pa	3	5	proposal_implementation	result_means	elaboration	elaboration	secondary	secondary	none	none	For learning from texts with known aspect ratings , the model performs multiple-instance regression ( MIR ) and assigns importance weights to each of the sentences or paragraphs of a text , uncovering their contribution to the aspect ratings .	We evaluate the model on seven multi-aspect sentiment analysis data sets , improving over four MIR baselines and two strong bag-of-words linear models , namely SVR and Lasso , by more than 10 % relative in terms of MSE .	CL_D14-1052_pa_3	CL_D14-1052_pa_5	CL_D14-1052_pa_3_5
CL	D14-1052	pa	4	5	result_means	result_means	support	elaboration	secondary	secondary	back	elaboration	Next , the model is used to predict aspect ratings in previously unseen texts , demonstrating interpretability and explanatory power for its predictions .	We evaluate the model on seven multi-aspect sentiment analysis data sets , improving over four MIR baselines and two strong bag-of-words linear models , namely SVR and Lasso , by more than 10 % relative in terms of MSE .	CL_D14-1052_pa_4	CL_D14-1052_pa_5	CL_D14-1052_pa_4_5
CL	D14-1052	pa	5	4	result_means	result_means	elaboration	support	secondary	secondary	forw	elaboration	We evaluate the model on seven multi-aspect sentiment analysis data sets , improving over four MIR baselines and two strong bag-of-words linear models , namely SVR and Lasso , by more than 10 % relative in terms of MSE .	Next , the model is used to predict aspect ratings in previously unseen texts , demonstrating interpretability and explanatory power for its predictions .	CL_D14-1052_pa_5	CL_D14-1052_pa_4	CL_D14-1052_pa_4_5
CL	D14-1053	ab	1	2	proposal	proposal	elaboration	none	secondary	main	forw	elaboration	We propose a semi-supervised bootstrapping algorithm for analyzing China's foreign relations from the People's Daily .	Our approach addresses sentiment target clustering , subjective lexicons extraction and sentiment prediction in a unified framework .	CL_D14-1053_ab_1	CL_D14-1053_ab_2	CL_D14-1053_ab_1_2
CL	D14-1053	ab	1	3	proposal	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	We propose a semi-supervised bootstrapping algorithm for analyzing China's foreign relations from the People's Daily .	Different from existing algorithms in the literature , time information is considered in our algorithm through a hierarchical bayesian model to guide the bootstrapping approach .	CL_D14-1053_ab_1	CL_D14-1053_ab_3	CL_D14-1053_ab_1_3
CL	D14-1053	ab	1	4	proposal	conclusion	elaboration	support	secondary	secondary	none	none	We propose a semi-supervised bootstrapping algorithm for analyzing China's foreign relations from the People's Daily .	We are hopeful that our approach can facilitate quantitative political analysis conducted by social scientists and politicians .	CL_D14-1053_ab_1	CL_D14-1053_ab_4	CL_D14-1053_ab_1_4
CL	D14-1053	ab	2	3	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	Our approach addresses sentiment target clustering , subjective lexicons extraction and sentiment prediction in a unified framework .	Different from existing algorithms in the literature , time information is considered in our algorithm through a hierarchical bayesian model to guide the bootstrapping approach .	CL_D14-1053_ab_2	CL_D14-1053_ab_3	CL_D14-1053_ab_2_3
CL	D14-1053	ab	2	4	proposal	conclusion	none	support	main	secondary	back	support	Our approach addresses sentiment target clustering , subjective lexicons extraction and sentiment prediction in a unified framework .	We are hopeful that our approach can facilitate quantitative political analysis conducted by social scientists and politicians .	CL_D14-1053_ab_2	CL_D14-1053_ab_4	CL_D14-1053_ab_2_4
CL	D14-1053	ab	3	4	proposal_implementation	conclusion	elaboration	support	secondary	secondary	none	none	Different from existing algorithms in the literature , time information is considered in our algorithm through a hierarchical bayesian model to guide the bootstrapping approach .	We are hopeful that our approach can facilitate quantitative political analysis conducted by social scientists and politicians .	CL_D14-1053_ab_3	CL_D14-1053_ab_4	CL_D14-1053_ab_3_4
CL	D14-1053	ab	4	3	conclusion	proposal_implementation	support	elaboration	secondary	secondary	none	none	We are hopeful that our approach can facilitate quantitative political analysis conducted by social scientists and politicians .	Different from existing algorithms in the literature , time information is considered in our algorithm through a hierarchical bayesian model to guide the bootstrapping approach .	CL_D14-1053_ab_4	CL_D14-1053_ab_3	CL_D14-1053_ab_3_4
CL	D14-1053	mn	1	2	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	We propose a semi-supervised bootstrapping algorithm for analyzing China's foreign relations from the People's Daily .	Our approach addresses sentiment target clustering , subjective lexicons extraction and sentiment prediction in a unified framework .	CL_D14-1053_mn_1	CL_D14-1053_mn_2	CL_D14-1053_mn_1_2
CL	D14-1053	mn	1	3	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	We propose a semi-supervised bootstrapping algorithm for analyzing China's foreign relations from the People's Daily .	Different from existing algorithms in the literature , time information is considered in our algorithm through a hierarchical bayesian model to guide the bootstrapping approach .	CL_D14-1053_mn_1	CL_D14-1053_mn_3	CL_D14-1053_mn_1_3
CL	D14-1053	mn	1	4	proposal	conclusion	none	support	main	secondary	back	support	We propose a semi-supervised bootstrapping algorithm for analyzing China's foreign relations from the People's Daily .	We are hopeful that our approach can facilitate quantitative political analysis conducted by social scientists and politicians .	CL_D14-1053_mn_1	CL_D14-1053_mn_4	CL_D14-1053_mn_1_4
CL	D14-1053	mn	2	3	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	Our approach addresses sentiment target clustering , subjective lexicons extraction and sentiment prediction in a unified framework .	Different from existing algorithms in the literature , time information is considered in our algorithm through a hierarchical bayesian model to guide the bootstrapping approach .	CL_D14-1053_mn_2	CL_D14-1053_mn_3	CL_D14-1053_mn_2_3
CL	D14-1053	mn	2	4	proposal_implementation	conclusion	elaboration	support	secondary	secondary	none	none	Our approach addresses sentiment target clustering , subjective lexicons extraction and sentiment prediction in a unified framework .	We are hopeful that our approach can facilitate quantitative political analysis conducted by social scientists and politicians .	CL_D14-1053_mn_2	CL_D14-1053_mn_4	CL_D14-1053_mn_2_4
CL	D14-1053	mn	3	4	proposal_implementation	conclusion	elaboration	support	secondary	secondary	none	none	Different from existing algorithms in the literature , time information is considered in our algorithm through a hierarchical bayesian model to guide the bootstrapping approach .	We are hopeful that our approach can facilitate quantitative political analysis conducted by social scientists and politicians .	CL_D14-1053_mn_3	CL_D14-1053_mn_4	CL_D14-1053_mn_3_4
CL	D14-1053	mn	4	3	conclusion	proposal_implementation	support	elaboration	secondary	secondary	none	none	We are hopeful that our approach can facilitate quantitative political analysis conducted by social scientists and politicians .	Different from existing algorithms in the literature , time information is considered in our algorithm through a hierarchical bayesian model to guide the bootstrapping approach .	CL_D14-1053_mn_4	CL_D14-1053_mn_3	CL_D14-1053_mn_3_4
CL	D14-1053	pa	1	2	proposal	proposal	none	elaboration	main	secondary	back	elaboration	We propose a semi-supervised bootstrapping algorithm for analyzing China's foreign relations from the People's Daily .	Our approach addresses sentiment target clustering , subjective lexicons extraction and sentiment prediction in a unified framework .	CL_D14-1053_pa_1	CL_D14-1053_pa_2	CL_D14-1053_pa_1_2
CL	D14-1053	pa	1	3	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	We propose a semi-supervised bootstrapping algorithm for analyzing China's foreign relations from the People's Daily .	Different from existing algorithms in the literature , time information is considered in our algorithm through a hierarchical bayesian model to guide the bootstrapping approach .	CL_D14-1053_pa_1	CL_D14-1053_pa_3	CL_D14-1053_pa_1_3
CL	D14-1053	pa	1	4	proposal	conclusion	none	support	main	secondary	back	support	We propose a semi-supervised bootstrapping algorithm for analyzing China's foreign relations from the People's Daily .	We are hopeful that our approach can facilitate quantitative political analysis conducted by social scientists and politicians .	CL_D14-1053_pa_1	CL_D14-1053_pa_4	CL_D14-1053_pa_1_4
CL	D14-1053	pa	2	3	proposal	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	Our approach addresses sentiment target clustering , subjective lexicons extraction and sentiment prediction in a unified framework .	Different from existing algorithms in the literature , time information is considered in our algorithm through a hierarchical bayesian model to guide the bootstrapping approach .	CL_D14-1053_pa_2	CL_D14-1053_pa_3	CL_D14-1053_pa_2_3
CL	D14-1053	pa	2	4	proposal	conclusion	elaboration	support	secondary	secondary	none	none	Our approach addresses sentiment target clustering , subjective lexicons extraction and sentiment prediction in a unified framework .	We are hopeful that our approach can facilitate quantitative political analysis conducted by social scientists and politicians .	CL_D14-1053_pa_2	CL_D14-1053_pa_4	CL_D14-1053_pa_2_4
CL	D14-1053	pa	3	4	proposal_implementation	conclusion	elaboration	support	secondary	secondary	none	none	Different from existing algorithms in the literature , time information is considered in our algorithm through a hierarchical bayesian model to guide the bootstrapping approach .	We are hopeful that our approach can facilitate quantitative political analysis conducted by social scientists and politicians .	CL_D14-1053_pa_3	CL_D14-1053_pa_4	CL_D14-1053_pa_3_4
CL	D14-1053	pa	4	3	conclusion	proposal_implementation	support	elaboration	secondary	secondary	none	none	We are hopeful that our approach can facilitate quantitative political analysis conducted by social scientists and politicians .	Different from existing algorithms in the literature , time information is considered in our algorithm through a hierarchical bayesian model to guide the bootstrapping approach .	CL_D14-1053_pa_4	CL_D14-1053_pa_3	CL_D14-1053_pa_3_4
CL	D14-1054	ab	1	2	proposal	motivation_problem	none	support	main	secondary	back	support	In this paper , we propose a joint segmentation and classification framework for sentiment analysis .	Existing sentiment classification algorithms typically split a sentence as a word sequence , which does not effectively handle the inconsistent sentiment polarity between a phrase and the words it contains , such as " not bad"  and " a great deal of " .	CL_D14-1054_ab_1	CL_D14-1054_ab_2	CL_D14-1054_ab_1_2
CL	D14-1054	ab	1	3	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	In this paper , we propose a joint segmentation and classification framework for sentiment analysis .	We address this issue by developing a joint segmentation and classification framework ( JSC ) , which simultaneously conducts sentence segmentation and sentence-level sentiment classification .	CL_D14-1054_ab_1	CL_D14-1054_ab_3	CL_D14-1054_ab_1_3
CL	D14-1054	ab	1	4	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	In this paper , we propose a joint segmentation and classification framework for sentiment analysis .	Specifically , we use a log-linear model to score each segmentation candidate , and exploit the phrasal information of top-ranked segmentations as features to build the sentiment classifier .	CL_D14-1054_ab_1	CL_D14-1054_ab_4	CL_D14-1054_ab_1_4
CL	D14-1054	ab	1	5	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	In this paper , we propose a joint segmentation and classification framework for sentiment analysis .	A marginal log-likelihood objective function is devised for the segmentation model , which is optimized for enhancing the sentiment classification performance .	CL_D14-1054_ab_1	CL_D14-1054_ab_5	CL_D14-1054_ab_1_5
CL	D14-1054	ab	1	6	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	In this paper , we propose a joint segmentation and classification framework for sentiment analysis .	The joint model is trained only based on the annotated sentiment polarity of sentences , without any segmentation annotations .	CL_D14-1054_ab_1	CL_D14-1054_ab_6	CL_D14-1054_ab_1_6
CL	D14-1054	ab	1	7	proposal	result_means	none	support	main	secondary	back	support	In this paper , we propose a joint segmentation and classification framework for sentiment analysis .	Experiments on a benchmark Twitter sentiment classification dataset in SemEval 2013 show that , our joint model performs comparably with the state-of-the-art methods .	CL_D14-1054_ab_1	CL_D14-1054_ab_7	CL_D14-1054_ab_1_7
CL	D14-1054	ab	2	3	motivation_problem	proposal_implementation	support	elaboration	secondary	secondary	none	none	Existing sentiment classification algorithms typically split a sentence as a word sequence , which does not effectively handle the inconsistent sentiment polarity between a phrase and the words it contains , such as " not bad"  and " a great deal of " .	We address this issue by developing a joint segmentation and classification framework ( JSC ) , which simultaneously conducts sentence segmentation and sentence-level sentiment classification .	CL_D14-1054_ab_2	CL_D14-1054_ab_3	CL_D14-1054_ab_2_3
CL	D14-1054	ab	2	4	motivation_problem	proposal_implementation	support	elaboration	secondary	secondary	none	none	Existing sentiment classification algorithms typically split a sentence as a word sequence , which does not effectively handle the inconsistent sentiment polarity between a phrase and the words it contains , such as " not bad"  and " a great deal of " .	Specifically , we use a log-linear model to score each segmentation candidate , and exploit the phrasal information of top-ranked segmentations as features to build the sentiment classifier .	CL_D14-1054_ab_2	CL_D14-1054_ab_4	CL_D14-1054_ab_2_4
CL	D14-1054	ab	2	5	motivation_problem	proposal_implementation	support	elaboration	secondary	secondary	none	none	Existing sentiment classification algorithms typically split a sentence as a word sequence , which does not effectively handle the inconsistent sentiment polarity between a phrase and the words it contains , such as " not bad"  and " a great deal of " .	A marginal log-likelihood objective function is devised for the segmentation model , which is optimized for enhancing the sentiment classification performance .	CL_D14-1054_ab_2	CL_D14-1054_ab_5	CL_D14-1054_ab_2_5
CL	D14-1054	ab	2	6	motivation_problem	proposal_implementation	support	elaboration	secondary	secondary	none	none	Existing sentiment classification algorithms typically split a sentence as a word sequence , which does not effectively handle the inconsistent sentiment polarity between a phrase and the words it contains , such as " not bad"  and " a great deal of " .	The joint model is trained only based on the annotated sentiment polarity of sentences , without any segmentation annotations .	CL_D14-1054_ab_2	CL_D14-1054_ab_6	CL_D14-1054_ab_2_6
CL	D14-1054	ab	2	7	motivation_problem	result_means	support	support	secondary	secondary	none	none	Existing sentiment classification algorithms typically split a sentence as a word sequence , which does not effectively handle the inconsistent sentiment polarity between a phrase and the words it contains , such as " not bad"  and " a great deal of " .	Experiments on a benchmark Twitter sentiment classification dataset in SemEval 2013 show that , our joint model performs comparably with the state-of-the-art methods .	CL_D14-1054_ab_2	CL_D14-1054_ab_7	CL_D14-1054_ab_2_7
CL	D14-1054	ab	3	4	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	We address this issue by developing a joint segmentation and classification framework ( JSC ) , which simultaneously conducts sentence segmentation and sentence-level sentiment classification .	Specifically , we use a log-linear model to score each segmentation candidate , and exploit the phrasal information of top-ranked segmentations as features to build the sentiment classifier .	CL_D14-1054_ab_3	CL_D14-1054_ab_4	CL_D14-1054_ab_3_4
CL	D14-1054	ab	3	5	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	none	none	We address this issue by developing a joint segmentation and classification framework ( JSC ) , which simultaneously conducts sentence segmentation and sentence-level sentiment classification .	A marginal log-likelihood objective function is devised for the segmentation model , which is optimized for enhancing the sentiment classification performance .	CL_D14-1054_ab_3	CL_D14-1054_ab_5	CL_D14-1054_ab_3_5
CL	D14-1054	ab	3	6	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	none	none	We address this issue by developing a joint segmentation and classification framework ( JSC ) , which simultaneously conducts sentence segmentation and sentence-level sentiment classification .	The joint model is trained only based on the annotated sentiment polarity of sentences , without any segmentation annotations .	CL_D14-1054_ab_3	CL_D14-1054_ab_6	CL_D14-1054_ab_3_6
CL	D14-1054	ab	3	7	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	We address this issue by developing a joint segmentation and classification framework ( JSC ) , which simultaneously conducts sentence segmentation and sentence-level sentiment classification .	Experiments on a benchmark Twitter sentiment classification dataset in SemEval 2013 show that , our joint model performs comparably with the state-of-the-art methods .	CL_D14-1054_ab_3	CL_D14-1054_ab_7	CL_D14-1054_ab_3_7
CL	D14-1054	ab	4	5	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	Specifically , we use a log-linear model to score each segmentation candidate , and exploit the phrasal information of top-ranked segmentations as features to build the sentiment classifier .	A marginal log-likelihood objective function is devised for the segmentation model , which is optimized for enhancing the sentiment classification performance .	CL_D14-1054_ab_4	CL_D14-1054_ab_5	CL_D14-1054_ab_4_5
CL	D14-1054	ab	4	6	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	none	none	Specifically , we use a log-linear model to score each segmentation candidate , and exploit the phrasal information of top-ranked segmentations as features to build the sentiment classifier .	The joint model is trained only based on the annotated sentiment polarity of sentences , without any segmentation annotations .	CL_D14-1054_ab_4	CL_D14-1054_ab_6	CL_D14-1054_ab_4_6
CL	D14-1054	ab	4	7	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	Specifically , we use a log-linear model to score each segmentation candidate , and exploit the phrasal information of top-ranked segmentations as features to build the sentiment classifier .	Experiments on a benchmark Twitter sentiment classification dataset in SemEval 2013 show that , our joint model performs comparably with the state-of-the-art methods .	CL_D14-1054_ab_4	CL_D14-1054_ab_7	CL_D14-1054_ab_4_7
CL	D14-1054	ab	5	6	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	A marginal log-likelihood objective function is devised for the segmentation model , which is optimized for enhancing the sentiment classification performance .	The joint model is trained only based on the annotated sentiment polarity of sentences , without any segmentation annotations .	CL_D14-1054_ab_5	CL_D14-1054_ab_6	CL_D14-1054_ab_5_6
CL	D14-1054	ab	5	7	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	A marginal log-likelihood objective function is devised for the segmentation model , which is optimized for enhancing the sentiment classification performance .	Experiments on a benchmark Twitter sentiment classification dataset in SemEval 2013 show that , our joint model performs comparably with the state-of-the-art methods .	CL_D14-1054_ab_5	CL_D14-1054_ab_7	CL_D14-1054_ab_5_7
CL	D14-1054	ab	6	7	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	The joint model is trained only based on the annotated sentiment polarity of sentences , without any segmentation annotations .	Experiments on a benchmark Twitter sentiment classification dataset in SemEval 2013 show that , our joint model performs comparably with the state-of-the-art methods .	CL_D14-1054_ab_6	CL_D14-1054_ab_7	CL_D14-1054_ab_6_7
CL	D14-1054	ab	7	6	result_means	proposal_implementation	support	elaboration	secondary	secondary	none	none	Experiments on a benchmark Twitter sentiment classification dataset in SemEval 2013 show that , our joint model performs comparably with the state-of-the-art methods .	The joint model is trained only based on the annotated sentiment polarity of sentences , without any segmentation annotations .	CL_D14-1054_ab_7	CL_D14-1054_ab_6	CL_D14-1054_ab_6_7
CL	D14-1054	mn	1	2	proposal	motivation_problem	none	info-required	main	secondary	none	none	In this paper , we propose a joint segmentation and classification framework for sentiment analysis .	Existing sentiment classification algorithms typically split a sentence as a word sequence , which does not effectively handle the inconsistent sentiment polarity between a phrase and the words it contains , such as " not bad"  and " a great deal of " .	CL_D14-1054_mn_1	CL_D14-1054_mn_2	CL_D14-1054_mn_1_2
CL	D14-1054	mn	1	3	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	In this paper , we propose a joint segmentation and classification framework for sentiment analysis .	We address this issue by developing a joint segmentation and classification framework ( JSC ) , which simultaneously conducts sentence segmentation and sentence-level sentiment classification .	CL_D14-1054_mn_1	CL_D14-1054_mn_3	CL_D14-1054_mn_1_3
CL	D14-1054	mn	1	4	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	In this paper , we propose a joint segmentation and classification framework for sentiment analysis .	Specifically , we use a log-linear model to score each segmentation candidate , and exploit the phrasal information of top-ranked segmentations as features to build the sentiment classifier .	CL_D14-1054_mn_1	CL_D14-1054_mn_4	CL_D14-1054_mn_1_4
CL	D14-1054	mn	1	5	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	In this paper , we propose a joint segmentation and classification framework for sentiment analysis .	A marginal log-likelihood objective function is devised for the segmentation model , which is optimized for enhancing the sentiment classification performance .	CL_D14-1054_mn_1	CL_D14-1054_mn_5	CL_D14-1054_mn_1_5
CL	D14-1054	mn	1	6	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	In this paper , we propose a joint segmentation and classification framework for sentiment analysis .	The joint model is trained only based on the annotated sentiment polarity of sentences , without any segmentation annotations .	CL_D14-1054_mn_1	CL_D14-1054_mn_6	CL_D14-1054_mn_1_6
CL	D14-1054	mn	1	7	proposal	result_means	none	support	main	secondary	back	support	In this paper , we propose a joint segmentation and classification framework for sentiment analysis .	Experiments on a benchmark Twitter sentiment classification dataset in SemEval 2013 show that , our joint model performs comparably with the state-of-the-art methods .	CL_D14-1054_mn_1	CL_D14-1054_mn_7	CL_D14-1054_mn_1_7
CL	D14-1054	mn	2	3	motivation_problem	proposal_implementation	info-required	elaboration	secondary	secondary	forw	info-required	Existing sentiment classification algorithms typically split a sentence as a word sequence , which does not effectively handle the inconsistent sentiment polarity between a phrase and the words it contains , such as " not bad"  and " a great deal of " .	We address this issue by developing a joint segmentation and classification framework ( JSC ) , which simultaneously conducts sentence segmentation and sentence-level sentiment classification .	CL_D14-1054_mn_2	CL_D14-1054_mn_3	CL_D14-1054_mn_2_3
CL	D14-1054	mn	2	4	motivation_problem	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	Existing sentiment classification algorithms typically split a sentence as a word sequence , which does not effectively handle the inconsistent sentiment polarity between a phrase and the words it contains , such as " not bad"  and " a great deal of " .	Specifically , we use a log-linear model to score each segmentation candidate , and exploit the phrasal information of top-ranked segmentations as features to build the sentiment classifier .	CL_D14-1054_mn_2	CL_D14-1054_mn_4	CL_D14-1054_mn_2_4
CL	D14-1054	mn	2	5	motivation_problem	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	Existing sentiment classification algorithms typically split a sentence as a word sequence , which does not effectively handle the inconsistent sentiment polarity between a phrase and the words it contains , such as " not bad"  and " a great deal of " .	A marginal log-likelihood objective function is devised for the segmentation model , which is optimized for enhancing the sentiment classification performance .	CL_D14-1054_mn_2	CL_D14-1054_mn_5	CL_D14-1054_mn_2_5
CL	D14-1054	mn	2	6	motivation_problem	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	Existing sentiment classification algorithms typically split a sentence as a word sequence , which does not effectively handle the inconsistent sentiment polarity between a phrase and the words it contains , such as " not bad"  and " a great deal of " .	The joint model is trained only based on the annotated sentiment polarity of sentences , without any segmentation annotations .	CL_D14-1054_mn_2	CL_D14-1054_mn_6	CL_D14-1054_mn_2_6
CL	D14-1054	mn	2	7	motivation_problem	result_means	info-required	support	secondary	secondary	none	none	Existing sentiment classification algorithms typically split a sentence as a word sequence , which does not effectively handle the inconsistent sentiment polarity between a phrase and the words it contains , such as " not bad"  and " a great deal of " .	Experiments on a benchmark Twitter sentiment classification dataset in SemEval 2013 show that , our joint model performs comparably with the state-of-the-art methods .	CL_D14-1054_mn_2	CL_D14-1054_mn_7	CL_D14-1054_mn_2_7
CL	D14-1054	mn	3	4	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	We address this issue by developing a joint segmentation and classification framework ( JSC ) , which simultaneously conducts sentence segmentation and sentence-level sentiment classification .	Specifically , we use a log-linear model to score each segmentation candidate , and exploit the phrasal information of top-ranked segmentations as features to build the sentiment classifier .	CL_D14-1054_mn_3	CL_D14-1054_mn_4	CL_D14-1054_mn_3_4
CL	D14-1054	mn	3	5	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	none	none	We address this issue by developing a joint segmentation and classification framework ( JSC ) , which simultaneously conducts sentence segmentation and sentence-level sentiment classification .	A marginal log-likelihood objective function is devised for the segmentation model , which is optimized for enhancing the sentiment classification performance .	CL_D14-1054_mn_3	CL_D14-1054_mn_5	CL_D14-1054_mn_3_5
CL	D14-1054	mn	3	6	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	We address this issue by developing a joint segmentation and classification framework ( JSC ) , which simultaneously conducts sentence segmentation and sentence-level sentiment classification .	The joint model is trained only based on the annotated sentiment polarity of sentences , without any segmentation annotations .	CL_D14-1054_mn_3	CL_D14-1054_mn_6	CL_D14-1054_mn_3_6
CL	D14-1054	mn	3	7	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	We address this issue by developing a joint segmentation and classification framework ( JSC ) , which simultaneously conducts sentence segmentation and sentence-level sentiment classification .	Experiments on a benchmark Twitter sentiment classification dataset in SemEval 2013 show that , our joint model performs comparably with the state-of-the-art methods .	CL_D14-1054_mn_3	CL_D14-1054_mn_7	CL_D14-1054_mn_3_7
CL	D14-1054	mn	4	5	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	Specifically , we use a log-linear model to score each segmentation candidate , and exploit the phrasal information of top-ranked segmentations as features to build the sentiment classifier .	A marginal log-likelihood objective function is devised for the segmentation model , which is optimized for enhancing the sentiment classification performance .	CL_D14-1054_mn_4	CL_D14-1054_mn_5	CL_D14-1054_mn_4_5
CL	D14-1054	mn	4	6	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	none	none	Specifically , we use a log-linear model to score each segmentation candidate , and exploit the phrasal information of top-ranked segmentations as features to build the sentiment classifier .	The joint model is trained only based on the annotated sentiment polarity of sentences , without any segmentation annotations .	CL_D14-1054_mn_4	CL_D14-1054_mn_6	CL_D14-1054_mn_4_6
CL	D14-1054	mn	4	7	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	Specifically , we use a log-linear model to score each segmentation candidate , and exploit the phrasal information of top-ranked segmentations as features to build the sentiment classifier .	Experiments on a benchmark Twitter sentiment classification dataset in SemEval 2013 show that , our joint model performs comparably with the state-of-the-art methods .	CL_D14-1054_mn_4	CL_D14-1054_mn_7	CL_D14-1054_mn_4_7
CL	D14-1054	mn	5	6	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	none	none	A marginal log-likelihood objective function is devised for the segmentation model , which is optimized for enhancing the sentiment classification performance .	The joint model is trained only based on the annotated sentiment polarity of sentences , without any segmentation annotations .	CL_D14-1054_mn_5	CL_D14-1054_mn_6	CL_D14-1054_mn_5_6
CL	D14-1054	mn	5	7	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	A marginal log-likelihood objective function is devised for the segmentation model , which is optimized for enhancing the sentiment classification performance .	Experiments on a benchmark Twitter sentiment classification dataset in SemEval 2013 show that , our joint model performs comparably with the state-of-the-art methods .	CL_D14-1054_mn_5	CL_D14-1054_mn_7	CL_D14-1054_mn_5_7
CL	D14-1054	mn	6	7	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	The joint model is trained only based on the annotated sentiment polarity of sentences , without any segmentation annotations .	Experiments on a benchmark Twitter sentiment classification dataset in SemEval 2013 show that , our joint model performs comparably with the state-of-the-art methods .	CL_D14-1054_mn_6	CL_D14-1054_mn_7	CL_D14-1054_mn_6_7
CL	D14-1054	mn	7	6	result_means	proposal_implementation	support	elaboration	secondary	secondary	none	none	Experiments on a benchmark Twitter sentiment classification dataset in SemEval 2013 show that , our joint model performs comparably with the state-of-the-art methods .	The joint model is trained only based on the annotated sentiment polarity of sentences , without any segmentation annotations .	CL_D14-1054_mn_7	CL_D14-1054_mn_6	CL_D14-1054_mn_6_7
CL	D14-1054	pa	1	2	proposal	motivation_problem	none	support	main	secondary	none	none	In this paper , we propose a joint segmentation and classification framework for sentiment analysis .	Existing sentiment classification algorithms typically split a sentence as a word sequence , which does not effectively handle the inconsistent sentiment polarity between a phrase and the words it contains , such as " not bad"  and " a great deal of " .	CL_D14-1054_pa_1	CL_D14-1054_pa_2	CL_D14-1054_pa_1_2
CL	D14-1054	pa	1	3	proposal	proposal	none	elaboration	main	secondary	back	elaboration	In this paper , we propose a joint segmentation and classification framework for sentiment analysis .	We address this issue by developing a joint segmentation and classification framework ( JSC ) , which simultaneously conducts sentence segmentation and sentence-level sentiment classification .	CL_D14-1054_pa_1	CL_D14-1054_pa_3	CL_D14-1054_pa_1_3
CL	D14-1054	pa	1	4	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	In this paper , we propose a joint segmentation and classification framework for sentiment analysis .	Specifically , we use a log-linear model to score each segmentation candidate , and exploit the phrasal information of top-ranked segmentations as features to build the sentiment classifier .	CL_D14-1054_pa_1	CL_D14-1054_pa_4	CL_D14-1054_pa_1_4
CL	D14-1054	pa	1	5	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	In this paper , we propose a joint segmentation and classification framework for sentiment analysis .	A marginal log-likelihood objective function is devised for the segmentation model , which is optimized for enhancing the sentiment classification performance .	CL_D14-1054_pa_1	CL_D14-1054_pa_5	CL_D14-1054_pa_1_5
CL	D14-1054	pa	1	6	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	In this paper , we propose a joint segmentation and classification framework for sentiment analysis .	The joint model is trained only based on the annotated sentiment polarity of sentences , without any segmentation annotations .	CL_D14-1054_pa_1	CL_D14-1054_pa_6	CL_D14-1054_pa_1_6
CL	D14-1054	pa	1	7	proposal	result_means	none	support	main	secondary	back	support	In this paper , we propose a joint segmentation and classification framework for sentiment analysis .	Experiments on a benchmark Twitter sentiment classification dataset in SemEval 2013 show that , our joint model performs comparably with the state-of-the-art methods .	CL_D14-1054_pa_1	CL_D14-1054_pa_7	CL_D14-1054_pa_1_7
CL	D14-1054	pa	2	3	motivation_problem	proposal	support	elaboration	secondary	secondary	forw	support	Existing sentiment classification algorithms typically split a sentence as a word sequence , which does not effectively handle the inconsistent sentiment polarity between a phrase and the words it contains , such as " not bad"  and " a great deal of " .	We address this issue by developing a joint segmentation and classification framework ( JSC ) , which simultaneously conducts sentence segmentation and sentence-level sentiment classification .	CL_D14-1054_pa_2	CL_D14-1054_pa_3	CL_D14-1054_pa_2_3
CL	D14-1054	pa	2	4	motivation_problem	proposal_implementation	support	elaboration	secondary	secondary	none	none	Existing sentiment classification algorithms typically split a sentence as a word sequence , which does not effectively handle the inconsistent sentiment polarity between a phrase and the words it contains , such as " not bad"  and " a great deal of " .	Specifically , we use a log-linear model to score each segmentation candidate , and exploit the phrasal information of top-ranked segmentations as features to build the sentiment classifier .	CL_D14-1054_pa_2	CL_D14-1054_pa_4	CL_D14-1054_pa_2_4
CL	D14-1054	pa	2	5	motivation_problem	proposal_implementation	support	elaboration	secondary	secondary	none	none	Existing sentiment classification algorithms typically split a sentence as a word sequence , which does not effectively handle the inconsistent sentiment polarity between a phrase and the words it contains , such as " not bad"  and " a great deal of " .	A marginal log-likelihood objective function is devised for the segmentation model , which is optimized for enhancing the sentiment classification performance .	CL_D14-1054_pa_2	CL_D14-1054_pa_5	CL_D14-1054_pa_2_5
CL	D14-1054	pa	2	6	motivation_problem	proposal_implementation	support	elaboration	secondary	secondary	none	none	Existing sentiment classification algorithms typically split a sentence as a word sequence , which does not effectively handle the inconsistent sentiment polarity between a phrase and the words it contains , such as " not bad"  and " a great deal of " .	The joint model is trained only based on the annotated sentiment polarity of sentences , without any segmentation annotations .	CL_D14-1054_pa_2	CL_D14-1054_pa_6	CL_D14-1054_pa_2_6
CL	D14-1054	pa	2	7	motivation_problem	result_means	support	support	secondary	secondary	none	none	Existing sentiment classification algorithms typically split a sentence as a word sequence , which does not effectively handle the inconsistent sentiment polarity between a phrase and the words it contains , such as " not bad"  and " a great deal of " .	Experiments on a benchmark Twitter sentiment classification dataset in SemEval 2013 show that , our joint model performs comparably with the state-of-the-art methods .	CL_D14-1054_pa_2	CL_D14-1054_pa_7	CL_D14-1054_pa_2_7
CL	D14-1054	pa	3	4	proposal	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	We address this issue by developing a joint segmentation and classification framework ( JSC ) , which simultaneously conducts sentence segmentation and sentence-level sentiment classification .	Specifically , we use a log-linear model to score each segmentation candidate , and exploit the phrasal information of top-ranked segmentations as features to build the sentiment classifier .	CL_D14-1054_pa_3	CL_D14-1054_pa_4	CL_D14-1054_pa_3_4
CL	D14-1054	pa	3	5	proposal	proposal_implementation	elaboration	elaboration	secondary	secondary	none	none	We address this issue by developing a joint segmentation and classification framework ( JSC ) , which simultaneously conducts sentence segmentation and sentence-level sentiment classification .	A marginal log-likelihood objective function is devised for the segmentation model , which is optimized for enhancing the sentiment classification performance .	CL_D14-1054_pa_3	CL_D14-1054_pa_5	CL_D14-1054_pa_3_5
CL	D14-1054	pa	3	6	proposal	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	We address this issue by developing a joint segmentation and classification framework ( JSC ) , which simultaneously conducts sentence segmentation and sentence-level sentiment classification .	The joint model is trained only based on the annotated sentiment polarity of sentences , without any segmentation annotations .	CL_D14-1054_pa_3	CL_D14-1054_pa_6	CL_D14-1054_pa_3_6
CL	D14-1054	pa	3	7	proposal	result_means	elaboration	support	secondary	secondary	none	none	We address this issue by developing a joint segmentation and classification framework ( JSC ) , which simultaneously conducts sentence segmentation and sentence-level sentiment classification .	Experiments on a benchmark Twitter sentiment classification dataset in SemEval 2013 show that , our joint model performs comparably with the state-of-the-art methods .	CL_D14-1054_pa_3	CL_D14-1054_pa_7	CL_D14-1054_pa_3_7
CL	D14-1054	pa	4	5	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	Specifically , we use a log-linear model to score each segmentation candidate , and exploit the phrasal information of top-ranked segmentations as features to build the sentiment classifier .	A marginal log-likelihood objective function is devised for the segmentation model , which is optimized for enhancing the sentiment classification performance .	CL_D14-1054_pa_4	CL_D14-1054_pa_5	CL_D14-1054_pa_4_5
CL	D14-1054	pa	4	6	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	none	none	Specifically , we use a log-linear model to score each segmentation candidate , and exploit the phrasal information of top-ranked segmentations as features to build the sentiment classifier .	The joint model is trained only based on the annotated sentiment polarity of sentences , without any segmentation annotations .	CL_D14-1054_pa_4	CL_D14-1054_pa_6	CL_D14-1054_pa_4_6
CL	D14-1054	pa	4	7	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	Specifically , we use a log-linear model to score each segmentation candidate , and exploit the phrasal information of top-ranked segmentations as features to build the sentiment classifier .	Experiments on a benchmark Twitter sentiment classification dataset in SemEval 2013 show that , our joint model performs comparably with the state-of-the-art methods .	CL_D14-1054_pa_4	CL_D14-1054_pa_7	CL_D14-1054_pa_4_7
CL	D14-1054	pa	5	6	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	none	none	A marginal log-likelihood objective function is devised for the segmentation model , which is optimized for enhancing the sentiment classification performance .	The joint model is trained only based on the annotated sentiment polarity of sentences , without any segmentation annotations .	CL_D14-1054_pa_5	CL_D14-1054_pa_6	CL_D14-1054_pa_5_6
CL	D14-1054	pa	5	7	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	A marginal log-likelihood objective function is devised for the segmentation model , which is optimized for enhancing the sentiment classification performance .	Experiments on a benchmark Twitter sentiment classification dataset in SemEval 2013 show that , our joint model performs comparably with the state-of-the-art methods .	CL_D14-1054_pa_5	CL_D14-1054_pa_7	CL_D14-1054_pa_5_7
CL	D14-1054	pa	6	7	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	The joint model is trained only based on the annotated sentiment polarity of sentences , without any segmentation annotations .	Experiments on a benchmark Twitter sentiment classification dataset in SemEval 2013 show that , our joint model performs comparably with the state-of-the-art methods .	CL_D14-1054_pa_6	CL_D14-1054_pa_7	CL_D14-1054_pa_6_7
CL	D14-1054	pa	7	6	result_means	proposal_implementation	support	elaboration	secondary	secondary	none	none	Experiments on a benchmark Twitter sentiment classification dataset in SemEval 2013 show that , our joint model performs comparably with the state-of-the-art methods .	The joint model is trained only based on the annotated sentiment polarity of sentences , without any segmentation annotations .	CL_D14-1054_pa_7	CL_D14-1054_pa_6	CL_D14-1054_pa_6_7
CL	D14-1055	ab	1	2	motivation_background	motivation_problem	support	elaboration	secondary	secondary	back	elaboration	Deceptive reviews detection has attracted significant attention from both business and research communities .	However , due to the difficulty of human labeling needed for supervised learning , the problem remains to be highly challenging .	CL_D14-1055_ab_1	CL_D14-1055_ab_2	CL_D14-1055_ab_1_2
CL	D14-1055	ab	1	3	motivation_background	proposal	support	none	secondary	main	forw	support	Deceptive reviews detection has attracted significant attention from both business and research communities .	This paper proposed a novel angle to the problem by modeling PU ( positive unlabeled ) learning .	CL_D14-1055_ab_1	CL_D14-1055_ab_3	CL_D14-1055_ab_1_3
CL	D14-1055	ab	1	4	motivation_background	proposal	support	elaboration	secondary	main	none	none	Deceptive reviews detection has attracted significant attention from both business and research communities .	A semi-supervised model , called mixing population and individual property PU learning ( MPIPUL ) , is proposed .	CL_D14-1055_ab_1	CL_D14-1055_ab_4	CL_D14-1055_ab_1_4
CL	D14-1055	ab	1	5	motivation_background	proposal_implementation	support	elaboration	secondary	secondary	none	none	Deceptive reviews detection has attracted significant attention from both business and research communities .	Firstly , some reliable negative examples are identified from the unlabeled dataset .	CL_D14-1055_ab_1	CL_D14-1055_ab_5	CL_D14-1055_ab_1_5
CL	D14-1055	ab	1	6	motivation_background	proposal_implementation	support	sequence	secondary	secondary	none	none	Deceptive reviews detection has attracted significant attention from both business and research communities .	Secondly , some representative positive examples and negative examples are generated based on LDA ( Latent Dirichlet Allocation ) .	CL_D14-1055_ab_1	CL_D14-1055_ab_6	CL_D14-1055_ab_1_6
CL	D14-1055	ab	1	7	motivation_background	proposal_implementation	support	sequence	secondary	secondary	none	none	Deceptive reviews detection has attracted significant attention from both business and research communities .	Thirdly , for the remaining unlabeled examples ( we call them spy examples ) , which can not be explicitly identified as positive and negative , two similarity weights are assigned , by which the probability of a spy example belonging to the positive class and the negative class are displayed .	CL_D14-1055_ab_1	CL_D14-1055_ab_7	CL_D14-1055_ab_1_7
CL	D14-1055	ab	1	8	motivation_background	proposal_implementation	support	sequence	secondary	secondary	none	none	Deceptive reviews detection has attracted significant attention from both business and research communities .	Finally , spy examples and their similarity weights are incorporated into SVM ( Support Vector Machine ) to build an accurate classifier .	CL_D14-1055_ab_1	CL_D14-1055_ab_8	CL_D14-1055_ab_1_8
CL	D14-1055	ab	1	9	motivation_background	result_means	support	support	secondary	secondary	none	none	Deceptive reviews detection has attracted significant attention from both business and research communities .	Experiments on gold-standard dataset demonstrate the effectiveness of MPIPUL which outperforms the state-of-the-art baselines .	CL_D14-1055_ab_1	CL_D14-1055_ab_9	CL_D14-1055_ab_1_9
CL	D14-1055	ab	2	3	motivation_problem	proposal	elaboration	none	secondary	main	none	none	However , due to the difficulty of human labeling needed for supervised learning , the problem remains to be highly challenging .	This paper proposed a novel angle to the problem by modeling PU ( positive unlabeled ) learning .	CL_D14-1055_ab_2	CL_D14-1055_ab_3	CL_D14-1055_ab_2_3
CL	D14-1055	ab	2	4	motivation_problem	proposal	elaboration	elaboration	secondary	main	none	none	However , due to the difficulty of human labeling needed for supervised learning , the problem remains to be highly challenging .	A semi-supervised model , called mixing population and individual property PU learning ( MPIPUL ) , is proposed .	CL_D14-1055_ab_2	CL_D14-1055_ab_4	CL_D14-1055_ab_2_4
CL	D14-1055	ab	2	5	motivation_problem	proposal_implementation	elaboration	elaboration	secondary	secondary	none	none	However , due to the difficulty of human labeling needed for supervised learning , the problem remains to be highly challenging .	Firstly , some reliable negative examples are identified from the unlabeled dataset .	CL_D14-1055_ab_2	CL_D14-1055_ab_5	CL_D14-1055_ab_2_5
CL	D14-1055	ab	2	6	motivation_problem	proposal_implementation	elaboration	sequence	secondary	secondary	none	none	However , due to the difficulty of human labeling needed for supervised learning , the problem remains to be highly challenging .	Secondly , some representative positive examples and negative examples are generated based on LDA ( Latent Dirichlet Allocation ) .	CL_D14-1055_ab_2	CL_D14-1055_ab_6	CL_D14-1055_ab_2_6
CL	D14-1055	ab	2	7	motivation_problem	proposal_implementation	elaboration	sequence	secondary	secondary	none	none	However , due to the difficulty of human labeling needed for supervised learning , the problem remains to be highly challenging .	Thirdly , for the remaining unlabeled examples ( we call them spy examples ) , which can not be explicitly identified as positive and negative , two similarity weights are assigned , by which the probability of a spy example belonging to the positive class and the negative class are displayed .	CL_D14-1055_ab_2	CL_D14-1055_ab_7	CL_D14-1055_ab_2_7
CL	D14-1055	ab	2	8	motivation_problem	proposal_implementation	elaboration	sequence	secondary	secondary	none	none	However , due to the difficulty of human labeling needed for supervised learning , the problem remains to be highly challenging .	Finally , spy examples and their similarity weights are incorporated into SVM ( Support Vector Machine ) to build an accurate classifier .	CL_D14-1055_ab_2	CL_D14-1055_ab_8	CL_D14-1055_ab_2_8
CL	D14-1055	ab	2	9	motivation_problem	result_means	elaboration	support	secondary	secondary	none	none	However , due to the difficulty of human labeling needed for supervised learning , the problem remains to be highly challenging .	Experiments on gold-standard dataset demonstrate the effectiveness of MPIPUL which outperforms the state-of-the-art baselines .	CL_D14-1055_ab_2	CL_D14-1055_ab_9	CL_D14-1055_ab_2_9
CL	D14-1055	ab	3	4	proposal	proposal	none	elaboration	main	main	back	elaboration	This paper proposed a novel angle to the problem by modeling PU ( positive unlabeled ) learning .	A semi-supervised model , called mixing population and individual property PU learning ( MPIPUL ) , is proposed .	CL_D14-1055_ab_3	CL_D14-1055_ab_4	CL_D14-1055_ab_3_4
CL	D14-1055	ab	3	5	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	This paper proposed a novel angle to the problem by modeling PU ( positive unlabeled ) learning .	Firstly , some reliable negative examples are identified from the unlabeled dataset .	CL_D14-1055_ab_3	CL_D14-1055_ab_5	CL_D14-1055_ab_3_5
CL	D14-1055	ab	3	6	proposal	proposal_implementation	none	sequence	main	secondary	none	none	This paper proposed a novel angle to the problem by modeling PU ( positive unlabeled ) learning .	Secondly , some representative positive examples and negative examples are generated based on LDA ( Latent Dirichlet Allocation ) .	CL_D14-1055_ab_3	CL_D14-1055_ab_6	CL_D14-1055_ab_3_6
CL	D14-1055	ab	3	7	proposal	proposal_implementation	none	sequence	main	secondary	none	none	This paper proposed a novel angle to the problem by modeling PU ( positive unlabeled ) learning .	Thirdly , for the remaining unlabeled examples ( we call them spy examples ) , which can not be explicitly identified as positive and negative , two similarity weights are assigned , by which the probability of a spy example belonging to the positive class and the negative class are displayed .	CL_D14-1055_ab_3	CL_D14-1055_ab_7	CL_D14-1055_ab_3_7
CL	D14-1055	ab	3	8	proposal	proposal_implementation	none	sequence	main	secondary	none	none	This paper proposed a novel angle to the problem by modeling PU ( positive unlabeled ) learning .	Finally , spy examples and their similarity weights are incorporated into SVM ( Support Vector Machine ) to build an accurate classifier .	CL_D14-1055_ab_3	CL_D14-1055_ab_8	CL_D14-1055_ab_3_8
CL	D14-1055	ab	3	9	proposal	result_means	none	support	main	secondary	back	support	This paper proposed a novel angle to the problem by modeling PU ( positive unlabeled ) learning .	Experiments on gold-standard dataset demonstrate the effectiveness of MPIPUL which outperforms the state-of-the-art baselines .	CL_D14-1055_ab_3	CL_D14-1055_ab_9	CL_D14-1055_ab_3_9
CL	D14-1055	ab	4	5	proposal	proposal_implementation	elaboration	elaboration	main	secondary	back	elaboration	A semi-supervised model , called mixing population and individual property PU learning ( MPIPUL ) , is proposed .	Firstly , some reliable negative examples are identified from the unlabeled dataset .	CL_D14-1055_ab_4	CL_D14-1055_ab_5	CL_D14-1055_ab_4_5
CL	D14-1055	ab	4	6	proposal	proposal_implementation	elaboration	sequence	main	secondary	none	none	A semi-supervised model , called mixing population and individual property PU learning ( MPIPUL ) , is proposed .	Secondly , some representative positive examples and negative examples are generated based on LDA ( Latent Dirichlet Allocation ) .	CL_D14-1055_ab_4	CL_D14-1055_ab_6	CL_D14-1055_ab_4_6
CL	D14-1055	ab	4	7	proposal	proposal_implementation	elaboration	sequence	main	secondary	none	none	A semi-supervised model , called mixing population and individual property PU learning ( MPIPUL ) , is proposed .	Thirdly , for the remaining unlabeled examples ( we call them spy examples ) , which can not be explicitly identified as positive and negative , two similarity weights are assigned , by which the probability of a spy example belonging to the positive class and the negative class are displayed .	CL_D14-1055_ab_4	CL_D14-1055_ab_7	CL_D14-1055_ab_4_7
CL	D14-1055	ab	4	8	proposal	proposal_implementation	elaboration	sequence	main	secondary	none	none	A semi-supervised model , called mixing population and individual property PU learning ( MPIPUL ) , is proposed .	Finally , spy examples and their similarity weights are incorporated into SVM ( Support Vector Machine ) to build an accurate classifier .	CL_D14-1055_ab_4	CL_D14-1055_ab_8	CL_D14-1055_ab_4_8
CL	D14-1055	ab	4	9	proposal	result_means	elaboration	support	main	secondary	none	none	A semi-supervised model , called mixing population and individual property PU learning ( MPIPUL ) , is proposed .	Experiments on gold-standard dataset demonstrate the effectiveness of MPIPUL which outperforms the state-of-the-art baselines .	CL_D14-1055_ab_4	CL_D14-1055_ab_9	CL_D14-1055_ab_4_9
CL	D14-1055	ab	5	6	proposal_implementation	proposal_implementation	elaboration	sequence	secondary	secondary	back	sequence	Firstly , some reliable negative examples are identified from the unlabeled dataset .	Secondly , some representative positive examples and negative examples are generated based on LDA ( Latent Dirichlet Allocation ) .	CL_D14-1055_ab_5	CL_D14-1055_ab_6	CL_D14-1055_ab_5_6
CL	D14-1055	ab	5	7	proposal_implementation	proposal_implementation	elaboration	sequence	secondary	secondary	none	none	Firstly , some reliable negative examples are identified from the unlabeled dataset .	Thirdly , for the remaining unlabeled examples ( we call them spy examples ) , which can not be explicitly identified as positive and negative , two similarity weights are assigned , by which the probability of a spy example belonging to the positive class and the negative class are displayed .	CL_D14-1055_ab_5	CL_D14-1055_ab_7	CL_D14-1055_ab_5_7
CL	D14-1055	ab	5	8	proposal_implementation	proposal_implementation	elaboration	sequence	secondary	secondary	none	none	Firstly , some reliable negative examples are identified from the unlabeled dataset .	Finally , spy examples and their similarity weights are incorporated into SVM ( Support Vector Machine ) to build an accurate classifier .	CL_D14-1055_ab_5	CL_D14-1055_ab_8	CL_D14-1055_ab_5_8
CL	D14-1055	ab	5	9	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	Firstly , some reliable negative examples are identified from the unlabeled dataset .	Experiments on gold-standard dataset demonstrate the effectiveness of MPIPUL which outperforms the state-of-the-art baselines .	CL_D14-1055_ab_5	CL_D14-1055_ab_9	CL_D14-1055_ab_5_9
CL	D14-1055	ab	6	7	proposal_implementation	proposal_implementation	sequence	sequence	secondary	secondary	back	sequence	Secondly , some representative positive examples and negative examples are generated based on LDA ( Latent Dirichlet Allocation ) .	Thirdly , for the remaining unlabeled examples ( we call them spy examples ) , which can not be explicitly identified as positive and negative , two similarity weights are assigned , by which the probability of a spy example belonging to the positive class and the negative class are displayed .	CL_D14-1055_ab_6	CL_D14-1055_ab_7	CL_D14-1055_ab_6_7
CL	D14-1055	ab	6	8	proposal_implementation	proposal_implementation	sequence	sequence	secondary	secondary	none	none	Secondly , some representative positive examples and negative examples are generated based on LDA ( Latent Dirichlet Allocation ) .	Finally , spy examples and their similarity weights are incorporated into SVM ( Support Vector Machine ) to build an accurate classifier .	CL_D14-1055_ab_6	CL_D14-1055_ab_8	CL_D14-1055_ab_6_8
CL	D14-1055	ab	6	9	proposal_implementation	result_means	sequence	support	secondary	secondary	none	none	Secondly , some representative positive examples and negative examples are generated based on LDA ( Latent Dirichlet Allocation ) .	Experiments on gold-standard dataset demonstrate the effectiveness of MPIPUL which outperforms the state-of-the-art baselines .	CL_D14-1055_ab_6	CL_D14-1055_ab_9	CL_D14-1055_ab_6_9
CL	D14-1055	ab	7	8	proposal_implementation	proposal_implementation	sequence	sequence	secondary	secondary	back	sequence	Thirdly , for the remaining unlabeled examples ( we call them spy examples ) , which can not be explicitly identified as positive and negative , two similarity weights are assigned , by which the probability of a spy example belonging to the positive class and the negative class are displayed .	Finally , spy examples and their similarity weights are incorporated into SVM ( Support Vector Machine ) to build an accurate classifier .	CL_D14-1055_ab_7	CL_D14-1055_ab_8	CL_D14-1055_ab_7_8
CL	D14-1055	ab	7	9	proposal_implementation	result_means	sequence	support	secondary	secondary	none	none	Thirdly , for the remaining unlabeled examples ( we call them spy examples ) , which can not be explicitly identified as positive and negative , two similarity weights are assigned , by which the probability of a spy example belonging to the positive class and the negative class are displayed .	Experiments on gold-standard dataset demonstrate the effectiveness of MPIPUL which outperforms the state-of-the-art baselines .	CL_D14-1055_ab_7	CL_D14-1055_ab_9	CL_D14-1055_ab_7_9
CL	D14-1055	ab	8	9	proposal_implementation	result_means	sequence	support	secondary	secondary	none	none	Finally , spy examples and their similarity weights are incorporated into SVM ( Support Vector Machine ) to build an accurate classifier .	Experiments on gold-standard dataset demonstrate the effectiveness of MPIPUL which outperforms the state-of-the-art baselines .	CL_D14-1055_ab_8	CL_D14-1055_ab_9	CL_D14-1055_ab_8_9
CL	D14-1055	ab	9	8	result_means	proposal_implementation	support	sequence	secondary	secondary	none	none	Experiments on gold-standard dataset demonstrate the effectiveness of MPIPUL which outperforms the state-of-the-art baselines .	Finally , spy examples and their similarity weights are incorporated into SVM ( Support Vector Machine ) to build an accurate classifier .	CL_D14-1055_ab_9	CL_D14-1055_ab_8	CL_D14-1055_ab_8_9
CL	D14-1055	mn	1	2	motivation_background	motivation_problem	info-required	support	secondary	secondary	forw	info-required	Deceptive reviews detection has attracted significant attention from both business and research communities .	However , due to the difficulty of human labeling needed for supervised learning , the problem remains to be highly challenging .	CL_D14-1055_mn_1	CL_D14-1055_mn_2	CL_D14-1055_mn_1_2
CL	D14-1055	mn	1	3	motivation_background	proposal	info-required	none	secondary	main	none	none	Deceptive reviews detection has attracted significant attention from both business and research communities .	This paper proposed a novel angle to the problem by modeling PU ( positive unlabeled ) learning .	CL_D14-1055_mn_1	CL_D14-1055_mn_3	CL_D14-1055_mn_1_3
CL	D14-1055	mn	1	4	motivation_background	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	Deceptive reviews detection has attracted significant attention from both business and research communities .	A semi-supervised model , called mixing population and individual property PU learning ( MPIPUL ) , is proposed .	CL_D14-1055_mn_1	CL_D14-1055_mn_4	CL_D14-1055_mn_1_4
CL	D14-1055	mn	1	5	motivation_background	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	Deceptive reviews detection has attracted significant attention from both business and research communities .	Firstly , some reliable negative examples are identified from the unlabeled dataset .	CL_D14-1055_mn_1	CL_D14-1055_mn_5	CL_D14-1055_mn_1_5
CL	D14-1055	mn	1	6	motivation_background	proposal_implementation	info-required	sequence	secondary	secondary	none	none	Deceptive reviews detection has attracted significant attention from both business and research communities .	Secondly , some representative positive examples and negative examples are generated based on LDA ( Latent Dirichlet Allocation ) .	CL_D14-1055_mn_1	CL_D14-1055_mn_6	CL_D14-1055_mn_1_6
CL	D14-1055	mn	1	7	motivation_background	proposal_implementation	info-required	sequence	secondary	secondary	none	none	Deceptive reviews detection has attracted significant attention from both business and research communities .	Thirdly , for the remaining unlabeled examples ( we call them spy examples ) , which can not be explicitly identified as positive and negative , two similarity weights are assigned , by which the probability of a spy example belonging to the positive class and the negative class are displayed .	CL_D14-1055_mn_1	CL_D14-1055_mn_7	CL_D14-1055_mn_1_7
CL	D14-1055	mn	1	8	motivation_background	proposal_implementation	info-required	sequence	secondary	secondary	none	none	Deceptive reviews detection has attracted significant attention from both business and research communities .	Finally , spy examples and their similarity weights are incorporated into SVM ( Support Vector Machine ) to build an accurate classifier .	CL_D14-1055_mn_1	CL_D14-1055_mn_8	CL_D14-1055_mn_1_8
CL	D14-1055	mn	1	9	motivation_background	result_means	info-required	support	secondary	secondary	none	none	Deceptive reviews detection has attracted significant attention from both business and research communities .	Experiments on gold-standard dataset demonstrate the effectiveness of MPIPUL which outperforms the state-of-the-art baselines .	CL_D14-1055_mn_1	CL_D14-1055_mn_9	CL_D14-1055_mn_1_9
CL	D14-1055	mn	2	3	motivation_problem	proposal	support	none	secondary	main	forw	support	However , due to the difficulty of human labeling needed for supervised learning , the problem remains to be highly challenging .	This paper proposed a novel angle to the problem by modeling PU ( positive unlabeled ) learning .	CL_D14-1055_mn_2	CL_D14-1055_mn_3	CL_D14-1055_mn_2_3
CL	D14-1055	mn	2	4	motivation_problem	proposal_implementation	support	elaboration	secondary	secondary	none	none	However , due to the difficulty of human labeling needed for supervised learning , the problem remains to be highly challenging .	A semi-supervised model , called mixing population and individual property PU learning ( MPIPUL ) , is proposed .	CL_D14-1055_mn_2	CL_D14-1055_mn_4	CL_D14-1055_mn_2_4
CL	D14-1055	mn	2	5	motivation_problem	proposal_implementation	support	elaboration	secondary	secondary	none	none	However , due to the difficulty of human labeling needed for supervised learning , the problem remains to be highly challenging .	Firstly , some reliable negative examples are identified from the unlabeled dataset .	CL_D14-1055_mn_2	CL_D14-1055_mn_5	CL_D14-1055_mn_2_5
CL	D14-1055	mn	2	6	motivation_problem	proposal_implementation	support	sequence	secondary	secondary	none	none	However , due to the difficulty of human labeling needed for supervised learning , the problem remains to be highly challenging .	Secondly , some representative positive examples and negative examples are generated based on LDA ( Latent Dirichlet Allocation ) .	CL_D14-1055_mn_2	CL_D14-1055_mn_6	CL_D14-1055_mn_2_6
CL	D14-1055	mn	2	7	motivation_problem	proposal_implementation	support	sequence	secondary	secondary	none	none	However , due to the difficulty of human labeling needed for supervised learning , the problem remains to be highly challenging .	Thirdly , for the remaining unlabeled examples ( we call them spy examples ) , which can not be explicitly identified as positive and negative , two similarity weights are assigned , by which the probability of a spy example belonging to the positive class and the negative class are displayed .	CL_D14-1055_mn_2	CL_D14-1055_mn_7	CL_D14-1055_mn_2_7
CL	D14-1055	mn	2	8	motivation_problem	proposal_implementation	support	sequence	secondary	secondary	none	none	However , due to the difficulty of human labeling needed for supervised learning , the problem remains to be highly challenging .	Finally , spy examples and their similarity weights are incorporated into SVM ( Support Vector Machine ) to build an accurate classifier .	CL_D14-1055_mn_2	CL_D14-1055_mn_8	CL_D14-1055_mn_2_8
CL	D14-1055	mn	2	9	motivation_problem	result_means	support	support	secondary	secondary	none	none	However , due to the difficulty of human labeling needed for supervised learning , the problem remains to be highly challenging .	Experiments on gold-standard dataset demonstrate the effectiveness of MPIPUL which outperforms the state-of-the-art baselines .	CL_D14-1055_mn_2	CL_D14-1055_mn_9	CL_D14-1055_mn_2_9
CL	D14-1055	mn	3	4	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	This paper proposed a novel angle to the problem by modeling PU ( positive unlabeled ) learning .	A semi-supervised model , called mixing population and individual property PU learning ( MPIPUL ) , is proposed .	CL_D14-1055_mn_3	CL_D14-1055_mn_4	CL_D14-1055_mn_3_4
CL	D14-1055	mn	3	5	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	This paper proposed a novel angle to the problem by modeling PU ( positive unlabeled ) learning .	Firstly , some reliable negative examples are identified from the unlabeled dataset .	CL_D14-1055_mn_3	CL_D14-1055_mn_5	CL_D14-1055_mn_3_5
CL	D14-1055	mn	3	6	proposal	proposal_implementation	none	sequence	main	secondary	none	none	This paper proposed a novel angle to the problem by modeling PU ( positive unlabeled ) learning .	Secondly , some representative positive examples and negative examples are generated based on LDA ( Latent Dirichlet Allocation ) .	CL_D14-1055_mn_3	CL_D14-1055_mn_6	CL_D14-1055_mn_3_6
CL	D14-1055	mn	3	7	proposal	proposal_implementation	none	sequence	main	secondary	none	none	This paper proposed a novel angle to the problem by modeling PU ( positive unlabeled ) learning .	Thirdly , for the remaining unlabeled examples ( we call them spy examples ) , which can not be explicitly identified as positive and negative , two similarity weights are assigned , by which the probability of a spy example belonging to the positive class and the negative class are displayed .	CL_D14-1055_mn_3	CL_D14-1055_mn_7	CL_D14-1055_mn_3_7
CL	D14-1055	mn	3	8	proposal	proposal_implementation	none	sequence	main	secondary	none	none	This paper proposed a novel angle to the problem by modeling PU ( positive unlabeled ) learning .	Finally , spy examples and their similarity weights are incorporated into SVM ( Support Vector Machine ) to build an accurate classifier .	CL_D14-1055_mn_3	CL_D14-1055_mn_8	CL_D14-1055_mn_3_8
CL	D14-1055	mn	3	9	proposal	result_means	none	support	main	secondary	back	support	This paper proposed a novel angle to the problem by modeling PU ( positive unlabeled ) learning .	Experiments on gold-standard dataset demonstrate the effectiveness of MPIPUL which outperforms the state-of-the-art baselines .	CL_D14-1055_mn_3	CL_D14-1055_mn_9	CL_D14-1055_mn_3_9
CL	D14-1055	mn	4	5	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	A semi-supervised model , called mixing population and individual property PU learning ( MPIPUL ) , is proposed .	Firstly , some reliable negative examples are identified from the unlabeled dataset .	CL_D14-1055_mn_4	CL_D14-1055_mn_5	CL_D14-1055_mn_4_5
CL	D14-1055	mn	4	6	proposal_implementation	proposal_implementation	elaboration	sequence	secondary	secondary	none	none	A semi-supervised model , called mixing population and individual property PU learning ( MPIPUL ) , is proposed .	Secondly , some representative positive examples and negative examples are generated based on LDA ( Latent Dirichlet Allocation ) .	CL_D14-1055_mn_4	CL_D14-1055_mn_6	CL_D14-1055_mn_4_6
CL	D14-1055	mn	4	7	proposal_implementation	proposal_implementation	elaboration	sequence	secondary	secondary	none	none	A semi-supervised model , called mixing population and individual property PU learning ( MPIPUL ) , is proposed .	Thirdly , for the remaining unlabeled examples ( we call them spy examples ) , which can not be explicitly identified as positive and negative , two similarity weights are assigned , by which the probability of a spy example belonging to the positive class and the negative class are displayed .	CL_D14-1055_mn_4	CL_D14-1055_mn_7	CL_D14-1055_mn_4_7
CL	D14-1055	mn	4	8	proposal_implementation	proposal_implementation	elaboration	sequence	secondary	secondary	none	none	A semi-supervised model , called mixing population and individual property PU learning ( MPIPUL ) , is proposed .	Finally , spy examples and their similarity weights are incorporated into SVM ( Support Vector Machine ) to build an accurate classifier .	CL_D14-1055_mn_4	CL_D14-1055_mn_8	CL_D14-1055_mn_4_8
CL	D14-1055	mn	4	9	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	A semi-supervised model , called mixing population and individual property PU learning ( MPIPUL ) , is proposed .	Experiments on gold-standard dataset demonstrate the effectiveness of MPIPUL which outperforms the state-of-the-art baselines .	CL_D14-1055_mn_4	CL_D14-1055_mn_9	CL_D14-1055_mn_4_9
CL	D14-1055	mn	5	6	proposal_implementation	proposal_implementation	elaboration	sequence	secondary	secondary	back	sequence	Firstly , some reliable negative examples are identified from the unlabeled dataset .	Secondly , some representative positive examples and negative examples are generated based on LDA ( Latent Dirichlet Allocation ) .	CL_D14-1055_mn_5	CL_D14-1055_mn_6	CL_D14-1055_mn_5_6
CL	D14-1055	mn	5	7	proposal_implementation	proposal_implementation	elaboration	sequence	secondary	secondary	none	none	Firstly , some reliable negative examples are identified from the unlabeled dataset .	Thirdly , for the remaining unlabeled examples ( we call them spy examples ) , which can not be explicitly identified as positive and negative , two similarity weights are assigned , by which the probability of a spy example belonging to the positive class and the negative class are displayed .	CL_D14-1055_mn_5	CL_D14-1055_mn_7	CL_D14-1055_mn_5_7
CL	D14-1055	mn	5	8	proposal_implementation	proposal_implementation	elaboration	sequence	secondary	secondary	none	none	Firstly , some reliable negative examples are identified from the unlabeled dataset .	Finally , spy examples and their similarity weights are incorporated into SVM ( Support Vector Machine ) to build an accurate classifier .	CL_D14-1055_mn_5	CL_D14-1055_mn_8	CL_D14-1055_mn_5_8
CL	D14-1055	mn	5	9	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	Firstly , some reliable negative examples are identified from the unlabeled dataset .	Experiments on gold-standard dataset demonstrate the effectiveness of MPIPUL which outperforms the state-of-the-art baselines .	CL_D14-1055_mn_5	CL_D14-1055_mn_9	CL_D14-1055_mn_5_9
CL	D14-1055	mn	6	7	proposal_implementation	proposal_implementation	sequence	sequence	secondary	secondary	back	sequence	Secondly , some representative positive examples and negative examples are generated based on LDA ( Latent Dirichlet Allocation ) .	Thirdly , for the remaining unlabeled examples ( we call them spy examples ) , which can not be explicitly identified as positive and negative , two similarity weights are assigned , by which the probability of a spy example belonging to the positive class and the negative class are displayed .	CL_D14-1055_mn_6	CL_D14-1055_mn_7	CL_D14-1055_mn_6_7
CL	D14-1055	mn	6	8	proposal_implementation	proposal_implementation	sequence	sequence	secondary	secondary	none	none	Secondly , some representative positive examples and negative examples are generated based on LDA ( Latent Dirichlet Allocation ) .	Finally , spy examples and their similarity weights are incorporated into SVM ( Support Vector Machine ) to build an accurate classifier .	CL_D14-1055_mn_6	CL_D14-1055_mn_8	CL_D14-1055_mn_6_8
CL	D14-1055	mn	6	9	proposal_implementation	result_means	sequence	support	secondary	secondary	none	none	Secondly , some representative positive examples and negative examples are generated based on LDA ( Latent Dirichlet Allocation ) .	Experiments on gold-standard dataset demonstrate the effectiveness of MPIPUL which outperforms the state-of-the-art baselines .	CL_D14-1055_mn_6	CL_D14-1055_mn_9	CL_D14-1055_mn_6_9
CL	D14-1055	mn	7	8	proposal_implementation	proposal_implementation	sequence	sequence	secondary	secondary	back	sequence	Thirdly , for the remaining unlabeled examples ( we call them spy examples ) , which can not be explicitly identified as positive and negative , two similarity weights are assigned , by which the probability of a spy example belonging to the positive class and the negative class are displayed .	Finally , spy examples and their similarity weights are incorporated into SVM ( Support Vector Machine ) to build an accurate classifier .	CL_D14-1055_mn_7	CL_D14-1055_mn_8	CL_D14-1055_mn_7_8
CL	D14-1055	mn	7	9	proposal_implementation	result_means	sequence	support	secondary	secondary	none	none	Thirdly , for the remaining unlabeled examples ( we call them spy examples ) , which can not be explicitly identified as positive and negative , two similarity weights are assigned , by which the probability of a spy example belonging to the positive class and the negative class are displayed .	Experiments on gold-standard dataset demonstrate the effectiveness of MPIPUL which outperforms the state-of-the-art baselines .	CL_D14-1055_mn_7	CL_D14-1055_mn_9	CL_D14-1055_mn_7_9
CL	D14-1055	mn	8	9	proposal_implementation	result_means	sequence	support	secondary	secondary	none	none	Finally , spy examples and their similarity weights are incorporated into SVM ( Support Vector Machine ) to build an accurate classifier .	Experiments on gold-standard dataset demonstrate the effectiveness of MPIPUL which outperforms the state-of-the-art baselines .	CL_D14-1055_mn_8	CL_D14-1055_mn_9	CL_D14-1055_mn_8_9
CL	D14-1055	mn	9	8	result_means	proposal_implementation	support	sequence	secondary	secondary	none	none	Experiments on gold-standard dataset demonstrate the effectiveness of MPIPUL which outperforms the state-of-the-art baselines .	Finally , spy examples and their similarity weights are incorporated into SVM ( Support Vector Machine ) to build an accurate classifier .	CL_D14-1055_mn_9	CL_D14-1055_mn_8	CL_D14-1055_mn_8_9
CL	D14-1055	pa	1	2	motivation_background	motivation_problem	info-required	support	secondary	secondary	forw	info-required	Deceptive reviews detection has attracted significant attention from both business and research communities .	However , due to the difficulty of human labeling needed for supervised learning , the problem remains to be highly challenging .	CL_D14-1055_pa_1	CL_D14-1055_pa_2	CL_D14-1055_pa_1_2
CL	D14-1055	pa	1	3	motivation_background	proposal	info-required	none	secondary	main	none	none	Deceptive reviews detection has attracted significant attention from both business and research communities .	This paper proposed a novel angle to the problem by modeling PU ( positive unlabeled ) learning .	CL_D14-1055_pa_1	CL_D14-1055_pa_3	CL_D14-1055_pa_1_3
CL	D14-1055	pa	1	4	motivation_background	proposal	info-required	elaboration	secondary	secondary	none	none	Deceptive reviews detection has attracted significant attention from both business and research communities .	A semi-supervised model , called mixing population and individual property PU learning ( MPIPUL ) , is proposed .	CL_D14-1055_pa_1	CL_D14-1055_pa_4	CL_D14-1055_pa_1_4
CL	D14-1055	pa	1	5	motivation_background	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	Deceptive reviews detection has attracted significant attention from both business and research communities .	Firstly , some reliable negative examples are identified from the unlabeled dataset .	CL_D14-1055_pa_1	CL_D14-1055_pa_5	CL_D14-1055_pa_1_5
CL	D14-1055	pa	1	6	motivation_background	proposal_implementation	info-required	sequence	secondary	secondary	none	none	Deceptive reviews detection has attracted significant attention from both business and research communities .	Secondly , some representative positive examples and negative examples are generated based on LDA ( Latent Dirichlet Allocation ) .	CL_D14-1055_pa_1	CL_D14-1055_pa_6	CL_D14-1055_pa_1_6
CL	D14-1055	pa	1	7	motivation_background	proposal_implementation	info-required	sequence	secondary	secondary	none	none	Deceptive reviews detection has attracted significant attention from both business and research communities .	Thirdly , for the remaining unlabeled examples ( we call them spy examples ) , which can not be explicitly identified as positive and negative , two similarity weights are assigned , by which the probability of a spy example belonging to the positive class and the negative class are displayed .	CL_D14-1055_pa_1	CL_D14-1055_pa_7	CL_D14-1055_pa_1_7
CL	D14-1055	pa	1	8	motivation_background	proposal_implementation	info-required	sequence	secondary	secondary	none	none	Deceptive reviews detection has attracted significant attention from both business and research communities .	Finally , spy examples and their similarity weights are incorporated into SVM ( Support Vector Machine ) to build an accurate classifier .	CL_D14-1055_pa_1	CL_D14-1055_pa_8	CL_D14-1055_pa_1_8
CL	D14-1055	pa	1	9	motivation_background	result_means	info-required	support	secondary	secondary	none	none	Deceptive reviews detection has attracted significant attention from both business and research communities .	Experiments on gold-standard dataset demonstrate the effectiveness of MPIPUL which outperforms the state-of-the-art baselines .	CL_D14-1055_pa_1	CL_D14-1055_pa_9	CL_D14-1055_pa_1_9
CL	D14-1055	pa	2	3	motivation_problem	proposal	support	none	secondary	main	forw	support	However , due to the difficulty of human labeling needed for supervised learning , the problem remains to be highly challenging .	This paper proposed a novel angle to the problem by modeling PU ( positive unlabeled ) learning .	CL_D14-1055_pa_2	CL_D14-1055_pa_3	CL_D14-1055_pa_2_3
CL	D14-1055	pa	2	4	motivation_problem	proposal	support	elaboration	secondary	secondary	none	none	However , due to the difficulty of human labeling needed for supervised learning , the problem remains to be highly challenging .	A semi-supervised model , called mixing population and individual property PU learning ( MPIPUL ) , is proposed .	CL_D14-1055_pa_2	CL_D14-1055_pa_4	CL_D14-1055_pa_2_4
CL	D14-1055	pa	2	5	motivation_problem	proposal_implementation	support	elaboration	secondary	secondary	none	none	However , due to the difficulty of human labeling needed for supervised learning , the problem remains to be highly challenging .	Firstly , some reliable negative examples are identified from the unlabeled dataset .	CL_D14-1055_pa_2	CL_D14-1055_pa_5	CL_D14-1055_pa_2_5
CL	D14-1055	pa	2	6	motivation_problem	proposal_implementation	support	sequence	secondary	secondary	none	none	However , due to the difficulty of human labeling needed for supervised learning , the problem remains to be highly challenging .	Secondly , some representative positive examples and negative examples are generated based on LDA ( Latent Dirichlet Allocation ) .	CL_D14-1055_pa_2	CL_D14-1055_pa_6	CL_D14-1055_pa_2_6
CL	D14-1055	pa	2	7	motivation_problem	proposal_implementation	support	sequence	secondary	secondary	none	none	However , due to the difficulty of human labeling needed for supervised learning , the problem remains to be highly challenging .	Thirdly , for the remaining unlabeled examples ( we call them spy examples ) , which can not be explicitly identified as positive and negative , two similarity weights are assigned , by which the probability of a spy example belonging to the positive class and the negative class are displayed .	CL_D14-1055_pa_2	CL_D14-1055_pa_7	CL_D14-1055_pa_2_7
CL	D14-1055	pa	2	8	motivation_problem	proposal_implementation	support	sequence	secondary	secondary	none	none	However , due to the difficulty of human labeling needed for supervised learning , the problem remains to be highly challenging .	Finally , spy examples and their similarity weights are incorporated into SVM ( Support Vector Machine ) to build an accurate classifier .	CL_D14-1055_pa_2	CL_D14-1055_pa_8	CL_D14-1055_pa_2_8
CL	D14-1055	pa	2	9	motivation_problem	result_means	support	support	secondary	secondary	none	none	However , due to the difficulty of human labeling needed for supervised learning , the problem remains to be highly challenging .	Experiments on gold-standard dataset demonstrate the effectiveness of MPIPUL which outperforms the state-of-the-art baselines .	CL_D14-1055_pa_2	CL_D14-1055_pa_9	CL_D14-1055_pa_2_9
CL	D14-1055	pa	3	4	proposal	proposal	none	elaboration	main	secondary	back	elaboration	This paper proposed a novel angle to the problem by modeling PU ( positive unlabeled ) learning .	A semi-supervised model , called mixing population and individual property PU learning ( MPIPUL ) , is proposed .	CL_D14-1055_pa_3	CL_D14-1055_pa_4	CL_D14-1055_pa_3_4
CL	D14-1055	pa	3	5	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	This paper proposed a novel angle to the problem by modeling PU ( positive unlabeled ) learning .	Firstly , some reliable negative examples are identified from the unlabeled dataset .	CL_D14-1055_pa_3	CL_D14-1055_pa_5	CL_D14-1055_pa_3_5
CL	D14-1055	pa	3	6	proposal	proposal_implementation	none	sequence	main	secondary	none	none	This paper proposed a novel angle to the problem by modeling PU ( positive unlabeled ) learning .	Secondly , some representative positive examples and negative examples are generated based on LDA ( Latent Dirichlet Allocation ) .	CL_D14-1055_pa_3	CL_D14-1055_pa_6	CL_D14-1055_pa_3_6
CL	D14-1055	pa	3	7	proposal	proposal_implementation	none	sequence	main	secondary	none	none	This paper proposed a novel angle to the problem by modeling PU ( positive unlabeled ) learning .	Thirdly , for the remaining unlabeled examples ( we call them spy examples ) , which can not be explicitly identified as positive and negative , two similarity weights are assigned , by which the probability of a spy example belonging to the positive class and the negative class are displayed .	CL_D14-1055_pa_3	CL_D14-1055_pa_7	CL_D14-1055_pa_3_7
CL	D14-1055	pa	3	8	proposal	proposal_implementation	none	sequence	main	secondary	none	none	This paper proposed a novel angle to the problem by modeling PU ( positive unlabeled ) learning .	Finally , spy examples and their similarity weights are incorporated into SVM ( Support Vector Machine ) to build an accurate classifier .	CL_D14-1055_pa_3	CL_D14-1055_pa_8	CL_D14-1055_pa_3_8
CL	D14-1055	pa	3	9	proposal	result_means	none	support	main	secondary	back	support	This paper proposed a novel angle to the problem by modeling PU ( positive unlabeled ) learning .	Experiments on gold-standard dataset demonstrate the effectiveness of MPIPUL which outperforms the state-of-the-art baselines .	CL_D14-1055_pa_3	CL_D14-1055_pa_9	CL_D14-1055_pa_3_9
CL	D14-1055	pa	4	5	proposal	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	A semi-supervised model , called mixing population and individual property PU learning ( MPIPUL ) , is proposed .	Firstly , some reliable negative examples are identified from the unlabeled dataset .	CL_D14-1055_pa_4	CL_D14-1055_pa_5	CL_D14-1055_pa_4_5
CL	D14-1055	pa	4	6	proposal	proposal_implementation	elaboration	sequence	secondary	secondary	none	none	A semi-supervised model , called mixing population and individual property PU learning ( MPIPUL ) , is proposed .	Secondly , some representative positive examples and negative examples are generated based on LDA ( Latent Dirichlet Allocation ) .	CL_D14-1055_pa_4	CL_D14-1055_pa_6	CL_D14-1055_pa_4_6
CL	D14-1055	pa	4	7	proposal	proposal_implementation	elaboration	sequence	secondary	secondary	none	none	A semi-supervised model , called mixing population and individual property PU learning ( MPIPUL ) , is proposed .	Thirdly , for the remaining unlabeled examples ( we call them spy examples ) , which can not be explicitly identified as positive and negative , two similarity weights are assigned , by which the probability of a spy example belonging to the positive class and the negative class are displayed .	CL_D14-1055_pa_4	CL_D14-1055_pa_7	CL_D14-1055_pa_4_7
CL	D14-1055	pa	4	8	proposal	proposal_implementation	elaboration	sequence	secondary	secondary	none	none	A semi-supervised model , called mixing population and individual property PU learning ( MPIPUL ) , is proposed .	Finally , spy examples and their similarity weights are incorporated into SVM ( Support Vector Machine ) to build an accurate classifier .	CL_D14-1055_pa_4	CL_D14-1055_pa_8	CL_D14-1055_pa_4_8
CL	D14-1055	pa	4	9	proposal	result_means	elaboration	support	secondary	secondary	none	none	A semi-supervised model , called mixing population and individual property PU learning ( MPIPUL ) , is proposed .	Experiments on gold-standard dataset demonstrate the effectiveness of MPIPUL which outperforms the state-of-the-art baselines .	CL_D14-1055_pa_4	CL_D14-1055_pa_9	CL_D14-1055_pa_4_9
CL	D14-1055	pa	5	6	proposal_implementation	proposal_implementation	elaboration	sequence	secondary	secondary	back	sequence	Firstly , some reliable negative examples are identified from the unlabeled dataset .	Secondly , some representative positive examples and negative examples are generated based on LDA ( Latent Dirichlet Allocation ) .	CL_D14-1055_pa_5	CL_D14-1055_pa_6	CL_D14-1055_pa_5_6
CL	D14-1055	pa	5	7	proposal_implementation	proposal_implementation	elaboration	sequence	secondary	secondary	none	none	Firstly , some reliable negative examples are identified from the unlabeled dataset .	Thirdly , for the remaining unlabeled examples ( we call them spy examples ) , which can not be explicitly identified as positive and negative , two similarity weights are assigned , by which the probability of a spy example belonging to the positive class and the negative class are displayed .	CL_D14-1055_pa_5	CL_D14-1055_pa_7	CL_D14-1055_pa_5_7
CL	D14-1055	pa	5	8	proposal_implementation	proposal_implementation	elaboration	sequence	secondary	secondary	none	none	Firstly , some reliable negative examples are identified from the unlabeled dataset .	Finally , spy examples and their similarity weights are incorporated into SVM ( Support Vector Machine ) to build an accurate classifier .	CL_D14-1055_pa_5	CL_D14-1055_pa_8	CL_D14-1055_pa_5_8
CL	D14-1055	pa	5	9	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	Firstly , some reliable negative examples are identified from the unlabeled dataset .	Experiments on gold-standard dataset demonstrate the effectiveness of MPIPUL which outperforms the state-of-the-art baselines .	CL_D14-1055_pa_5	CL_D14-1055_pa_9	CL_D14-1055_pa_5_9
CL	D14-1055	pa	6	7	proposal_implementation	proposal_implementation	sequence	sequence	secondary	secondary	back	sequence	Secondly , some representative positive examples and negative examples are generated based on LDA ( Latent Dirichlet Allocation ) .	Thirdly , for the remaining unlabeled examples ( we call them spy examples ) , which can not be explicitly identified as positive and negative , two similarity weights are assigned , by which the probability of a spy example belonging to the positive class and the negative class are displayed .	CL_D14-1055_pa_6	CL_D14-1055_pa_7	CL_D14-1055_pa_6_7
CL	D14-1055	pa	6	8	proposal_implementation	proposal_implementation	sequence	sequence	secondary	secondary	none	none	Secondly , some representative positive examples and negative examples are generated based on LDA ( Latent Dirichlet Allocation ) .	Finally , spy examples and their similarity weights are incorporated into SVM ( Support Vector Machine ) to build an accurate classifier .	CL_D14-1055_pa_6	CL_D14-1055_pa_8	CL_D14-1055_pa_6_8
CL	D14-1055	pa	6	9	proposal_implementation	result_means	sequence	support	secondary	secondary	none	none	Secondly , some representative positive examples and negative examples are generated based on LDA ( Latent Dirichlet Allocation ) .	Experiments on gold-standard dataset demonstrate the effectiveness of MPIPUL which outperforms the state-of-the-art baselines .	CL_D14-1055_pa_6	CL_D14-1055_pa_9	CL_D14-1055_pa_6_9
CL	D14-1055	pa	7	8	proposal_implementation	proposal_implementation	sequence	sequence	secondary	secondary	back	sequence	Thirdly , for the remaining unlabeled examples ( we call them spy examples ) , which can not be explicitly identified as positive and negative , two similarity weights are assigned , by which the probability of a spy example belonging to the positive class and the negative class are displayed .	Finally , spy examples and their similarity weights are incorporated into SVM ( Support Vector Machine ) to build an accurate classifier .	CL_D14-1055_pa_7	CL_D14-1055_pa_8	CL_D14-1055_pa_7_8
CL	D14-1055	pa	7	9	proposal_implementation	result_means	sequence	support	secondary	secondary	none	none	Thirdly , for the remaining unlabeled examples ( we call them spy examples ) , which can not be explicitly identified as positive and negative , two similarity weights are assigned , by which the probability of a spy example belonging to the positive class and the negative class are displayed .	Experiments on gold-standard dataset demonstrate the effectiveness of MPIPUL which outperforms the state-of-the-art baselines .	CL_D14-1055_pa_7	CL_D14-1055_pa_9	CL_D14-1055_pa_7_9
CL	D14-1055	pa	8	9	proposal_implementation	result_means	sequence	support	secondary	secondary	none	none	Finally , spy examples and their similarity weights are incorporated into SVM ( Support Vector Machine ) to build an accurate classifier .	Experiments on gold-standard dataset demonstrate the effectiveness of MPIPUL which outperforms the state-of-the-art baselines .	CL_D14-1055_pa_8	CL_D14-1055_pa_9	CL_D14-1055_pa_8_9
CL	D14-1055	pa	9	8	result_means	proposal_implementation	support	sequence	secondary	secondary	none	none	Experiments on gold-standard dataset demonstrate the effectiveness of MPIPUL which outperforms the state-of-the-art baselines .	Finally , spy examples and their similarity weights are incorporated into SVM ( Support Vector Machine ) to build an accurate classifier .	CL_D14-1055_pa_9	CL_D14-1055_pa_8	CL_D14-1055_pa_8_9
CL	D14-1056	ab	1	2	motivation_background	motivation_problem	info-required	support	secondary	secondary	forw	info-required	Shell nouns , such as fact and problem , occur frequently in all kinds of texts .	These nouns themselves are unspecific , and can only be interpreted together with the shell content .	CL_D14-1056_ab_1	CL_D14-1056_ab_2	CL_D14-1056_ab_1_2
CL	D14-1056	ab	1	3	motivation_background	proposal	info-required	none	secondary	main	none	none	Shell nouns , such as fact and problem , occur frequently in all kinds of texts .	We propose a general approach to automatically identify shell content of shell nouns .	CL_D14-1056_ab_1	CL_D14-1056_ab_3	CL_D14-1056_ab_1_3
CL	D14-1056	ab	1	4	motivation_background	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	Shell nouns , such as fact and problem , occur frequently in all kinds of texts .	Our approach exploits lexico-syntactic knowledge derived from the linguistics literature .	CL_D14-1056_ab_1	CL_D14-1056_ab_4	CL_D14-1056_ab_1_4
CL	D14-1056	ab	1	5	motivation_background	result	info-required	support	secondary	secondary	none	none	Shell nouns , such as fact and problem , occur frequently in all kinds of texts .	We evaluate the approach on a variety of shell nouns with a variety of syntactic expectations , achieving accuracies in the range of 62 % ( baseline = 33 % ) to 83 % ( baseline = 74 % ) on crowd-annotated data .	CL_D14-1056_ab_1	CL_D14-1056_ab_5	CL_D14-1056_ab_1_5
CL	D14-1056	ab	2	3	motivation_problem	proposal	support	none	secondary	main	forw	support	These nouns themselves are unspecific , and can only be interpreted together with the shell content .	We propose a general approach to automatically identify shell content of shell nouns .	CL_D14-1056_ab_2	CL_D14-1056_ab_3	CL_D14-1056_ab_2_3
CL	D14-1056	ab	2	4	motivation_problem	proposal_implementation	support	elaboration	secondary	secondary	none	none	These nouns themselves are unspecific , and can only be interpreted together with the shell content .	Our approach exploits lexico-syntactic knowledge derived from the linguistics literature .	CL_D14-1056_ab_2	CL_D14-1056_ab_4	CL_D14-1056_ab_2_4
CL	D14-1056	ab	2	5	motivation_problem	result	support	support	secondary	secondary	none	none	These nouns themselves are unspecific , and can only be interpreted together with the shell content .	We evaluate the approach on a variety of shell nouns with a variety of syntactic expectations , achieving accuracies in the range of 62 % ( baseline = 33 % ) to 83 % ( baseline = 74 % ) on crowd-annotated data .	CL_D14-1056_ab_2	CL_D14-1056_ab_5	CL_D14-1056_ab_2_5
CL	D14-1056	ab	3	4	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	We propose a general approach to automatically identify shell content of shell nouns .	Our approach exploits lexico-syntactic knowledge derived from the linguistics literature .	CL_D14-1056_ab_3	CL_D14-1056_ab_4	CL_D14-1056_ab_3_4
CL	D14-1056	ab	3	5	proposal	result	none	support	main	secondary	back	support	We propose a general approach to automatically identify shell content of shell nouns .	We evaluate the approach on a variety of shell nouns with a variety of syntactic expectations , achieving accuracies in the range of 62 % ( baseline = 33 % ) to 83 % ( baseline = 74 % ) on crowd-annotated data .	CL_D14-1056_ab_3	CL_D14-1056_ab_5	CL_D14-1056_ab_3_5
CL	D14-1056	ab	4	5	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	Our approach exploits lexico-syntactic knowledge derived from the linguistics literature .	We evaluate the approach on a variety of shell nouns with a variety of syntactic expectations , achieving accuracies in the range of 62 % ( baseline = 33 % ) to 83 % ( baseline = 74 % ) on crowd-annotated data .	CL_D14-1056_ab_4	CL_D14-1056_ab_5	CL_D14-1056_ab_4_5
CL	D14-1056	ab	5	4	result	proposal_implementation	support	elaboration	secondary	secondary	none	none	We evaluate the approach on a variety of shell nouns with a variety of syntactic expectations , achieving accuracies in the range of 62 % ( baseline = 33 % ) to 83 % ( baseline = 74 % ) on crowd-annotated data .	Our approach exploits lexico-syntactic knowledge derived from the linguistics literature .	CL_D14-1056_ab_5	CL_D14-1056_ab_4	CL_D14-1056_ab_4_5
CL	D14-1056	mn	1	2	motivation_background	motivation_problem	info-required	support	secondary	secondary	forw	info-required	Shell nouns , such as fact and problem , occur frequently in all kinds of texts .	These nouns themselves are unspecific , and can only be interpreted together with the shell content .	CL_D14-1056_mn_1	CL_D14-1056_mn_2	CL_D14-1056_mn_1_2
CL	D14-1056	mn	1	3	motivation_background	proposal	info-required	none	secondary	main	none	none	Shell nouns , such as fact and problem , occur frequently in all kinds of texts .	We propose a general approach to automatically identify shell content of shell nouns .	CL_D14-1056_mn_1	CL_D14-1056_mn_3	CL_D14-1056_mn_1_3
CL	D14-1056	mn	1	4	motivation_background	proposal_implementation	info-required	elaboration	secondary	secondary	none	none	Shell nouns , such as fact and problem , occur frequently in all kinds of texts .	Our approach exploits lexico-syntactic knowledge derived from the linguistics literature .	CL_D14-1056_mn_1	CL_D14-1056_mn_4	CL_D14-1056_mn_1_4
CL	D14-1056	mn	1	5	motivation_background	observation	info-required	support	secondary	secondary	none	none	Shell nouns , such as fact and problem , occur frequently in all kinds of texts .	We evaluate the approach on a variety of shell nouns with a variety of syntactic expectations , achieving accuracies in the range of 62 % ( baseline = 33 % ) to 83 % ( baseline = 74 % ) on crowd-annotated data .	CL_D14-1056_mn_1	CL_D14-1056_mn_5	CL_D14-1056_mn_1_5
CL	D14-1056	mn	2	3	motivation_problem	proposal	support	none	secondary	main	forw	support	These nouns themselves are unspecific , and can only be interpreted together with the shell content .	We propose a general approach to automatically identify shell content of shell nouns .	CL_D14-1056_mn_2	CL_D14-1056_mn_3	CL_D14-1056_mn_2_3
CL	D14-1056	mn	2	4	motivation_problem	proposal_implementation	support	elaboration	secondary	secondary	none	none	These nouns themselves are unspecific , and can only be interpreted together with the shell content .	Our approach exploits lexico-syntactic knowledge derived from the linguistics literature .	CL_D14-1056_mn_2	CL_D14-1056_mn_4	CL_D14-1056_mn_2_4
CL	D14-1056	mn	2	5	motivation_problem	observation	support	support	secondary	secondary	none	none	These nouns themselves are unspecific , and can only be interpreted together with the shell content .	We evaluate the approach on a variety of shell nouns with a variety of syntactic expectations , achieving accuracies in the range of 62 % ( baseline = 33 % ) to 83 % ( baseline = 74 % ) on crowd-annotated data .	CL_D14-1056_mn_2	CL_D14-1056_mn_5	CL_D14-1056_mn_2_5
CL	D14-1056	mn	3	4	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	We propose a general approach to automatically identify shell content of shell nouns .	Our approach exploits lexico-syntactic knowledge derived from the linguistics literature .	CL_D14-1056_mn_3	CL_D14-1056_mn_4	CL_D14-1056_mn_3_4
CL	D14-1056	mn	3	5	proposal	observation	none	support	main	secondary	back	support	We propose a general approach to automatically identify shell content of shell nouns .	We evaluate the approach on a variety of shell nouns with a variety of syntactic expectations , achieving accuracies in the range of 62 % ( baseline = 33 % ) to 83 % ( baseline = 74 % ) on crowd-annotated data .	CL_D14-1056_mn_3	CL_D14-1056_mn_5	CL_D14-1056_mn_3_5
CL	D14-1056	mn	4	5	proposal_implementation	observation	elaboration	support	secondary	secondary	none	none	Our approach exploits lexico-syntactic knowledge derived from the linguistics literature .	We evaluate the approach on a variety of shell nouns with a variety of syntactic expectations , achieving accuracies in the range of 62 % ( baseline = 33 % ) to 83 % ( baseline = 74 % ) on crowd-annotated data .	CL_D14-1056_mn_4	CL_D14-1056_mn_5	CL_D14-1056_mn_4_5
CL	D14-1056	mn	5	4	observation	proposal_implementation	support	elaboration	secondary	secondary	none	none	We evaluate the approach on a variety of shell nouns with a variety of syntactic expectations , achieving accuracies in the range of 62 % ( baseline = 33 % ) to 83 % ( baseline = 74 % ) on crowd-annotated data .	Our approach exploits lexico-syntactic knowledge derived from the linguistics literature .	CL_D14-1056_mn_5	CL_D14-1056_mn_4	CL_D14-1056_mn_4_5
CL	D14-1056	pa	1	2	motivation_background	motivation_problem	info-required	support	secondary	secondary	forw	info-required	Shell nouns , such as fact and problem , occur frequently in all kinds of texts .	These nouns themselves are unspecific , and can only be interpreted together with the shell content .	CL_D14-1056_pa_1	CL_D14-1056_pa_2	CL_D14-1056_pa_1_2
CL	D14-1056	pa	1	3	motivation_background	proposal	info-required	none	secondary	main	none	none	Shell nouns , such as fact and problem , occur frequently in all kinds of texts .	We propose a general approach to automatically identify shell content of shell nouns .	CL_D14-1056_pa_1	CL_D14-1056_pa_3	CL_D14-1056_pa_1_3
CL	D14-1056	pa	1	4	motivation_background	proposal	info-required	elaboration	secondary	secondary	none	none	Shell nouns , such as fact and problem , occur frequently in all kinds of texts .	Our approach exploits lexico-syntactic knowledge derived from the linguistics literature .	CL_D14-1056_pa_1	CL_D14-1056_pa_4	CL_D14-1056_pa_1_4
CL	D14-1056	pa	1	5	motivation_background	observation	info-required	support	secondary	secondary	none	none	Shell nouns , such as fact and problem , occur frequently in all kinds of texts .	We evaluate the approach on a variety of shell nouns with a variety of syntactic expectations , achieving accuracies in the range of 62 % ( baseline = 33 % ) to 83 % ( baseline = 74 % ) on crowd-annotated data .	CL_D14-1056_pa_1	CL_D14-1056_pa_5	CL_D14-1056_pa_1_5
CL	D14-1056	pa	2	3	motivation_problem	proposal	support	none	secondary	main	forw	support	These nouns themselves are unspecific , and can only be interpreted together with the shell content .	We propose a general approach to automatically identify shell content of shell nouns .	CL_D14-1056_pa_2	CL_D14-1056_pa_3	CL_D14-1056_pa_2_3
CL	D14-1056	pa	2	4	motivation_problem	proposal	support	elaboration	secondary	secondary	none	none	These nouns themselves are unspecific , and can only be interpreted together with the shell content .	Our approach exploits lexico-syntactic knowledge derived from the linguistics literature .	CL_D14-1056_pa_2	CL_D14-1056_pa_4	CL_D14-1056_pa_2_4
CL	D14-1056	pa	2	5	motivation_problem	observation	support	support	secondary	secondary	none	none	These nouns themselves are unspecific , and can only be interpreted together with the shell content .	We evaluate the approach on a variety of shell nouns with a variety of syntactic expectations , achieving accuracies in the range of 62 % ( baseline = 33 % ) to 83 % ( baseline = 74 % ) on crowd-annotated data .	CL_D14-1056_pa_2	CL_D14-1056_pa_5	CL_D14-1056_pa_2_5
CL	D14-1056	pa	3	4	proposal	proposal	none	elaboration	main	secondary	back	elaboration	We propose a general approach to automatically identify shell content of shell nouns .	Our approach exploits lexico-syntactic knowledge derived from the linguistics literature .	CL_D14-1056_pa_3	CL_D14-1056_pa_4	CL_D14-1056_pa_3_4
CL	D14-1056	pa	3	5	proposal	observation	none	support	main	secondary	back	support	We propose a general approach to automatically identify shell content of shell nouns .	We evaluate the approach on a variety of shell nouns with a variety of syntactic expectations , achieving accuracies in the range of 62 % ( baseline = 33 % ) to 83 % ( baseline = 74 % ) on crowd-annotated data .	CL_D14-1056_pa_3	CL_D14-1056_pa_5	CL_D14-1056_pa_3_5
CL	D14-1056	pa	4	5	proposal	observation	elaboration	support	secondary	secondary	none	none	Our approach exploits lexico-syntactic knowledge derived from the linguistics literature .	We evaluate the approach on a variety of shell nouns with a variety of syntactic expectations , achieving accuracies in the range of 62 % ( baseline = 33 % ) to 83 % ( baseline = 74 % ) on crowd-annotated data .	CL_D14-1056_pa_4	CL_D14-1056_pa_5	CL_D14-1056_pa_4_5
CL	D14-1056	pa	5	4	observation	proposal	support	elaboration	secondary	secondary	none	none	We evaluate the approach on a variety of shell nouns with a variety of syntactic expectations , achieving accuracies in the range of 62 % ( baseline = 33 % ) to 83 % ( baseline = 74 % ) on crowd-annotated data .	Our approach exploits lexico-syntactic knowledge derived from the linguistics literature .	CL_D14-1056_pa_5	CL_D14-1056_pa_4	CL_D14-1056_pa_4_5
CL	D14-1057	ab	1	2	proposal	result	none	support	main	secondary	back	support	We present a comparison of different selectional preference models and evaluate them on an automatic verb classification task in German .	We find that all the models we compare are effective for verb clustering ; the best-performing model uses syntactic information to induce nouns classes from unlabelled data in an unsupervised manner .	CL_D14-1057_ab_1	CL_D14-1057_ab_2	CL_D14-1057_ab_1_2
CL	D14-1057	ab	1	3	proposal	result	none	elaboration	main	secondary	none	none	We present a comparison of different selectional preference models and evaluate them on an automatic verb classification task in German .	A very simple model based on lexical preferences is also found to perform well .	CL_D14-1057_ab_1	CL_D14-1057_ab_3	CL_D14-1057_ab_1_3
CL	D14-1057	ab	2	3	result	result	support	elaboration	secondary	secondary	back	elaboration	We find that all the models we compare are effective for verb clustering ; the best-performing model uses syntactic information to induce nouns classes from unlabelled data in an unsupervised manner .	A very simple model based on lexical preferences is also found to perform well .	CL_D14-1057_ab_2	CL_D14-1057_ab_3	CL_D14-1057_ab_2_3
CL	D14-1057	ab	3	2	result	result	elaboration	support	secondary	secondary	forw	elaboration	A very simple model based on lexical preferences is also found to perform well .	We find that all the models we compare are effective for verb clustering ; the best-performing model uses syntactic information to induce nouns classes from unlabelled data in an unsupervised manner .	CL_D14-1057_ab_3	CL_D14-1057_ab_2	CL_D14-1057_ab_2_3
CL	D14-1057	mn	1	2	proposal	result	none	support	main	secondary	back	support	We present a comparison of different selectional preference models and evaluate them on an automatic verb classification task in German .	We find that all the models we compare are effective for verb clustering ; the best-performing model uses syntactic information to induce nouns classes from unlabelled data in an unsupervised manner .	CL_D14-1057_mn_1	CL_D14-1057_mn_2	CL_D14-1057_mn_1_2
CL	D14-1057	mn	1	3	proposal	result	none	elaboration	main	secondary	none	none	We present a comparison of different selectional preference models and evaluate them on an automatic verb classification task in German .	A very simple model based on lexical preferences is also found to perform well .	CL_D14-1057_mn_1	CL_D14-1057_mn_3	CL_D14-1057_mn_1_3
CL	D14-1057	mn	2	3	result	result	support	elaboration	secondary	secondary	back	elaboration	We find that all the models we compare are effective for verb clustering ; the best-performing model uses syntactic information to induce nouns classes from unlabelled data in an unsupervised manner .	A very simple model based on lexical preferences is also found to perform well .	CL_D14-1057_mn_2	CL_D14-1057_mn_3	CL_D14-1057_mn_2_3
CL	D14-1057	mn	3	2	result	result	elaboration	support	secondary	secondary	forw	elaboration	A very simple model based on lexical preferences is also found to perform well .	We find that all the models we compare are effective for verb clustering ; the best-performing model uses syntactic information to induce nouns classes from unlabelled data in an unsupervised manner .	CL_D14-1057_mn_3	CL_D14-1057_mn_2	CL_D14-1057_mn_2_3
CL	D14-1057	pa	1	2	proposal	result	none	support	main	secondary	back	support	We present a comparison of different selectional preference models and evaluate them on an automatic verb classification task in German .	We find that all the models we compare are effective for verb clustering ; the best-performing model uses syntactic information to induce nouns classes from unlabelled data in an unsupervised manner .	CL_D14-1057_pa_1	CL_D14-1057_pa_2	CL_D14-1057_pa_1_2
CL	D14-1057	pa	1	3	proposal	result	none	elaboration	main	secondary	none	none	We present a comparison of different selectional preference models and evaluate them on an automatic verb classification task in German .	A very simple model based on lexical preferences is also found to perform well .	CL_D14-1057_pa_1	CL_D14-1057_pa_3	CL_D14-1057_pa_1_3
CL	D14-1057	pa	2	3	result	result	support	elaboration	secondary	secondary	back	elaboration	We find that all the models we compare are effective for verb clustering ; the best-performing model uses syntactic information to induce nouns classes from unlabelled data in an unsupervised manner .	A very simple model based on lexical preferences is also found to perform well .	CL_D14-1057_pa_2	CL_D14-1057_pa_3	CL_D14-1057_pa_2_3
CL	D14-1057	pa	3	2	result	result	elaboration	support	secondary	secondary	forw	elaboration	A very simple model based on lexical preferences is also found to perform well .	We find that all the models we compare are effective for verb clustering ; the best-performing model uses syntactic information to induce nouns classes from unlabelled data in an unsupervised manner .	CL_D14-1057_pa_3	CL_D14-1057_pa_2	CL_D14-1057_pa_2_3
CL	D14-1058	ab	1	2	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	This paper presents a novel approach to learning to solve simple arithmetic word problems .	Our system , ARIS , analyzes each of the sentences in the problem state-ment to identify the relevant variables and their values .	CL_D14-1058_ab_1	CL_D14-1058_ab_2	CL_D14-1058_ab_1_2
CL	D14-1058	ab	1	3	proposal	proposal_implementation	none	sequence	main	secondary	none	none	This paper presents a novel approach to learning to solve simple arithmetic word problems .	ARIS then maps this information into an equation that represents the problem , and enables its ( trivial ) solution as shown in Figure 1 .	CL_D14-1058_ab_1	CL_D14-1058_ab_3	CL_D14-1058_ab_1_3
CL	D14-1058	ab	1	4	proposal	proposal	none	elaboration	main	secondary	back	elaboration	This paper presents a novel approach to learning to solve simple arithmetic word problems .	The paper analyzes the arithmetic-word problems "genre" , identifying seven categories of verbs used in such problems .	CL_D14-1058_ab_1	CL_D14-1058_ab_4	CL_D14-1058_ab_1_4
CL	D14-1058	ab	1	5	proposal	observation	none	support	main	secondary	none	none	This paper presents a novel approach to learning to solve simple arithmetic word problems .	ARIS learns to categorize verbs with 81.2 % accuracy , and is able to solve 77.7 % of the problems in a corpus of standard primary school test questions .	CL_D14-1058_ab_1	CL_D14-1058_ab_5	CL_D14-1058_ab_1_5
CL	D14-1058	ab	1	6	proposal	result	none	support	main	secondary	back	support	This paper presents a novel approach to learning to solve simple arithmetic word problems .	We report the first learning results on this task without reliance on pre-defined templates and make our data publicly available .	CL_D14-1058_ab_1	CL_D14-1058_ab_6	CL_D14-1058_ab_1_6
CL	D14-1058	ab	2	3	proposal_implementation	proposal_implementation	elaboration	sequence	secondary	secondary	back	sequence	Our system , ARIS , analyzes each of the sentences in the problem state-ment to identify the relevant variables and their values .	ARIS then maps this information into an equation that represents the problem , and enables its ( trivial ) solution as shown in Figure 1 .	CL_D14-1058_ab_2	CL_D14-1058_ab_3	CL_D14-1058_ab_2_3
CL	D14-1058	ab	2	4	proposal_implementation	proposal	elaboration	elaboration	secondary	secondary	none	none	Our system , ARIS , analyzes each of the sentences in the problem state-ment to identify the relevant variables and their values .	The paper analyzes the arithmetic-word problems "genre" , identifying seven categories of verbs used in such problems .	CL_D14-1058_ab_2	CL_D14-1058_ab_4	CL_D14-1058_ab_2_4
CL	D14-1058	ab	2	5	proposal_implementation	observation	elaboration	support	secondary	secondary	none	none	Our system , ARIS , analyzes each of the sentences in the problem state-ment to identify the relevant variables and their values .	ARIS learns to categorize verbs with 81.2 % accuracy , and is able to solve 77.7 % of the problems in a corpus of standard primary school test questions .	CL_D14-1058_ab_2	CL_D14-1058_ab_5	CL_D14-1058_ab_2_5
CL	D14-1058	ab	2	6	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	Our system , ARIS , analyzes each of the sentences in the problem state-ment to identify the relevant variables and their values .	We report the first learning results on this task without reliance on pre-defined templates and make our data publicly available .	CL_D14-1058_ab_2	CL_D14-1058_ab_6	CL_D14-1058_ab_2_6
CL	D14-1058	ab	3	4	proposal_implementation	proposal	sequence	elaboration	secondary	secondary	none	none	ARIS then maps this information into an equation that represents the problem , and enables its ( trivial ) solution as shown in Figure 1 .	The paper analyzes the arithmetic-word problems "genre" , identifying seven categories of verbs used in such problems .	CL_D14-1058_ab_3	CL_D14-1058_ab_4	CL_D14-1058_ab_3_4
CL	D14-1058	ab	3	5	proposal_implementation	observation	sequence	support	secondary	secondary	none	none	ARIS then maps this information into an equation that represents the problem , and enables its ( trivial ) solution as shown in Figure 1 .	ARIS learns to categorize verbs with 81.2 % accuracy , and is able to solve 77.7 % of the problems in a corpus of standard primary school test questions .	CL_D14-1058_ab_3	CL_D14-1058_ab_5	CL_D14-1058_ab_3_5
CL	D14-1058	ab	3	6	proposal_implementation	result	sequence	support	secondary	secondary	none	none	ARIS then maps this information into an equation that represents the problem , and enables its ( trivial ) solution as shown in Figure 1 .	We report the first learning results on this task without reliance on pre-defined templates and make our data publicly available .	CL_D14-1058_ab_3	CL_D14-1058_ab_6	CL_D14-1058_ab_3_6
CL	D14-1058	ab	4	5	proposal	observation	elaboration	support	secondary	secondary	none	none	The paper analyzes the arithmetic-word problems "genre" , identifying seven categories of verbs used in such problems .	ARIS learns to categorize verbs with 81.2 % accuracy , and is able to solve 77.7 % of the problems in a corpus of standard primary school test questions .	CL_D14-1058_ab_4	CL_D14-1058_ab_5	CL_D14-1058_ab_4_5
CL	D14-1058	ab	4	6	proposal	result	elaboration	support	secondary	secondary	none	none	The paper analyzes the arithmetic-word problems "genre" , identifying seven categories of verbs used in such problems .	We report the first learning results on this task without reliance on pre-defined templates and make our data publicly available .	CL_D14-1058_ab_4	CL_D14-1058_ab_6	CL_D14-1058_ab_4_6
CL	D14-1058	ab	5	6	observation	result	support	support	secondary	secondary	forw	support	ARIS learns to categorize verbs with 81.2 % accuracy , and is able to solve 77.7 % of the problems in a corpus of standard primary school test questions .	We report the first learning results on this task without reliance on pre-defined templates and make our data publicly available .	CL_D14-1058_ab_5	CL_D14-1058_ab_6	CL_D14-1058_ab_5_6
CL	D14-1058	ab	6	5	result	observation	support	support	secondary	secondary	back	support	We report the first learning results on this task without reliance on pre-defined templates and make our data publicly available .	ARIS learns to categorize verbs with 81.2 % accuracy , and is able to solve 77.7 % of the problems in a corpus of standard primary school test questions .	CL_D14-1058_ab_6	CL_D14-1058_ab_5	CL_D14-1058_ab_5_6
CL	D14-1058	mn	1	2	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	This paper presents a novel approach to learning to solve simple arithmetic word problems .	Our system , ARIS , analyzes each of the sentences in the problem state-ment to identify the relevant variables and their values .	CL_D14-1058_mn_1	CL_D14-1058_mn_2	CL_D14-1058_mn_1_2
CL	D14-1058	mn	1	3	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	This paper presents a novel approach to learning to solve simple arithmetic word problems .	ARIS then maps this information into an equation that represents the problem , and enables its ( trivial ) solution as shown in Figure 1 .	CL_D14-1058_mn_1	CL_D14-1058_mn_3	CL_D14-1058_mn_1_3
CL	D14-1058	mn	1	4	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	This paper presents a novel approach to learning to solve simple arithmetic word problems .	The paper analyzes the arithmetic-word problems "genre" , identifying seven categories of verbs used in such problems .	CL_D14-1058_mn_1	CL_D14-1058_mn_4	CL_D14-1058_mn_1_4
CL	D14-1058	mn	1	5	proposal	observation	none	support	main	secondary	none	none	This paper presents a novel approach to learning to solve simple arithmetic word problems .	ARIS learns to categorize verbs with 81.2 % accuracy , and is able to solve 77.7 % of the problems in a corpus of standard primary school test questions .	CL_D14-1058_mn_1	CL_D14-1058_mn_5	CL_D14-1058_mn_1_5
CL	D14-1058	mn	1	6	proposal	result	none	support	main	secondary	back	support	This paper presents a novel approach to learning to solve simple arithmetic word problems .	We report the first learning results on this task without reliance on pre-defined templates and make our data publicly available .	CL_D14-1058_mn_1	CL_D14-1058_mn_6	CL_D14-1058_mn_1_6
CL	D14-1058	mn	2	3	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	Our system , ARIS , analyzes each of the sentences in the problem state-ment to identify the relevant variables and their values .	ARIS then maps this information into an equation that represents the problem , and enables its ( trivial ) solution as shown in Figure 1 .	CL_D14-1058_mn_2	CL_D14-1058_mn_3	CL_D14-1058_mn_2_3
CL	D14-1058	mn	2	4	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	none	none	Our system , ARIS , analyzes each of the sentences in the problem state-ment to identify the relevant variables and their values .	The paper analyzes the arithmetic-word problems "genre" , identifying seven categories of verbs used in such problems .	CL_D14-1058_mn_2	CL_D14-1058_mn_4	CL_D14-1058_mn_2_4
CL	D14-1058	mn	2	5	proposal_implementation	observation	elaboration	support	secondary	secondary	none	none	Our system , ARIS , analyzes each of the sentences in the problem state-ment to identify the relevant variables and their values .	ARIS learns to categorize verbs with 81.2 % accuracy , and is able to solve 77.7 % of the problems in a corpus of standard primary school test questions .	CL_D14-1058_mn_2	CL_D14-1058_mn_5	CL_D14-1058_mn_2_5
CL	D14-1058	mn	2	6	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	Our system , ARIS , analyzes each of the sentences in the problem state-ment to identify the relevant variables and their values .	We report the first learning results on this task without reliance on pre-defined templates and make our data publicly available .	CL_D14-1058_mn_2	CL_D14-1058_mn_6	CL_D14-1058_mn_2_6
CL	D14-1058	mn	3	4	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	none	none	ARIS then maps this information into an equation that represents the problem , and enables its ( trivial ) solution as shown in Figure 1 .	The paper analyzes the arithmetic-word problems "genre" , identifying seven categories of verbs used in such problems .	CL_D14-1058_mn_3	CL_D14-1058_mn_4	CL_D14-1058_mn_3_4
CL	D14-1058	mn	3	5	proposal_implementation	observation	elaboration	support	secondary	secondary	none	none	ARIS then maps this information into an equation that represents the problem , and enables its ( trivial ) solution as shown in Figure 1 .	ARIS learns to categorize verbs with 81.2 % accuracy , and is able to solve 77.7 % of the problems in a corpus of standard primary school test questions .	CL_D14-1058_mn_3	CL_D14-1058_mn_5	CL_D14-1058_mn_3_5
CL	D14-1058	mn	3	6	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	ARIS then maps this information into an equation that represents the problem , and enables its ( trivial ) solution as shown in Figure 1 .	We report the first learning results on this task without reliance on pre-defined templates and make our data publicly available .	CL_D14-1058_mn_3	CL_D14-1058_mn_6	CL_D14-1058_mn_3_6
CL	D14-1058	mn	4	5	proposal_implementation	observation	elaboration	support	secondary	secondary	none	none	The paper analyzes the arithmetic-word problems "genre" , identifying seven categories of verbs used in such problems .	ARIS learns to categorize verbs with 81.2 % accuracy , and is able to solve 77.7 % of the problems in a corpus of standard primary school test questions .	CL_D14-1058_mn_4	CL_D14-1058_mn_5	CL_D14-1058_mn_4_5
CL	D14-1058	mn	4	6	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	The paper analyzes the arithmetic-word problems "genre" , identifying seven categories of verbs used in such problems .	We report the first learning results on this task without reliance on pre-defined templates and make our data publicly available .	CL_D14-1058_mn_4	CL_D14-1058_mn_6	CL_D14-1058_mn_4_6
CL	D14-1058	mn	5	6	observation	result	support	support	secondary	secondary	forw	support	ARIS learns to categorize verbs with 81.2 % accuracy , and is able to solve 77.7 % of the problems in a corpus of standard primary school test questions .	We report the first learning results on this task without reliance on pre-defined templates and make our data publicly available .	CL_D14-1058_mn_5	CL_D14-1058_mn_6	CL_D14-1058_mn_5_6
CL	D14-1058	mn	6	5	result	observation	support	support	secondary	secondary	back	support	We report the first learning results on this task without reliance on pre-defined templates and make our data publicly available .	ARIS learns to categorize verbs with 81.2 % accuracy , and is able to solve 77.7 % of the problems in a corpus of standard primary school test questions .	CL_D14-1058_mn_6	CL_D14-1058_mn_5	CL_D14-1058_mn_5_6
CL	D14-1058	pa	1	2	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	This paper presents a novel approach to learning to solve simple arithmetic word problems .	Our system , ARIS , analyzes each of the sentences in the problem state-ment to identify the relevant variables and their values .	CL_D14-1058_pa_1	CL_D14-1058_pa_2	CL_D14-1058_pa_1_2
CL	D14-1058	pa	1	3	proposal	proposal_implementation	none	sequence	main	secondary	none	none	This paper presents a novel approach to learning to solve simple arithmetic word problems .	ARIS then maps this information into an equation that represents the problem , and enables its ( trivial ) solution as shown in Figure 1 .	CL_D14-1058_pa_1	CL_D14-1058_pa_3	CL_D14-1058_pa_1_3
CL	D14-1058	pa	1	4	proposal	proposal	none	elaboration	main	secondary	back	elaboration	This paper presents a novel approach to learning to solve simple arithmetic word problems .	The paper analyzes the arithmetic-word problems "genre" , identifying seven categories of verbs used in such problems .	CL_D14-1058_pa_1	CL_D14-1058_pa_4	CL_D14-1058_pa_1_4
CL	D14-1058	pa	1	5	proposal	observation	none	support	main	secondary	none	none	This paper presents a novel approach to learning to solve simple arithmetic word problems .	ARIS learns to categorize verbs with 81.2 % accuracy , and is able to solve 77.7 % of the problems in a corpus of standard primary school test questions .	CL_D14-1058_pa_1	CL_D14-1058_pa_5	CL_D14-1058_pa_1_5
CL	D14-1058	pa	1	6	proposal	conclusion	none	support	main	secondary	back	support	This paper presents a novel approach to learning to solve simple arithmetic word problems .	We report the first learning results on this task without reliance on pre-defined templates and make our data publicly available .	CL_D14-1058_pa_1	CL_D14-1058_pa_6	CL_D14-1058_pa_1_6
CL	D14-1058	pa	2	3	proposal_implementation	proposal_implementation	elaboration	sequence	secondary	secondary	back	sequence	Our system , ARIS , analyzes each of the sentences in the problem state-ment to identify the relevant variables and their values .	ARIS then maps this information into an equation that represents the problem , and enables its ( trivial ) solution as shown in Figure 1 .	CL_D14-1058_pa_2	CL_D14-1058_pa_3	CL_D14-1058_pa_2_3
CL	D14-1058	pa	2	4	proposal_implementation	proposal	elaboration	elaboration	secondary	secondary	none	none	Our system , ARIS , analyzes each of the sentences in the problem state-ment to identify the relevant variables and their values .	The paper analyzes the arithmetic-word problems "genre" , identifying seven categories of verbs used in such problems .	CL_D14-1058_pa_2	CL_D14-1058_pa_4	CL_D14-1058_pa_2_4
CL	D14-1058	pa	2	5	proposal_implementation	observation	elaboration	support	secondary	secondary	back	support	Our system , ARIS , analyzes each of the sentences in the problem state-ment to identify the relevant variables and their values .	ARIS learns to categorize verbs with 81.2 % accuracy , and is able to solve 77.7 % of the problems in a corpus of standard primary school test questions .	CL_D14-1058_pa_2	CL_D14-1058_pa_5	CL_D14-1058_pa_2_5
CL	D14-1058	pa	2	6	proposal_implementation	conclusion	elaboration	support	secondary	secondary	none	none	Our system , ARIS , analyzes each of the sentences in the problem state-ment to identify the relevant variables and their values .	We report the first learning results on this task without reliance on pre-defined templates and make our data publicly available .	CL_D14-1058_pa_2	CL_D14-1058_pa_6	CL_D14-1058_pa_2_6
CL	D14-1058	pa	3	4	proposal_implementation	proposal	sequence	elaboration	secondary	secondary	none	none	ARIS then maps this information into an equation that represents the problem , and enables its ( trivial ) solution as shown in Figure 1 .	The paper analyzes the arithmetic-word problems "genre" , identifying seven categories of verbs used in such problems .	CL_D14-1058_pa_3	CL_D14-1058_pa_4	CL_D14-1058_pa_3_4
CL	D14-1058	pa	3	5	proposal_implementation	observation	sequence	support	secondary	secondary	none	none	ARIS then maps this information into an equation that represents the problem , and enables its ( trivial ) solution as shown in Figure 1 .	ARIS learns to categorize verbs with 81.2 % accuracy , and is able to solve 77.7 % of the problems in a corpus of standard primary school test questions .	CL_D14-1058_pa_3	CL_D14-1058_pa_5	CL_D14-1058_pa_3_5
CL	D14-1058	pa	3	6	proposal_implementation	conclusion	sequence	support	secondary	secondary	none	none	ARIS then maps this information into an equation that represents the problem , and enables its ( trivial ) solution as shown in Figure 1 .	We report the first learning results on this task without reliance on pre-defined templates and make our data publicly available .	CL_D14-1058_pa_3	CL_D14-1058_pa_6	CL_D14-1058_pa_3_6
CL	D14-1058	pa	4	5	proposal	observation	elaboration	support	secondary	secondary	none	none	The paper analyzes the arithmetic-word problems "genre" , identifying seven categories of verbs used in such problems .	ARIS learns to categorize verbs with 81.2 % accuracy , and is able to solve 77.7 % of the problems in a corpus of standard primary school test questions .	CL_D14-1058_pa_4	CL_D14-1058_pa_5	CL_D14-1058_pa_4_5
CL	D14-1058	pa	4	6	proposal	conclusion	elaboration	support	secondary	secondary	none	none	The paper analyzes the arithmetic-word problems "genre" , identifying seven categories of verbs used in such problems .	We report the first learning results on this task without reliance on pre-defined templates and make our data publicly available .	CL_D14-1058_pa_4	CL_D14-1058_pa_6	CL_D14-1058_pa_4_6
CL	D14-1058	pa	5	6	observation	conclusion	support	support	secondary	secondary	none	none	ARIS learns to categorize verbs with 81.2 % accuracy , and is able to solve 77.7 % of the problems in a corpus of standard primary school test questions .	We report the first learning results on this task without reliance on pre-defined templates and make our data publicly available .	CL_D14-1058_pa_5	CL_D14-1058_pa_6	CL_D14-1058_pa_5_6
CL	D14-1058	pa	6	5	conclusion	observation	support	support	secondary	secondary	none	none	We report the first learning results on this task without reliance on pre-defined templates and make our data publicly available .	ARIS learns to categorize verbs with 81.2 % accuracy , and is able to solve 77.7 % of the problems in a corpus of standard primary school test questions .	CL_D14-1058_pa_6	CL_D14-1058_pa_5	CL_D14-1058_pa_5_6
CL	D14-1059	ab	1	2	motivation_background	proposal	support	none	secondary	main	forw	support	Common-sense reasoning is important for AI applications , both in NLP and many vision and robotics tasks .	We propose NaturalLI : a Natural Logic inference system for inferring common sense facts - for instance , that cats have tails or tomatoes are round - from a very large database of known facts .	CL_D14-1059_ab_1	CL_D14-1059_ab_2	CL_D14-1059_ab_1_2
CL	D14-1059	ab	1	3	motivation_background	proposal	support	elaboration	secondary	secondary	none	none	Common-sense reasoning is important for AI applications , both in NLP and many vision and robotics tasks .	In addition to being able to provide strictly valid derivations , the system is also able to produce derivations which are only likely valid , accompanied by an associated confidence .	CL_D14-1059_ab_1	CL_D14-1059_ab_3	CL_D14-1059_ab_1_3
CL	D14-1059	ab	1	4	motivation_background	result_means	support	support	secondary	secondary	none	none	Common-sense reasoning is important for AI applications , both in NLP and many vision and robotics tasks .	We both show that our system is able to capture strict Natural Logic inferences on the FraCaS test suite , and demonstrate its ability to predict common sense facts with 49 % recall and 91 % precision .	CL_D14-1059_ab_1	CL_D14-1059_ab_4	CL_D14-1059_ab_1_4
CL	D14-1059	ab	2	3	proposal	proposal	none	elaboration	main	secondary	back	elaboration	We propose NaturalLI : a Natural Logic inference system for inferring common sense facts - for instance , that cats have tails or tomatoes are round - from a very large database of known facts .	In addition to being able to provide strictly valid derivations , the system is also able to produce derivations which are only likely valid , accompanied by an associated confidence .	CL_D14-1059_ab_2	CL_D14-1059_ab_3	CL_D14-1059_ab_2_3
CL	D14-1059	ab	2	4	proposal	result_means	none	support	main	secondary	back	support	We propose NaturalLI : a Natural Logic inference system for inferring common sense facts - for instance , that cats have tails or tomatoes are round - from a very large database of known facts .	We both show that our system is able to capture strict Natural Logic inferences on the FraCaS test suite , and demonstrate its ability to predict common sense facts with 49 % recall and 91 % precision .	CL_D14-1059_ab_2	CL_D14-1059_ab_4	CL_D14-1059_ab_2_4
CL	D14-1059	ab	3	4	proposal	result_means	elaboration	support	secondary	secondary	none	none	In addition to being able to provide strictly valid derivations , the system is also able to produce derivations which are only likely valid , accompanied by an associated confidence .	We both show that our system is able to capture strict Natural Logic inferences on the FraCaS test suite , and demonstrate its ability to predict common sense facts with 49 % recall and 91 % precision .	CL_D14-1059_ab_3	CL_D14-1059_ab_4	CL_D14-1059_ab_3_4
CL	D14-1059	ab	4	3	result_means	proposal	support	elaboration	secondary	secondary	none	none	We both show that our system is able to capture strict Natural Logic inferences on the FraCaS test suite , and demonstrate its ability to predict common sense facts with 49 % recall and 91 % precision .	In addition to being able to provide strictly valid derivations , the system is also able to produce derivations which are only likely valid , accompanied by an associated confidence .	CL_D14-1059_ab_4	CL_D14-1059_ab_3	CL_D14-1059_ab_3_4
CL	D14-1059	mn	1	2	motivation_background	proposal	support	none	secondary	main	forw	support	Common-sense reasoning is important for AI applications , both in NLP and many vision and robotics tasks .	We propose NaturalLI : a Natural Logic inference system for inferring common sense facts - for instance , that cats have tails or tomatoes are round - from a very large database of known facts .	CL_D14-1059_mn_1	CL_D14-1059_mn_2	CL_D14-1059_mn_1_2
CL	D14-1059	mn	1	3	motivation_background	proposal_implementation	support	elaboration	secondary	secondary	none	none	Common-sense reasoning is important for AI applications , both in NLP and many vision and robotics tasks .	In addition to being able to provide strictly valid derivations , the system is also able to produce derivations which are only likely valid , accompanied by an associated confidence .	CL_D14-1059_mn_1	CL_D14-1059_mn_3	CL_D14-1059_mn_1_3
CL	D14-1059	mn	1	4	motivation_background	result_means	support	support	secondary	secondary	none	none	Common-sense reasoning is important for AI applications , both in NLP and many vision and robotics tasks .	We both show that our system is able to capture strict Natural Logic inferences on the FraCaS test suite , and demonstrate its ability to predict common sense facts with 49 % recall and 91 % precision .	CL_D14-1059_mn_1	CL_D14-1059_mn_4	CL_D14-1059_mn_1_4
CL	D14-1059	mn	2	3	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	We propose NaturalLI : a Natural Logic inference system for inferring common sense facts - for instance , that cats have tails or tomatoes are round - from a very large database of known facts .	In addition to being able to provide strictly valid derivations , the system is also able to produce derivations which are only likely valid , accompanied by an associated confidence .	CL_D14-1059_mn_2	CL_D14-1059_mn_3	CL_D14-1059_mn_2_3
CL	D14-1059	mn	2	4	proposal	result_means	none	support	main	secondary	back	support	We propose NaturalLI : a Natural Logic inference system for inferring common sense facts - for instance , that cats have tails or tomatoes are round - from a very large database of known facts .	We both show that our system is able to capture strict Natural Logic inferences on the FraCaS test suite , and demonstrate its ability to predict common sense facts with 49 % recall and 91 % precision .	CL_D14-1059_mn_2	CL_D14-1059_mn_4	CL_D14-1059_mn_2_4
CL	D14-1059	mn	3	4	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	In addition to being able to provide strictly valid derivations , the system is also able to produce derivations which are only likely valid , accompanied by an associated confidence .	We both show that our system is able to capture strict Natural Logic inferences on the FraCaS test suite , and demonstrate its ability to predict common sense facts with 49 % recall and 91 % precision .	CL_D14-1059_mn_3	CL_D14-1059_mn_4	CL_D14-1059_mn_3_4
CL	D14-1059	mn	4	3	result_means	proposal_implementation	support	elaboration	secondary	secondary	none	none	We both show that our system is able to capture strict Natural Logic inferences on the FraCaS test suite , and demonstrate its ability to predict common sense facts with 49 % recall and 91 % precision .	In addition to being able to provide strictly valid derivations , the system is also able to produce derivations which are only likely valid , accompanied by an associated confidence .	CL_D14-1059_mn_4	CL_D14-1059_mn_3	CL_D14-1059_mn_3_4
CL	D14-1059	pa	1	2	motivation_background	proposal	support	none	secondary	main	forw	support	Common-sense reasoning is important for AI applications , both in NLP and many vision and robotics tasks .	We propose NaturalLI : a Natural Logic inference system for inferring common sense facts - for instance , that cats have tails or tomatoes are round - from a very large database of known facts .	CL_D14-1059_pa_1	CL_D14-1059_pa_2	CL_D14-1059_pa_1_2
CL	D14-1059	pa	1	3	motivation_background	proposal	support	elaboration	secondary	secondary	none	none	Common-sense reasoning is important for AI applications , both in NLP and many vision and robotics tasks .	In addition to being able to provide strictly valid derivations , the system is also able to produce derivations which are only likely valid , accompanied by an associated confidence .	CL_D14-1059_pa_1	CL_D14-1059_pa_3	CL_D14-1059_pa_1_3
CL	D14-1059	pa	1	4	motivation_background	result_means	support	support	secondary	secondary	none	none	Common-sense reasoning is important for AI applications , both in NLP and many vision and robotics tasks .	We both show that our system is able to capture strict Natural Logic inferences on the FraCaS test suite , and demonstrate its ability to predict common sense facts with 49 % recall and 91 % precision .	CL_D14-1059_pa_1	CL_D14-1059_pa_4	CL_D14-1059_pa_1_4
CL	D14-1059	pa	2	3	proposal	proposal	none	elaboration	main	secondary	back	elaboration	We propose NaturalLI : a Natural Logic inference system for inferring common sense facts - for instance , that cats have tails or tomatoes are round - from a very large database of known facts .	In addition to being able to provide strictly valid derivations , the system is also able to produce derivations which are only likely valid , accompanied by an associated confidence .	CL_D14-1059_pa_2	CL_D14-1059_pa_3	CL_D14-1059_pa_2_3
CL	D14-1059	pa	2	4	proposal	result_means	none	support	main	secondary	back	support	We propose NaturalLI : a Natural Logic inference system for inferring common sense facts - for instance , that cats have tails or tomatoes are round - from a very large database of known facts .	We both show that our system is able to capture strict Natural Logic inferences on the FraCaS test suite , and demonstrate its ability to predict common sense facts with 49 % recall and 91 % precision .	CL_D14-1059_pa_2	CL_D14-1059_pa_4	CL_D14-1059_pa_2_4
CL	D14-1059	pa	3	4	proposal	result_means	elaboration	support	secondary	secondary	none	none	In addition to being able to provide strictly valid derivations , the system is also able to produce derivations which are only likely valid , accompanied by an associated confidence .	We both show that our system is able to capture strict Natural Logic inferences on the FraCaS test suite , and demonstrate its ability to predict common sense facts with 49 % recall and 91 % precision .	CL_D14-1059_pa_3	CL_D14-1059_pa_4	CL_D14-1059_pa_3_4
CL	D14-1059	pa	4	3	result_means	proposal	support	elaboration	secondary	secondary	none	none	We both show that our system is able to capture strict Natural Logic inferences on the FraCaS test suite , and demonstrate its ability to predict common sense facts with 49 % recall and 91 % precision .	In addition to being able to provide strictly valid derivations , the system is also able to produce derivations which are only likely valid , accompanied by an associated confidence .	CL_D14-1059_pa_4	CL_D14-1059_pa_3	CL_D14-1059_pa_3_4
CL	D14-1060	ab	1	2	motivation_background	proposal	support	none	secondary	main	forw	support	Term translation is of great importance for statistical machine translation ( SMT ) , especially document-informed SMT .	In this paper , we investigate three issues of term translation in the context of document-informed SMT and propose three corresponding models : ( a ) a term translation disambiguation model which selects desirable translations for terms in the source language with domain information , ( b ) a term translation consistency model that encourages consistent translations for terms with a high strength of translation consistency throughout a document , and ( c ) a term bracketing model that rewards translation hypotheses where bracketable source terms are translated as a whole unit .	CL_D14-1060_ab_1	CL_D14-1060_ab_2	CL_D14-1060_ab_1_2
CL	D14-1060	ab	1	3	motivation_background	proposal	support	elaboration	secondary	secondary	none	none	Term translation is of great importance for statistical machine translation ( SMT ) , especially document-informed SMT .	We integrate the three models into hierarchical phrase-based SMT and evaluate their effectiveness on NIST Chinese-English translation tasks with large-scale training data .	CL_D14-1060_ab_1	CL_D14-1060_ab_3	CL_D14-1060_ab_1_3
CL	D14-1060	ab	1	4	motivation_background	result	support	support	secondary	secondary	none	none	Term translation is of great importance for statistical machine translation ( SMT ) , especially document-informed SMT .	Experiment results show that all three models can achieve significant improvements over the baseline .	CL_D14-1060_ab_1	CL_D14-1060_ab_4	CL_D14-1060_ab_1_4
CL	D14-1060	ab	1	5	motivation_background	result	support	elaboration	secondary	secondary	none	none	Term translation is of great importance for statistical machine translation ( SMT ) , especially document-informed SMT .	Additionally , we can obtain a further improvement when combining the three models .	CL_D14-1060_ab_1	CL_D14-1060_ab_5	CL_D14-1060_ab_1_5
CL	D14-1060	ab	2	3	proposal	proposal	none	elaboration	main	secondary	back	elaboration	In this paper , we investigate three issues of term translation in the context of document-informed SMT and propose three corresponding models : ( a ) a term translation disambiguation model which selects desirable translations for terms in the source language with domain information , ( b ) a term translation consistency model that encourages consistent translations for terms with a high strength of translation consistency throughout a document , and ( c ) a term bracketing model that rewards translation hypotheses where bracketable source terms are translated as a whole unit .	We integrate the three models into hierarchical phrase-based SMT and evaluate their effectiveness on NIST Chinese-English translation tasks with large-scale training data .	CL_D14-1060_ab_2	CL_D14-1060_ab_3	CL_D14-1060_ab_2_3
CL	D14-1060	ab	2	4	proposal	result	none	support	main	secondary	back	support	In this paper , we investigate three issues of term translation in the context of document-informed SMT and propose three corresponding models : ( a ) a term translation disambiguation model which selects desirable translations for terms in the source language with domain information , ( b ) a term translation consistency model that encourages consistent translations for terms with a high strength of translation consistency throughout a document , and ( c ) a term bracketing model that rewards translation hypotheses where bracketable source terms are translated as a whole unit .	Experiment results show that all three models can achieve significant improvements over the baseline .	CL_D14-1060_ab_2	CL_D14-1060_ab_4	CL_D14-1060_ab_2_4
CL	D14-1060	ab	2	5	proposal	result	none	elaboration	main	secondary	none	none	In this paper , we investigate three issues of term translation in the context of document-informed SMT and propose three corresponding models : ( a ) a term translation disambiguation model which selects desirable translations for terms in the source language with domain information , ( b ) a term translation consistency model that encourages consistent translations for terms with a high strength of translation consistency throughout a document , and ( c ) a term bracketing model that rewards translation hypotheses where bracketable source terms are translated as a whole unit .	Additionally , we can obtain a further improvement when combining the three models .	CL_D14-1060_ab_2	CL_D14-1060_ab_5	CL_D14-1060_ab_2_5
CL	D14-1060	ab	3	4	proposal	result	elaboration	support	secondary	secondary	none	none	We integrate the three models into hierarchical phrase-based SMT and evaluate their effectiveness on NIST Chinese-English translation tasks with large-scale training data .	Experiment results show that all three models can achieve significant improvements over the baseline .	CL_D14-1060_ab_3	CL_D14-1060_ab_4	CL_D14-1060_ab_3_4
CL	D14-1060	ab	3	5	proposal	result	elaboration	elaboration	secondary	secondary	none	none	We integrate the three models into hierarchical phrase-based SMT and evaluate their effectiveness on NIST Chinese-English translation tasks with large-scale training data .	Additionally , we can obtain a further improvement when combining the three models .	CL_D14-1060_ab_3	CL_D14-1060_ab_5	CL_D14-1060_ab_3_5
CL	D14-1060	ab	4	5	result	result	support	elaboration	secondary	secondary	back	elaboration	Experiment results show that all three models can achieve significant improvements over the baseline .	Additionally , we can obtain a further improvement when combining the three models .	CL_D14-1060_ab_4	CL_D14-1060_ab_5	CL_D14-1060_ab_4_5
CL	D14-1060	ab	5	4	result	result	elaboration	support	secondary	secondary	forw	elaboration	Additionally , we can obtain a further improvement when combining the three models .	Experiment results show that all three models can achieve significant improvements over the baseline .	CL_D14-1060_ab_5	CL_D14-1060_ab_4	CL_D14-1060_ab_4_5
CL	D14-1060	mn	1	2	motivation_background	proposal	support	none	secondary	main	forw	support	Term translation is of great importance for statistical machine translation ( SMT ) , especially document-informed SMT .	In this paper , we investigate three issues of term translation in the context of document-informed SMT and propose three corresponding models : ( a ) a term translation disambiguation model which selects desirable translations for terms in the source language with domain information , ( b ) a term translation consistency model that encourages consistent translations for terms with a high strength of translation consistency throughout a document , and ( c ) a term bracketing model that rewards translation hypotheses where bracketable source terms are translated as a whole unit .	CL_D14-1060_mn_1	CL_D14-1060_mn_2	CL_D14-1060_mn_1_2
CL	D14-1060	mn	1	3	motivation_background	proposal_implementation	support	elaboration	secondary	secondary	none	none	Term translation is of great importance for statistical machine translation ( SMT ) , especially document-informed SMT .	We integrate the three models into hierarchical phrase-based SMT and evaluate their effectiveness on NIST Chinese-English translation tasks with large-scale training data .	CL_D14-1060_mn_1	CL_D14-1060_mn_3	CL_D14-1060_mn_1_3
CL	D14-1060	mn	1	4	motivation_background	result	support	support	secondary	secondary	none	none	Term translation is of great importance for statistical machine translation ( SMT ) , especially document-informed SMT .	Experiment results show that all three models can achieve significant improvements over the baseline .	CL_D14-1060_mn_1	CL_D14-1060_mn_4	CL_D14-1060_mn_1_4
CL	D14-1060	mn	1	5	motivation_background	result	support	elaboration	secondary	secondary	none	none	Term translation is of great importance for statistical machine translation ( SMT ) , especially document-informed SMT .	Additionally , we can obtain a further improvement when combining the three models .	CL_D14-1060_mn_1	CL_D14-1060_mn_5	CL_D14-1060_mn_1_5
CL	D14-1060	mn	2	3	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	In this paper , we investigate three issues of term translation in the context of document-informed SMT and propose three corresponding models : ( a ) a term translation disambiguation model which selects desirable translations for terms in the source language with domain information , ( b ) a term translation consistency model that encourages consistent translations for terms with a high strength of translation consistency throughout a document , and ( c ) a term bracketing model that rewards translation hypotheses where bracketable source terms are translated as a whole unit .	We integrate the three models into hierarchical phrase-based SMT and evaluate their effectiveness on NIST Chinese-English translation tasks with large-scale training data .	CL_D14-1060_mn_2	CL_D14-1060_mn_3	CL_D14-1060_mn_2_3
CL	D14-1060	mn	2	4	proposal	result	none	support	main	secondary	back	support	In this paper , we investigate three issues of term translation in the context of document-informed SMT and propose three corresponding models : ( a ) a term translation disambiguation model which selects desirable translations for terms in the source language with domain information , ( b ) a term translation consistency model that encourages consistent translations for terms with a high strength of translation consistency throughout a document , and ( c ) a term bracketing model that rewards translation hypotheses where bracketable source terms are translated as a whole unit .	Experiment results show that all three models can achieve significant improvements over the baseline .	CL_D14-1060_mn_2	CL_D14-1060_mn_4	CL_D14-1060_mn_2_4
CL	D14-1060	mn	2	5	proposal	result	none	elaboration	main	secondary	none	none	In this paper , we investigate three issues of term translation in the context of document-informed SMT and propose three corresponding models : ( a ) a term translation disambiguation model which selects desirable translations for terms in the source language with domain information , ( b ) a term translation consistency model that encourages consistent translations for terms with a high strength of translation consistency throughout a document , and ( c ) a term bracketing model that rewards translation hypotheses where bracketable source terms are translated as a whole unit .	Additionally , we can obtain a further improvement when combining the three models .	CL_D14-1060_mn_2	CL_D14-1060_mn_5	CL_D14-1060_mn_2_5
CL	D14-1060	mn	3	4	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	We integrate the three models into hierarchical phrase-based SMT and evaluate their effectiveness on NIST Chinese-English translation tasks with large-scale training data .	Experiment results show that all three models can achieve significant improvements over the baseline .	CL_D14-1060_mn_3	CL_D14-1060_mn_4	CL_D14-1060_mn_3_4
CL	D14-1060	mn	3	5	proposal_implementation	result	elaboration	elaboration	secondary	secondary	none	none	We integrate the three models into hierarchical phrase-based SMT and evaluate their effectiveness on NIST Chinese-English translation tasks with large-scale training data .	Additionally , we can obtain a further improvement when combining the three models .	CL_D14-1060_mn_3	CL_D14-1060_mn_5	CL_D14-1060_mn_3_5
CL	D14-1060	mn	4	5	result	result	support	elaboration	secondary	secondary	back	elaboration	Experiment results show that all three models can achieve significant improvements over the baseline .	Additionally , we can obtain a further improvement when combining the three models .	CL_D14-1060_mn_4	CL_D14-1060_mn_5	CL_D14-1060_mn_4_5
CL	D14-1060	mn	5	4	result	result	elaboration	support	secondary	secondary	forw	elaboration	Additionally , we can obtain a further improvement when combining the three models .	Experiment results show that all three models can achieve significant improvements over the baseline .	CL_D14-1060_mn_5	CL_D14-1060_mn_4	CL_D14-1060_mn_4_5
CL	D14-1060	pa	1	2	motivation_background	proposal	support	none	secondary	main	forw	support	Term translation is of great importance for statistical machine translation ( SMT ) , especially document-informed SMT .	In this paper , we investigate three issues of term translation in the context of document-informed SMT and propose three corresponding models : ( a ) a term translation disambiguation model which selects desirable translations for terms in the source language with domain information , ( b ) a term translation consistency model that encourages consistent translations for terms with a high strength of translation consistency throughout a document , and ( c ) a term bracketing model that rewards translation hypotheses where bracketable source terms are translated as a whole unit .	CL_D14-1060_pa_1	CL_D14-1060_pa_2	CL_D14-1060_pa_1_2
CL	D14-1060	pa	1	3	motivation_background	proposal_implementation	support	elaboration	secondary	secondary	none	none	Term translation is of great importance for statistical machine translation ( SMT ) , especially document-informed SMT .	We integrate the three models into hierarchical phrase-based SMT and evaluate their effectiveness on NIST Chinese-English translation tasks with large-scale training data .	CL_D14-1060_pa_1	CL_D14-1060_pa_3	CL_D14-1060_pa_1_3
CL	D14-1060	pa	1	4	motivation_background	result	support	support	secondary	secondary	none	none	Term translation is of great importance for statistical machine translation ( SMT ) , especially document-informed SMT .	Experiment results show that all three models can achieve significant improvements over the baseline .	CL_D14-1060_pa_1	CL_D14-1060_pa_4	CL_D14-1060_pa_1_4
CL	D14-1060	pa	1	5	motivation_background	result_means	support	elaboration	secondary	secondary	none	none	Term translation is of great importance for statistical machine translation ( SMT ) , especially document-informed SMT .	Additionally , we can obtain a further improvement when combining the three models .	CL_D14-1060_pa_1	CL_D14-1060_pa_5	CL_D14-1060_pa_1_5
CL	D14-1060	pa	2	3	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	In this paper , we investigate three issues of term translation in the context of document-informed SMT and propose three corresponding models : ( a ) a term translation disambiguation model which selects desirable translations for terms in the source language with domain information , ( b ) a term translation consistency model that encourages consistent translations for terms with a high strength of translation consistency throughout a document , and ( c ) a term bracketing model that rewards translation hypotheses where bracketable source terms are translated as a whole unit .	We integrate the three models into hierarchical phrase-based SMT and evaluate their effectiveness on NIST Chinese-English translation tasks with large-scale training data .	CL_D14-1060_pa_2	CL_D14-1060_pa_3	CL_D14-1060_pa_2_3
CL	D14-1060	pa	2	4	proposal	result	none	support	main	secondary	back	support	In this paper , we investigate three issues of term translation in the context of document-informed SMT and propose three corresponding models : ( a ) a term translation disambiguation model which selects desirable translations for terms in the source language with domain information , ( b ) a term translation consistency model that encourages consistent translations for terms with a high strength of translation consistency throughout a document , and ( c ) a term bracketing model that rewards translation hypotheses where bracketable source terms are translated as a whole unit .	Experiment results show that all three models can achieve significant improvements over the baseline .	CL_D14-1060_pa_2	CL_D14-1060_pa_4	CL_D14-1060_pa_2_4
CL	D14-1060	pa	2	5	proposal	result_means	none	elaboration	main	secondary	none	none	In this paper , we investigate three issues of term translation in the context of document-informed SMT and propose three corresponding models : ( a ) a term translation disambiguation model which selects desirable translations for terms in the source language with domain information , ( b ) a term translation consistency model that encourages consistent translations for terms with a high strength of translation consistency throughout a document , and ( c ) a term bracketing model that rewards translation hypotheses where bracketable source terms are translated as a whole unit .	Additionally , we can obtain a further improvement when combining the three models .	CL_D14-1060_pa_2	CL_D14-1060_pa_5	CL_D14-1060_pa_2_5
CL	D14-1060	pa	3	4	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	We integrate the three models into hierarchical phrase-based SMT and evaluate their effectiveness on NIST Chinese-English translation tasks with large-scale training data .	Experiment results show that all three models can achieve significant improvements over the baseline .	CL_D14-1060_pa_3	CL_D14-1060_pa_4	CL_D14-1060_pa_3_4
CL	D14-1060	pa	3	5	proposal_implementation	result_means	elaboration	elaboration	secondary	secondary	none	none	We integrate the three models into hierarchical phrase-based SMT and evaluate their effectiveness on NIST Chinese-English translation tasks with large-scale training data .	Additionally , we can obtain a further improvement when combining the three models .	CL_D14-1060_pa_3	CL_D14-1060_pa_5	CL_D14-1060_pa_3_5
CL	D14-1060	pa	4	5	result	result_means	support	elaboration	secondary	secondary	back	elaboration	Experiment results show that all three models can achieve significant improvements over the baseline .	Additionally , we can obtain a further improvement when combining the three models .	CL_D14-1060_pa_4	CL_D14-1060_pa_5	CL_D14-1060_pa_4_5
CL	D14-1060	pa	5	4	result_means	result	elaboration	support	secondary	secondary	forw	elaboration	Additionally , we can obtain a further improvement when combining the three models .	Experiment results show that all three models can achieve significant improvements over the baseline .	CL_D14-1060_pa_5	CL_D14-1060_pa_4	CL_D14-1060_pa_4_5
CL	D14-1061	ab	1	2	proposal	proposal	none	elaboration	main	secondary	back	elaboration	Inspired by previous work , where decipherment is used to improve machine translation , we propose a new idea to combine word alignment and decipherment into a single learning process .	We use EM to estimate the model parameters , not only to maximize the probability of parallel corpus , but also the monolingual corpus .	CL_D14-1061_ab_1	CL_D14-1061_ab_2	CL_D14-1061_ab_1_2
CL	D14-1061	ab	1	3	proposal	proposal	none	elaboration	main	secondary	none	none	Inspired by previous work , where decipherment is used to improve machine translation , we propose a new idea to combine word alignment and decipherment into a single learning process .	We apply our approach to improve Malagasy-English machine translation , where only a small amount of parallel data is available .	CL_D14-1061_ab_1	CL_D14-1061_ab_3	CL_D14-1061_ab_1_3
CL	D14-1061	ab	1	4	proposal	observation	none	support	main	secondary	back	support	Inspired by previous work , where decipherment is used to improve machine translation , we propose a new idea to combine word alignment and decipherment into a single learning process .	In our experiments , we observe gains of 0.9 to 2.1 Bleu over a strong baseline .	CL_D14-1061_ab_1	CL_D14-1061_ab_4	CL_D14-1061_ab_1_4
CL	D14-1061	ab	2	3	proposal	proposal	elaboration	elaboration	secondary	secondary	back	elaboration	We use EM to estimate the model parameters , not only to maximize the probability of parallel corpus , but also the monolingual corpus .	We apply our approach to improve Malagasy-English machine translation , where only a small amount of parallel data is available .	CL_D14-1061_ab_2	CL_D14-1061_ab_3	CL_D14-1061_ab_2_3
CL	D14-1061	ab	2	4	proposal	observation	elaboration	support	secondary	secondary	none	none	We use EM to estimate the model parameters , not only to maximize the probability of parallel corpus , but also the monolingual corpus .	In our experiments , we observe gains of 0.9 to 2.1 Bleu over a strong baseline .	CL_D14-1061_ab_2	CL_D14-1061_ab_4	CL_D14-1061_ab_2_4
CL	D14-1061	ab	3	4	proposal	observation	elaboration	support	secondary	secondary	none	none	We apply our approach to improve Malagasy-English machine translation , where only a small amount of parallel data is available .	In our experiments , we observe gains of 0.9 to 2.1 Bleu over a strong baseline .	CL_D14-1061_ab_3	CL_D14-1061_ab_4	CL_D14-1061_ab_3_4
CL	D14-1061	ab	4	3	observation	proposal	support	elaboration	secondary	secondary	none	none	In our experiments , we observe gains of 0.9 to 2.1 Bleu over a strong baseline .	We apply our approach to improve Malagasy-English machine translation , where only a small amount of parallel data is available .	CL_D14-1061_ab_4	CL_D14-1061_ab_3	CL_D14-1061_ab_3_4
CL	D14-1061	mn	1	2	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	Inspired by previous work , where decipherment is used to improve machine translation , we propose a new idea to combine word alignment and decipherment into a single learning process .	We use EM to estimate the model parameters , not only to maximize the probability of parallel corpus , but also the monolingual corpus .	CL_D14-1061_mn_1	CL_D14-1061_mn_2	CL_D14-1061_mn_1_2
CL	D14-1061	mn	1	3	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	Inspired by previous work , where decipherment is used to improve machine translation , we propose a new idea to combine word alignment and decipherment into a single learning process .	We apply our approach to improve Malagasy-English machine translation , where only a small amount of parallel data is available .	CL_D14-1061_mn_1	CL_D14-1061_mn_3	CL_D14-1061_mn_1_3
CL	D14-1061	mn	1	4	proposal	observation	none	support	main	secondary	back	support	Inspired by previous work , where decipherment is used to improve machine translation , we propose a new idea to combine word alignment and decipherment into a single learning process .	In our experiments , we observe gains of 0.9 to 2.1 Bleu over a strong baseline .	CL_D14-1061_mn_1	CL_D14-1061_mn_4	CL_D14-1061_mn_1_4
CL	D14-1061	mn	2	3	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	We use EM to estimate the model parameters , not only to maximize the probability of parallel corpus , but also the monolingual corpus .	We apply our approach to improve Malagasy-English machine translation , where only a small amount of parallel data is available .	CL_D14-1061_mn_2	CL_D14-1061_mn_3	CL_D14-1061_mn_2_3
CL	D14-1061	mn	2	4	proposal_implementation	observation	elaboration	support	secondary	secondary	none	none	We use EM to estimate the model parameters , not only to maximize the probability of parallel corpus , but also the monolingual corpus .	In our experiments , we observe gains of 0.9 to 2.1 Bleu over a strong baseline .	CL_D14-1061_mn_2	CL_D14-1061_mn_4	CL_D14-1061_mn_2_4
CL	D14-1061	mn	3	4	proposal_implementation	observation	elaboration	support	secondary	secondary	none	none	We apply our approach to improve Malagasy-English machine translation , where only a small amount of parallel data is available .	In our experiments , we observe gains of 0.9 to 2.1 Bleu over a strong baseline .	CL_D14-1061_mn_3	CL_D14-1061_mn_4	CL_D14-1061_mn_3_4
CL	D14-1061	mn	4	3	observation	proposal_implementation	support	elaboration	secondary	secondary	none	none	In our experiments , we observe gains of 0.9 to 2.1 Bleu over a strong baseline .	We apply our approach to improve Malagasy-English machine translation , where only a small amount of parallel data is available .	CL_D14-1061_mn_4	CL_D14-1061_mn_3	CL_D14-1061_mn_3_4
CL	D14-1061	pa	1	2	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	Inspired by previous work , where decipherment is used to improve machine translation , we propose a new idea to combine word alignment and decipherment into a single learning process .	We use EM to estimate the model parameters , not only to maximize the probability of parallel corpus , but also the monolingual corpus .	CL_D14-1061_pa_1	CL_D14-1061_pa_2	CL_D14-1061_pa_1_2
CL	D14-1061	pa	1	3	proposal	proposal	none	elaboration	main	secondary	back	elaboration	Inspired by previous work , where decipherment is used to improve machine translation , we propose a new idea to combine word alignment and decipherment into a single learning process .	We apply our approach to improve Malagasy-English machine translation , where only a small amount of parallel data is available .	CL_D14-1061_pa_1	CL_D14-1061_pa_3	CL_D14-1061_pa_1_3
CL	D14-1061	pa	1	4	proposal	observation	none	support	main	secondary	back	support	Inspired by previous work , where decipherment is used to improve machine translation , we propose a new idea to combine word alignment and decipherment into a single learning process .	In our experiments , we observe gains of 0.9 to 2.1 Bleu over a strong baseline .	CL_D14-1061_pa_1	CL_D14-1061_pa_4	CL_D14-1061_pa_1_4
CL	D14-1061	pa	2	3	proposal_implementation	proposal	elaboration	elaboration	secondary	secondary	none	none	We use EM to estimate the model parameters , not only to maximize the probability of parallel corpus , but also the monolingual corpus .	We apply our approach to improve Malagasy-English machine translation , where only a small amount of parallel data is available .	CL_D14-1061_pa_2	CL_D14-1061_pa_3	CL_D14-1061_pa_2_3
CL	D14-1061	pa	2	4	proposal_implementation	observation	elaboration	support	secondary	secondary	none	none	We use EM to estimate the model parameters , not only to maximize the probability of parallel corpus , but also the monolingual corpus .	In our experiments , we observe gains of 0.9 to 2.1 Bleu over a strong baseline .	CL_D14-1061_pa_2	CL_D14-1061_pa_4	CL_D14-1061_pa_2_4
CL	D14-1061	pa	3	4	proposal	observation	elaboration	support	secondary	secondary	none	none	We apply our approach to improve Malagasy-English machine translation , where only a small amount of parallel data is available .	In our experiments , we observe gains of 0.9 to 2.1 Bleu over a strong baseline .	CL_D14-1061_pa_3	CL_D14-1061_pa_4	CL_D14-1061_pa_3_4
CL	D14-1061	pa	4	3	observation	proposal	support	elaboration	secondary	secondary	none	none	In our experiments , we observe gains of 0.9 to 2.1 Bleu over a strong baseline .	We apply our approach to improve Malagasy-English machine translation , where only a small amount of parallel data is available .	CL_D14-1061_pa_4	CL_D14-1061_pa_3	CL_D14-1061_pa_3_4
CL	D14-1062	ab	1	2	motivation_problem	proposal	support	none	secondary	main	forw	support	Phrase-based models directly trained on mix-of-domain corpora can be sub-optimal.	In this paper we equip phrase-based models with a latent domain variable and present a novel method for adapting them to an in-domain task represented by a seed corpus .	CL_D14-1062_ab_1	CL_D14-1062_ab_2	CL_D14-1062_ab_1_2
CL	D14-1062	ab	1	3	motivation_problem	proposal_implementation	support	elaboration	secondary	secondary	none	none	Phrase-based models directly trained on mix-of-domain corpora can be sub-optimal.	We derive an EM algorithm which alternates between inducing domain-focused phrase pair estimates , and weights for mix-domain sentence pairs reflecting their relevance for the in-domain task .	CL_D14-1062_ab_1	CL_D14-1062_ab_3	CL_D14-1062_ab_1_3
CL	D14-1062	ab	1	4	motivation_problem	proposal_implementation	support	elaboration	secondary	secondary	none	none	Phrase-based models directly trained on mix-of-domain corpora can be sub-optimal.	By embedding our latent domain phrase model in a sentence-level model and training the two in tandem , we are able to adapt all core translation components together - phrase , lexical and reordering .	CL_D14-1062_ab_1	CL_D14-1062_ab_4	CL_D14-1062_ab_1_4
CL	D14-1062	ab	1	5	motivation_problem	result	support	support	secondary	secondary	none	none	Phrase-based models directly trained on mix-of-domain corpora can be sub-optimal.	We show experiments on weighing sentence pairs for relevance as well as adapting phrase-based models , showing significant performance improvement in both tasks .	CL_D14-1062_ab_1	CL_D14-1062_ab_5	CL_D14-1062_ab_1_5
CL	D14-1062	ab	2	3	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	In this paper we equip phrase-based models with a latent domain variable and present a novel method for adapting them to an in-domain task represented by a seed corpus .	We derive an EM algorithm which alternates between inducing domain-focused phrase pair estimates , and weights for mix-domain sentence pairs reflecting their relevance for the in-domain task .	CL_D14-1062_ab_2	CL_D14-1062_ab_3	CL_D14-1062_ab_2_3
CL	D14-1062	ab	2	4	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	In this paper we equip phrase-based models with a latent domain variable and present a novel method for adapting them to an in-domain task represented by a seed corpus .	By embedding our latent domain phrase model in a sentence-level model and training the two in tandem , we are able to adapt all core translation components together - phrase , lexical and reordering .	CL_D14-1062_ab_2	CL_D14-1062_ab_4	CL_D14-1062_ab_2_4
CL	D14-1062	ab	2	5	proposal	result	none	support	main	secondary	back	support	In this paper we equip phrase-based models with a latent domain variable and present a novel method for adapting them to an in-domain task represented by a seed corpus .	We show experiments on weighing sentence pairs for relevance as well as adapting phrase-based models , showing significant performance improvement in both tasks .	CL_D14-1062_ab_2	CL_D14-1062_ab_5	CL_D14-1062_ab_2_5
CL	D14-1062	ab	3	4	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	We derive an EM algorithm which alternates between inducing domain-focused phrase pair estimates , and weights for mix-domain sentence pairs reflecting their relevance for the in-domain task .	By embedding our latent domain phrase model in a sentence-level model and training the two in tandem , we are able to adapt all core translation components together - phrase , lexical and reordering .	CL_D14-1062_ab_3	CL_D14-1062_ab_4	CL_D14-1062_ab_3_4
CL	D14-1062	ab	3	5	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	We derive an EM algorithm which alternates between inducing domain-focused phrase pair estimates , and weights for mix-domain sentence pairs reflecting their relevance for the in-domain task .	We show experiments on weighing sentence pairs for relevance as well as adapting phrase-based models , showing significant performance improvement in both tasks .	CL_D14-1062_ab_3	CL_D14-1062_ab_5	CL_D14-1062_ab_3_5
CL	D14-1062	ab	4	5	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	By embedding our latent domain phrase model in a sentence-level model and training the two in tandem , we are able to adapt all core translation components together - phrase , lexical and reordering .	We show experiments on weighing sentence pairs for relevance as well as adapting phrase-based models , showing significant performance improvement in both tasks .	CL_D14-1062_ab_4	CL_D14-1062_ab_5	CL_D14-1062_ab_4_5
CL	D14-1062	ab	5	4	result	proposal_implementation	support	elaboration	secondary	secondary	none	none	We show experiments on weighing sentence pairs for relevance as well as adapting phrase-based models , showing significant performance improvement in both tasks .	By embedding our latent domain phrase model in a sentence-level model and training the two in tandem , we are able to adapt all core translation components together - phrase , lexical and reordering .	CL_D14-1062_ab_5	CL_D14-1062_ab_4	CL_D14-1062_ab_4_5
CL	D14-1062	mn	1	2	motivation_problem	proposal	support	none	secondary	main	forw	support	Phrase-based models directly trained on mix-of-domain corpora can be sub-optimal.	In this paper we equip phrase-based models with a latent domain variable and present a novel method for adapting them to an in-domain task represented by a seed corpus .	CL_D14-1062_mn_1	CL_D14-1062_mn_2	CL_D14-1062_mn_1_2
CL	D14-1062	mn	1	3	motivation_problem	proposal_implementation	support	elaboration	secondary	secondary	none	none	Phrase-based models directly trained on mix-of-domain corpora can be sub-optimal.	We derive an EM algorithm which alternates between inducing domain-focused phrase pair estimates , and weights for mix-domain sentence pairs reflecting their relevance for the in-domain task .	CL_D14-1062_mn_1	CL_D14-1062_mn_3	CL_D14-1062_mn_1_3
CL	D14-1062	mn	1	4	motivation_problem	proposal_implementation	support	elaboration	secondary	secondary	none	none	Phrase-based models directly trained on mix-of-domain corpora can be sub-optimal.	By embedding our latent domain phrase model in a sentence-level model and training the two in tandem , we are able to adapt all core translation components together - phrase , lexical and reordering .	CL_D14-1062_mn_1	CL_D14-1062_mn_4	CL_D14-1062_mn_1_4
CL	D14-1062	mn	1	5	motivation_problem	result	support	support	secondary	secondary	none	none	Phrase-based models directly trained on mix-of-domain corpora can be sub-optimal.	We show experiments on weighing sentence pairs for relevance as well as adapting phrase-based models , showing significant performance improvement in both tasks .	CL_D14-1062_mn_1	CL_D14-1062_mn_5	CL_D14-1062_mn_1_5
CL	D14-1062	mn	2	3	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	In this paper we equip phrase-based models with a latent domain variable and present a novel method for adapting them to an in-domain task represented by a seed corpus .	We derive an EM algorithm which alternates between inducing domain-focused phrase pair estimates , and weights for mix-domain sentence pairs reflecting their relevance for the in-domain task .	CL_D14-1062_mn_2	CL_D14-1062_mn_3	CL_D14-1062_mn_2_3
CL	D14-1062	mn	2	4	proposal	proposal_implementation	none	elaboration	main	secondary	none	none	In this paper we equip phrase-based models with a latent domain variable and present a novel method for adapting them to an in-domain task represented by a seed corpus .	By embedding our latent domain phrase model in a sentence-level model and training the two in tandem , we are able to adapt all core translation components together - phrase , lexical and reordering .	CL_D14-1062_mn_2	CL_D14-1062_mn_4	CL_D14-1062_mn_2_4
CL	D14-1062	mn	2	5	proposal	result	none	support	main	secondary	back	support	In this paper we equip phrase-based models with a latent domain variable and present a novel method for adapting them to an in-domain task represented by a seed corpus .	We show experiments on weighing sentence pairs for relevance as well as adapting phrase-based models , showing significant performance improvement in both tasks .	CL_D14-1062_mn_2	CL_D14-1062_mn_5	CL_D14-1062_mn_2_5
CL	D14-1062	mn	3	4	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	back	elaboration	We derive an EM algorithm which alternates between inducing domain-focused phrase pair estimates , and weights for mix-domain sentence pairs reflecting their relevance for the in-domain task .	By embedding our latent domain phrase model in a sentence-level model and training the two in tandem , we are able to adapt all core translation components together - phrase , lexical and reordering .	CL_D14-1062_mn_3	CL_D14-1062_mn_4	CL_D14-1062_mn_3_4
CL	D14-1062	mn	3	5	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	We derive an EM algorithm which alternates between inducing domain-focused phrase pair estimates , and weights for mix-domain sentence pairs reflecting their relevance for the in-domain task .	We show experiments on weighing sentence pairs for relevance as well as adapting phrase-based models , showing significant performance improvement in both tasks .	CL_D14-1062_mn_3	CL_D14-1062_mn_5	CL_D14-1062_mn_3_5
CL	D14-1062	mn	4	5	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	By embedding our latent domain phrase model in a sentence-level model and training the two in tandem , we are able to adapt all core translation components together - phrase , lexical and reordering .	We show experiments on weighing sentence pairs for relevance as well as adapting phrase-based models , showing significant performance improvement in both tasks .	CL_D14-1062_mn_4	CL_D14-1062_mn_5	CL_D14-1062_mn_4_5
CL	D14-1062	mn	5	4	result	proposal_implementation	support	elaboration	secondary	secondary	none	none	We show experiments on weighing sentence pairs for relevance as well as adapting phrase-based models , showing significant performance improvement in both tasks .	By embedding our latent domain phrase model in a sentence-level model and training the two in tandem , we are able to adapt all core translation components together - phrase , lexical and reordering .	CL_D14-1062_mn_5	CL_D14-1062_mn_4	CL_D14-1062_mn_4_5
CL	D14-1062	pa	1	2	motivation_problem	proposal	support	none	secondary	main	forw	support	Phrase-based models directly trained on mix-of-domain corpora can be sub-optimal.	In this paper we equip phrase-based models with a latent domain variable and present a novel method for adapting them to an in-domain task represented by a seed corpus .	CL_D14-1062_pa_1	CL_D14-1062_pa_2	CL_D14-1062_pa_1_2
CL	D14-1062	pa	1	3	motivation_problem	proposal_implementation	support	elaboration	secondary	secondary	none	none	Phrase-based models directly trained on mix-of-domain corpora can be sub-optimal.	We derive an EM algorithm which alternates between inducing domain-focused phrase pair estimates , and weights for mix-domain sentence pairs reflecting their relevance for the in-domain task .	CL_D14-1062_pa_1	CL_D14-1062_pa_3	CL_D14-1062_pa_1_3
CL	D14-1062	pa	1	4	motivation_problem	proposal_implementation	support	elaboration	secondary	secondary	none	none	Phrase-based models directly trained on mix-of-domain corpora can be sub-optimal.	By embedding our latent domain phrase model in a sentence-level model and training the two in tandem , we are able to adapt all core translation components together - phrase , lexical and reordering .	CL_D14-1062_pa_1	CL_D14-1062_pa_4	CL_D14-1062_pa_1_4
CL	D14-1062	pa	1	5	motivation_problem	result_means	support	support	secondary	secondary	none	none	Phrase-based models directly trained on mix-of-domain corpora can be sub-optimal.	We show experiments on weighing sentence pairs for relevance as well as adapting phrase-based models , showing significant performance improvement in both tasks .	CL_D14-1062_pa_1	CL_D14-1062_pa_5	CL_D14-1062_pa_1_5
CL	D14-1062	pa	2	3	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	In this paper we equip phrase-based models with a latent domain variable and present a novel method for adapting them to an in-domain task represented by a seed corpus .	We derive an EM algorithm which alternates between inducing domain-focused phrase pair estimates , and weights for mix-domain sentence pairs reflecting their relevance for the in-domain task .	CL_D14-1062_pa_2	CL_D14-1062_pa_3	CL_D14-1062_pa_2_3
CL	D14-1062	pa	2	4	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	In this paper we equip phrase-based models with a latent domain variable and present a novel method for adapting them to an in-domain task represented by a seed corpus .	By embedding our latent domain phrase model in a sentence-level model and training the two in tandem , we are able to adapt all core translation components together - phrase , lexical and reordering .	CL_D14-1062_pa_2	CL_D14-1062_pa_4	CL_D14-1062_pa_2_4
CL	D14-1062	pa	2	5	proposal	result_means	none	support	main	secondary	back	support	In this paper we equip phrase-based models with a latent domain variable and present a novel method for adapting them to an in-domain task represented by a seed corpus .	We show experiments on weighing sentence pairs for relevance as well as adapting phrase-based models , showing significant performance improvement in both tasks .	CL_D14-1062_pa_2	CL_D14-1062_pa_5	CL_D14-1062_pa_2_5
CL	D14-1062	pa	3	4	proposal_implementation	proposal_implementation	elaboration	elaboration	secondary	secondary	none	none	We derive an EM algorithm which alternates between inducing domain-focused phrase pair estimates , and weights for mix-domain sentence pairs reflecting their relevance for the in-domain task .	By embedding our latent domain phrase model in a sentence-level model and training the two in tandem , we are able to adapt all core translation components together - phrase , lexical and reordering .	CL_D14-1062_pa_3	CL_D14-1062_pa_4	CL_D14-1062_pa_3_4
CL	D14-1062	pa	3	5	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	We derive an EM algorithm which alternates between inducing domain-focused phrase pair estimates , and weights for mix-domain sentence pairs reflecting their relevance for the in-domain task .	We show experiments on weighing sentence pairs for relevance as well as adapting phrase-based models , showing significant performance improvement in both tasks .	CL_D14-1062_pa_3	CL_D14-1062_pa_5	CL_D14-1062_pa_3_5
CL	D14-1062	pa	4	5	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	By embedding our latent domain phrase model in a sentence-level model and training the two in tandem , we are able to adapt all core translation components together - phrase , lexical and reordering .	We show experiments on weighing sentence pairs for relevance as well as adapting phrase-based models , showing significant performance improvement in both tasks .	CL_D14-1062_pa_4	CL_D14-1062_pa_5	CL_D14-1062_pa_4_5
CL	D14-1062	pa	5	4	result_means	proposal_implementation	support	elaboration	secondary	secondary	none	none	We show experiments on weighing sentence pairs for relevance as well as adapting phrase-based models , showing significant performance improvement in both tasks .	By embedding our latent domain phrase model in a sentence-level model and training the two in tandem , we are able to adapt all core translation components together - phrase , lexical and reordering .	CL_D14-1062_pa_5	CL_D14-1062_pa_4	CL_D14-1062_pa_4_5
CL	D14-1063	ab	1	2	motivation_background	motivation_background	support	elaboration	secondary	secondary	back	elaboration	In Corpus-Based Machine Translation , the search space of the translation candidates for a given input sentence is often defined by a set of ( cycle-free ) context-free grammar rules .	This happens naturally in Syntax-Based Machine Translation and Hierarchical Phrase-Based Machine Translation ( where the representation will be the set of the target-side half of the synchronous rules used to parse the input sentence ) .	CL_D14-1063_ab_1	CL_D14-1063_ab_2	CL_D14-1063_ab_1_2
CL	D14-1063	ab	1	3	motivation_background	motivation_hypothesis	support	elaboration	secondary	secondary	none	none	In Corpus-Based Machine Translation , the search space of the translation candidates for a given input sentence is often defined by a set of ( cycle-free ) context-free grammar rules .	But it is also possible to describe Phrase-Based Machine Translation in this framework .	CL_D14-1063_ab_1	CL_D14-1063_ab_3	CL_D14-1063_ab_1_3
CL	D14-1063	ab	1	4	motivation_background	proposal	support	none	secondary	main	forw	support	In Corpus-Based Machine Translation , the search space of the translation candidates for a given input sentence is often defined by a set of ( cycle-free ) context-free grammar rules .	We propose a natural extension to this representation by using lattice-rules that allow to easily encode an exponential number of variations of each rules .	CL_D14-1063_ab_1	CL_D14-1063_ab_4	CL_D14-1063_ab_1_4
CL	D14-1063	ab	1	5	motivation_background	proposal	support	elaboration	secondary	secondary	none	none	In Corpus-Based Machine Translation , the search space of the translation candidates for a given input sentence is often defined by a set of ( cycle-free ) context-free grammar rules .	We also demonstrate how the representation of the search space has an impact on decoding efficiency , and how it is possible to optimize this representation .	CL_D14-1063_ab_1	CL_D14-1063_ab_5	CL_D14-1063_ab_1_5
CL	D14-1063	ab	2	3	motivation_background	motivation_hypothesis	elaboration	elaboration	secondary	secondary	back	elaboration	This happens naturally in Syntax-Based Machine Translation and Hierarchical Phrase-Based Machine Translation ( where the representation will be the set of the target-side half of the synchronous rules used to parse the input sentence ) .	But it is also possible to describe Phrase-Based Machine Translation in this framework .	CL_D14-1063_ab_2	CL_D14-1063_ab_3	CL_D14-1063_ab_2_3
CL	D14-1063	ab	2	4	motivation_background	proposal	elaboration	none	secondary	main	none	none	This happens naturally in Syntax-Based Machine Translation and Hierarchical Phrase-Based Machine Translation ( where the representation will be the set of the target-side half of the synchronous rules used to parse the input sentence ) .	We propose a natural extension to this representation by using lattice-rules that allow to easily encode an exponential number of variations of each rules .	CL_D14-1063_ab_2	CL_D14-1063_ab_4	CL_D14-1063_ab_2_4
CL	D14-1063	ab	2	5	motivation_background	proposal	elaboration	elaboration	secondary	secondary	none	none	This happens naturally in Syntax-Based Machine Translation and Hierarchical Phrase-Based Machine Translation ( where the representation will be the set of the target-side half of the synchronous rules used to parse the input sentence ) .	We also demonstrate how the representation of the search space has an impact on decoding efficiency , and how it is possible to optimize this representation .	CL_D14-1063_ab_2	CL_D14-1063_ab_5	CL_D14-1063_ab_2_5
CL	D14-1063	ab	3	4	motivation_hypothesis	proposal	elaboration	none	secondary	main	none	none	But it is also possible to describe Phrase-Based Machine Translation in this framework .	We propose a natural extension to this representation by using lattice-rules that allow to easily encode an exponential number of variations of each rules .	CL_D14-1063_ab_3	CL_D14-1063_ab_4	CL_D14-1063_ab_3_4
CL	D14-1063	ab	3	5	motivation_hypothesis	proposal	elaboration	elaboration	secondary	secondary	none	none	But it is also possible to describe Phrase-Based Machine Translation in this framework .	We also demonstrate how the representation of the search space has an impact on decoding efficiency , and how it is possible to optimize this representation .	CL_D14-1063_ab_3	CL_D14-1063_ab_5	CL_D14-1063_ab_3_5
CL	D14-1063	ab	4	5	proposal	proposal	none	elaboration	main	secondary	back	elaboration	We propose a natural extension to this representation by using lattice-rules that allow to easily encode an exponential number of variations of each rules .	We also demonstrate how the representation of the search space has an impact on decoding efficiency , and how it is possible to optimize this representation .	CL_D14-1063_ab_4	CL_D14-1063_ab_5	CL_D14-1063_ab_4_5
CL	D14-1063	ab	5	4	proposal	proposal	elaboration	none	secondary	main	forw	elaboration	We also demonstrate how the representation of the search space has an impact on decoding efficiency , and how it is possible to optimize this representation .	We propose a natural extension to this representation by using lattice-rules that allow to easily encode an exponential number of variations of each rules .	CL_D14-1063_ab_5	CL_D14-1063_ab_4	CL_D14-1063_ab_4_5
CL	D14-1063	mn	1	2	motivation_background	motivation_background	info-required	info-required	secondary	secondary	forw	info-required	In Corpus-Based Machine Translation , the search space of the translation candidates for a given input sentence is often defined by a set of ( cycle-free ) context-free grammar rules .	This happens naturally in Syntax-Based Machine Translation and Hierarchical Phrase-Based Machine Translation ( where the representation will be the set of the target-side half of the synchronous rules used to parse the input sentence ) .	CL_D14-1063_mn_1	CL_D14-1063_mn_2	CL_D14-1063_mn_1_2
CL	D14-1063	mn	1	3	motivation_background	motivation_background	info-required	support	secondary	secondary	none	none	In Corpus-Based Machine Translation , the search space of the translation candidates for a given input sentence is often defined by a set of ( cycle-free ) context-free grammar rules .	But it is also possible to describe Phrase-Based Machine Translation in this framework .	CL_D14-1063_mn_1	CL_D14-1063_mn_3	CL_D14-1063_mn_1_3
CL	D14-1063	mn	1	4	motivation_background	proposal	info-required	none	secondary	main	none	none	In Corpus-Based Machine Translation , the search space of the translation candidates for a given input sentence is often defined by a set of ( cycle-free ) context-free grammar rules .	We propose a natural extension to this representation by using lattice-rules that allow to easily encode an exponential number of variations of each rules .	CL_D14-1063_mn_1	CL_D14-1063_mn_4	CL_D14-1063_mn_1_4
CL	D14-1063	mn	1	5	motivation_background	conclusion	info-required	support	secondary	secondary	none	none	In Corpus-Based Machine Translation , the search space of the translation candidates for a given input sentence is often defined by a set of ( cycle-free ) context-free grammar rules .	We also demonstrate how the representation of the search space has an impact on decoding efficiency , and how it is possible to optimize this representation .	CL_D14-1063_mn_1	CL_D14-1063_mn_5	CL_D14-1063_mn_1_5
CL	D14-1063	mn	2	3	motivation_background	motivation_background	info-required	support	secondary	secondary	forw	info-required	This happens naturally in Syntax-Based Machine Translation and Hierarchical Phrase-Based Machine Translation ( where the representation will be the set of the target-side half of the synchronous rules used to parse the input sentence ) .	But it is also possible to describe Phrase-Based Machine Translation in this framework .	CL_D14-1063_mn_2	CL_D14-1063_mn_3	CL_D14-1063_mn_2_3
CL	D14-1063	mn	2	4	motivation_background	proposal	info-required	none	secondary	main	none	none	This happens naturally in Syntax-Based Machine Translation and Hierarchical Phrase-Based Machine Translation ( where the representation will be the set of the target-side half of the synchronous rules used to parse the input sentence ) .	We propose a natural extension to this representation by using lattice-rules that allow to easily encode an exponential number of variations of each rules .	CL_D14-1063_mn_2	CL_D14-1063_mn_4	CL_D14-1063_mn_2_4
CL	D14-1063	mn	2	5	motivation_background	conclusion	info-required	support	secondary	secondary	none	none	This happens naturally in Syntax-Based Machine Translation and Hierarchical Phrase-Based Machine Translation ( where the representation will be the set of the target-side half of the synchronous rules used to parse the input sentence ) .	We also demonstrate how the representation of the search space has an impact on decoding efficiency , and how it is possible to optimize this representation .	CL_D14-1063_mn_2	CL_D14-1063_mn_5	CL_D14-1063_mn_2_5
CL	D14-1063	mn	3	4	motivation_background	proposal	support	none	secondary	main	forw	support	But it is also possible to describe Phrase-Based Machine Translation in this framework .	We propose a natural extension to this representation by using lattice-rules that allow to easily encode an exponential number of variations of each rules .	CL_D14-1063_mn_3	CL_D14-1063_mn_4	CL_D14-1063_mn_3_4
CL	D14-1063	mn	3	5	motivation_background	conclusion	support	support	secondary	secondary	none	none	But it is also possible to describe Phrase-Based Machine Translation in this framework .	We also demonstrate how the representation of the search space has an impact on decoding efficiency , and how it is possible to optimize this representation .	CL_D14-1063_mn_3	CL_D14-1063_mn_5	CL_D14-1063_mn_3_5
CL	D14-1063	mn	4	5	proposal	conclusion	none	support	main	secondary	back	support	We propose a natural extension to this representation by using lattice-rules that allow to easily encode an exponential number of variations of each rules .	We also demonstrate how the representation of the search space has an impact on decoding efficiency , and how it is possible to optimize this representation .	CL_D14-1063_mn_4	CL_D14-1063_mn_5	CL_D14-1063_mn_4_5
CL	D14-1063	mn	5	4	conclusion	proposal	support	none	secondary	main	forw	support	We also demonstrate how the representation of the search space has an impact on decoding efficiency , and how it is possible to optimize this representation .	We propose a natural extension to this representation by using lattice-rules that allow to easily encode an exponential number of variations of each rules .	CL_D14-1063_mn_5	CL_D14-1063_mn_4	CL_D14-1063_mn_4_5
CL	D14-1063	pa	1	2	motivation_background	motivation_background	info-required	info-required	secondary	secondary	forw	info-required	In Corpus-Based Machine Translation , the search space of the translation candidates for a given input sentence is often defined by a set of ( cycle-free ) context-free grammar rules .	This happens naturally in Syntax-Based Machine Translation and Hierarchical Phrase-Based Machine Translation ( where the representation will be the set of the target-side half of the synchronous rules used to parse the input sentence ) .	CL_D14-1063_pa_1	CL_D14-1063_pa_2	CL_D14-1063_pa_1_2
CL	D14-1063	pa	1	3	motivation_background	motivation_background	info-required	support	secondary	secondary	none	none	In Corpus-Based Machine Translation , the search space of the translation candidates for a given input sentence is often defined by a set of ( cycle-free ) context-free grammar rules .	But it is also possible to describe Phrase-Based Machine Translation in this framework .	CL_D14-1063_pa_1	CL_D14-1063_pa_3	CL_D14-1063_pa_1_3
CL	D14-1063	pa	1	4	motivation_background	proposal	info-required	none	secondary	main	none	none	In Corpus-Based Machine Translation , the search space of the translation candidates for a given input sentence is often defined by a set of ( cycle-free ) context-free grammar rules .	We propose a natural extension to this representation by using lattice-rules that allow to easily encode an exponential number of variations of each rules .	CL_D14-1063_pa_1	CL_D14-1063_pa_4	CL_D14-1063_pa_1_4
CL	D14-1063	pa	1	5	motivation_background	conclusion	info-required	support	secondary	secondary	none	none	In Corpus-Based Machine Translation , the search space of the translation candidates for a given input sentence is often defined by a set of ( cycle-free ) context-free grammar rules .	We also demonstrate how the representation of the search space has an impact on decoding efficiency , and how it is possible to optimize this representation .	CL_D14-1063_pa_1	CL_D14-1063_pa_5	CL_D14-1063_pa_1_5
CL	D14-1063	pa	2	3	motivation_background	motivation_background	info-required	support	secondary	secondary	forw	info-required	This happens naturally in Syntax-Based Machine Translation and Hierarchical Phrase-Based Machine Translation ( where the representation will be the set of the target-side half of the synchronous rules used to parse the input sentence ) .	But it is also possible to describe Phrase-Based Machine Translation in this framework .	CL_D14-1063_pa_2	CL_D14-1063_pa_3	CL_D14-1063_pa_2_3
CL	D14-1063	pa	2	4	motivation_background	proposal	info-required	none	secondary	main	none	none	This happens naturally in Syntax-Based Machine Translation and Hierarchical Phrase-Based Machine Translation ( where the representation will be the set of the target-side half of the synchronous rules used to parse the input sentence ) .	We propose a natural extension to this representation by using lattice-rules that allow to easily encode an exponential number of variations of each rules .	CL_D14-1063_pa_2	CL_D14-1063_pa_4	CL_D14-1063_pa_2_4
CL	D14-1063	pa	2	5	motivation_background	conclusion	info-required	support	secondary	secondary	none	none	This happens naturally in Syntax-Based Machine Translation and Hierarchical Phrase-Based Machine Translation ( where the representation will be the set of the target-side half of the synchronous rules used to parse the input sentence ) .	We also demonstrate how the representation of the search space has an impact on decoding efficiency , and how it is possible to optimize this representation .	CL_D14-1063_pa_2	CL_D14-1063_pa_5	CL_D14-1063_pa_2_5
CL	D14-1063	pa	3	4	motivation_background	proposal	support	none	secondary	main	forw	support	But it is also possible to describe Phrase-Based Machine Translation in this framework .	We propose a natural extension to this representation by using lattice-rules that allow to easily encode an exponential number of variations of each rules .	CL_D14-1063_pa_3	CL_D14-1063_pa_4	CL_D14-1063_pa_3_4
CL	D14-1063	pa	3	5	motivation_background	conclusion	support	support	secondary	secondary	none	none	But it is also possible to describe Phrase-Based Machine Translation in this framework .	We also demonstrate how the representation of the search space has an impact on decoding efficiency , and how it is possible to optimize this representation .	CL_D14-1063_pa_3	CL_D14-1063_pa_5	CL_D14-1063_pa_3_5
CL	D14-1063	pa	4	5	proposal	conclusion	none	support	main	secondary	back	support	We propose a natural extension to this representation by using lattice-rules that allow to easily encode an exponential number of variations of each rules .	We also demonstrate how the representation of the search space has an impact on decoding efficiency , and how it is possible to optimize this representation .	CL_D14-1063_pa_4	CL_D14-1063_pa_5	CL_D14-1063_pa_4_5
CL	D14-1063	pa	5	4	conclusion	proposal	support	none	secondary	main	forw	support	We also demonstrate how the representation of the search space has an impact on decoding efficiency , and how it is possible to optimize this representation .	We propose a natural extension to this representation by using lattice-rules that allow to easily encode an exponential number of variations of each rules .	CL_D14-1063_pa_5	CL_D14-1063_pa_4	CL_D14-1063_pa_4_5
CL	D14-1064	ab	1	2	motivation_background	motivation_hypothesis	info-required	support	secondary	secondary	forw	info-required	When documents and queries are presented in different languages , the common approach is to translate the query into the document language .	While there are a variety of query translation approaches , recent research suggests that combining multiple methods into a single "structured query" is the most effective .	CL_D14-1064_ab_1	CL_D14-1064_ab_2	CL_D14-1064_ab_1_2
CL	D14-1064	ab	1	3	motivation_background	proposal	info-required	none	secondary	main	none	none	When documents and queries are presented in different languages , the common approach is to translate the query into the document language .	In this paper , we introduce a novel approach for producing a unique combination recipe for each query , as it has also been shown that the optimal combination weights differ substantially across queries and other task specifics .	CL_D14-1064_ab_1	CL_D14-1064_ab_3	CL_D14-1064_ab_1_3
CL	D14-1064	ab	1	4	motivation_background	result	info-required	support	secondary	secondary	none	none	When documents and queries are presented in different languages , the common approach is to translate the query into the document language .	Our query-specific combination method generates statistically significant improvements over other combination strategies presented in the literature , such as uniform and task-specific weighting .	CL_D14-1064_ab_1	CL_D14-1064_ab_4	CL_D14-1064_ab_1_4
CL	D14-1064	ab	1	5	motivation_background	conclusion	info-required	support	secondary	secondary	none	none	When documents and queries are presented in different languages , the common approach is to translate the query into the document language .	An in-depth empirical analysis presents insights about the effect of data size , domain differences , labeling and tuning on the end performance of our approach .	CL_D14-1064_ab_1	CL_D14-1064_ab_5	CL_D14-1064_ab_1_5
CL	D14-1064	ab	2	3	motivation_hypothesis	proposal	support	none	secondary	main	forw	support	While there are a variety of query translation approaches , recent research suggests that combining multiple methods into a single "structured query" is the most effective .	In this paper , we introduce a novel approach for producing a unique combination recipe for each query , as it has also been shown that the optimal combination weights differ substantially across queries and other task specifics .	CL_D14-1064_ab_2	CL_D14-1064_ab_3	CL_D14-1064_ab_2_3
CL	D14-1064	ab	2	4	motivation_hypothesis	result	support	support	secondary	secondary	none	none	While there are a variety of query translation approaches , recent research suggests that combining multiple methods into a single "structured query" is the most effective .	Our query-specific combination method generates statistically significant improvements over other combination strategies presented in the literature , such as uniform and task-specific weighting .	CL_D14-1064_ab_2	CL_D14-1064_ab_4	CL_D14-1064_ab_2_4
CL	D14-1064	ab	2	5	motivation_hypothesis	conclusion	support	support	secondary	secondary	none	none	While there are a variety of query translation approaches , recent research suggests that combining multiple methods into a single "structured query" is the most effective .	An in-depth empirical analysis presents insights about the effect of data size , domain differences , labeling and tuning on the end performance of our approach .	CL_D14-1064_ab_2	CL_D14-1064_ab_5	CL_D14-1064_ab_2_5
CL	D14-1064	ab	3	4	proposal	result	none	support	main	secondary	none	none	In this paper , we introduce a novel approach for producing a unique combination recipe for each query , as it has also been shown that the optimal combination weights differ substantially across queries and other task specifics .	Our query-specific combination method generates statistically significant improvements over other combination strategies presented in the literature , such as uniform and task-specific weighting .	CL_D14-1064_ab_3	CL_D14-1064_ab_4	CL_D14-1064_ab_3_4
CL	D14-1064	ab	3	5	proposal	conclusion	none	support	main	secondary	back	support	In this paper , we introduce a novel approach for producing a unique combination recipe for each query , as it has also been shown that the optimal combination weights differ substantially across queries and other task specifics .	An in-depth empirical analysis presents insights about the effect of data size , domain differences , labeling and tuning on the end performance of our approach .	CL_D14-1064_ab_3	CL_D14-1064_ab_5	CL_D14-1064_ab_3_5
CL	D14-1064	ab	4	5	result	conclusion	support	support	secondary	secondary	forw	support	Our query-specific combination method generates statistically significant improvements over other combination strategies presented in the literature , such as uniform and task-specific weighting .	An in-depth empirical analysis presents insights about the effect of data size , domain differences , labeling and tuning on the end performance of our approach .	CL_D14-1064_ab_4	CL_D14-1064_ab_5	CL_D14-1064_ab_4_5
CL	D14-1064	ab	5	4	conclusion	result	support	support	secondary	secondary	back	support	An in-depth empirical analysis presents insights about the effect of data size , domain differences , labeling and tuning on the end performance of our approach .	Our query-specific combination method generates statistically significant improvements over other combination strategies presented in the literature , such as uniform and task-specific weighting .	CL_D14-1064_ab_5	CL_D14-1064_ab_4	CL_D14-1064_ab_4_5
CL	D14-1064	mn	1	2	motivation_background	motivation_background	info-required	support	secondary	secondary	forw	info-required	When documents and queries are presented in different languages , the common approach is to translate the query into the document language .	While there are a variety of query translation approaches , recent research suggests that combining multiple methods into a single "structured query" is the most effective .	CL_D14-1064_mn_1	CL_D14-1064_mn_2	CL_D14-1064_mn_1_2
CL	D14-1064	mn	1	3	motivation_background	proposal	info-required	none	secondary	main	none	none	When documents and queries are presented in different languages , the common approach is to translate the query into the document language .	In this paper , we introduce a novel approach for producing a unique combination recipe for each query , as it has also been shown that the optimal combination weights differ substantially across queries and other task specifics .	CL_D14-1064_mn_1	CL_D14-1064_mn_3	CL_D14-1064_mn_1_3
CL	D14-1064	mn	1	4	motivation_background	conclusion	info-required	support	secondary	secondary	none	none	When documents and queries are presented in different languages , the common approach is to translate the query into the document language .	Our query-specific combination method generates statistically significant improvements over other combination strategies presented in the literature , such as uniform and task-specific weighting .	CL_D14-1064_mn_1	CL_D14-1064_mn_4	CL_D14-1064_mn_1_4
CL	D14-1064	mn	1	5	motivation_background	conclusion	info-required	elaboration	secondary	secondary	none	none	When documents and queries are presented in different languages , the common approach is to translate the query into the document language .	An in-depth empirical analysis presents insights about the effect of data size , domain differences , labeling and tuning on the end performance of our approach .	CL_D14-1064_mn_1	CL_D14-1064_mn_5	CL_D14-1064_mn_1_5
CL	D14-1064	mn	2	3	motivation_background	proposal	support	none	secondary	main	forw	support	While there are a variety of query translation approaches , recent research suggests that combining multiple methods into a single "structured query" is the most effective .	In this paper , we introduce a novel approach for producing a unique combination recipe for each query , as it has also been shown that the optimal combination weights differ substantially across queries and other task specifics .	CL_D14-1064_mn_2	CL_D14-1064_mn_3	CL_D14-1064_mn_2_3
CL	D14-1064	mn	2	4	motivation_background	conclusion	support	support	secondary	secondary	none	none	While there are a variety of query translation approaches , recent research suggests that combining multiple methods into a single "structured query" is the most effective .	Our query-specific combination method generates statistically significant improvements over other combination strategies presented in the literature , such as uniform and task-specific weighting .	CL_D14-1064_mn_2	CL_D14-1064_mn_4	CL_D14-1064_mn_2_4
CL	D14-1064	mn	2	5	motivation_background	conclusion	support	elaboration	secondary	secondary	none	none	While there are a variety of query translation approaches , recent research suggests that combining multiple methods into a single "structured query" is the most effective .	An in-depth empirical analysis presents insights about the effect of data size , domain differences , labeling and tuning on the end performance of our approach .	CL_D14-1064_mn_2	CL_D14-1064_mn_5	CL_D14-1064_mn_2_5
CL	D14-1064	mn	3	4	proposal	conclusion	none	support	main	secondary	back	support	In this paper , we introduce a novel approach for producing a unique combination recipe for each query , as it has also been shown that the optimal combination weights differ substantially across queries and other task specifics .	Our query-specific combination method generates statistically significant improvements over other combination strategies presented in the literature , such as uniform and task-specific weighting .	CL_D14-1064_mn_3	CL_D14-1064_mn_4	CL_D14-1064_mn_3_4
CL	D14-1064	mn	3	5	proposal	conclusion	none	elaboration	main	secondary	none	none	In this paper , we introduce a novel approach for producing a unique combination recipe for each query , as it has also been shown that the optimal combination weights differ substantially across queries and other task specifics .	An in-depth empirical analysis presents insights about the effect of data size , domain differences , labeling and tuning on the end performance of our approach .	CL_D14-1064_mn_3	CL_D14-1064_mn_5	CL_D14-1064_mn_3_5
CL	D14-1064	mn	4	5	conclusion	conclusion	support	elaboration	secondary	secondary	back	elaboration	Our query-specific combination method generates statistically significant improvements over other combination strategies presented in the literature , such as uniform and task-specific weighting .	An in-depth empirical analysis presents insights about the effect of data size , domain differences , labeling and tuning on the end performance of our approach .	CL_D14-1064_mn_4	CL_D14-1064_mn_5	CL_D14-1064_mn_4_5
CL	D14-1064	mn	5	4	conclusion	conclusion	elaboration	support	secondary	secondary	forw	elaboration	An in-depth empirical analysis presents insights about the effect of data size , domain differences , labeling and tuning on the end performance of our approach .	Our query-specific combination method generates statistically significant improvements over other combination strategies presented in the literature , such as uniform and task-specific weighting .	CL_D14-1064_mn_5	CL_D14-1064_mn_4	CL_D14-1064_mn_4_5
CL	D14-1064	pa	1	2	motivation_background	motivation_background	info-required	support	secondary	secondary	forw	info-required	When documents and queries are presented in different languages , the common approach is to translate the query into the document language .	While there are a variety of query translation approaches , recent research suggests that combining multiple methods into a single "structured query" is the most effective .	CL_D14-1064_pa_1	CL_D14-1064_pa_2	CL_D14-1064_pa_1_2
CL	D14-1064	pa	1	3	motivation_background	proposal	info-required	none	secondary	main	none	none	When documents and queries are presented in different languages , the common approach is to translate the query into the document language .	In this paper , we introduce a novel approach for producing a unique combination recipe for each query , as it has also been shown that the optimal combination weights differ substantially across queries and other task specifics .	CL_D14-1064_pa_1	CL_D14-1064_pa_3	CL_D14-1064_pa_1_3
CL	D14-1064	pa	1	4	motivation_background	conclusion	info-required	support	secondary	secondary	none	none	When documents and queries are presented in different languages , the common approach is to translate the query into the document language .	Our query-specific combination method generates statistically significant improvements over other combination strategies presented in the literature , such as uniform and task-specific weighting .	CL_D14-1064_pa_1	CL_D14-1064_pa_4	CL_D14-1064_pa_1_4
CL	D14-1064	pa	1	5	motivation_background	result_means	info-required	support	secondary	secondary	none	none	When documents and queries are presented in different languages , the common approach is to translate the query into the document language .	An in-depth empirical analysis presents insights about the effect of data size , domain differences , labeling and tuning on the end performance of our approach .	CL_D14-1064_pa_1	CL_D14-1064_pa_5	CL_D14-1064_pa_1_5
CL	D14-1064	pa	2	3	motivation_background	proposal	support	none	secondary	main	forw	support	While there are a variety of query translation approaches , recent research suggests that combining multiple methods into a single "structured query" is the most effective .	In this paper , we introduce a novel approach for producing a unique combination recipe for each query , as it has also been shown that the optimal combination weights differ substantially across queries and other task specifics .	CL_D14-1064_pa_2	CL_D14-1064_pa_3	CL_D14-1064_pa_2_3
CL	D14-1064	pa	2	4	motivation_background	conclusion	support	support	secondary	secondary	none	none	While there are a variety of query translation approaches , recent research suggests that combining multiple methods into a single "structured query" is the most effective .	Our query-specific combination method generates statistically significant improvements over other combination strategies presented in the literature , such as uniform and task-specific weighting .	CL_D14-1064_pa_2	CL_D14-1064_pa_4	CL_D14-1064_pa_2_4
CL	D14-1064	pa	2	5	motivation_background	result_means	support	support	secondary	secondary	none	none	While there are a variety of query translation approaches , recent research suggests that combining multiple methods into a single "structured query" is the most effective .	An in-depth empirical analysis presents insights about the effect of data size , domain differences , labeling and tuning on the end performance of our approach .	CL_D14-1064_pa_2	CL_D14-1064_pa_5	CL_D14-1064_pa_2_5
CL	D14-1064	pa	3	4	proposal	conclusion	none	support	main	secondary	back	support	In this paper , we introduce a novel approach for producing a unique combination recipe for each query , as it has also been shown that the optimal combination weights differ substantially across queries and other task specifics .	Our query-specific combination method generates statistically significant improvements over other combination strategies presented in the literature , such as uniform and task-specific weighting .	CL_D14-1064_pa_3	CL_D14-1064_pa_4	CL_D14-1064_pa_3_4
CL	D14-1064	pa	3	5	proposal	result_means	none	support	main	secondary	none	none	In this paper , we introduce a novel approach for producing a unique combination recipe for each query , as it has also been shown that the optimal combination weights differ substantially across queries and other task specifics .	An in-depth empirical analysis presents insights about the effect of data size , domain differences , labeling and tuning on the end performance of our approach .	CL_D14-1064_pa_3	CL_D14-1064_pa_5	CL_D14-1064_pa_3_5
CL	D14-1064	pa	4	5	conclusion	result_means	support	support	secondary	secondary	back	support	Our query-specific combination method generates statistically significant improvements over other combination strategies presented in the literature , such as uniform and task-specific weighting .	An in-depth empirical analysis presents insights about the effect of data size , domain differences , labeling and tuning on the end performance of our approach .	CL_D14-1064_pa_4	CL_D14-1064_pa_5	CL_D14-1064_pa_4_5
CL	D14-1064	pa	5	4	result_means	conclusion	support	support	secondary	secondary	forw	support	An in-depth empirical analysis presents insights about the effect of data size , domain differences , labeling and tuning on the end performance of our approach .	Our query-specific combination method generates statistically significant improvements over other combination strategies presented in the literature , such as uniform and task-specific weighting .	CL_D14-1064_pa_5	CL_D14-1064_pa_4	CL_D14-1064_pa_4_5
CL	D14-1065	ab	1	2	motivation_background	motivation_background	info-required	support	secondary	secondary	forw	info-required	A major challenge in document clustering research arises from the growing amount of text data written in different languages .	Previous approaches depend on language-specific solutions ( e.g. , bilingual dictionaries , sequential machine translation ) to evaluate document similarities , and the required transformations may alter the original document semantics .	CL_D14-1065_ab_1	CL_D14-1065_ab_2	CL_D14-1065_ab_1_2
CL	D14-1065	ab	1	3	motivation_background	proposal	info-required	none	secondary	main	none	none	A major challenge in document clustering research arises from the growing amount of text data written in different languages .	To cope with this issue we propose a new document clustering approach for multilingual corpora that ( i ) exploits a large-scale multilingual knowledge base , ( ii ) takes advantage of the multi-topic nature of the text documents , and ( iii ) employs a tensor-based model to deal with high dimensionality and sparseness .	CL_D14-1065_ab_1	CL_D14-1065_ab_3	CL_D14-1065_ab_1_3
CL	D14-1065	ab	1	4	motivation_background	result	info-required	support	secondary	secondary	none	none	A major challenge in document clustering research arises from the growing amount of text data written in different languages .	Results have shown the significance of our approach and its better performance w.r.t. classic document clustering approaches , in both a balanced and an unbalanced corpus evaluation .	CL_D14-1065_ab_1	CL_D14-1065_ab_4	CL_D14-1065_ab_1_4
CL	D14-1065	ab	2	3	motivation_background	proposal	support	none	secondary	main	forw	support	Previous approaches depend on language-specific solutions ( e.g. , bilingual dictionaries , sequential machine translation ) to evaluate document similarities , and the required transformations may alter the original document semantics .	To cope with this issue we propose a new document clustering approach for multilingual corpora that ( i ) exploits a large-scale multilingual knowledge base , ( ii ) takes advantage of the multi-topic nature of the text documents , and ( iii ) employs a tensor-based model to deal with high dimensionality and sparseness .	CL_D14-1065_ab_2	CL_D14-1065_ab_3	CL_D14-1065_ab_2_3
CL	D14-1065	ab	2	4	motivation_background	result	support	support	secondary	secondary	none	none	Previous approaches depend on language-specific solutions ( e.g. , bilingual dictionaries , sequential machine translation ) to evaluate document similarities , and the required transformations may alter the original document semantics .	Results have shown the significance of our approach and its better performance w.r.t. classic document clustering approaches , in both a balanced and an unbalanced corpus evaluation .	CL_D14-1065_ab_2	CL_D14-1065_ab_4	CL_D14-1065_ab_2_4
CL	D14-1065	ab	3	4	proposal	result	none	support	main	secondary	back	support	To cope with this issue we propose a new document clustering approach for multilingual corpora that ( i ) exploits a large-scale multilingual knowledge base , ( ii ) takes advantage of the multi-topic nature of the text documents , and ( iii ) employs a tensor-based model to deal with high dimensionality and sparseness .	Results have shown the significance of our approach and its better performance w.r.t. classic document clustering approaches , in both a balanced and an unbalanced corpus evaluation .	CL_D14-1065_ab_3	CL_D14-1065_ab_4	CL_D14-1065_ab_3_4
CL	D14-1065	ab	4	3	result	proposal	support	none	secondary	main	forw	support	Results have shown the significance of our approach and its better performance w.r.t. classic document clustering approaches , in both a balanced and an unbalanced corpus evaluation .	To cope with this issue we propose a new document clustering approach for multilingual corpora that ( i ) exploits a large-scale multilingual knowledge base , ( ii ) takes advantage of the multi-topic nature of the text documents , and ( iii ) employs a tensor-based model to deal with high dimensionality and sparseness .	CL_D14-1065_ab_4	CL_D14-1065_ab_3	CL_D14-1065_ab_3_4
CL	D14-1065	mn	1	2	motivation_problem	motivation_problem	info-required	support	secondary	secondary	forw	info-required	A major challenge in document clustering research arises from the growing amount of text data written in different languages .	Previous approaches depend on language-specific solutions ( e.g. , bilingual dictionaries , sequential machine translation ) to evaluate document similarities , and the required transformations may alter the original document semantics .	CL_D14-1065_mn_1	CL_D14-1065_mn_2	CL_D14-1065_mn_1_2
CL	D14-1065	mn	1	3	motivation_problem	proposal	info-required	none	secondary	main	none	none	A major challenge in document clustering research arises from the growing amount of text data written in different languages .	To cope with this issue we propose a new document clustering approach for multilingual corpora that ( i ) exploits a large-scale multilingual knowledge base , ( ii ) takes advantage of the multi-topic nature of the text documents , and ( iii ) employs a tensor-based model to deal with high dimensionality and sparseness .	CL_D14-1065_mn_1	CL_D14-1065_mn_3	CL_D14-1065_mn_1_3
CL	D14-1065	mn	1	4	motivation_problem	conclusion	info-required	support	secondary	secondary	none	none	A major challenge in document clustering research arises from the growing amount of text data written in different languages .	Results have shown the significance of our approach and its better performance w.r.t. classic document clustering approaches , in both a balanced and an unbalanced corpus evaluation .	CL_D14-1065_mn_1	CL_D14-1065_mn_4	CL_D14-1065_mn_1_4
CL	D14-1065	mn	2	3	motivation_problem	proposal	support	none	secondary	main	forw	support	Previous approaches depend on language-specific solutions ( e.g. , bilingual dictionaries , sequential machine translation ) to evaluate document similarities , and the required transformations may alter the original document semantics .	To cope with this issue we propose a new document clustering approach for multilingual corpora that ( i ) exploits a large-scale multilingual knowledge base , ( ii ) takes advantage of the multi-topic nature of the text documents , and ( iii ) employs a tensor-based model to deal with high dimensionality and sparseness .	CL_D14-1065_mn_2	CL_D14-1065_mn_3	CL_D14-1065_mn_2_3
CL	D14-1065	mn	2	4	motivation_problem	conclusion	support	support	secondary	secondary	none	none	Previous approaches depend on language-specific solutions ( e.g. , bilingual dictionaries , sequential machine translation ) to evaluate document similarities , and the required transformations may alter the original document semantics .	Results have shown the significance of our approach and its better performance w.r.t. classic document clustering approaches , in both a balanced and an unbalanced corpus evaluation .	CL_D14-1065_mn_2	CL_D14-1065_mn_4	CL_D14-1065_mn_2_4
CL	D14-1065	mn	3	4	proposal	conclusion	none	support	main	secondary	back	support	To cope with this issue we propose a new document clustering approach for multilingual corpora that ( i ) exploits a large-scale multilingual knowledge base , ( ii ) takes advantage of the multi-topic nature of the text documents , and ( iii ) employs a tensor-based model to deal with high dimensionality and sparseness .	Results have shown the significance of our approach and its better performance w.r.t. classic document clustering approaches , in both a balanced and an unbalanced corpus evaluation .	CL_D14-1065_mn_3	CL_D14-1065_mn_4	CL_D14-1065_mn_3_4
CL	D14-1065	mn	4	3	conclusion	proposal	support	none	secondary	main	forw	support	Results have shown the significance of our approach and its better performance w.r.t. classic document clustering approaches , in both a balanced and an unbalanced corpus evaluation .	To cope with this issue we propose a new document clustering approach for multilingual corpora that ( i ) exploits a large-scale multilingual knowledge base , ( ii ) takes advantage of the multi-topic nature of the text documents , and ( iii ) employs a tensor-based model to deal with high dimensionality and sparseness .	CL_D14-1065_mn_4	CL_D14-1065_mn_3	CL_D14-1065_mn_3_4
CL	D14-1065	pa	1	2	motivation_background	motivation_problem	info-required	support	secondary	secondary	forw	info-required	A major challenge in document clustering research arises from the growing amount of text data written in different languages .	Previous approaches depend on language-specific solutions ( e.g. , bilingual dictionaries , sequential machine translation ) to evaluate document similarities , and the required transformations may alter the original document semantics .	CL_D14-1065_pa_1	CL_D14-1065_pa_2	CL_D14-1065_pa_1_2
CL	D14-1065	pa	1	3	motivation_background	proposal	info-required	none	secondary	main	none	none	A major challenge in document clustering research arises from the growing amount of text data written in different languages .	To cope with this issue we propose a new document clustering approach for multilingual corpora that ( i ) exploits a large-scale multilingual knowledge base , ( ii ) takes advantage of the multi-topic nature of the text documents , and ( iii ) employs a tensor-based model to deal with high dimensionality and sparseness .	CL_D14-1065_pa_1	CL_D14-1065_pa_3	CL_D14-1065_pa_1_3
CL	D14-1065	pa	1	4	motivation_background	result	info-required	support	secondary	secondary	none	none	A major challenge in document clustering research arises from the growing amount of text data written in different languages .	Results have shown the significance of our approach and its better performance w.r.t. classic document clustering approaches , in both a balanced and an unbalanced corpus evaluation .	CL_D14-1065_pa_1	CL_D14-1065_pa_4	CL_D14-1065_pa_1_4
CL	D14-1065	pa	2	3	motivation_problem	proposal	support	none	secondary	main	forw	support	Previous approaches depend on language-specific solutions ( e.g. , bilingual dictionaries , sequential machine translation ) to evaluate document similarities , and the required transformations may alter the original document semantics .	To cope with this issue we propose a new document clustering approach for multilingual corpora that ( i ) exploits a large-scale multilingual knowledge base , ( ii ) takes advantage of the multi-topic nature of the text documents , and ( iii ) employs a tensor-based model to deal with high dimensionality and sparseness .	CL_D14-1065_pa_2	CL_D14-1065_pa_3	CL_D14-1065_pa_2_3
CL	D14-1065	pa	2	4	motivation_problem	result	support	support	secondary	secondary	none	none	Previous approaches depend on language-specific solutions ( e.g. , bilingual dictionaries , sequential machine translation ) to evaluate document similarities , and the required transformations may alter the original document semantics .	Results have shown the significance of our approach and its better performance w.r.t. classic document clustering approaches , in both a balanced and an unbalanced corpus evaluation .	CL_D14-1065_pa_2	CL_D14-1065_pa_4	CL_D14-1065_pa_2_4
CL	D14-1065	pa	3	4	proposal	result	none	support	main	secondary	back	support	To cope with this issue we propose a new document clustering approach for multilingual corpora that ( i ) exploits a large-scale multilingual knowledge base , ( ii ) takes advantage of the multi-topic nature of the text documents , and ( iii ) employs a tensor-based model to deal with high dimensionality and sparseness .	Results have shown the significance of our approach and its better performance w.r.t. classic document clustering approaches , in both a balanced and an unbalanced corpus evaluation .	CL_D14-1065_pa_3	CL_D14-1065_pa_4	CL_D14-1065_pa_3_4
CL	D14-1065	pa	4	3	result	proposal	support	none	secondary	main	forw	support	Results have shown the significance of our approach and its better performance w.r.t. classic document clustering approaches , in both a balanced and an unbalanced corpus evaluation .	To cope with this issue we propose a new document clustering approach for multilingual corpora that ( i ) exploits a large-scale multilingual knowledge base , ( ii ) takes advantage of the multi-topic nature of the text documents , and ( iii ) employs a tensor-based model to deal with high dimensionality and sparseness .	CL_D14-1065_pa_4	CL_D14-1065_pa_3	CL_D14-1065_pa_3_4
CL	D14-1066	ab	1	2	proposal	proposal	none	elaboration	main	secondary	back	elaboration	In this paper we examine the lexical substitution task for the medical domain .	We adapt the current best system from the open domain , which trains a single classifier for all instances using delexicalized features .	CL_D14-1066_ab_1	CL_D14-1066_ab_2	CL_D14-1066_ab_1_2
CL	D14-1066	ab	1	3	proposal	result_means	none	support	main	secondary	back	support	In this paper we examine the lexical substitution task for the medical domain .	We show significant improvements over a strong baseline coming from a distributional thesaurus ( DT ) .	CL_D14-1066_ab_1	CL_D14-1066_ab_3	CL_D14-1066_ab_1_3
CL	D14-1066	ab	1	4	proposal	result	none	elaboration	main	secondary	none	none	In this paper we examine the lexical substitution task for the medical domain .	Whereas in the open domain system , features derived from WordNet show only slight improvements , we show that its counterpart for the medical domain ( UMLS ) shows a significant additional benefit when used for feature generation .	CL_D14-1066_ab_1	CL_D14-1066_ab_4	CL_D14-1066_ab_1_4
CL	D14-1066	ab	2	3	proposal	result_means	elaboration	support	secondary	secondary	none	none	We adapt the current best system from the open domain , which trains a single classifier for all instances using delexicalized features .	We show significant improvements over a strong baseline coming from a distributional thesaurus ( DT ) .	CL_D14-1066_ab_2	CL_D14-1066_ab_3	CL_D14-1066_ab_2_3
CL	D14-1066	ab	2	4	proposal	result	elaboration	elaboration	secondary	secondary	none	none	We adapt the current best system from the open domain , which trains a single classifier for all instances using delexicalized features .	Whereas in the open domain system , features derived from WordNet show only slight improvements , we show that its counterpart for the medical domain ( UMLS ) shows a significant additional benefit when used for feature generation .	CL_D14-1066_ab_2	CL_D14-1066_ab_4	CL_D14-1066_ab_2_4
CL	D14-1066	ab	3	4	result_means	result	support	elaboration	secondary	secondary	back	elaboration	We show significant improvements over a strong baseline coming from a distributional thesaurus ( DT ) .	Whereas in the open domain system , features derived from WordNet show only slight improvements , we show that its counterpart for the medical domain ( UMLS ) shows a significant additional benefit when used for feature generation .	CL_D14-1066_ab_3	CL_D14-1066_ab_4	CL_D14-1066_ab_3_4
CL	D14-1066	ab	4	3	result	result_means	elaboration	support	secondary	secondary	forw	elaboration	Whereas in the open domain system , features derived from WordNet show only slight improvements , we show that its counterpart for the medical domain ( UMLS ) shows a significant additional benefit when used for feature generation .	We show significant improvements over a strong baseline coming from a distributional thesaurus ( DT ) .	CL_D14-1066_ab_4	CL_D14-1066_ab_3	CL_D14-1066_ab_3_4
CL	D14-1066	mn	1	2	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	In this paper we examine the lexical substitution task for the medical domain .	We adapt the current best system from the open domain , which trains a single classifier for all instances using delexicalized features .	CL_D14-1066_mn_1	CL_D14-1066_mn_2	CL_D14-1066_mn_1_2
CL	D14-1066	mn	1	3	proposal	result	none	elaboration	main	secondary	none	none	In this paper we examine the lexical substitution task for the medical domain .	We show significant improvements over a strong baseline coming from a distributional thesaurus ( DT ) .	CL_D14-1066_mn_1	CL_D14-1066_mn_3	CL_D14-1066_mn_1_3
CL	D14-1066	mn	1	4	proposal	result	none	support	main	secondary	back	support	In this paper we examine the lexical substitution task for the medical domain .	Whereas in the open domain system , features derived from WordNet show only slight improvements , we show that its counterpart for the medical domain ( UMLS ) shows a significant additional benefit when used for feature generation .	CL_D14-1066_mn_1	CL_D14-1066_mn_4	CL_D14-1066_mn_1_4
CL	D14-1066	mn	2	3	proposal_implementation	result	elaboration	elaboration	secondary	secondary	none	none	We adapt the current best system from the open domain , which trains a single classifier for all instances using delexicalized features .	We show significant improvements over a strong baseline coming from a distributional thesaurus ( DT ) .	CL_D14-1066_mn_2	CL_D14-1066_mn_3	CL_D14-1066_mn_2_3
CL	D14-1066	mn	2	4	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	We adapt the current best system from the open domain , which trains a single classifier for all instances using delexicalized features .	Whereas in the open domain system , features derived from WordNet show only slight improvements , we show that its counterpart for the medical domain ( UMLS ) shows a significant additional benefit when used for feature generation .	CL_D14-1066_mn_2	CL_D14-1066_mn_4	CL_D14-1066_mn_2_4
CL	D14-1066	mn	3	4	result	result	elaboration	support	secondary	secondary	forw	elaboration	We show significant improvements over a strong baseline coming from a distributional thesaurus ( DT ) .	Whereas in the open domain system , features derived from WordNet show only slight improvements , we show that its counterpart for the medical domain ( UMLS ) shows a significant additional benefit when used for feature generation .	CL_D14-1066_mn_3	CL_D14-1066_mn_4	CL_D14-1066_mn_3_4
CL	D14-1066	mn	4	3	result	result	support	elaboration	secondary	secondary	back	elaboration	Whereas in the open domain system , features derived from WordNet show only slight improvements , we show that its counterpart for the medical domain ( UMLS ) shows a significant additional benefit when used for feature generation .	We show significant improvements over a strong baseline coming from a distributional thesaurus ( DT ) .	CL_D14-1066_mn_4	CL_D14-1066_mn_3	CL_D14-1066_mn_3_4
CL	D14-1066	pa	1	2	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	In this paper we examine the lexical substitution task for the medical domain .	We adapt the current best system from the open domain , which trains a single classifier for all instances using delexicalized features .	CL_D14-1066_pa_1	CL_D14-1066_pa_2	CL_D14-1066_pa_1_2
CL	D14-1066	pa	1	3	proposal	result	none	support	main	secondary	back	support	In this paper we examine the lexical substitution task for the medical domain .	We show significant improvements over a strong baseline coming from a distributional thesaurus ( DT ) .	CL_D14-1066_pa_1	CL_D14-1066_pa_3	CL_D14-1066_pa_1_3
CL	D14-1066	pa	1	4	proposal	result	none	support	main	secondary	back	support	In this paper we examine the lexical substitution task for the medical domain .	Whereas in the open domain system , features derived from WordNet show only slight improvements , we show that its counterpart for the medical domain ( UMLS ) shows a significant additional benefit when used for feature generation .	CL_D14-1066_pa_1	CL_D14-1066_pa_4	CL_D14-1066_pa_1_4
CL	D14-1066	pa	2	3	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	We adapt the current best system from the open domain , which trains a single classifier for all instances using delexicalized features .	We show significant improvements over a strong baseline coming from a distributional thesaurus ( DT ) .	CL_D14-1066_pa_2	CL_D14-1066_pa_3	CL_D14-1066_pa_2_3
CL	D14-1066	pa	2	4	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	We adapt the current best system from the open domain , which trains a single classifier for all instances using delexicalized features .	Whereas in the open domain system , features derived from WordNet show only slight improvements , we show that its counterpart for the medical domain ( UMLS ) shows a significant additional benefit when used for feature generation .	CL_D14-1066_pa_2	CL_D14-1066_pa_4	CL_D14-1066_pa_2_4
CL	D14-1066	pa	3	4	result	result	support	support	secondary	secondary	none	none	We show significant improvements over a strong baseline coming from a distributional thesaurus ( DT ) .	Whereas in the open domain system , features derived from WordNet show only slight improvements , we show that its counterpart for the medical domain ( UMLS ) shows a significant additional benefit when used for feature generation .	CL_D14-1066_pa_3	CL_D14-1066_pa_4	CL_D14-1066_pa_3_4
CL	D14-1066	pa	4	3	result	result	support	support	secondary	secondary	none	none	Whereas in the open domain system , features derived from WordNet show only slight improvements , we show that its counterpart for the medical domain ( UMLS ) shows a significant additional benefit when used for feature generation .	We show significant improvements over a strong baseline coming from a distributional thesaurus ( DT ) .	CL_D14-1066_pa_4	CL_D14-1066_pa_3	CL_D14-1066_pa_3_4
CL	D14-1067	ab	1	2	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	This paper presents a system which learns to answer questions on a broad range of topics from a knowledge base using few hand-crafted features .	Our model learns low-dimensional embeddings of words and knowledge base constituents ; these representations are used to score natural language questions against candidate answers .	CL_D14-1067_ab_1	CL_D14-1067_ab_2	CL_D14-1067_ab_1_2
CL	D14-1067	ab	1	3	proposal	result	none	support	main	secondary	back	support	This paper presents a system which learns to answer questions on a broad range of topics from a knowledge base using few hand-crafted features .	Training our system using pairs of questions and structured representations of their answers , and pairs of question paraphrases , yields competitive results on a recent benchmark of the literature .	CL_D14-1067_ab_1	CL_D14-1067_ab_3	CL_D14-1067_ab_1_3
CL	D14-1067	ab	2	3	proposal_implementation	result	elaboration	support	secondary	secondary	none	none	Our model learns low-dimensional embeddings of words and knowledge base constituents ; these representations are used to score natural language questions against candidate answers .	Training our system using pairs of questions and structured representations of their answers , and pairs of question paraphrases , yields competitive results on a recent benchmark of the literature .	CL_D14-1067_ab_2	CL_D14-1067_ab_3	CL_D14-1067_ab_2_3
CL	D14-1067	ab	3	2	result	proposal_implementation	support	elaboration	secondary	secondary	none	none	Training our system using pairs of questions and structured representations of their answers , and pairs of question paraphrases , yields competitive results on a recent benchmark of the literature .	Our model learns low-dimensional embeddings of words and knowledge base constituents ; these representations are used to score natural language questions against candidate answers .	CL_D14-1067_ab_3	CL_D14-1067_ab_2	CL_D14-1067_ab_2_3
CL	D14-1067	mn	1	2	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	This paper presents a system which learns to answer questions on a broad range of topics from a knowledge base using few hand-crafted features .	Our model learns low-dimensional embeddings of words and knowledge base constituents ; these representations are used to score natural language questions against candidate answers .	CL_D14-1067_mn_1	CL_D14-1067_mn_2	CL_D14-1067_mn_1_2
CL	D14-1067	mn	1	3	proposal	result_means	none	support	main	secondary	back	support	This paper presents a system which learns to answer questions on a broad range of topics from a knowledge base using few hand-crafted features .	Training our system using pairs of questions and structured representations of their answers , and pairs of question paraphrases , yields competitive results on a recent benchmark of the literature .	CL_D14-1067_mn_1	CL_D14-1067_mn_3	CL_D14-1067_mn_1_3
CL	D14-1067	mn	2	3	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	Our model learns low-dimensional embeddings of words and knowledge base constituents ; these representations are used to score natural language questions against candidate answers .	Training our system using pairs of questions and structured representations of their answers , and pairs of question paraphrases , yields competitive results on a recent benchmark of the literature .	CL_D14-1067_mn_2	CL_D14-1067_mn_3	CL_D14-1067_mn_2_3
CL	D14-1067	mn	3	2	result_means	proposal_implementation	support	elaboration	secondary	secondary	none	none	Training our system using pairs of questions and structured representations of their answers , and pairs of question paraphrases , yields competitive results on a recent benchmark of the literature .	Our model learns low-dimensional embeddings of words and knowledge base constituents ; these representations are used to score natural language questions against candidate answers .	CL_D14-1067_mn_3	CL_D14-1067_mn_2	CL_D14-1067_mn_2_3
CL	D14-1067	pa	1	2	proposal	proposal_implementation	none	elaboration	main	secondary	back	elaboration	This paper presents a system which learns to answer questions on a broad range of topics from a knowledge base using few hand-crafted features .	Our model learns low-dimensional embeddings of words and knowledge base constituents ; these representations are used to score natural language questions against candidate answers .	CL_D14-1067_pa_1	CL_D14-1067_pa_2	CL_D14-1067_pa_1_2
CL	D14-1067	pa	1	3	proposal	result_means	none	support	main	secondary	back	support	This paper presents a system which learns to answer questions on a broad range of topics from a knowledge base using few hand-crafted features .	Training our system using pairs of questions and structured representations of their answers , and pairs of question paraphrases , yields competitive results on a recent benchmark of the literature .	CL_D14-1067_pa_1	CL_D14-1067_pa_3	CL_D14-1067_pa_1_3
CL	D14-1067	pa	2	3	proposal_implementation	result_means	elaboration	support	secondary	secondary	none	none	Our model learns low-dimensional embeddings of words and knowledge base constituents ; these representations are used to score natural language questions against candidate answers .	Training our system using pairs of questions and structured representations of their answers , and pairs of question paraphrases , yields competitive results on a recent benchmark of the literature .	CL_D14-1067_pa_2	CL_D14-1067_pa_3	CL_D14-1067_pa_2_3
CL	D14-1067	pa	3	2	result_means	proposal_implementation	support	elaboration	secondary	secondary	none	none	Training our system using pairs of questions and structured representations of their answers , and pairs of question paraphrases , yields competitive results on a recent benchmark of the literature .	Our model learns low-dimensional embeddings of words and knowledge base constituents ; these representations are used to score natural language questions against candidate answers .	CL_D14-1067_pa_3	CL_D14-1067_pa_2	CL_D14-1067_pa_2_3
CL	D14-1068	ab	1	2	motivation_background	proposal	support	none	secondary	main	forw	support	Keyboard layout errors and homoglyphs in cross-language queries impact our ability to correctly interpret user information needs and offer relevant results .	We present a machine learning approach to correcting these errors , based largely on character-level n-gram features .	CL_D14-1068_ab_1	CL_D14-1068_ab_2	CL_D14-1068_ab_1_2
CL	D14-1068	ab	1	3	motivation_background	result	support	support	secondary	secondary	none	none	Keyboard layout errors and homoglyphs in cross-language queries impact our ability to correctly interpret user information needs and offer relevant results .	We demonstrate superior performance over rule-based methods , as well as a significant reduction in the number of queries that yield null search results .	CL_D14-1068_ab_1	CL_D14-1068_ab_3	CL_D14-1068_ab_1_3
CL	D14-1068	ab	2	3	proposal	result	none	support	main	secondary	back	support	We present a machine learning approach to correcting these errors , based largely on character-level n-gram features .	We demonstrate superior performance over rule-based methods , as well as a significant reduction in the number of queries that yield null search results .	CL_D14-1068_ab_2	CL_D14-1068_ab_3	CL_D14-1068_ab_2_3
CL	D14-1068	ab	3	2	result	proposal	support	none	secondary	main	forw	support	We demonstrate superior performance over rule-based methods , as well as a significant reduction in the number of queries that yield null search results .	We present a machine learning approach to correcting these errors , based largely on character-level n-gram features .	CL_D14-1068_ab_3	CL_D14-1068_ab_2	CL_D14-1068_ab_2_3
CL	D14-1068	mn	1	2	motivation_background	proposal	support	none	secondary	main	forw	support	Keyboard layout errors and homoglyphs in cross-language queries impact our ability to correctly interpret user information needs and offer relevant results .	We present a machine learning approach to correcting these errors , based largely on character-level n-gram features .	CL_D14-1068_mn_1	CL_D14-1068_mn_2	CL_D14-1068_mn_1_2
CL	D14-1068	mn	1	3	motivation_background	conclusion	support	support	secondary	secondary	none	none	Keyboard layout errors and homoglyphs in cross-language queries impact our ability to correctly interpret user information needs and offer relevant results .	We demonstrate superior performance over rule-based methods , as well as a significant reduction in the number of queries that yield null search results .	CL_D14-1068_mn_1	CL_D14-1068_mn_3	CL_D14-1068_mn_1_3
CL	D14-1068	mn	2	3	proposal	conclusion	none	support	main	secondary	back	support	We present a machine learning approach to correcting these errors , based largely on character-level n-gram features .	We demonstrate superior performance over rule-based methods , as well as a significant reduction in the number of queries that yield null search results .	CL_D14-1068_mn_2	CL_D14-1068_mn_3	CL_D14-1068_mn_2_3
CL	D14-1068	mn	3	2	conclusion	proposal	support	none	secondary	main	forw	support	We demonstrate superior performance over rule-based methods , as well as a significant reduction in the number of queries that yield null search results .	We present a machine learning approach to correcting these errors , based largely on character-level n-gram features .	CL_D14-1068_mn_3	CL_D14-1068_mn_2	CL_D14-1068_mn_2_3
CL	D14-1068	pa	1	2	motivation_background	proposal	support	none	secondary	main	forw	support	Keyboard layout errors and homoglyphs in cross-language queries impact our ability to correctly interpret user information needs and offer relevant results .	We present a machine learning approach to correcting these errors , based largely on character-level n-gram features .	CL_D14-1068_pa_1	CL_D14-1068_pa_2	CL_D14-1068_pa_1_2
CL	D14-1068	pa	1	3	motivation_background	result	support	support	secondary	secondary	none	none	Keyboard layout errors and homoglyphs in cross-language queries impact our ability to correctly interpret user information needs and offer relevant results .	We demonstrate superior performance over rule-based methods , as well as a significant reduction in the number of queries that yield null search results .	CL_D14-1068_pa_1	CL_D14-1068_pa_3	CL_D14-1068_pa_1_3
CL	D14-1068	pa	2	3	proposal	result	none	support	main	secondary	back	support	We present a machine learning approach to correcting these errors , based largely on character-level n-gram features .	We demonstrate superior performance over rule-based methods , as well as a significant reduction in the number of queries that yield null search results .	CL_D14-1068_pa_2	CL_D14-1068_pa_3	CL_D14-1068_pa_2_3
CL	D14-1068	pa	3	2	result	proposal	support	none	secondary	main	forw	support	We demonstrate superior performance over rule-based methods , as well as a significant reduction in the number of queries that yield null search results .	We present a machine learning approach to correcting these errors , based largely on character-level n-gram features .	CL_D14-1068_pa_3	CL_D14-1068_pa_2	CL_D14-1068_pa_2_3
CL	D14-1069	ab	1	2	proposal_implementation	observation	none	support	main	secondary	back	support	Non-linear mappings of the form P ( ngram ) γ andlog ( 1+τP ( ngram )) log ( 1+τ ) are applied to the n-gram probabilities in five trainable open-source language identifiers .	The first mapping reduces classification errors by 4.0 % to 83.9 % over a test set of more than one million 65-character strings in 1366 languages , and by 2.6 % to 76.7 % over a subset of 781 languages .	CL_D14-1069_ab_1	CL_D14-1069_ab_2	CL_D14-1069_ab_1_2
CL	D14-1069	ab	1	3	proposal_implementation	observation	none	elaboration	main	secondary	none	none	Non-linear mappings of the form P ( ngram ) γ andlog ( 1+τP ( ngram )) log ( 1+τ ) are applied to the n-gram probabilities in five trainable open-source language identifiers .	The second mapping improves four of the five identifiers by 10.6 % to 83.8 % on the larger corpus and 14.4 % to 76.7 % on the smaller corpus .	CL_D14-1069_ab_1	CL_D14-1069_ab_3	CL_D14-1069_ab_1_3
CL	D14-1069	ab	1	4	proposal_implementation	information_additional	none	info-optional	main	secondary	back	info-optional	Non-linear mappings of the form P ( ngram ) γ andlog ( 1+τP ( ngram )) log ( 1+τ ) are applied to the n-gram probabilities in five trainable open-source language identifiers .	The subset corpus and the modified programs are made freely available for download at http : //www.cs.cmu.edu/∼ralf/langid.html .	CL_D14-1069_ab_1	CL_D14-1069_ab_4	CL_D14-1069_ab_1_4
CL	D14-1069	ab	2	3	observation	observation	support	elaboration	secondary	secondary	back	elaboration	The first mapping reduces classification errors by 4.0 % to 83.9 % over a test set of more than one million 65-character strings in 1366 languages , and by 2.6 % to 76.7 % over a subset of 781 languages .	The second mapping improves four of the five identifiers by 10.6 % to 83.8 % on the larger corpus and 14.4 % to 76.7 % on the smaller corpus .	CL_D14-1069_ab_2	CL_D14-1069_ab_3	CL_D14-1069_ab_2_3
CL	D14-1069	ab	2	4	observation	information_additional	support	info-optional	secondary	secondary	none	none	The first mapping reduces classification errors by 4.0 % to 83.9 % over a test set of more than one million 65-character strings in 1366 languages , and by 2.6 % to 76.7 % over a subset of 781 languages .	The subset corpus and the modified programs are made freely available for download at http : //www.cs.cmu.edu/∼ralf/langid.html .	CL_D14-1069_ab_2	CL_D14-1069_ab_4	CL_D14-1069_ab_2_4
CL	D14-1069	ab	3	4	observation	information_additional	elaboration	info-optional	secondary	secondary	none	none	The second mapping improves four of the five identifiers by 10.6 % to 83.8 % on the larger corpus and 14.4 % to 76.7 % on the smaller corpus .	The subset corpus and the modified programs are made freely available for download at http : //www.cs.cmu.edu/∼ralf/langid.html .	CL_D14-1069_ab_3	CL_D14-1069_ab_4	CL_D14-1069_ab_3_4
CL	D14-1069	ab	4	3	information_additional	observation	info-optional	elaboration	secondary	secondary	none	none	The subset corpus and the modified programs are made freely available for download at http : //www.cs.cmu.edu/∼ralf/langid.html .	The second mapping improves four of the five identifiers by 10.6 % to 83.8 % on the larger corpus and 14.4 % to 76.7 % on the smaller corpus .	CL_D14-1069_ab_4	CL_D14-1069_ab_3	CL_D14-1069_ab_3_4
CL	D14-1069	mn	1	2	proposal	observation	none	support	main	secondary	back	support	Non-linear mappings of the form P ( ngram ) γ andlog ( 1+τP ( ngram )) log ( 1+τ ) are applied to the n-gram probabilities in five trainable open-source language identifiers .	The first mapping reduces classification errors by 4.0 % to 83.9 % over a test set of more than one million 65-character strings in 1366 languages , and by 2.6 % to 76.7 % over a subset of 781 languages .	CL_D14-1069_mn_1	CL_D14-1069_mn_2	CL_D14-1069_mn_1_2
CL	D14-1069	mn	1	3	proposal	observation	none	elaboration	main	secondary	none	none	Non-linear mappings of the form P ( ngram ) γ andlog ( 1+τP ( ngram )) log ( 1+τ ) are applied to the n-gram probabilities in five trainable open-source language identifiers .	The second mapping improves four of the five identifiers by 10.6 % to 83.8 % on the larger corpus and 14.4 % to 76.7 % on the smaller corpus .	CL_D14-1069_mn_1	CL_D14-1069_mn_3	CL_D14-1069_mn_1_3
CL	D14-1069	mn	1	4	proposal	information_additional	none	info-optional	main	secondary	back	info-optional	Non-linear mappings of the form P ( ngram ) γ andlog ( 1+τP ( ngram )) log ( 1+τ ) are applied to the n-gram probabilities in five trainable open-source language identifiers .	The subset corpus and the modified programs are made freely available for download at http : //www.cs.cmu.edu/∼ralf/langid.html .	CL_D14-1069_mn_1	CL_D14-1069_mn_4	CL_D14-1069_mn_1_4
CL	D14-1069	mn	2	3	observation	observation	support	elaboration	secondary	secondary	back	elaboration	The first mapping reduces classification errors by 4.0 % to 83.9 % over a test set of more than one million 65-character strings in 1366 languages , and by 2.6 % to 76.7 % over a subset of 781 languages .	The second mapping improves four of the five identifiers by 10.6 % to 83.8 % on the larger corpus and 14.4 % to 76.7 % on the smaller corpus .	CL_D14-1069_mn_2	CL_D14-1069_mn_3	CL_D14-1069_mn_2_3
CL	D14-1069	mn	2	4	observation	information_additional	support	info-optional	secondary	secondary	none	none	The first mapping reduces classification errors by 4.0 % to 83.9 % over a test set of more than one million 65-character strings in 1366 languages , and by 2.6 % to 76.7 % over a subset of 781 languages .	The subset corpus and the modified programs are made freely available for download at http : //www.cs.cmu.edu/∼ralf/langid.html .	CL_D14-1069_mn_2	CL_D14-1069_mn_4	CL_D14-1069_mn_2_4
CL	D14-1069	mn	3	4	observation	information_additional	elaboration	info-optional	secondary	secondary	none	none	The second mapping improves four of the five identifiers by 10.6 % to 83.8 % on the larger corpus and 14.4 % to 76.7 % on the smaller corpus .	The subset corpus and the modified programs are made freely available for download at http : //www.cs.cmu.edu/∼ralf/langid.html .	CL_D14-1069_mn_3	CL_D14-1069_mn_4	CL_D14-1069_mn_3_4
CL	D14-1069	mn	4	3	information_additional	observation	info-optional	elaboration	secondary	secondary	none	none	The subset corpus and the modified programs are made freely available for download at http : //www.cs.cmu.edu/∼ralf/langid.html .	The second mapping improves four of the five identifiers by 10.6 % to 83.8 % on the larger corpus and 14.4 % to 76.7 % on the smaller corpus .	CL_D14-1069_mn_4	CL_D14-1069_mn_3	CL_D14-1069_mn_3_4
CL	D14-1069	pa	1	2	proposal	observation	none	info-required	main	secondary	none	none	Non-linear mappings of the form P ( ngram ) γ andlog ( 1+τP ( ngram )) log ( 1+τ ) are applied to the n-gram probabilities in five trainable open-source language identifiers .	The first mapping reduces classification errors by 4.0 % to 83.9 % over a test set of more than one million 65-character strings in 1366 languages , and by 2.6 % to 76.7 % over a subset of 781 languages .	CL_D14-1069_pa_1	CL_D14-1069_pa_2	CL_D14-1069_pa_1_2
CL	D14-1069	pa	1	3	proposal	observation	none	support	main	secondary	back	support	Non-linear mappings of the form P ( ngram ) γ andlog ( 1+τP ( ngram )) log ( 1+τ ) are applied to the n-gram probabilities in five trainable open-source language identifiers .	The second mapping improves four of the five identifiers by 10.6 % to 83.8 % on the larger corpus and 14.4 % to 76.7 % on the smaller corpus .	CL_D14-1069_pa_1	CL_D14-1069_pa_3	CL_D14-1069_pa_1_3
CL	D14-1069	pa	1	4	proposal	information_additional	none	info-optional	main	secondary	back	info-optional	Non-linear mappings of the form P ( ngram ) γ andlog ( 1+τP ( ngram )) log ( 1+τ ) are applied to the n-gram probabilities in five trainable open-source language identifiers .	The subset corpus and the modified programs are made freely available for download at http : //www.cs.cmu.edu/∼ralf/langid.html .	CL_D14-1069_pa_1	CL_D14-1069_pa_4	CL_D14-1069_pa_1_4
CL	D14-1069	pa	2	3	observation	observation	info-required	support	secondary	secondary	forw	info-required	The first mapping reduces classification errors by 4.0 % to 83.9 % over a test set of more than one million 65-character strings in 1366 languages , and by 2.6 % to 76.7 % over a subset of 781 languages .	The second mapping improves four of the five identifiers by 10.6 % to 83.8 % on the larger corpus and 14.4 % to 76.7 % on the smaller corpus .	CL_D14-1069_pa_2	CL_D14-1069_pa_3	CL_D14-1069_pa_2_3
CL	D14-1069	pa	2	4	observation	information_additional	info-required	info-optional	secondary	secondary	none	none	The first mapping reduces classification errors by 4.0 % to 83.9 % over a test set of more than one million 65-character strings in 1366 languages , and by 2.6 % to 76.7 % over a subset of 781 languages .	The subset corpus and the modified programs are made freely available for download at http : //www.cs.cmu.edu/∼ralf/langid.html .	CL_D14-1069_pa_2	CL_D14-1069_pa_4	CL_D14-1069_pa_2_4
CL	D14-1069	pa	3	4	observation	information_additional	support	info-optional	secondary	secondary	none	none	The second mapping improves four of the five identifiers by 10.6 % to 83.8 % on the larger corpus and 14.4 % to 76.7 % on the smaller corpus .	The subset corpus and the modified programs are made freely available for download at http : //www.cs.cmu.edu/∼ralf/langid.html .	CL_D14-1069_pa_3	CL_D14-1069_pa_4	CL_D14-1069_pa_3_4
CL	D14-1069	pa	4	3	information_additional	observation	info-optional	support	secondary	secondary	none	none	The subset corpus and the modified programs are made freely available for download at http : //www.cs.cmu.edu/∼ralf/langid.html .	The second mapping improves four of the five identifiers by 10.6 % to 83.8 % on the larger corpus and 14.4 % to 76.7 % on the smaller corpus .	CL_D14-1069_pa_4	CL_D14-1069_pa_3	CL_D14-1069_pa_3_4
